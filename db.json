{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/archer/source/assets/algolia_logo.svg","path":"assets/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/archer/source/assets/beian.png","path":"assets/beian.png","modified":1,"renderable":1},{"_id":"themes/archer/source/assets/blog-solid.svg","path":"assets/blog-solid.svg","modified":1,"renderable":1},{"_id":"themes/archer/source/assets/example_qr.png","path":"assets/example_qr.png","modified":1,"renderable":1},{"_id":"themes/archer/source/assets/favicon.ico","path":"assets/favicon.ico","modified":1,"renderable":1},{"_id":"themes/archer/source/assets/loading.svg","path":"assets/loading.svg","modified":1,"renderable":1},{"_id":"themes/archer/source/avatar/Misaka.jpg","path":"avatar/Misaka.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/avatar/medium.jpg","path":"avatar/medium.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/css/dark.css","path":"css/dark.css","modified":1,"renderable":1},{"_id":"themes/archer/source/css/dark.css.map","path":"css/dark.css.map","modified":1,"renderable":1},{"_id":"themes/archer/source/css/mobile.css","path":"css/mobile.css","modified":1,"renderable":1},{"_id":"themes/archer/source/css/mobile.css.map","path":"css/mobile.css.map","modified":1,"renderable":1},{"_id":"themes/archer/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/archer/source/css/style.css.map","path":"css/style.css.map","modified":1,"renderable":1},{"_id":"themes/archer/source/font/Oswald-Regular.ttf","path":"font/Oswald-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/archer/source/font/Source Sans Pro.woff","path":"font/Source Sans Pro.woff","modified":1,"renderable":1},{"_id":"themes/archer/source/font/Source Sans Pro.woff2","path":"font/Source Sans Pro.woff2","modified":1,"renderable":1},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff","path":"font/SourceCodePro-Regular.ttf.woff","modified":1,"renderable":1},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff2","path":"font/SourceCodePro-Regular.ttf.woff2","modified":1,"renderable":1},{"_id":"themes/archer/source/intro/404-bg.jpg","path":"intro/404-bg.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/intro/about-bg.jpg","path":"intro/about-bg.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/intro/index-bg.jpg","path":"intro/index-bg.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/intro/post-bg.jpg","path":"intro/post-bg.jpg","modified":1,"renderable":1},{"_id":"themes/archer/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/archer/source/lib/webfontloader.min.js","path":"lib/webfontloader.min.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/customFontLoader.js","path":"scripts/customFontLoader.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/customFontLoader.js.map","path":"scripts/customFontLoader.js.map","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/dark.js","path":"scripts/dark.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/dark.js.map","path":"scripts/dark.js.map","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/main.js","path":"scripts/main.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/main.js.LICENSE.txt","path":"scripts/main.js.LICENSE.txt","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/main.js.map","path":"scripts/main.js.map","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/search.js","path":"scripts/search.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/search.js.LICENSE.txt","path":"scripts/search.js.LICENSE.txt","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/search.js.map","path":"scripts/search.js.map","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/share.js","path":"scripts/share.js","modified":1,"renderable":1},{"_id":"themes/archer/source/scripts/share.js.map","path":"scripts/share.js.map","modified":1,"renderable":1}],"Cache":[{"_id":"source/about/index.md","hash":"4d6eb52b9cab9747a724811ceb0fc29044546fcf","modified":1655134879770},{"_id":"source/_posts/DE-Linux-commandline.md","hash":"f187f7a804c5dfd14fe0182b9b8d2e89de06fcdf","modified":1649974222000},{"_id":"source/Projects/index.md","hash":"41cb26c0d03d4429ea0a8ec9e526638a8d93287c","modified":1649974222000},{"_id":"source/_posts/DE-SQL-case-when.md","hash":"fa6da2bc1c47ce181eafb07a11be14874ceba0f0","modified":1649974222000},{"_id":"source/_posts/DE-SQL-Where.md","hash":"7fd48621a63f448f7a8b151ea19a8bbe299cad07","modified":1649974222000},{"_id":"source/_posts/DE-SQL-DDL-tables.md","hash":"b1259cd6fbb5cd766a995b4d863dccf6e7b18bd5","modified":1649974222000},{"_id":"source/_posts/DL-RNN.md","hash":"dd9dfd4ddebaeac633614920339f6e31cfe57902","modified":1649974222000},{"_id":"source/_posts/DL-lossfunction.md","hash":"d041a6160f83185ec4eb8d6ea52f70cfea343891","modified":1649974222000},{"_id":"source/_posts/DE-SQL-subquery.md","hash":"cd3740e48c4bc3a4e3d8a2aa1c5b667b3832cb8a","modified":1649974222000},{"_id":"source/_posts/DL-backpropagation.md","hash":"9e3bc996fcdcd219e6f0da28c03118cb6fe28cbc","modified":1649974222000},{"_id":"source/_posts/DL-regularization.md","hash":"3d60a04c795d85b60f4a3e60f879e1af41a446c1","modified":1649974222000},{"_id":"source/_posts/DL-hyperparameter.md","hash":"e3b41512590d8e597e1b712c97ea502efa22f498","modified":1649974222000},{"_id":"source/_posts/DL-optimizer.md","hash":"73676914b7e2f5fb477e0af65666b3951c41be66","modified":1649974222000},{"_id":"source/_posts/ML-Metrics-Regression.md","hash":"5eb4685d1ba704d3c5fbdfadedcb922bcea9d4f9","modified":1649974222000},{"_id":"source/_posts/DL-perceptron.md","hash":"55e92293da968d67162f37e6217e6821ad2592f2","modified":1649974222000},{"_id":"source/_posts/ML-SP-Decision_Tree.md","hash":"d9d5aefcb9775d6beb85b4f018f6c1a120bf1026","modified":1649974222000},{"_id":"source/_posts/ML-SP-Main-Decision-Tree-Algorithms.md","hash":"01f4417d1715046530f0a4cfe6ac65a540f49ac6","modified":1649974222000},{"_id":"source/_posts/ML-SP-simple_regression.md","hash":"7d7dd52a5e8f762fa6e11399ab20c3d92de45eaa","modified":1649974222000},{"_id":"source/_posts/ML-SP-Random_Forest.md","hash":"91d6ef3f3596ca0714659c1e50f23d1adc5da3ab","modified":1649974222000},{"_id":"source/_posts/ML-SP-SVM.md","hash":"5276b5435c2042b68a9fec90e4a6c4241ab5157b","modified":1649974222000},{"_id":"source/_posts/ML-SP-optimization.md","hash":"0afd4f691eb1ceceb16531087fac3d3fc9bf3e0d","modified":1649974222000},{"_id":"source/_posts/ML-SP-logistic_regression.md","hash":"6d510f6df4e7aa9abbd2a044ab9f6d5939f4e195","modified":1649974222000},{"_id":"source/_posts/ML-US-PCA.md","hash":"c1c64f413239da349347961adb077e6843c97619","modified":1649974222000},{"_id":"source/_posts/NLP-NLU.md","hash":"6392ca5bd3a5d1e2f10d9ff96200dbdefd8099a4","modified":1649974222000},{"_id":"source/_posts/ML-XAI-PDP.md","hash":"96539ad249b800d390ed395324080c6bd9815834","modified":1649974222000},{"_id":"source/_posts/ML-XAI-shap.md","hash":"df812a56dd77d7ee3afff522669f31b382aacbdc","modified":1649974222000},{"_id":"source/_posts/ML-US-knn.md","hash":"69150db2e484dc6ef07e84b237b02da17b9ee622","modified":1649974222000},{"_id":"source/_posts/NLP-wordembedding.md","hash":"82881acf48e91f6927d4b80ad5e31586509b3ea2","modified":1649974222000},{"_id":"source/_posts/Preprocessing-dt-Scaler.md","hash":"a19430b394790e4e4b4e30043513a37ba5611670","modified":1649974222000},{"_id":"source/_posts/Preprocessing-numpy-basics.md","hash":"642f5b5c480c0268d264525343cacf30b8cb9f02","modified":1649974222000},{"_id":"source/_posts/Preprocessing-pandas-collection-to-df.md","hash":"aed3941bcdab9ad0da9b1b3168b885f8e90120dd","modified":1649974222000},{"_id":"source/_posts/Preprocessing-pandas-remove_col.md","hash":"64484fa17d2363bdcf55b9c92b96ebf75c228614","modified":1649974222000},{"_id":"source/_posts/Preprocessing-pandas_groupby.md","hash":"d5223c4275e3b5b74af399ebe58cc95e6ba2d306","modified":1649974222000},{"_id":"source/_posts/Preprocessing-pandas_overview.md","hash":"9e6ed33c02dba1b331ebe459bf2a344ce8241ce5","modified":1649974222000},{"_id":"source/_posts/Programming-Python-Generator.md","hash":"9bfbefd08eeeec0f9d19bb701e43f157e2cf45f6","modified":1649974222000},{"_id":"source/_posts/Preprocessing-sampling-imbalance-data.md","hash":"fa61aeabf42c47c18d409ef1422cb0355d46a9f3","modified":1649974222000},{"_id":"source/_posts/Preprocessing-pandas_tricks.md","hash":"41f8eac5a6c433d8da8c2b92c689926c5b0b2a1e","modified":1649974222000},{"_id":"source/_posts/Programming-Python-lambda-highorder.md","hash":"3acaf74406d50265d7cc1ffd6dfadf1bb8432f4b","modified":1649974222000},{"_id":"source/_posts/Programming-Python-Graph-basic.md","hash":"4894bea6f8a72e6bced5fc2a7539d2b6ac0432b6","modified":1649974222000},{"_id":"source/_posts/Programming-Python-hash-table.md","hash":"9168b61a6e26381389ecdd9636b022a1dc0bbbd7","modified":1649974222000},{"_id":"source/_posts/Programming-Python-unpacking.md","hash":"5f955018322558057a497e3568331d84286144fc","modified":1649974222000},{"_id":"source/_posts/Programming-python-regex.md","hash":"5a6d9c1028f8ab60262126b61d6bb48c6ce3f5b1","modified":1649974222000},{"_id":"source/_posts/Programming-Python-zip-enumerate.md","hash":"c41508fb9f1f57d8d696c965b74ea795d63669ff","modified":1649974222000},{"_id":"source/_posts/R-Programming-operators.md","hash":"f178a9bcf4d7ae083a3fc3619988d483e13168e4","modified":1649974222000},{"_id":"source/_posts/Statistics-Prob-multinomial-dist.md","hash":"b8f49089c8f6b5988804a1b75d653730a94b11df","modified":1649974222000},{"_id":"source/_posts/TS-SQL-ts-1.md","hash":"396b86861d30d72fbb96dd272a5b30b61c7ca265","modified":1649974222000},{"_id":"source/_posts/TS-R-lib-1.md","hash":"6e84f308abfb211b48d3afd65e19595ed9e7d918","modified":1649974222000},{"_id":"source/_posts/Statistics-Prob-chi-square-dist.md","hash":"98b99d609e377ec4034a67550a5812c974af2017","modified":1649974222000},{"_id":"source/_posts/TS-linux-ts-1.md","hash":"9ad7baae2a44439dbab7e0507b299a83faf84b58","modified":1649974222000},{"_id":"source/_posts/tools-docker-basic.md","hash":"74d1fcdfdf02628181af5a87c056e3fc8637b827","modified":1649974222000},{"_id":"source/_posts/thoughts-1.md","hash":"519bc6f2a43452d5075da1d8a4541c23d7530bea","modified":1649974222000},{"_id":"source/_posts/tools-conda-install.md","hash":"86a89dfcea399c0b25ff177444f6690d85d70584","modified":1649974222000},{"_id":"source/_posts/tools-git-basic.md","hash":"20a09fafa4d90defb5684d260353361ce00b4ad1","modified":1649974222000},{"_id":"source/_posts/tools-git-log.md","hash":"e6d0bc81a4091a944f50d4beb622420a2da679b3","modified":1649974222000},{"_id":"source/_posts/tools-git-private-repo.md","hash":"2eb436fc0d29714d5af945441252aabf397ecebd","modified":1649974222000},{"_id":"source/_posts/DL-backpropagation/bp_model.png","hash":"76492bc58d6ef89dc21c452628b8a5df11ce944c","modified":1649974222000},{"_id":"source/_posts/DL-backpropagation/gb_ex.png","hash":"1caa6b5fb63ed1e6aeee3485b55f57bfcfb36bb4","modified":1649974222000},{"_id":"source/_posts/Data-Wangling-pandas_groupby/groupby_9_1.png","hash":"31baf0f66f1891a223bbab8f17b6f00914493f3d","modified":1649974222000},{"_id":"source/_posts/Statistics-Math-derivatives.md","hash":"65a754bf16c4fd8507dbe4fd5c9684e575848ee5","modified":1649974222000},{"_id":"source/_posts/Data-Wangling-pandas_groupby/groupby_9_1.svg","hash":"383a69dfbb9dd707016efbff035bdb4b8e1f075d","modified":1649974222000},{"_id":"source/_posts/ML-US-PCA/FS_FE.png","hash":"53859997510237799d6b789355f8aff99014a0e7","modified":1649974222000},{"_id":"source/_posts/ML-SP-simple_regression/1_simple-regression_8_0.png","hash":"61faa846f2d4c0ddd3adc33b391022ca299c056f","modified":1649974222000},{"_id":"source/_posts/ML-SP-simple_regression/1_simple-regression_6_0.png","hash":"a779a3bf17f9ae408d4bb6fbbc20ff7558090340","modified":1649974222000},{"_id":"source/_posts/ML-US-PCA/covariance_matrix.png","hash":"4dca4a7391e9b39e019d2ec30c58e9b39d51600d","modified":1649974222000},{"_id":"source/_posts/tools-git-basic/git-remote.png","hash":"c7e327ba54a68f2004095f72ef1a4fae5b4b6f27","modified":1649974222000},{"_id":"source/_posts/tools-git-basic/git-branch.png","hash":"6f18fac6409cc917e55a66f91733327a10982588","modified":1649974222000},{"_id":"source/_posts/tools-git-basic/git-simple.png","hash":"9174ca5e88c8642b874d02ada8f8b92c90103ecf","modified":1649974222000},{"_id":"source/_posts/ML-DL-perceptron/perception_rule.png","hash":"d3bb7630bed0d5a8e5e05ea3da5175073fe98afa","modified":1649974222000},{"_id":"source/_posts/ML-DL-perceptron/perceptron_al.png","hash":"81bb0f2ef08a5d3dbc5e27e2ba8728d956304829","modified":1649974222000},{"_id":"source/_posts/ML-US-PCA/PCA_lc.png","hash":"d78c44c057fb6bc7c368e7cba1d9fca7cf206be5","modified":1649974222000},{"_id":"source/_posts/tools-conda-install/conda_venv.png","hash":"31feccec0cb9be5f30c0f75ade776b473fbbbb7e","modified":1649974222000},{"_id":"source/_posts/tools-git-basic/github_remote.png","hash":"65231ba5ecf084a439a9d014eb77e8b2af6e550d","modified":1649974222000},{"_id":"themes/archer/layout/_partial/comment/custom.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1649391170000},{"_id":"themes/archer/.editorconfig","hash":"5bccaaa5c39bfb2c7f8174853942bcf150023b41","modified":1649391170000},{"_id":"themes/archer/.eslintrc.json","hash":"3f53b2d1eb42d1d221b408159a6d65e34d2c115a","modified":1649391170000},{"_id":"themes/archer/.babelrc","hash":"afa50eba6e461a2fffa7f7df19ffcc2ffe160c2e","modified":1649391170000},{"_id":"themes/archer/.gitattributes","hash":"82c1a621642d5b620275ae1ed59845c3f7015a64","modified":1649391170000},{"_id":"themes/archer/.gitignore","hash":"703a5c7132f1604d25ccc32233a1d81eb9255ec7","modified":1649391170000},{"_id":"themes/archer/.prettierrc.js","hash":"3a31e8e081b4e1bdd1174f70d89489114095a578","modified":1649391170000},{"_id":"themes/archer/.prettierignore","hash":"f1489a0426112ccd5a243b8ffb84ba8c9311d70f","modified":1649391170000},{"_id":"themes/archer/LICENSE","hash":"35f4fb806270f8243459c870a2141e795dfab166","modified":1649391170000},{"_id":"themes/archer/CHANGELOG.md","hash":"152a607acacfc4893276d8e2f15379238fd0eb7d","modified":1649391170000},{"_id":"themes/archer/_config.yml","hash":"e674aa5e12cd79b041b7ae8d2fde6915b1dfdd25","modified":1655129042000},{"_id":"source/_posts/DL-backpropagation/backward_propagation.png","hash":"b0b52e411aa192fe2efb5bae3e4140ce562bb004","modified":1649974222000},{"_id":"themes/archer/gulpfile.js","hash":"25b788354e20c9b99866dea35959006dcf837fe1","modified":1649391170000},{"_id":"source/_posts/DL-backpropagation/chainrule.png","hash":"1a6488625fa49ba7fb926c31473b472c5a9cf8c7","modified":1649974222000},{"_id":"themes/archer/package.json","hash":"95630d252e575ee98b9d43d92669f780ea031906","modified":1649391170000},{"_id":"themes/archer/webpack.config.js","hash":"d8359ceb1df91202dee1600a00a0f4dd28fdc729","modified":1649391170000},{"_id":"themes/archer/webpack.dev.js","hash":"9508d5fef1acb87dcc71bf6a228f3f86eb9c6408","modified":1649391170000},{"_id":"themes/archer/webpack.prod.js","hash":"9b036689235595d86394e01bb9e93c77f9d35f36","modified":1649391170000},{"_id":"themes/archer/dev/archer.sh","hash":"9474c501c1c55f47f02cccdd9e2039498ebc5e43","modified":1649391170000},{"_id":"themes/archer/README.md","hash":"5dc90ec5f722a9d03fbc325be2ffab8a13c74c1f","modified":1649391170000},{"_id":"themes/archer/docs/develop-guide-en.md","hash":"c10293eb8ccad5d02412a1369ec1c7e77516b929","modified":1649391170000},{"_id":"themes/archer/docs/develop-guide-zh.md","hash":"6d1e62f7c2d45ff0168fffc4cfe43341d3f89ff9","modified":1649391170000},{"_id":"themes/archer/languages/default.yml","hash":"77a3a6f64410611ac732d5a3f537d810fcaa60d9","modified":1649391170000},{"_id":"themes/archer/languages/en.yml","hash":"5790207a1c8f4ed7763961af036e014ce8884901","modified":1649391170000},{"_id":"themes/archer/docs/README-en.md","hash":"e4fff6fc13f3296c2b168ab220f847192bf1273b","modified":1649391170000},{"_id":"themes/archer/layout/404.ejs","hash":"a35e0e478ff84592c4a9d129fb24260e3ddc28ee","modified":1649391170000},{"_id":"themes/archer/layout/about.ejs","hash":"79eb9f77fcf72c07f8399bdf571e912c5588c647","modified":1649391170000},{"_id":"themes/archer/layout/index.ejs","hash":"ee164984b6a824c6b2caef224e169ef8a2827fa5","modified":1649391170000},{"_id":"themes/archer/layout/layout.ejs","hash":"05285ac99ca71d3d5ad73cf927065182bb0bf005","modified":1655133628612},{"_id":"themes/archer/layout/site-meta.ejs","hash":"7a5a65c84cb0573b510f87eff2b812ee05a17041","modified":1649391170000},{"_id":"themes/archer/layout/post.ejs","hash":"a255b022714add34098bd3d50ed8dd91dbdf3c5e","modified":1650035168000},{"_id":"themes/archer/layout/_partial/algolia.ejs","hash":"f119abb7e9f594a1516df45520bc45d0c620bdb6","modified":1649391170000},{"_id":"themes/archer/layout/_partial/base-footer-fixed.ejs","hash":"e4dbde6594c0c2d1c5de71ddf968be0879ceddb7","modified":1649391170000},{"_id":"themes/archer/layout/_partial/base-footer.ejs","hash":"c74fcd3f6e8a58eeb4ecf3aad33d8ac926da77b4","modified":1649391170000},{"_id":"themes/archer/layout/_partial/base-background-image.ejs","hash":"aa09e2d060b55764dd3e3195184951bca5dd6779","modified":1649391170000},{"_id":"themes/archer/layout/_partial/base-header.ejs","hash":"f57c5023139b129bcdfd50f85f43e3159f2c9621","modified":1655132700611},{"_id":"themes/archer/layout/_partial/base-head.ejs","hash":"0353c58011e559f1606168b897e6cdef0a1f3cf5","modified":1649437600000},{"_id":"themes/archer/layout/_partial/base-social.ejs","hash":"0cfba91d1a7273f349a5ca676a1fb8d23fb5e4e9","modified":1649419606000},{"_id":"themes/archer/layout/_partial/base-preload-polyfill.ejs","hash":"52445702f566f2d75407455fb4724e7469dc4b85","modified":1649391170000},{"_id":"themes/archer/layout/_partial/base-profile.ejs","hash":"aee834b34b2921448c53763dc0b164cf8c10efd9","modified":1650031896000},{"_id":"themes/archer/layout/_partial/base-title-tags.ejs","hash":"fc4c44f8f21ed8a3ad011a86c956a750c5ff4db2","modified":1655132009787},{"_id":"themes/archer/layout/_partial/custom-font.ejs","hash":"d75ca325a2253ae8ea498a6ca02bf7d556ec3066","modified":1649391170000},{"_id":"themes/archer/layout/_partial/intro-height.ejs","hash":"03b295e12f90373042faf3ddc00baa7ea66f682e","modified":1649391170000},{"_id":"themes/archer/source/assets/algolia_logo.svg","hash":"16505f61f19ba65f629dfd033f14ee9abcf18756","modified":1649391170000},{"_id":"themes/archer/source/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1649391170000},{"_id":"themes/archer/source/assets/blog-solid.svg","hash":"6b3d36900d5064217234bc241df5ef4208cf185c","modified":1649741754000},{"_id":"themes/archer/source/assets/loading.svg","hash":"85082b002bae1335114b71550350907884187e38","modified":1649391170000},{"_id":"themes/archer/source/assets/beian.png","hash":"a99df13e8eb11db86edebf6e5ac246eb59f4b3c4","modified":1649391170000},{"_id":"themes/archer/source/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1649391170000},{"_id":"themes/archer/source/css/dark.css","hash":"721c12c1714e6fa0b1b0a2a330bd3ec59d591ad3","modified":1649391170000},{"_id":"themes/archer/source/assets/favicon.ico","hash":"8b200c575d273d41a179c102442e191414e74eae","modified":1649391170000},{"_id":"themes/archer/source/css/mobile.css","hash":"c929d93afd5c30f6b1587d6f53da5dabe6a00b60","modified":1649391170000},{"_id":"themes/archer/source/css/style.css","hash":"4b12b68e3a952913b714d98782f49421eb67d9c9","modified":1649391170000},{"_id":"themes/archer/source/css/mobile.css.map","hash":"1f26e4000a416870ccf4e85083f7209dc55f4482","modified":1649391170000},{"_id":"themes/archer/source/css/dark.css.map","hash":"f5b60d05584b47dea744edff92c44bc389675e05","modified":1649391170000},{"_id":"themes/archer/source/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1649391170000},{"_id":"themes/archer/source/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1649391170000},{"_id":"themes/archer/source/lib/webfontloader.min.js","hash":"03f379e646bdedbbbcc53c634d7e0039519cb0ad","modified":1649391170000},{"_id":"themes/archer/source/scripts/customFontLoader.js","hash":"9237010b227c2cc5d0b30223640eee689801629f","modified":1649391170000},{"_id":"themes/archer/source/scripts/dark.js","hash":"469ff6acbb00deb0d62ce765f06a2fc76dadbfc0","modified":1649391170000},{"_id":"themes/archer/source/scripts/main.js.LICENSE.txt","hash":"8a6ae61d6eedc0fa08c18b34a7fe7bdd61059720","modified":1649391170000},{"_id":"themes/archer/source/scripts/search.js.LICENSE.txt","hash":"c2e89128b9d5f31e3702384033f4b118114c10d5","modified":1649391170000},{"_id":"themes/archer/source/scripts/share.js","hash":"3c5d3a97a33d1f5caa39cd168f6f82aff6ec02aa","modified":1649391170000},{"_id":"themes/archer/source/scripts/dark.js.map","hash":"512d8ca5ed89950d8fad95ba1200f0b21ba82388","modified":1649391170000},{"_id":"themes/archer/src/js/browser.js","hash":"1c2672917d07adc8e74c065264a520e081154191","modified":1649391170000},{"_id":"themes/archer/src/js/customFontLoader.js","hash":"7dbdaf6ab1d0553129f827768cab793360cf1d63","modified":1649391170000},{"_id":"themes/archer/src/js/dark.js","hash":"7f03e978af9d59ff990a68dfa939a37cb6e87af1","modified":1649391170000},{"_id":"themes/archer/src/js/fancybox.js","hash":"2907a27a3ba897cfc7c3ab3dd7bc6a3e8a7c504c","modified":1649391170000},{"_id":"themes/archer/src/js/fontawsome.js","hash":"5694c5c42a3990c034dcb2ed9baf73264805b039","modified":1649391170000},{"_id":"themes/archer/src/js/init.js","hash":"1eed1591f6387f847b9800a3f7ca87e16d0c325e","modified":1649391170000},{"_id":"themes/archer/src/js/initSidebar.js","hash":"ad88bef0bf1c71c6278072d203aaad8c84561fde","modified":1649391170000},{"_id":"themes/archer/src/js/main.js","hash":"16a5203c9fb46cc81a3bed4c5baf99012bd794c3","modified":1649391170000},{"_id":"themes/archer/src/js/mobile.js","hash":"5064036c3c8c77a44c62fadb4d3c34ad1943fd6e","modified":1649391170000},{"_id":"themes/archer/src/js/scroll.js","hash":"17959aabae6af0d5e27ae4cfbb13300293aedcb9","modified":1649391170000},{"_id":"themes/archer/src/js/search.js","hash":"2751aee7fc0e494b0f5fb1273ef24317c40f819a","modified":1649443324000},{"_id":"themes/archer/src/js/share.js","hash":"eb8990823f7931b0f71478f6da665664c9e36e81","modified":1649391170000},{"_id":"themes/archer/src/js/sidebar.js","hash":"ea4b8ad7af9d3e8673a21591eac980761e01a7e3","modified":1649391170000},{"_id":"themes/archer/src/js/tag.js","hash":"ddf2723fe7cce2aae12ad0479287d5a3c53baa07","modified":1649391170000},{"_id":"themes/archer/src/js/toc.js","hash":"070dc7760f80802afd71faf3b39772c392504eb2","modified":1649391170000},{"_id":"themes/archer/src/js/util.js","hash":"5347de39208195bd5ba67479e3c00945b0f89d56","modified":1649391170000},{"_id":"themes/archer/src/scss/_common.scss","hash":"d5ac6df18ba0bed9fae2a7c0858c2dcc5f497904","modified":1649391170000},{"_id":"themes/archer/src/scss/_mixin.scss","hash":"ede5b572a3a29fd01902b1947057508441bc1a7e","modified":1649391170000},{"_id":"themes/archer/src/scss/_normalize.scss","hash":"fb6a1349bab25b65cf89b47e136d958d10947533","modified":1649391170000},{"_id":"themes/archer/src/scss/_variables.scss","hash":"99915c1ae8562f475eddc55ec296d531262d82ff","modified":1649391170000},{"_id":"themes/archer/src/scss/dark.scss","hash":"f3a3dcd962a3ca60967757a174999d22f26884aa","modified":1649391170000},{"_id":"themes/archer/src/scss/mobile.scss","hash":"e024acc944fcc4d97774855e5735a8c4b6fb3528","modified":1649391170000},{"_id":"themes/archer/layout/_partial/_third_party/localsearch.ejs","hash":"90ab413e51ab7289b015f016237a52989feefa06","modified":1649431534000},{"_id":"themes/archer/src/scss/style.scss","hash":"bcd8df4e180903820938324201ccc24a464c7cde","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/changyan.ejs","hash":"f35a8bb655ba782676493c87d94f4021d1f8d4e7","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/gitment.ejs","hash":"d3040d5613313bee8010c6b77359bf96ece0e84b","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/livere.ejs","hash":"15dd256b459522515d7601239e21cf5d849930ca","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/gitalk.ejs","hash":"609d9b867d9aae7984dcc233ed31f6d073f9d3a0","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/utteranc.ejs","hash":"d120a1b0732f7d6033dbd789dea58e23603c10aa","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/valine.ejs","hash":"e96796398f3ade42b8ba4354b51890ec586d7843","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/waline.ejs","hash":"4eafb4aab531b5997061c435b461712d6385b4e1","modified":1649391170000},{"_id":"themes/archer/layout/_partial/critical-css/critical-style.ejs","hash":"74988d738cbd070cc4a921f947b7fcdb5183c405","modified":1649391170000},{"_id":"themes/archer/layout/_partial/comment/youyan.ejs","hash":"b56d493eebace62db42b30c12fd9df8fde961d0d","modified":1649391170000},{"_id":"themes/archer/layout/_partial/math/mathjax.ejs","hash":"65c1203fcb652894b5edaa7b686009cfd5211f72","modified":1649391170000},{"_id":"themes/archer/layout/_partial/script/font-loader.ejs","hash":"245943b4153047c9efb874c4c27e88fd9ed3c341","modified":1649391170000},{"_id":"themes/archer/layout/_partial/search/localsearch_view.ejs","hash":"7195bdd991a4147c245ad3df6e0fe3353789f42e","modified":1649426726000},{"_id":"themes/archer/layout/_partial/search/localsearch_button.ejs","hash":"6b8858191f7595170f8e7bc8abcef75a833b9958","modified":1649475540000},{"_id":"themes/archer/layout/_partial/comment/disqus.ejs","hash":"9859e0717d8e76a31001c3cafd320f85c78f3776","modified":1649391170000},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-archives.ejs","hash":"89bf250c14366682def9b1667339d55dff234f21","modified":1649391170000},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-categories.ejs","hash":"9316847cb59357787c3ef4b9a3f572a96d14cb00","modified":1649391170000},{"_id":"themes/archer/layout/_partial/sidebar/sidebar-tags.ejs","hash":"3b856404884c52173af2c9be165a1b23fd3caa70","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_404.scss","hash":"b5eaa29de016b5c7838a5791198bcc6003385b65","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_common-dark.scss","hash":"d768ff4ded26f321a73e425748400779659f51b6","modified":1649391170000},{"_id":"themes/archer/layout/_partial/sidebar/base-sidebar.ejs","hash":"84c9747859051f5df7843ad704f8fac6c3021847","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_index-page.scss","hash":"150821e84dac9338b15bc43d1c9bfeca7a17a70d","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_algolia.scss","hash":"2bf49a3ffdc12cb7c1581690a383b66f0d0f2ddd","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_index-page-dark.scss","hash":"fe31715d8ab240e9207bb907786854badfdd1e72","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_algolia-dark.scss","hash":"2576dd0957e01ba76f90e21b91db3e4b48f84bd5","modified":1649391170000},{"_id":"themes/archer/src/scss/_mobile/_partial/_index-page-mobile.scss","hash":"b8af32962b11463b5f94891aa1d7de9ae95a68cf","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_post-page-dark.scss","hash":"8f30799613a40a55edfea2c90e525fb356099813","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_post-page.scss","hash":"b557cec951b5b5f707ed297a4b1159edaa1c2124","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_comment/_gitalk.scss","hash":"846893051295b8b66301b31274f0c787cc32cc0a","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_footer.scss","hash":"0e27384366412a4da56ece046cc9e7530abf679d","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_footer-fixed.scss","hash":"806c3eefac1e07493eab03849420143635e45828","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_intro.scss","hash":"32ef36e1b2fc298323ea09f2b7ffce3be175d58d","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_paginator.scss","hash":"1e4510959f51b5d4d3c5781468c77d25546ce905","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_profile.scss","hash":"eba73d0e1ff0997446efbe5655b449a80967c8c1","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_header.scss","hash":"24adf03f093ed8f3d28936a9ee2b7598c4f4799f","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar-tags.scss","hash":"7f20bdb96b402663a0e2bfcf7f4827198611a3c4","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_partial/_scrollbar.scss","hash":"04f57efb3c9a586dfba742fe63606ebac98d936a","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar-archive.scss","hash":"f97ea429dcb4d371cecf34c9c8f7d3a40fc11e11","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_post/_code.scss","hash":"87840eec79fb6970516e735d26a5b320d1cb846d","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_post/_writing-enhance.scss","hash":"12f52673d487c720a8ea49a31db5a37630e5b33f","modified":1649391170000},{"_id":"themes/archer/src/scss/_partial/_sidebar/_sidebar.scss","hash":"3ecc4abc4de9a3f134e8c032d46e7562afab6916","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_partial/_footer-dark.scss","hash":"b169fbdf0e85497021ea521db247939a723aa9b3","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_partial/_footer-fixed-dark.scss","hash":"7da8ef87e04efce102140ac8dcce01b9596f513b","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_partial/_header-dark.scss","hash":"6e18d4d079a4fd151c8d81bdc70c91f63faf6c47","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_partial/_profile-dark.scss","hash":"f6f21504c2212fadc80fc0157b03c14acad29b4a","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_post/_code-dark.scss","hash":"310ac0c38accd652b16eafbb068fdb7d27fce729","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_comment/_gitalk-dark.scss","hash":"f32dbf8a00900f16d2b65dff5336a9a112a76662","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_sidebar/_sidebar-dark.scss","hash":"8ad0191162833248ba821d4f2f4453c344254649","modified":1649391170000},{"_id":"themes/archer/src/scss/_mobile/_partial/_post/_writing-enhance-mobile.scss","hash":"27f6775c5fbb04f53440050fca312b94f48088f1","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_sidebar/_sidebar-archive-dark.scss","hash":"71f50003fe109ed40d6fdad835b7cdc8ab19bf97","modified":1649391170000},{"_id":"themes/archer/src/scss/_dark/_partial/_sidebar/_sidebar-tags-dark.scss","hash":"2e2f208e25dd2d86a8ecff1bc2d20f604084fa05","modified":1649391170000},{"_id":"themes/archer/src/scss/_mobile/_partial/_sidebar/_sidebar-tags-mobile.scss","hash":"68acfc8b7c5df5cd945365ba9c0620df2cd5e515","modified":1649391170000},{"_id":"themes/archer/src/js/local-search.js","hash":"0dd62adebe36cf937b57f621f58e73a237370e0a","modified":1649475350000},{"_id":"themes/archer/source/avatar/medium.jpg","hash":"a9a45491c713d8121c8df40282433671fe041e66","modified":1581220472000},{"_id":"themes/archer/source/css/style.css.map","hash":"c9495e0d815109b6cb17ddaa1a99e10714e0a6e8","modified":1649391170000},{"_id":"themes/archer/source/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1649391170000},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1649391170000},{"_id":"themes/archer/source/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1649391170000},{"_id":"themes/archer/source/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1649391170000},{"_id":"themes/archer/source/lib/jquery.min.js","hash":"156837f75f6600ccb602b4efcbd393636c33f35e","modified":1649391170000},{"_id":"themes/archer/source/scripts/customFontLoader.js.map","hash":"0fb40aed5a9c1d3f97dbad07ae3279229eaebea9","modified":1649391170000},{"_id":"themes/archer/source/scripts/share.js.map","hash":"b6a3e931cf44fdac5fd717f64019f2aa3b0d62ef","modified":1649391170000},{"_id":"themes/archer/source/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1649391170000},{"_id":"themes/archer/source/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1649391170000},{"_id":"themes/archer/source/scripts/search.js","hash":"2b822ad6a686ba8d5f08e7218437053e84e5f344","modified":1649391170000},{"_id":"themes/archer/package-lock.json","hash":"5c2fae8360c4c4ff6535b7cb7b7855d745919af2","modified":1649391170000},{"_id":"themes/archer/docs/snap.png","hash":"0b2a8bf016f6eed576abfdcdb7dcf8de51c12562","modified":1649391170000},{"_id":"themes/archer/source/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1649391170000},{"_id":"themes/archer/source/scripts/search.js.map","hash":"1ecdd768a261ebede8b51ca383c9bf519a4f6c7c","modified":1649391170000},{"_id":"themes/archer/source/scripts/main.js","hash":"8d7b985669cc76db1d98b8d899e810790807c5ad","modified":1649391170000},{"_id":"themes/archer/source/scripts/main.js.map","hash":"b2f3bab368e872ecc910e48fff47797432ab019f","modified":1649391170000},{"_id":"public/rss2.xml","hash":"5af7e815a77d1d9abcc1448823c9e8cd06b61179","modified":1655209069473},{"_id":"public/content.json","hash":"6123d69883dd5a82e6ec5a7a725ae159396278f5","modified":1655209069473},{"_id":"public/search.xml","hash":"92cb3e07837bf7d6ac7870a618893710750dbe1f","modified":1655209069473},{"_id":"public/post-sitemap.xml","hash":"60e9be83960ce54fbee5535e93906b9160e968a3","modified":1655209069473},{"_id":"public/page-sitemap.xml","hash":"3163fb9715726a466e28d8ebb9d1094b927c1e70","modified":1655209069473},{"_id":"public/category-sitemap.xml","hash":"6a4222f6d9df692c0267d1505e88a5bd0c600350","modified":1655209069473},{"_id":"public/sitemap.xsl","hash":"4321fa69dc1b8811d32b7a1478e5603e038cea1a","modified":1655209069473},{"_id":"public/sitemap.xml","hash":"d5cebd13ebf056731551218345a4ddbe31789901","modified":1655209069473},{"_id":"public/about/index.html","hash":"d31686772b4497a1204e8eca710fe261f9816f83","modified":1655209069473},{"_id":"public/Projects/index.html","hash":"eba69ef7ce75ac5ef322ba988ec729a0d62d8e66","modified":1655209069473},{"_id":"public/2022/06/13/tools-docker-basic/index.html","hash":"79b077677c3ba23cf3bb0307129d49c419dcc40b","modified":1655209069473},{"_id":"public/2022/06/13/tools-git-log/index.html","hash":"d3df7cc435523fa21d3ca00c21f9afc1c21d975b","modified":1655209069473},{"_id":"public/2022/06/13/tools-git-private-repo/index.html","hash":"98f2ed34f9fe10583656e58099eac8c46f545499","modified":1655209069473},{"_id":"public/2022/06/13/DL-hyperparameter/index.html","hash":"ebc56335d2d1786d82148cffc551dff73aa9993d","modified":1655209069473},{"_id":"public/2022/06/13/DL-lossfunction/index.html","hash":"7fd7153e6ab2324e7cf03e32c662822a13e2d205","modified":1655209069473},{"_id":"public/2022/06/13/DL-optimizer/index.html","hash":"d864abf42e6b609e4e8a552c66e0028cc452ba58","modified":1655209069473},{"_id":"public/2022/06/13/DL-perceptron/index.html","hash":"454deae3eedf14e69da7ea6d471ac96f83b2d49c","modified":1655209069473},{"_id":"public/2022/06/13/DL-regularization/index.html","hash":"7364f6dcd86048b6550c982b0aacfeec643e726c","modified":1655209069473},{"_id":"public/2022/06/13/ML-Metrics-Regression/index.html","hash":"c9223e33f18b6a318e132ca9edadef6834394b49","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-Decision_Tree/index.html","hash":"c3b77a2679467e705a3cc4514a6a9d9f823cc207","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-Main-Decision-Tree-Algorithms/index.html","hash":"9aad89403b6f5e8b58571900d366018a076add0f","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-Random_Forest/index.html","hash":"3a234cb7e6668433326e777cf3e9e6932bfe5975","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-SVM/index.html","hash":"63ff430ff6878f4d8195f1dc066e830e1a7910a2","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-logistic_regression/index.html","hash":"280e6908cc2ebfe9fafd8d7b0c035c3e03edab5d","modified":1655209069473},{"_id":"public/2022/06/13/ML-SP-optimization/index.html","hash":"fe15b97696b6b8aa9b47133ed59fabbd446dd2a4","modified":1655209069473},{"_id":"public/2022/06/13/ML-US-knn/index.html","hash":"c6dba019f745a8decbc39e8f8dae63532e34a1b8","modified":1655209069473},{"_id":"public/2022/06/13/ML-XAI-PDP/index.html","hash":"a826632954390f6f90d72c518e4179fbbf149f94","modified":1655209069473},{"_id":"public/2022/06/13/ML-XAI-shap/index.html","hash":"3c6872fc464c835d8277c4d746500e59a3724bd1","modified":1655209069473},{"_id":"public/2022/06/13/NLP-NLU/index.html","hash":"2312422d0300db1fcf56f878d97dd013760e746a","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-dt-Scaler/index.html","hash":"4444fa25cc8aadc5b932ea6da92b921b1b82da39","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-numpy-basics/index.html","hash":"aea7cf9bf9e1fb8e824be7a59d4e8a339cfde76e","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-pandas-collection-to-df/index.html","hash":"a0596b881ee39267a3e54221596d90dbaebaa2eb","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-pandas-remove_col/index.html","hash":"b85d1e5d46c662a91e111a66c141e89550e5e20e","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-pandas_groupby/index.html","hash":"c962b516ebeb9ab861383a60366b46f16abdeedd","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-pandas_overview/index.html","hash":"0732f7f42a67cb9486d9bad1580ac82618ac8700","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-pandas_tricks/index.html","hash":"29b2d5f0d8c15c3527dd02886040845868cae27e","modified":1655209069473},{"_id":"public/2022/06/13/Preprocessing-sampling-imbalance-data/index.html","hash":"038baed05ace1b4c33885070827adee9c0e3461a","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-Generator/index.html","hash":"ebc44afc6f6047faf646b383a38c55afc2e1dea0","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-Graph-basic/index.html","hash":"34e9f5456d49fa7a6d16275caffd72bf90fb16c9","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-hash-table/index.html","hash":"e33191a1ce985f539d80e9a0f86b573de2e9cd0f","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-lambda-highorder/index.html","hash":"b3b5dc8ffbd1b11d60c84e248907f6ef827adad7","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-unpacking/index.html","hash":"f5bad6b437761a5fb4ead1a42c68c24b59958324","modified":1655209069473},{"_id":"public/2022/06/13/Programming-Python-zip-enumerate/index.html","hash":"b024c0f414c79334a8adce1072f3dcac1d12d334","modified":1655209069473},{"_id":"public/2022/06/13/Programming-python-regex/index.html","hash":"c80a14d6bac2f0bc623b058ce4ac60257ad5ed9f","modified":1655209069473},{"_id":"public/2022/06/13/Statistics-Math-derivatives/index.html","hash":"d1a75fe2b4d4efa9c38587981a48215bc66e8092","modified":1655209069473},{"_id":"public/2022/06/13/Statistics-Prob-chi-square-dist/index.html","hash":"df33e627ccf2a86073fef657003db7f4e9e2871f","modified":1655209069473},{"_id":"public/2022/06/13/Statistics-Prob-multinomial-dist/index.html","hash":"56042d875549ecd579d1b5d5244c413fb001b15d","modified":1655209069473},{"_id":"public/2022/06/13/TS-R-lib-1/index.html","hash":"6430664a36d64662e0e3d15083d7eeff2d5d7f42","modified":1655209069473},{"_id":"public/2022/06/13/TS-SQL-ts-1/index.html","hash":"c2f4c006ad551f02919a2711f9db3d03534e216b","modified":1655209069473},{"_id":"public/2022/06/13/TS-linux-ts-1/index.html","hash":"e81512a28e6fe37fbeaaa7b0cb89f1ca821b1aaf","modified":1655209069473},{"_id":"public/2022/06/13/thoughts-1/index.html","hash":"b0bff414c2dd6e276ab7f4143edb764b7fc2d6c9","modified":1655209069473},{"_id":"public/2022/06/13/DE-Linux-commandline/index.html","hash":"9c4a5cebbbe3c4ae5d12ef7b522ceb8299bc5707","modified":1655209069473},{"_id":"public/2022/06/13/DE-SQL-DDL-tables/index.html","hash":"6d1c5f443142842d1cb55ea342d1b7b6c88a76ed","modified":1655209069473},{"_id":"public/2022/06/13/DE-SQL-Where/index.html","hash":"03156f20a589284fde3aea317c3be8d2bb39eead","modified":1655209069473},{"_id":"public/2022/06/13/DE-SQL-case-when/index.html","hash":"16762ea1f70cbbe42b310db2eb5267913007c729","modified":1655209069473},{"_id":"public/2022/06/13/DE-SQL-subquery/index.html","hash":"9c89035945ea66cc899336f6468cd5e121812eb9","modified":1655209069473},{"_id":"public/2022/06/13/DL-RNN/index.html","hash":"53b299ca7fbf93113c0c2de3e898e85f31caa106","modified":1655209069473},{"_id":"public/2022/03/02/NLP-wordembedding/index.html","hash":"d20d0c0740b4449571fa9c3d5c57563d81c54c8e","modified":1655209069473},{"_id":"public/2021/09/28/DL-backpropagation/index.html","hash":"89839c74b2ca209a1c45fee9dc2935fcc7b67cd6","modified":1655209069473},{"_id":"public/2021/07/17/tools-git-basic/index.html","hash":"4c4ffebb8ca94e4d0e06a3bac97930ae1bc56170","modified":1655209069473},{"_id":"public/2021/07/08/tools-conda-install/index.html","hash":"10c24203a29e44a5eab359b4a28cfa33c87af819","modified":1655209069473},{"_id":"public/2021/06/08/ML-SP-simple_regression/index.html","hash":"f40a078af459bef96f2c423b98b66416ea62f1d5","modified":1655209069473},{"_id":"public/2021/05/29/ML-US-PCA/index.html","hash":"ab8e08595ea60f95fc193fa03d7379254f0df1c4","modified":1655209069473},{"_id":"public/2021/03/02/R-Programming-operators/index.html","hash":"9a0c8019dcfcca77633e517047713712113cc1c5","modified":1655209069473},{"_id":"public/archives/index.html","hash":"8f36b3254f223b6614f906dcb602550b64c6fe3a","modified":1655209069473},{"_id":"public/archives/page/2/index.html","hash":"f2af85b3838889ec9baf6a7d2f3649b09aa9b3ee","modified":1655209069473},{"_id":"public/archives/page/3/index.html","hash":"1b63fd7d7fa408976b2774b51f942907d83bb92e","modified":1655209069473},{"_id":"public/archives/page/4/index.html","hash":"30bf17ef88ec142a5344a8c8dcbe358217b41c92","modified":1655209069473},{"_id":"public/archives/page/5/index.html","hash":"e48006fce4d1d7a08b20270778a35cdec1012178","modified":1655209069473},{"_id":"public/archives/page/6/index.html","hash":"d1b2263e11a9720a9de0ef492492ff3bc16efcd2","modified":1655209069473},{"_id":"public/archives/2021/index.html","hash":"05f075bafa33779662d9e5a20be2947160f2c44e","modified":1655209069473},{"_id":"public/archives/2021/03/index.html","hash":"4497aec842d3aa953899a75f10c4c6a6ed1be6da","modified":1655209069473},{"_id":"public/archives/2021/05/index.html","hash":"ddc647a32a57a543eb6c8d3b366ddc16735e0a6b","modified":1655209069473},{"_id":"public/archives/2021/06/index.html","hash":"7608f11fa6ddaedd18fee72d6123d8fd640d270a","modified":1655209069473},{"_id":"public/archives/2021/07/index.html","hash":"42dc249ae202251cf545993c01c02209e1f08d3b","modified":1655209069473},{"_id":"public/archives/2021/09/index.html","hash":"ce6de0310d554e0c5831f03581fb910417e6c4a4","modified":1655209069473},{"_id":"public/archives/2022/index.html","hash":"b5fb27de16cdcf26eac183a4aa20019838307edf","modified":1655209069473},{"_id":"public/archives/2022/page/2/index.html","hash":"5785c0b1c5eb7f3df5c54bf94898b2a7da6a0020","modified":1655209069473},{"_id":"public/archives/2022/page/3/index.html","hash":"576b028d4a393b76168a1a42e29ede3727cc8618","modified":1655209069473},{"_id":"public/archives/2022/page/4/index.html","hash":"b75ea22301d83bf8621c4546cccdf9e45ed36390","modified":1655209069473},{"_id":"public/archives/2022/page/5/index.html","hash":"74b09d998b8a1857cb45467ee90c9a65669160d9","modified":1655209069473},{"_id":"public/archives/2022/03/index.html","hash":"5ddd33904b56fdce7ba83d344603f273cb49b240","modified":1655209069473},{"_id":"public/archives/2022/06/index.html","hash":"1b008ed7db1049a5e68688bbc56cad273ea9e595","modified":1655209069473},{"_id":"public/archives/2022/06/page/2/index.html","hash":"ea638d002251c902d05782b86dcd2003014dcba8","modified":1655209069473},{"_id":"public/archives/2022/06/page/3/index.html","hash":"a2865d14b5910ba1c0fe0fb56787be52c31a90e4","modified":1655209069473},{"_id":"public/archives/2022/06/page/4/index.html","hash":"e7b1136232875ae27b8431b7270affb28995fdb3","modified":1655209069473},{"_id":"public/archives/2022/06/page/5/index.html","hash":"625f083b77d4038543b19db0900984a4c18eeec2","modified":1655209069473},{"_id":"public/categories/Data-Engineering/index.html","hash":"39dccc62c6d961693e785c2ce28431e46da3002c","modified":1655209069473},{"_id":"public/categories/Data-Engineering/Linux/index.html","hash":"0a83188c21ee27fc4e411545df9e2c66d878b81f","modified":1655209069473},{"_id":"public/categories/Neural-Network/index.html","hash":"6c74211e2f081eb195ee19df17c5d4226becf11a","modified":1655209069473},{"_id":"public/categories/Machine-Learning/index.html","hash":"41907a95ba488c2cb0309924c980d128875efe8f","modified":1655209069473},{"_id":"public/categories/Machine-Learning/page/2/index.html","hash":"3f2445a1f36077fd161845922356ff33ec8f042d","modified":1655209069473},{"_id":"public/categories/NLP/index.html","hash":"a71455990181d2da7e9834c84d5757e212e4171a","modified":1655209069473},{"_id":"public/categories/Preprocessing/index.html","hash":"45bb78b616fb9f0996311ebe96878e1a65bf647a","modified":1655209069473},{"_id":"public/categories/Machine-Learning/Supervised-Learning/index.html","hash":"f1f53f2f4f9799ed8a96fd6fde8c10d921cfa564","modified":1655209069473},{"_id":"public/categories/Programming/index.html","hash":"723728651569f5c1fddd72fa3c974ea49e9aa4a7","modified":1655209069473},{"_id":"public/categories/PCA/index.html","hash":"f096ff8c00f6006a97d218b648d13f5e0be5e6b9","modified":1655209069473},{"_id":"public/categories/Statistics/index.html","hash":"cd45190ca76e581b1d5c409cfc1a49734428fca0","modified":1655209069473},{"_id":"public/categories/Troubleshooting/index.html","hash":"749b9ca43608741c65c3eb4510c5faad9fed6ab6","modified":1655209069473},{"_id":"public/categories/Unsorted/index.html","hash":"b08636463ae45f25c508c6271af2df7d97af9c51","modified":1655209069473},{"_id":"public/categories/Tools/index.html","hash":"c7077bfb68057810d79f2ed38423b71a74ca9d79","modified":1655209069473},{"_id":"public/index.html","hash":"9fce92633785fa44d9fbe4caa3eddefd89d9ed52","modified":1655209069473},{"_id":"public/page/2/index.html","hash":"7cc1430fccad803b354fb988ee43014f66c7ec4d","modified":1655209069473},{"_id":"public/page/3/index.html","hash":"7014d81905766b32e1013b1ed2cc7aa378d5c277","modified":1655209069473},{"_id":"public/page/4/index.html","hash":"ae80416501d6f0ac8eb9a248ef212bf47e522848","modified":1655209069473},{"_id":"public/page/5/index.html","hash":"7bb660d6808f2c50c0687adf684b369886461289","modified":1655209069473},{"_id":"public/page/6/index.html","hash":"f9e0e4fc9d9c3d83fc54ad769fbc4eb31efd6cf3","modified":1655209069473},{"_id":"public/tags/commandline/index.html","hash":"c3adf876042b180c7dd2574c2c6397e43d45c7e4","modified":1655209069473},{"_id":"public/tags/Linux/index.html","hash":"e0a1332cbfd47ad35c6d29e86813f1b3c4678448","modified":1655209069473},{"_id":"public/tags/SQL/index.html","hash":"3b1815d35ca47c91376bc7557c75895b7be394a2","modified":1655209069473},{"_id":"public/tags/Subquery/index.html","hash":"b7f6fdfe9f976990d01e58b5b2b43437c48fe96e","modified":1655209069473},{"_id":"public/tags/RNN/index.html","hash":"9709da09af93f274393c5602dd80c2e651687332","modified":1655209069473},{"_id":"public/tags/Optimizer/index.html","hash":"c944cd79df4135ba5562c594801a85cf054eed03","modified":1655209069473},{"_id":"public/tags/Regression/index.html","hash":"b38044d1bba159b3f970ddca976f8dc1eae2a789","modified":1655209069473},{"_id":"public/tags/Metrics/index.html","hash":"7901d1117992d6e12b413d9c9a4ddb5b5f0c3b6b","modified":1655209069473},{"_id":"public/tags/Supervised-Learning/index.html","hash":"dbbbfcd500854a233f20d3855d424d0b6dcbc21e","modified":1655209069473},{"_id":"public/tags/Decision-Tree/index.html","hash":"3d927763e86cc7fe4511774383a3d08c9e529ab5","modified":1655209069473},{"_id":"public/tags/Random-Forest/index.html","hash":"b359c38f12572406300cac4dab19f77f29d33883","modified":1655209069473},{"_id":"public/tags/SVM/index.html","hash":"474141b0e048132954e0d02cebc8491db38dd5c9","modified":1655209069473},{"_id":"public/tags/hyperplane/index.html","hash":"c5316473e5c33876e9813f36216ad0f92c2e7ae8","modified":1655209069473},{"_id":"public/tags/Logistic-Regression/index.html","hash":"f888098eb18c3a7b9bcad8e1b8bcf6a0968290e1","modified":1655209069473},{"_id":"public/tags/Cross-Entropy/index.html","hash":"75f0869bee907fa53d654feaff5f34443a6ce530","modified":1655209069473},{"_id":"public/tags/Python/index.html","hash":"853b7f8f0d3fce0d6b27ab2a4900dbb3c9e4c29e","modified":1655209069473},{"_id":"public/tags/Unsupervised-Learning/index.html","hash":"ebc9fb978f0c30a4e1b362c4c9563ccdbfa7dbdb","modified":1655209069473},{"_id":"public/tags/PCA/index.html","hash":"51632ce1ae315a6094ad4beffae4a07d3eb35e8c","modified":1655209069473},{"_id":"public/tags/KNN/index.html","hash":"1596e2dd1f5c96e20241f567e41c75ed03fe7cec","modified":1655209069473},{"_id":"public/tags/XAI/index.html","hash":"4869a3ddfbe4345c5ced51c600e63ab0d7339097","modified":1655209069473},{"_id":"public/tags/NLP/index.html","hash":"63844ea8a733ccb3f02a3ab923f1cb19c9b894a9","modified":1655209069473},{"_id":"public/tags/NLU/index.html","hash":"ef3c81546c17bfeb93cc6f1117c5105eef5b299a","modified":1655209069473},{"_id":"public/tags/QA/index.html","hash":"5f3a681e798bee30ee3f5e3a7dce9e03470cb20f","modified":1655209069473},{"_id":"public/tags/Deep-Learning/index.html","hash":"dd55fe22edb802b6937f8331b767f9099e12cfab","modified":1655209069473},{"_id":"public/tags/WordEmbedding/index.html","hash":"bb7d6bfc161ee88c3bb39bc682ddc767e5149c53","modified":1655209069473},{"_id":"public/tags/numpy/index.html","hash":"4216a784552d22e0fc683bf72a46e073986177f9","modified":1655209069473},{"_id":"public/tags/pandas/index.html","hash":"3f6897ff8f19e4ad463b1049d4387c5be2d4c0c1","modified":1655209069473},{"_id":"public/tags/Sampling/index.html","hash":"33627f5de87b2a8cece2fb1a6cbd03d22cab689a","modified":1655209069473},{"_id":"public/tags/Algorithm/index.html","hash":"225a37f9e79e6be97cb488c81c48649f0ab32387","modified":1655209069473},{"_id":"public/tags/Data-Structure/index.html","hash":"f39b667bf2df44e4306878c0b3737226919e089c","modified":1655209069473},{"_id":"public/tags/regex/index.html","hash":"3c71f69da244b4655c29364e1e6a0533e8fb4a6c","modified":1655209069473},{"_id":"public/tags/R/index.html","hash":"4ac737c44a3dd2b7bb20084ed4fe800d7a68bff1","modified":1655209069473},{"_id":"public/tags/Probability/index.html","hash":"a25b96d6f0ea074b8d27243157c451e5f783b41c","modified":1655209069473},{"_id":"public/tags/Unsorted/index.html","hash":"ee86076d2aed34db01d3f30d0782cc0cef8e6205","modified":1655209069473},{"_id":"public/tags/Git/index.html","hash":"58d78128ffedfce4141041ff3cf95fc5372c2039","modified":1655209069473},{"_id":"public/assets/beian.png","hash":"a99df13e8eb11db86edebf6e5ac246eb59f4b3c4","modified":1655209069473},{"_id":"public/assets/algolia_logo.svg","hash":"16505f61f19ba65f629dfd033f14ee9abcf18756","modified":1655209069473},{"_id":"public/assets/blog-solid.svg","hash":"6b3d36900d5064217234bc241df5ef4208cf185c","modified":1655209069473},{"_id":"public/assets/loading.svg","hash":"85082b002bae1335114b71550350907884187e38","modified":1655209069473},{"_id":"public/assets/favicon.ico","hash":"8b200c575d273d41a179c102442e191414e74eae","modified":1655209069473},{"_id":"public/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1655209069473},{"_id":"public/css/dark.css.map","hash":"f5b60d05584b47dea744edff92c44bc389675e05","modified":1655209069473},{"_id":"public/css/mobile.css.map","hash":"1f26e4000a416870ccf4e85083f7209dc55f4482","modified":1655209069473},{"_id":"public/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1655209069473},{"_id":"public/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1655209069473},{"_id":"public/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1655209069473},{"_id":"public/scripts/dark.js.map","hash":"512d8ca5ed89950d8fad95ba1200f0b21ba82388","modified":1655209069473},{"_id":"public/scripts/main.js.LICENSE.txt","hash":"8a6ae61d6eedc0fa08c18b34a7fe7bdd61059720","modified":1655209069473},{"_id":"public/scripts/search.js.LICENSE.txt","hash":"c2e89128b9d5f31e3702384033f4b118114c10d5","modified":1655209069473},{"_id":"public/2021/09/28/DL-backpropagation/bp_model.png","hash":"76492bc58d6ef89dc21c452628b8a5df11ce944c","modified":1655209069473},{"_id":"public/2021/09/28/DL-backpropagation/gb_ex.png","hash":"1caa6b5fb63ed1e6aeee3485b55f57bfcfb36bb4","modified":1655209069473},{"_id":"public/2021/06/08/ML-SP-simple_regression/1_simple-regression_6_0.png","hash":"a779a3bf17f9ae408d4bb6fbbc20ff7558090340","modified":1655209069473},{"_id":"public/2021/06/08/ML-SP-simple_regression/1_simple-regression_8_0.png","hash":"61faa846f2d4c0ddd3adc33b391022ca299c056f","modified":1655209069473},{"_id":"public/2021/05/29/ML-US-PCA/FS_FE.png","hash":"53859997510237799d6b789355f8aff99014a0e7","modified":1655209069473},{"_id":"public/2021/05/29/ML-US-PCA/covariance_matrix.png","hash":"4dca4a7391e9b39e019d2ec30c58e9b39d51600d","modified":1655209069473},{"_id":"public/2021/07/17/tools-git-basic/git-branch.png","hash":"6f18fac6409cc917e55a66f91733327a10982588","modified":1655209069473},{"_id":"public/2021/07/17/tools-git-basic/git-simple.png","hash":"9174ca5e88c8642b874d02ada8f8b92c90103ecf","modified":1655209069473},{"_id":"public/2021/07/17/tools-git-basic/git-remote.png","hash":"c7e327ba54a68f2004095f72ef1a4fae5b4b6f27","modified":1655209069473},{"_id":"public/assets/algolia/algoliasearchLite.min.js","hash":"284416885e4e80e27fa4eae6fc305f4de15b914c","modified":1655209069473},{"_id":"public/avatar/medium.jpg","hash":"a9a45491c713d8121c8df40282433671fe041e66","modified":1655209069473},{"_id":"public/css/style.css.map","hash":"c9495e0d815109b6cb17ddaa1a99e10714e0a6e8","modified":1655209069473},{"_id":"public/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1655209069473},{"_id":"public/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1655209069473},{"_id":"public/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1655209069473},{"_id":"public/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1655209069473},{"_id":"public/scripts/customFontLoader.js.map","hash":"0fb40aed5a9c1d3f97dbad07ae3279229eaebea9","modified":1655209069473},{"_id":"public/2021/05/29/ML-US-PCA/PCA_lc.png","hash":"d78c44c057fb6bc7c368e7cba1d9fca7cf206be5","modified":1655209069473},{"_id":"public/2021/07/08/tools-conda-install/conda_venv.png","hash":"31feccec0cb9be5f30c0f75ade776b473fbbbb7e","modified":1655209069473},{"_id":"public/2021/07/17/tools-git-basic/github_remote.png","hash":"65231ba5ecf084a439a9d014eb77e8b2af6e550d","modified":1655209069473},{"_id":"public/assets/algolia/algoliasearchLite.js","hash":"e56ad6b82caf69066de545201014291fc961635e","modified":1655209069473},{"_id":"public/assets/algolia/algoliasearch.min.js","hash":"a3b131a9a47ccc16f4dd8988fabb6d306548db2f","modified":1655209069473},{"_id":"public/css/mobile.css","hash":"7b73350f80ee39ec30b4c61895d274b4889329f4","modified":1655209069473},{"_id":"public/css/dark.css","hash":"32b2072ca8d39803cbd8f7395dcf02f6390dbee3","modified":1655209069473},{"_id":"public/lib/webfontloader.min.js","hash":"4c69aeb4e4f355912503d1c460e8e7aa6ea6963e","modified":1655209069473},{"_id":"public/scripts/dark.js","hash":"b0b3b400432f1913c57c817e3d8ad40dbfeebabc","modified":1655209069473},{"_id":"public/css/style.css","hash":"2a3733ec29221a6f9a74f08b821ca079d18dbea6","modified":1655209069473},{"_id":"public/scripts/customFontLoader.js","hash":"e95e00bdb80b5b2abef477eabe59005cc8d196c5","modified":1655209069473},{"_id":"public/lib/jquery.min.js","hash":"ad886e472b3557f3dc7dfa2bc43468ab8d1cef5b","modified":1655209069473},{"_id":"public/scripts/share.js","hash":"f47267d78d896ae1ec2b4daba9f7ed098712a95b","modified":1655209069473},{"_id":"public/scripts/search.js","hash":"6f2e0cefb575617be37bfb2c2954f48e568e8f00","modified":1655209069473},{"_id":"public/scripts/share.js.map","hash":"b6a3e931cf44fdac5fd717f64019f2aa3b0d62ef","modified":1655209069473},{"_id":"public/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1655209069473},{"_id":"public/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1655209069473},{"_id":"public/assets/algolia/algoliasearch.js","hash":"6948fcdf071e4983e784e8c458cf201536f77792","modified":1655209069473},{"_id":"public/scripts/main.js","hash":"95f0284e68221e5f2f50e8a59de6f7fdda66608b","modified":1655209069473},{"_id":"public/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1655209069473},{"_id":"public/2021/09/28/DL-backpropagation/backward_propagation.png","hash":"b0b52e411aa192fe2efb5bae3e4140ce562bb004","modified":1655209069473},{"_id":"public/2021/09/28/DL-backpropagation/chainrule.png","hash":"1a6488625fa49ba7fb926c31473b472c5a9cf8c7","modified":1655209069473},{"_id":"public/scripts/search.js.map","hash":"1ecdd768a261ebede8b51ca383c9bf519a4f6c7c","modified":1655209069473},{"_id":"public/scripts/main.js.map","hash":"b2f3bab368e872ecc910e48fff47797432ab019f","modified":1655209069473}],"Category":[{"name":"Data Engineering","_id":"cl4e4qsby0006b36qh7rsh89z"},{"name":"Linux","parent":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc3000lb36q3fx1a729"},{"name":"Neural Network","_id":"cl4e4qsc5000sb36qa9cqdy9f"},{"name":"Machine Learning","_id":"cl4e4qscd001tb36q1v7h2bpn"},{"name":"NLP","_id":"cl4e4qscp003pb36qarth6e08"},{"name":"Preprocessing","_id":"cl4e4qscr0042b36q8j7e1is0"},{"name":"Supervised Learning","parent":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscu004xb36qagsr6j3p"},{"name":"Programming","_id":"cl4e4qscv0056b36q1dszdal0"},{"name":"PCA","_id":"cl4e4qscx0060b36q9q1i10dp"},{"name":"Statistics","_id":"cl4e4qscx0065b36q61bsgran"},{"name":"Troubleshooting","_id":"cl4e4qscy006ib36q1tj64v4y"},{"name":"Unsorted","_id":"cl4e4qscz006ub36qbvz08mci"},{"name":"Tools","_id":"cl4e4qsd00070b36qggzv5a1c"}],"Data":[],"Page":[{"title":"About","date":"2022-06-13T15:41:19.770Z","layout":"about","_content":"\n안녕하세요 이것저것 하고 있습니다.\n\n<!--\nnbviewer 형태로 넣기\n-->","source":"about/index.md","raw":"---\ntitle: About\ndate: \nlayout: about\n---\n\n안녕하세요 이것저것 하고 있습니다.\n\n<!--\nnbviewer 형태로 넣기\n-->","updated":"2022-06-13T15:41:19.770Z","path":"about/index.html","comments":1,"_id":"cl4e4qsbq0000b36qchid2ige","content":"<p>안녕하세요 이것저것 하고 있습니다.</p>\n<!--\nnbviewer 형태로 넣기\n-->","site":{"data":{}},"excerpt":"","more":"<p>안녕하세요 이것저것 하고 있습니다.</p>\n<!--\nnbviewer 형태로 넣기\n-->"},{"title":"Projects","date":"2021-12-01T04:21:48.000Z","layout":"about","_content":"\n<!--\nnbviewer 형태로 넣기\n-->\n\n## Personal Project\n\n### 1. 머신러닝을 활용한 사회초년생의 이직의도 예측\n\n- 분석과제 :  사회초년생의 이직준비 여부를 예측하는 머신러닝 모형 설계\n- 분석 데이터 : 2019년 대졸자직업이동경로 조사(GOMS)\n- 프로젝트 링크 : [https://github.com/yjinheon/project-turnover-behavior](https://github.com/yjinheon/project-turnover-behavior)\n\n### 2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축\n\n- 분석과제 :  멜론 플레이리스하는 딥러닝 모형 구축\n- 분석 데이터 : 카카오 멜론 플레이리스트 데이터셋 ([https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/](https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/))\n- 프로젝트 링크 : []()\n\n## Work\n\n### 1. 재정지원 일자리사업 참여자 만족도조사\n\n- 담당기관 : 한국고용정보원\n- 주요 업무 : 재정지원 일자리사업 참여자들의 리얼보이스 및 재정지원 사업 관련 SNS데이터에 대한 텍스트마이닝 및 감성분석, 네트워크 분석\n- 업무 기간 : 2020.05 ~ 2021.01 (약 9개월)\n- 상세 내용 : 170여개 재정지원 일자리 사업 참여자들의 주관식 응답을 만족도 조사의 보조지표로 활용\n- 주요 Skill\n    : **R** , **PPT** , **Excel**     \n\n### 2. 소상공인 경기동향조사(BSI)\n\n- 담당기관 : 소상공인시장진흥공단\n- 주요 업무 : R을 활용한 뉴스 데이터 수집 및 시각화 및 BSI 인덱스 시계열 분석 및 계절조정\n- 업무 기간 : 2020.05 ~ 2020.12(약 8개월)\n- 상세 내용 : 소상공인 경기동향을 반영하는 지표인 BSI의 계절적인 변동성을 제거\n- 주요 Skill\n    : **R** , **PPT** , **Excel** \n    \n\n### 3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사\n\n- 담당기관 : 소상공인시장진흥공단\n- 주요 업무 : R을 활용한 공공상권데이터 분석 및 상권 표본선정\n- 업무 기간 : 2020.05 ~ 2020.12(약 8개월)\n- 상세 내용 : 소상공인 사이트에서 제공하는 상권정보시스템의 정확도를 검증하기 위해 상권내 업소 수 증가율을 바탕으로 표본 선정 및 조사\n- 주요 Skill\n    : **R** , **PPT** , **Excel**     \n    \n\n### 4. 2020년 강원지역 훈련수급 조사\n\n- 담당기관 : 강원도인적자원개발위원회\n- 주요 업무 :\n    - 프로젝트 전반 관리 및 보고서 작성\n    - R을 활용한 직업훈련 공급데이터 분석 및 설문데이터 분석\n- 업무 기간 : 2020.05 ~ 2020.11 (약 7개월)\n- 상세 내용 : 강원지역의 일자리 수요,공급 조사를 통해 직무교육공급관련 인사이트 제공\n- 주요 Skill \n    : **R** , **PPT** , **Excel** , **Python**     \n\n\n## Publications\n\n- 임혜빈, **윤진헌**, 최준영 and 이병관. (2021). 한국판 선망 척도 개발 및 타당화 연구. 한국심리학회지: 소비자·광고, 22(2), 271-291.\n- **윤진헌**, 임혜빈, 이병관. (2020). 소비자의 의사결정 방식이 랜덤박스 구매의도에 미치는 영향: 긍정적 불확실성과 위험성 평가를 중심으로. 한국심리학회지: 소비자·광고, 21(1), 109-135.\n- 임혜빈, **윤진헌**, 이병관 (2019). 심적 회계에서 지출 항목의 정당성의 역할. 한국심리학회지: 소비자광고, 20(1), 153-172\n- 임혜빈, 노환호, 도은영, **윤진헌**, 이병관. (2018). 대학생의 시간계획성향 척도 개발 및 타당화 연구. 한국청소년연구, 29(4), 59-87.","source":"Projects/index.md","raw":"---\ntitle: Projects\ndate: 2021-12-01 13:21:48\nlayout: about\n---\n\n<!--\nnbviewer 형태로 넣기\n-->\n\n## Personal Project\n\n### 1. 머신러닝을 활용한 사회초년생의 이직의도 예측\n\n- 분석과제 :  사회초년생의 이직준비 여부를 예측하는 머신러닝 모형 설계\n- 분석 데이터 : 2019년 대졸자직업이동경로 조사(GOMS)\n- 프로젝트 링크 : [https://github.com/yjinheon/project-turnover-behavior](https://github.com/yjinheon/project-turnover-behavior)\n\n### 2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축\n\n- 분석과제 :  멜론 플레이리스하는 딥러닝 모형 구축\n- 분석 데이터 : 카카오 멜론 플레이리스트 데이터셋 ([https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/](https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/))\n- 프로젝트 링크 : []()\n\n## Work\n\n### 1. 재정지원 일자리사업 참여자 만족도조사\n\n- 담당기관 : 한국고용정보원\n- 주요 업무 : 재정지원 일자리사업 참여자들의 리얼보이스 및 재정지원 사업 관련 SNS데이터에 대한 텍스트마이닝 및 감성분석, 네트워크 분석\n- 업무 기간 : 2020.05 ~ 2021.01 (약 9개월)\n- 상세 내용 : 170여개 재정지원 일자리 사업 참여자들의 주관식 응답을 만족도 조사의 보조지표로 활용\n- 주요 Skill\n    : **R** , **PPT** , **Excel**     \n\n### 2. 소상공인 경기동향조사(BSI)\n\n- 담당기관 : 소상공인시장진흥공단\n- 주요 업무 : R을 활용한 뉴스 데이터 수집 및 시각화 및 BSI 인덱스 시계열 분석 및 계절조정\n- 업무 기간 : 2020.05 ~ 2020.12(약 8개월)\n- 상세 내용 : 소상공인 경기동향을 반영하는 지표인 BSI의 계절적인 변동성을 제거\n- 주요 Skill\n    : **R** , **PPT** , **Excel** \n    \n\n### 3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사\n\n- 담당기관 : 소상공인시장진흥공단\n- 주요 업무 : R을 활용한 공공상권데이터 분석 및 상권 표본선정\n- 업무 기간 : 2020.05 ~ 2020.12(약 8개월)\n- 상세 내용 : 소상공인 사이트에서 제공하는 상권정보시스템의 정확도를 검증하기 위해 상권내 업소 수 증가율을 바탕으로 표본 선정 및 조사\n- 주요 Skill\n    : **R** , **PPT** , **Excel**     \n    \n\n### 4. 2020년 강원지역 훈련수급 조사\n\n- 담당기관 : 강원도인적자원개발위원회\n- 주요 업무 :\n    - 프로젝트 전반 관리 및 보고서 작성\n    - R을 활용한 직업훈련 공급데이터 분석 및 설문데이터 분석\n- 업무 기간 : 2020.05 ~ 2020.11 (약 7개월)\n- 상세 내용 : 강원지역의 일자리 수요,공급 조사를 통해 직무교육공급관련 인사이트 제공\n- 주요 Skill \n    : **R** , **PPT** , **Excel** , **Python**     \n\n\n## Publications\n\n- 임혜빈, **윤진헌**, 최준영 and 이병관. (2021). 한국판 선망 척도 개발 및 타당화 연구. 한국심리학회지: 소비자·광고, 22(2), 271-291.\n- **윤진헌**, 임혜빈, 이병관. (2020). 소비자의 의사결정 방식이 랜덤박스 구매의도에 미치는 영향: 긍정적 불확실성과 위험성 평가를 중심으로. 한국심리학회지: 소비자·광고, 21(1), 109-135.\n- 임혜빈, **윤진헌**, 이병관 (2019). 심적 회계에서 지출 항목의 정당성의 역할. 한국심리학회지: 소비자광고, 20(1), 153-172\n- 임혜빈, 노환호, 도은영, **윤진헌**, 이병관. (2018). 대학생의 시간계획성향 척도 개발 및 타당화 연구. 한국청소년연구, 29(4), 59-87.","updated":"2022-04-14T22:10:22.000Z","path":"Projects/index.html","comments":1,"_id":"cl4e4qsbu0002b36q242k5fdd","content":"<!--\nnbviewer 형태로 넣기\n-->\n\n<h2 id=\"Personal-Project\"><a href=\"#Personal-Project\" class=\"headerlink\" title=\"Personal Project\"></a>Personal Project</h2><h3 id=\"1-머신러닝을-활용한-사회초년생의-이직의도-예측\"><a href=\"#1-머신러닝을-활용한-사회초년생의-이직의도-예측\" class=\"headerlink\" title=\"1. 머신러닝을 활용한 사회초년생의 이직의도 예측\"></a>1. 머신러닝을 활용한 사회초년생의 이직의도 예측</h3><ul>\n<li>분석과제 :  사회초년생의 이직준비 여부를 예측하는 머신러닝 모형 설계</li>\n<li>분석 데이터 : 2019년 대졸자직업이동경로 조사(GOMS)</li>\n<li>프로젝트 링크 : <a href=\"https://github.com/yjinheon/project-turnover-behavior\">https://github.com/yjinheon/project-turnover-behavior</a></li>\n</ul>\n<h3 id=\"2-Deep-Autoencoder을-활용한-플레이리스트-기반-추천시스템-구축\"><a href=\"#2-Deep-Autoencoder을-활용한-플레이리스트-기반-추천시스템-구축\" class=\"headerlink\" title=\"2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축\"></a>2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축</h3><ul>\n<li>분석과제 :  멜론 플레이리스하는 딥러닝 모형 구축</li>\n<li>분석 데이터 : 카카오 멜론 플레이리스트 데이터셋 (<a href=\"https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/\">https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/</a>)</li>\n<li>프로젝트 링크 : <a href=\"\"></a></li>\n</ul>\n<h2 id=\"Work\"><a href=\"#Work\" class=\"headerlink\" title=\"Work\"></a>Work</h2><h3 id=\"1-재정지원-일자리사업-참여자-만족도조사\"><a href=\"#1-재정지원-일자리사업-참여자-만족도조사\" class=\"headerlink\" title=\"1. 재정지원 일자리사업 참여자 만족도조사\"></a>1. 재정지원 일자리사업 참여자 만족도조사</h3><ul>\n<li>담당기관 : 한국고용정보원</li>\n<li>주요 업무 : 재정지원 일자리사업 참여자들의 리얼보이스 및 재정지원 사업 관련 SNS데이터에 대한 텍스트마이닝 및 감성분석, 네트워크 분석</li>\n<li>업무 기간 : 2020.05 ~ 2021.01 (약 9개월)</li>\n<li>상세 내용 : 170여개 재정지원 일자리 사업 참여자들의 주관식 응답을 만족도 조사의 보조지표로 활용</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"2-소상공인-경기동향조사-BSI\"><a href=\"#2-소상공인-경기동향조사-BSI\" class=\"headerlink\" title=\"2. 소상공인 경기동향조사(BSI)\"></a>2. 소상공인 경기동향조사(BSI)</h3><ul>\n<li>담당기관 : 소상공인시장진흥공단</li>\n<li>주요 업무 : R을 활용한 뉴스 데이터 수집 및 시각화 및 BSI 인덱스 시계열 분석 및 계절조정</li>\n<li>업무 기간 : 2020.05 ~ 2020.12(약 8개월)</li>\n<li>상세 내용 : 소상공인 경기동향을 반영하는 지표인 BSI의 계절적인 변동성을 제거</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"3-2020년-상가업소DB-정확도-수준-및-이용현황-실태조사\"><a href=\"#3-2020년-상가업소DB-정확도-수준-및-이용현황-실태조사\" class=\"headerlink\" title=\"3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사\"></a>3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사</h3><ul>\n<li>담당기관 : 소상공인시장진흥공단</li>\n<li>주요 업무 : R을 활용한 공공상권데이터 분석 및 상권 표본선정</li>\n<li>업무 기간 : 2020.05 ~ 2020.12(약 8개월)</li>\n<li>상세 내용 : 소상공인 사이트에서 제공하는 상권정보시스템의 정확도를 검증하기 위해 상권내 업소 수 증가율을 바탕으로 표본 선정 및 조사</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"4-2020년-강원지역-훈련수급-조사\"><a href=\"#4-2020년-강원지역-훈련수급-조사\" class=\"headerlink\" title=\"4. 2020년 강원지역 훈련수급 조사\"></a>4. 2020년 강원지역 훈련수급 조사</h3><ul>\n<li>담당기관 : 강원도인적자원개발위원회</li>\n<li>주요 업무 :<ul>\n<li>프로젝트 전반 관리 및 보고서 작성</li>\n<li>R을 활용한 직업훈련 공급데이터 분석 및 설문데이터 분석</li>\n</ul>\n</li>\n<li>업무 기간 : 2020.05 ~ 2020.11 (약 7개월)</li>\n<li>상세 내용 : 강원지역의 일자리 수요,공급 조사를 통해 직무교육공급관련 인사이트 제공</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong> , <strong>Python</strong></li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><ul>\n<li>임혜빈, <strong>윤진헌</strong>, 최준영 and 이병관. (2021). 한국판 선망 척도 개발 및 타당화 연구. 한국심리학회지: 소비자·광고, 22(2), 271-291.</li>\n<li><strong>윤진헌</strong>, 임혜빈, 이병관. (2020). 소비자의 의사결정 방식이 랜덤박스 구매의도에 미치는 영향: 긍정적 불확실성과 위험성 평가를 중심으로. 한국심리학회지: 소비자·광고, 21(1), 109-135.</li>\n<li>임혜빈, <strong>윤진헌</strong>, 이병관 (2019). 심적 회계에서 지출 항목의 정당성의 역할. 한국심리학회지: 소비자광고, 20(1), 153-172</li>\n<li>임혜빈, 노환호, 도은영, <strong>윤진헌</strong>, 이병관. (2018). 대학생의 시간계획성향 척도 개발 및 타당화 연구. 한국청소년연구, 29(4), 59-87.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\nnbviewer 형태로 넣기\n-->\n\n<h2 id=\"Personal-Project\"><a href=\"#Personal-Project\" class=\"headerlink\" title=\"Personal Project\"></a>Personal Project</h2><h3 id=\"1-머신러닝을-활용한-사회초년생의-이직의도-예측\"><a href=\"#1-머신러닝을-활용한-사회초년생의-이직의도-예측\" class=\"headerlink\" title=\"1. 머신러닝을 활용한 사회초년생의 이직의도 예측\"></a>1. 머신러닝을 활용한 사회초년생의 이직의도 예측</h3><ul>\n<li>분석과제 :  사회초년생의 이직준비 여부를 예측하는 머신러닝 모형 설계</li>\n<li>분석 데이터 : 2019년 대졸자직업이동경로 조사(GOMS)</li>\n<li>프로젝트 링크 : <a href=\"https://github.com/yjinheon/project-turnover-behavior\">https://github.com/yjinheon/project-turnover-behavior</a></li>\n</ul>\n<h3 id=\"2-Deep-Autoencoder을-활용한-플레이리스트-기반-추천시스템-구축\"><a href=\"#2-Deep-Autoencoder을-활용한-플레이리스트-기반-추천시스템-구축\" class=\"headerlink\" title=\"2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축\"></a>2. Deep Autoencoder을 활용한 플레이리스트 기반 추천시스템 구축</h3><ul>\n<li>분석과제 :  멜론 플레이리스하는 딥러닝 모형 구축</li>\n<li>분석 데이터 : 카카오 멜론 플레이리스트 데이터셋 (<a href=\"https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/\">https://tech.kakao.com/2020/04/29/kakaoarena-3rd-part2/</a>)</li>\n<li>프로젝트 링크 : <a href=\"\"></a></li>\n</ul>\n<h2 id=\"Work\"><a href=\"#Work\" class=\"headerlink\" title=\"Work\"></a>Work</h2><h3 id=\"1-재정지원-일자리사업-참여자-만족도조사\"><a href=\"#1-재정지원-일자리사업-참여자-만족도조사\" class=\"headerlink\" title=\"1. 재정지원 일자리사업 참여자 만족도조사\"></a>1. 재정지원 일자리사업 참여자 만족도조사</h3><ul>\n<li>담당기관 : 한국고용정보원</li>\n<li>주요 업무 : 재정지원 일자리사업 참여자들의 리얼보이스 및 재정지원 사업 관련 SNS데이터에 대한 텍스트마이닝 및 감성분석, 네트워크 분석</li>\n<li>업무 기간 : 2020.05 ~ 2021.01 (약 9개월)</li>\n<li>상세 내용 : 170여개 재정지원 일자리 사업 참여자들의 주관식 응답을 만족도 조사의 보조지표로 활용</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"2-소상공인-경기동향조사-BSI\"><a href=\"#2-소상공인-경기동향조사-BSI\" class=\"headerlink\" title=\"2. 소상공인 경기동향조사(BSI)\"></a>2. 소상공인 경기동향조사(BSI)</h3><ul>\n<li>담당기관 : 소상공인시장진흥공단</li>\n<li>주요 업무 : R을 활용한 뉴스 데이터 수집 및 시각화 및 BSI 인덱스 시계열 분석 및 계절조정</li>\n<li>업무 기간 : 2020.05 ~ 2020.12(약 8개월)</li>\n<li>상세 내용 : 소상공인 경기동향을 반영하는 지표인 BSI의 계절적인 변동성을 제거</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"3-2020년-상가업소DB-정확도-수준-및-이용현황-실태조사\"><a href=\"#3-2020년-상가업소DB-정확도-수준-및-이용현황-실태조사\" class=\"headerlink\" title=\"3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사\"></a>3. 2020년 상가업소DB 정확도 수준 및 이용현황 실태조사</h3><ul>\n<li>담당기관 : 소상공인시장진흥공단</li>\n<li>주요 업무 : R을 활용한 공공상권데이터 분석 및 상권 표본선정</li>\n<li>업무 기간 : 2020.05 ~ 2020.12(약 8개월)</li>\n<li>상세 내용 : 소상공인 사이트에서 제공하는 상권정보시스템의 정확도를 검증하기 위해 상권내 업소 수 증가율을 바탕으로 표본 선정 및 조사</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong></li>\n</ul>\n<h3 id=\"4-2020년-강원지역-훈련수급-조사\"><a href=\"#4-2020년-강원지역-훈련수급-조사\" class=\"headerlink\" title=\"4. 2020년 강원지역 훈련수급 조사\"></a>4. 2020년 강원지역 훈련수급 조사</h3><ul>\n<li>담당기관 : 강원도인적자원개발위원회</li>\n<li>주요 업무 :<ul>\n<li>프로젝트 전반 관리 및 보고서 작성</li>\n<li>R을 활용한 직업훈련 공급데이터 분석 및 설문데이터 분석</li>\n</ul>\n</li>\n<li>업무 기간 : 2020.05 ~ 2020.11 (약 7개월)</li>\n<li>상세 내용 : 강원지역의 일자리 수요,공급 조사를 통해 직무교육공급관련 인사이트 제공</li>\n<li>주요 Skill<br>  : <strong>R</strong> , <strong>PPT</strong> , <strong>Excel</strong> , <strong>Python</strong></li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><ul>\n<li>임혜빈, <strong>윤진헌</strong>, 최준영 and 이병관. (2021). 한국판 선망 척도 개발 및 타당화 연구. 한국심리학회지: 소비자·광고, 22(2), 271-291.</li>\n<li><strong>윤진헌</strong>, 임혜빈, 이병관. (2020). 소비자의 의사결정 방식이 랜덤박스 구매의도에 미치는 영향: 긍정적 불확실성과 위험성 평가를 중심으로. 한국심리학회지: 소비자·광고, 21(1), 109-135.</li>\n<li>임혜빈, <strong>윤진헌</strong>, 이병관 (2019). 심적 회계에서 지출 항목의 정당성의 역할. 한국심리학회지: 소비자광고, 20(1), 153-172</li>\n<li>임혜빈, 노환호, 도은영, <strong>윤진헌</strong>, 이병관. (2018). 대학생의 시간계획성향 척도 개발 및 타당화 연구. 한국청소년연구, 29(4), 59-87.</li>\n</ul>\n"}],"Post":[{"title":"[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## 데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음\n\n### 도움말\n\n- man\n\n### 파일관리\n\n- pwd\n- cd\n- ls\n- mkdir\n- rmdir\n- cp\n- mv\n- rm\n- ln\n- chmod\n\n\n### 파일처리\n\n- cat\n- echo\n- head\n- tail\n- more/less\n- grep\n- sed*\n- awk*\n- find*\n- which\n- sort\n- uniq\n- cut\n- tr\n- zip\n- unzip\n- gunzip\n- tar\n\n### 프로세스 관리\n\n- top\n- ps\n- kill\n- fg\n- bg\n\n\n### 네트워크\n\n- ssh\n- scp\n- ping\n- traceroute\n- curl\n- finger\n- who\n\n\n### 편집기\n\n- vi\n- vim\n- nvim \n- nano\n\n### 파이프와 리디렉션\n\n### 셀 환경변수\n\n- export\n- $path\n- $ps1\n\n### Unsorted\n\n- cal\n- history\n\n**References & annotation**\n---\n- 따라하며 배우는 데이터과학 (책)","source":"_posts/DE-Linux-commandline.md","raw":"---\ntitle: '[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음'\ncategories:\n  - - Data Engineering\n    - Linux\ndate:\nupdated:\ntags:\n  - commandline\n  - Linux\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## 데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음\n\n### 도움말\n\n- man\n\n### 파일관리\n\n- pwd\n- cd\n- ls\n- mkdir\n- rmdir\n- cp\n- mv\n- rm\n- ln\n- chmod\n\n\n### 파일처리\n\n- cat\n- echo\n- head\n- tail\n- more/less\n- grep\n- sed*\n- awk*\n- find*\n- which\n- sort\n- uniq\n- cut\n- tr\n- zip\n- unzip\n- gunzip\n- tar\n\n### 프로세스 관리\n\n- top\n- ps\n- kill\n- fg\n- bg\n\n\n### 네트워크\n\n- ssh\n- scp\n- ping\n- traceroute\n- curl\n- finger\n- who\n\n\n### 편집기\n\n- vi\n- vim\n- nvim \n- nano\n\n### 파이프와 리디렉션\n\n### 셀 환경변수\n\n- export\n- $path\n- $ps1\n\n### Unsorted\n\n- cal\n- history\n\n**References & annotation**\n---\n- 따라하며 배우는 데이터과학 (책)","slug":"DE-Linux-commandline","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsbs0001b36q4lco4f4q","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음\"><a href=\"#데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음\" class=\"headerlink\" title=\"데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음\"></a>데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음</h2><h3 id=\"도움말\"><a href=\"#도움말\" class=\"headerlink\" title=\"도움말\"></a>도움말</h3><ul>\n<li>man</li>\n</ul>\n<h3 id=\"파일관리\"><a href=\"#파일관리\" class=\"headerlink\" title=\"파일관리\"></a>파일관리</h3><ul>\n<li>pwd</li>\n<li>cd</li>\n<li>ls</li>\n<li>mkdir</li>\n<li>rmdir</li>\n<li>cp</li>\n<li>mv</li>\n<li>rm</li>\n<li>ln</li>\n<li>chmod</li>\n</ul>\n<h3 id=\"파일처리\"><a href=\"#파일처리\" class=\"headerlink\" title=\"파일처리\"></a>파일처리</h3><ul>\n<li>cat</li>\n<li>echo</li>\n<li>head</li>\n<li>tail</li>\n<li>more&#x2F;less</li>\n<li>grep</li>\n<li>sed*</li>\n<li>awk*</li>\n<li>find*</li>\n<li>which</li>\n<li>sort</li>\n<li>uniq</li>\n<li>cut</li>\n<li>tr</li>\n<li>zip</li>\n<li>unzip</li>\n<li>gunzip</li>\n<li>tar</li>\n</ul>\n<h3 id=\"프로세스-관리\"><a href=\"#프로세스-관리\" class=\"headerlink\" title=\"프로세스 관리\"></a>프로세스 관리</h3><ul>\n<li>top</li>\n<li>ps</li>\n<li>kill</li>\n<li>fg</li>\n<li>bg</li>\n</ul>\n<h3 id=\"네트워크\"><a href=\"#네트워크\" class=\"headerlink\" title=\"네트워크\"></a>네트워크</h3><ul>\n<li>ssh</li>\n<li>scp</li>\n<li>ping</li>\n<li>traceroute</li>\n<li>curl</li>\n<li>finger</li>\n<li>who</li>\n</ul>\n<h3 id=\"편집기\"><a href=\"#편집기\" class=\"headerlink\" title=\"편집기\"></a>편집기</h3><ul>\n<li>vi</li>\n<li>vim</li>\n<li>nvim </li>\n<li>nano</li>\n</ul>\n<h3 id=\"파이프와-리디렉션\"><a href=\"#파이프와-리디렉션\" class=\"headerlink\" title=\"파이프와 리디렉션\"></a>파이프와 리디렉션</h3><h3 id=\"셀-환경변수\"><a href=\"#셀-환경변수\" class=\"headerlink\" title=\"셀 환경변수\"></a>셀 환경변수</h3><ul>\n<li>export</li>\n<li>$path</li>\n<li>$ps1</li>\n</ul>\n<h3 id=\"Unsorted\"><a href=\"#Unsorted\" class=\"headerlink\" title=\"Unsorted\"></a>Unsorted</h3><ul>\n<li>cal</li>\n<li>history</li>\n</ul>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li>따라하며 배우는 데이터과학 (책)</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음\"><a href=\"#데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음\" class=\"headerlink\" title=\"데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음\"></a>데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음</h2><h3 id=\"도움말\"><a href=\"#도움말\" class=\"headerlink\" title=\"도움말\"></a>도움말</h3><ul>\n<li>man</li>\n</ul>\n<h3 id=\"파일관리\"><a href=\"#파일관리\" class=\"headerlink\" title=\"파일관리\"></a>파일관리</h3><ul>\n<li>pwd</li>\n<li>cd</li>\n<li>ls</li>\n<li>mkdir</li>\n<li>rmdir</li>\n<li>cp</li>\n<li>mv</li>\n<li>rm</li>\n<li>ln</li>\n<li>chmod</li>\n</ul>\n<h3 id=\"파일처리\"><a href=\"#파일처리\" class=\"headerlink\" title=\"파일처리\"></a>파일처리</h3><ul>\n<li>cat</li>\n<li>echo</li>\n<li>head</li>\n<li>tail</li>\n<li>more&#x2F;less</li>\n<li>grep</li>\n<li>sed*</li>\n<li>awk*</li>\n<li>find*</li>\n<li>which</li>\n<li>sort</li>\n<li>uniq</li>\n<li>cut</li>\n<li>tr</li>\n<li>zip</li>\n<li>unzip</li>\n<li>gunzip</li>\n<li>tar</li>\n</ul>\n<h3 id=\"프로세스-관리\"><a href=\"#프로세스-관리\" class=\"headerlink\" title=\"프로세스 관리\"></a>프로세스 관리</h3><ul>\n<li>top</li>\n<li>ps</li>\n<li>kill</li>\n<li>fg</li>\n<li>bg</li>\n</ul>\n<h3 id=\"네트워크\"><a href=\"#네트워크\" class=\"headerlink\" title=\"네트워크\"></a>네트워크</h3><ul>\n<li>ssh</li>\n<li>scp</li>\n<li>ping</li>\n<li>traceroute</li>\n<li>curl</li>\n<li>finger</li>\n<li>who</li>\n</ul>\n<h3 id=\"편집기\"><a href=\"#편집기\" class=\"headerlink\" title=\"편집기\"></a>편집기</h3><ul>\n<li>vi</li>\n<li>vim</li>\n<li>nvim </li>\n<li>nano</li>\n</ul>\n<h3 id=\"파이프와-리디렉션\"><a href=\"#파이프와-리디렉션\" class=\"headerlink\" title=\"파이프와 리디렉션\"></a>파이프와 리디렉션</h3><h3 id=\"셀-환경변수\"><a href=\"#셀-환경변수\" class=\"headerlink\" title=\"셀 환경변수\"></a>셀 환경변수</h3><ul>\n<li>export</li>\n<li>$path</li>\n<li>$ps1</li>\n</ul>\n<h3 id=\"Unsorted\"><a href=\"#Unsorted\" class=\"headerlink\" title=\"Unsorted\"></a>Unsorted</h3><ul>\n<li>cal</li>\n<li>history</li>\n</ul>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li>따라하며 배우는 데이터과학 (책)</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음","path":"2022/06/13/DE-Linux-commandline/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Data Engineering > Linux","tags":["commandline","Linux"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[DDL]Table 구조 다루기","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Data Definition\n\n- `ALTER`는 DB구조를 변경하는데 쓴다.\n- `TRUNCATE`는 데이터를 삭제하는데 쓴다.\n- `DROP`은 테이블 자체를 삭제한다.\n\n### 테이블 데이터 다루기\n\n\n- 테이블 생성\n\n```sql\nCREATE TABLE t ( id INT PRIMARY KEY,\n                 name VARCHAR NOT NULL, \n                 price INT DEFAULT 0);\n\n```\n\n- 테이블 삭제\n\n```sql\nDROP TABLE t;\n\n```\n\n- 새 컬럼 추가\n\n```sql\nALTER TABLE t ADD COLUMN;\n\n```\n\n- 새 제약조건 추가\n\n```sql\nALTER TABLE t ADD CONSTRAINT;\n\n```\n\n- 제약조건 삭제\n\n```sql\nALTER TABLE t DROP CONSTRAINT;\n\n```\n\n- 테이블명 변경\n\n```sql\nALTER TABLE t RENAME to t2;\n\n```\n\n- 컬럼명 변경\n\n```sql\n\nALTER table t1 RENAME c1 to c2;\n\n```\n\n- 테이블 데이터 삭제\n\n테이블 구조는 남기고 데이터를 전부 날린다.\n\n```sql\n\nTRUNCATE TABLE t;\n\n```\n\n**References & annotation**\n---\n\n- [mysql tutorial](https://www.mysqltutorial.org/mysql-join/)\n","source":"_posts/DE-SQL-DDL-tables.md","raw":"---\ntitle: '[DDL]Table 구조 다루기'\ncategories:\n  - Data Engineering\ndate:\nupdated:\ntags: \n  - SQL\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Data Definition\n\n- `ALTER`는 DB구조를 변경하는데 쓴다.\n- `TRUNCATE`는 데이터를 삭제하는데 쓴다.\n- `DROP`은 테이블 자체를 삭제한다.\n\n### 테이블 데이터 다루기\n\n\n- 테이블 생성\n\n```sql\nCREATE TABLE t ( id INT PRIMARY KEY,\n                 name VARCHAR NOT NULL, \n                 price INT DEFAULT 0);\n\n```\n\n- 테이블 삭제\n\n```sql\nDROP TABLE t;\n\n```\n\n- 새 컬럼 추가\n\n```sql\nALTER TABLE t ADD COLUMN;\n\n```\n\n- 새 제약조건 추가\n\n```sql\nALTER TABLE t ADD CONSTRAINT;\n\n```\n\n- 제약조건 삭제\n\n```sql\nALTER TABLE t DROP CONSTRAINT;\n\n```\n\n- 테이블명 변경\n\n```sql\nALTER TABLE t RENAME to t2;\n\n```\n\n- 컬럼명 변경\n\n```sql\n\nALTER table t1 RENAME c1 to c2;\n\n```\n\n- 테이블 데이터 삭제\n\n테이블 구조는 남기고 데이터를 전부 날린다.\n\n```sql\n\nTRUNCATE TABLE t;\n\n```\n\n**References & annotation**\n---\n\n- [mysql tutorial](https://www.mysqltutorial.org/mysql-join/)\n","slug":"DE-SQL-DDL-tables","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsbv0003b36qbikja07i","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Data-Definition\"><a href=\"#Data-Definition\" class=\"headerlink\" title=\"Data Definition\"></a>Data Definition</h2><ul>\n<li><code>ALTER</code>는 DB구조를 변경하는데 쓴다.</li>\n<li><code>TRUNCATE</code>는 데이터를 삭제하는데 쓴다.</li>\n<li><code>DROP</code>은 테이블 자체를 삭제한다.</li>\n</ul>\n<h3 id=\"테이블-데이터-다루기\"><a href=\"#테이블-데이터-다루기\" class=\"headerlink\" title=\"테이블 데이터 다루기\"></a>테이블 데이터 다루기</h3><ul>\n<li>테이블 생성</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> t ( id <span class=\"type\">INT</span> <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">                 name <span class=\"type\">VARCHAR</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>, </span><br><span class=\"line\">                 price <span class=\"type\">INT</span> <span class=\"keyword\">DEFAULT</span> <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블 삭제</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DROP</span> <span class=\"keyword\">TABLE</span> t;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>새 컬럼 추가</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">ADD</span> <span class=\"keyword\">COLUMN</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>새 제약조건 추가</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">ADD</span> <span class=\"keyword\">CONSTRAINT</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>제약조건 삭제</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">DROP</span> <span class=\"keyword\">CONSTRAINT</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블명 변경</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t RENAME <span class=\"keyword\">to</span> t2;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>컬럼명 변경</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">table</span> t1 RENAME c1 <span class=\"keyword\">to</span> c2;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블 데이터 삭제</li>\n</ul>\n<p>테이블 구조는 남기고 데이터를 전부 날린다.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">TRUNCATE</span> <span class=\"keyword\">TABLE</span> t;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.mysqltutorial.org/mysql-join/\">mysql tutorial</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Data-Definition\"><a href=\"#Data-Definition\" class=\"headerlink\" title=\"Data Definition\"></a>Data Definition</h2><ul>\n<li><code>ALTER</code>는 DB구조를 변경하는데 쓴다.</li>\n<li><code>TRUNCATE</code>는 데이터를 삭제하는데 쓴다.</li>\n<li><code>DROP</code>은 테이블 자체를 삭제한다.</li>\n</ul>\n<h3 id=\"테이블-데이터-다루기\"><a href=\"#테이블-데이터-다루기\" class=\"headerlink\" title=\"테이블 데이터 다루기\"></a>테이블 데이터 다루기</h3><ul>\n<li>테이블 생성</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> t ( id <span class=\"type\">INT</span> <span class=\"keyword\">PRIMARY</span> KEY,</span><br><span class=\"line\">                 name <span class=\"type\">VARCHAR</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>, </span><br><span class=\"line\">                 price <span class=\"type\">INT</span> <span class=\"keyword\">DEFAULT</span> <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블 삭제</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DROP</span> <span class=\"keyword\">TABLE</span> t;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>새 컬럼 추가</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">ADD</span> <span class=\"keyword\">COLUMN</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>새 제약조건 추가</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">ADD</span> <span class=\"keyword\">CONSTRAINT</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>제약조건 삭제</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t <span class=\"keyword\">DROP</span> <span class=\"keyword\">CONSTRAINT</span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블명 변경</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">TABLE</span> t RENAME <span class=\"keyword\">to</span> t2;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>컬럼명 변경</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">table</span> t1 RENAME c1 <span class=\"keyword\">to</span> c2;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>테이블 데이터 삭제</li>\n</ul>\n<p>테이블 구조는 남기고 데이터를 전부 날린다.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">TRUNCATE</span> <span class=\"keyword\">TABLE</span> t;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.mysqltutorial.org/mysql-join/\">mysql tutorial</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[DDL]Table 구조 다루기","path":"2022/06/13/DE-SQL-DDL-tables/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Data Engineering","tags":["SQL"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[SQL]WHERE절의 이해","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Where 절\n\n조건생성을 위해 Where절을 사용한다.\n\n#### BETWEEN\n\n- 특정 칼럼의 값이 시작점, 끝점인 데이터만 출력\n\n```sql\nselect *\nfrom ordersdetails\nwhere priceeach between 30 and 50;\n```\n\n#### IN\n\n- or 연산자\n\n```sql\nselect customernumber\nfrom customers\nwhere country in ('USA','CANADA')\n\n```\n\n#### IS NULL\n\n- null 데이터 출력\n\n```sql\nSELECT * FROM copang_main.member where ADDRESS is NULL;\n\nSELECT * FROM copang_main.member where address is NOT NULL;\n\nSELECT * FROM copang_main.member where\naddress is NULL\nOR height IS NULL\nOR weight IS NULL; # 세 컬럼중 하나라도 null이 있는 로우만 조회\n\n\nSELECT # coalesce\n    coalesce(height,'####'), # null이 아닌 값은 그대로 반환, null일 경우 입력한 값 반환\n    coalesce(weight,'----'),\n    coalesce(address,'@@@@')\nFROM copang_main.`member`;\n\n```\n\n**NULL 변환함수**\n\n1. coalesce : 첫번째로 null이 아닌 값을 반환\n\n2. ifnull : 첫번째 인자가 null인 경우 두번째 인자, 아닐 경우 해당 값 표현\n\n3. if(a1,a2,a3) : ifelse 처럼 사용가능\n\n\n#### LIKE\n\n- 문자열 매칭하기\n\n```sql\nSELECT * FROM main.`member` WHERE address like '서울%'; # address가 서울로 시작하는 row 조회\nSELECT * FROM main.`member` WHERE address like '%고양시%'; # 고양시라는 단어 앞뒤로 임의의 길이를 가진 문자열 조건\n```\n\n\n#### 이스케이핑 문제\n\n\n- 어떤 문자가 그것에 부여된 특정한 의미,기능으로 해석되는 것이 아니라 단순한 문자 하나도 해석되게끔 하는 것을 `이스케이핑`이라 한다.\n- ' 이스케이핑 -> select * from copang_main.test where sentence like '%\\'%'\n- _ 이스케이핑 -> select * from copang_main.test where sentence like '%\\_%'\n- \" 이스케이핑 -> select * from copang_main.test where sentence like '%\\\"\\\"%'\n- 대문자 제외 소문자 찾기 select * from copang_main.test where sentence like binary '%g%'\n\n\n#### ANY\n\n- **ANY는 나올 수 있는 모든 조건에 OR 연산을 수행한것과 동일한 결과 반환**\n- 수량이 10개인 제품 전부 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ANY\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity = 10);\n```\n\n- 수량이 1000개 초과인 제품 전부 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ANY\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity > 1000);\n```\n\n#### ALL\n\n- 조건을 모두 만족하는 행을 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ALL\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity = 10);\n```\n\n#### EXISTS\n\n- EXISTS는 행의 존재 여부를 확인하여 True/False 값을 반환\n\n```sql\nSELECT SupplierName\nFROM Suppliers\nWHERE EXISTS (SELECT ProductName FROM Products WHERE Products.SupplierID = Suppliers.supplierID AND Price = 22);\n```\n\n\n## References\n\n- SQL로 맛보는 데이터 전처리분석","source":"_posts/DE-SQL-Where.md","raw":"---\ntitle: '[SQL]WHERE절의 이해'\ncategories:\n   - Data Engineering\ntags:\n  - SQL\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Where 절\n\n조건생성을 위해 Where절을 사용한다.\n\n#### BETWEEN\n\n- 특정 칼럼의 값이 시작점, 끝점인 데이터만 출력\n\n```sql\nselect *\nfrom ordersdetails\nwhere priceeach between 30 and 50;\n```\n\n#### IN\n\n- or 연산자\n\n```sql\nselect customernumber\nfrom customers\nwhere country in ('USA','CANADA')\n\n```\n\n#### IS NULL\n\n- null 데이터 출력\n\n```sql\nSELECT * FROM copang_main.member where ADDRESS is NULL;\n\nSELECT * FROM copang_main.member where address is NOT NULL;\n\nSELECT * FROM copang_main.member where\naddress is NULL\nOR height IS NULL\nOR weight IS NULL; # 세 컬럼중 하나라도 null이 있는 로우만 조회\n\n\nSELECT # coalesce\n    coalesce(height,'####'), # null이 아닌 값은 그대로 반환, null일 경우 입력한 값 반환\n    coalesce(weight,'----'),\n    coalesce(address,'@@@@')\nFROM copang_main.`member`;\n\n```\n\n**NULL 변환함수**\n\n1. coalesce : 첫번째로 null이 아닌 값을 반환\n\n2. ifnull : 첫번째 인자가 null인 경우 두번째 인자, 아닐 경우 해당 값 표현\n\n3. if(a1,a2,a3) : ifelse 처럼 사용가능\n\n\n#### LIKE\n\n- 문자열 매칭하기\n\n```sql\nSELECT * FROM main.`member` WHERE address like '서울%'; # address가 서울로 시작하는 row 조회\nSELECT * FROM main.`member` WHERE address like '%고양시%'; # 고양시라는 단어 앞뒤로 임의의 길이를 가진 문자열 조건\n```\n\n\n#### 이스케이핑 문제\n\n\n- 어떤 문자가 그것에 부여된 특정한 의미,기능으로 해석되는 것이 아니라 단순한 문자 하나도 해석되게끔 하는 것을 `이스케이핑`이라 한다.\n- ' 이스케이핑 -> select * from copang_main.test where sentence like '%\\'%'\n- _ 이스케이핑 -> select * from copang_main.test where sentence like '%\\_%'\n- \" 이스케이핑 -> select * from copang_main.test where sentence like '%\\\"\\\"%'\n- 대문자 제외 소문자 찾기 select * from copang_main.test where sentence like binary '%g%'\n\n\n#### ANY\n\n- **ANY는 나올 수 있는 모든 조건에 OR 연산을 수행한것과 동일한 결과 반환**\n- 수량이 10개인 제품 전부 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ANY\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity = 10);\n```\n\n- 수량이 1000개 초과인 제품 전부 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ANY\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity > 1000);\n```\n\n#### ALL\n\n- 조건을 모두 만족하는 행을 반환\n\n```sql\nSELECT ProductName\nFROM Products\nWHERE ProductID = ALL\n  (SELECT ProductID\n  FROM OrderDetails\n  WHERE Quantity = 10);\n```\n\n#### EXISTS\n\n- EXISTS는 행의 존재 여부를 확인하여 True/False 값을 반환\n\n```sql\nSELECT SupplierName\nFROM Suppliers\nWHERE EXISTS (SELECT ProductName FROM Products WHERE Products.SupplierID = Suppliers.supplierID AND Price = 22);\n```\n\n\n## References\n\n- SQL로 맛보는 데이터 전처리분석","slug":"DE-SQL-Where","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsbx0005b36qdwzx18db","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Where-절\"><a href=\"#Where-절\" class=\"headerlink\" title=\"Where 절\"></a>Where 절</h2><p>조건생성을 위해 Where절을 사용한다.</p>\n<h4 id=\"BETWEEN\"><a href=\"#BETWEEN\" class=\"headerlink\" title=\"BETWEEN\"></a>BETWEEN</h4><ul>\n<li>특정 칼럼의 값이 시작점, 끝점인 데이터만 출력</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"operator\">*</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> ordersdetails</span><br><span class=\"line\"><span class=\"keyword\">where</span> priceeach <span class=\"keyword\">between</span> <span class=\"number\">30</span> <span class=\"keyword\">and</span> <span class=\"number\">50</span>;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"IN\"><a href=\"#IN\" class=\"headerlink\" title=\"IN\"></a>IN</h4><ul>\n<li>or 연산자</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> customernumber</span><br><span class=\"line\"><span class=\"keyword\">from</span> customers</span><br><span class=\"line\"><span class=\"keyword\">where</span> country <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;USA&#x27;</span>,<span class=\"string\">&#x27;CANADA&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"IS-NULL\"><a href=\"#IS-NULL\" class=\"headerlink\" title=\"IS NULL\"></a>IS NULL</h4><ul>\n<li>null 데이터 출력</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span> ADDRESS <span class=\"keyword\">is</span> <span class=\"keyword\">NULL</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span> address <span class=\"keyword\">is</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span></span><br><span class=\"line\">address <span class=\"keyword\">is</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\"><span class=\"keyword\">OR</span> height <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\"><span class=\"keyword\">OR</span> weight <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span>; # 세 컬럼중 하나라도 <span class=\"keyword\">null</span>이 있는 로우만 조회</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> # coalesce</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(height,<span class=\"string\">&#x27;####&#x27;</span>), # <span class=\"keyword\">null</span>이 아닌 값은 그대로 반환, <span class=\"keyword\">null</span>일 경우 입력한 값 반환</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(weight,<span class=\"string\">&#x27;----&#x27;</span>),</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(address,<span class=\"string\">&#x27;@@@@&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> copang_main.`<span class=\"keyword\">member</span>`;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>NULL 변환함수</strong></p>\n<ol>\n<li><p>coalesce : 첫번째로 null이 아닌 값을 반환</p>\n</li>\n<li><p>ifnull : 첫번째 인자가 null인 경우 두번째 인자, 아닐 경우 해당 값 표현</p>\n</li>\n<li><p>if(a1,a2,a3) : ifelse 처럼 사용가능</p>\n</li>\n</ol>\n<h4 id=\"LIKE\"><a href=\"#LIKE\" class=\"headerlink\" title=\"LIKE\"></a>LIKE</h4><ul>\n<li>문자열 매칭하기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> main.`<span class=\"keyword\">member</span>` <span class=\"keyword\">WHERE</span> address <span class=\"keyword\">like</span> <span class=\"string\">&#x27;서울%&#x27;</span>; # address가 서울로 시작하는 <span class=\"type\">row</span> 조회</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> main.`<span class=\"keyword\">member</span>` <span class=\"keyword\">WHERE</span> address <span class=\"keyword\">like</span> <span class=\"string\">&#x27;%고양시%&#x27;</span>; # 고양시라는 단어 앞뒤로 임의의 길이를 가진 문자열 조건</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"이스케이핑-문제\"><a href=\"#이스케이핑-문제\" class=\"headerlink\" title=\"이스케이핑 문제\"></a>이스케이핑 문제</h4><ul>\n<li>어떤 문자가 그것에 부여된 특정한 의미,기능으로 해석되는 것이 아니라 단순한 문자 하나도 해석되게끔 하는 것을 <code>이스케이핑</code>이라 한다.</li>\n<li>‘ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%&#39;%’</li>\n<li>_ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%_%’</li>\n<li>“ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%&quot;&quot;%’</li>\n<li>대문자 제외 소문자 찾기 select * from copang_main.test where sentence like binary ‘%g%’</li>\n</ul>\n<h4 id=\"ANY\"><a href=\"#ANY\" class=\"headerlink\" title=\"ANY\"></a>ANY</h4><ul>\n<li><strong>ANY는 나올 수 있는 모든 조건에 OR 연산을 수행한것과 동일한 결과 반환</strong></li>\n<li>수량이 10개인 제품 전부 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ANY</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">=</span> <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>수량이 1000개 초과인 제품 전부 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ANY</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">&gt;</span> <span class=\"number\">1000</span>);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"ALL\"><a href=\"#ALL\" class=\"headerlink\" title=\"ALL\"></a>ALL</h4><ul>\n<li>조건을 모두 만족하는 행을 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ALL</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">=</span> <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"EXISTS\"><a href=\"#EXISTS\" class=\"headerlink\" title=\"EXISTS\"></a>EXISTS</h4><ul>\n<li>EXISTS는 행의 존재 여부를 확인하여 True&#x2F;False 값을 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> SupplierName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Suppliers</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> <span class=\"keyword\">EXISTS</span> (<span class=\"keyword\">SELECT</span> ProductName <span class=\"keyword\">FROM</span> Products <span class=\"keyword\">WHERE</span> Products.SupplierID <span class=\"operator\">=</span> Suppliers.supplierID <span class=\"keyword\">AND</span> Price <span class=\"operator\">=</span> <span class=\"number\">22</span>);</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li>SQL로 맛보는 데이터 전처리분석</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Where-절\"><a href=\"#Where-절\" class=\"headerlink\" title=\"Where 절\"></a>Where 절</h2><p>조건생성을 위해 Where절을 사용한다.</p>\n<h4 id=\"BETWEEN\"><a href=\"#BETWEEN\" class=\"headerlink\" title=\"BETWEEN\"></a>BETWEEN</h4><ul>\n<li>특정 칼럼의 값이 시작점, 끝점인 데이터만 출력</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"operator\">*</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> ordersdetails</span><br><span class=\"line\"><span class=\"keyword\">where</span> priceeach <span class=\"keyword\">between</span> <span class=\"number\">30</span> <span class=\"keyword\">and</span> <span class=\"number\">50</span>;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"IN\"><a href=\"#IN\" class=\"headerlink\" title=\"IN\"></a>IN</h4><ul>\n<li>or 연산자</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> customernumber</span><br><span class=\"line\"><span class=\"keyword\">from</span> customers</span><br><span class=\"line\"><span class=\"keyword\">where</span> country <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;USA&#x27;</span>,<span class=\"string\">&#x27;CANADA&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"IS-NULL\"><a href=\"#IS-NULL\" class=\"headerlink\" title=\"IS NULL\"></a>IS NULL</h4><ul>\n<li>null 데이터 출력</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span> ADDRESS <span class=\"keyword\">is</span> <span class=\"keyword\">NULL</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span> address <span class=\"keyword\">is</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> copang_main.member <span class=\"keyword\">where</span></span><br><span class=\"line\">address <span class=\"keyword\">is</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\"><span class=\"keyword\">OR</span> height <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span></span><br><span class=\"line\"><span class=\"keyword\">OR</span> weight <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span>; # 세 컬럼중 하나라도 <span class=\"keyword\">null</span>이 있는 로우만 조회</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> # coalesce</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(height,<span class=\"string\">&#x27;####&#x27;</span>), # <span class=\"keyword\">null</span>이 아닌 값은 그대로 반환, <span class=\"keyword\">null</span>일 경우 입력한 값 반환</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(weight,<span class=\"string\">&#x27;----&#x27;</span>),</span><br><span class=\"line\">    <span class=\"built_in\">coalesce</span>(address,<span class=\"string\">&#x27;@@@@&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> copang_main.`<span class=\"keyword\">member</span>`;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>NULL 변환함수</strong></p>\n<ol>\n<li><p>coalesce : 첫번째로 null이 아닌 값을 반환</p>\n</li>\n<li><p>ifnull : 첫번째 인자가 null인 경우 두번째 인자, 아닐 경우 해당 값 표현</p>\n</li>\n<li><p>if(a1,a2,a3) : ifelse 처럼 사용가능</p>\n</li>\n</ol>\n<h4 id=\"LIKE\"><a href=\"#LIKE\" class=\"headerlink\" title=\"LIKE\"></a>LIKE</h4><ul>\n<li>문자열 매칭하기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> main.`<span class=\"keyword\">member</span>` <span class=\"keyword\">WHERE</span> address <span class=\"keyword\">like</span> <span class=\"string\">&#x27;서울%&#x27;</span>; # address가 서울로 시작하는 <span class=\"type\">row</span> 조회</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> main.`<span class=\"keyword\">member</span>` <span class=\"keyword\">WHERE</span> address <span class=\"keyword\">like</span> <span class=\"string\">&#x27;%고양시%&#x27;</span>; # 고양시라는 단어 앞뒤로 임의의 길이를 가진 문자열 조건</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"이스케이핑-문제\"><a href=\"#이스케이핑-문제\" class=\"headerlink\" title=\"이스케이핑 문제\"></a>이스케이핑 문제</h4><ul>\n<li>어떤 문자가 그것에 부여된 특정한 의미,기능으로 해석되는 것이 아니라 단순한 문자 하나도 해석되게끔 하는 것을 <code>이스케이핑</code>이라 한다.</li>\n<li>‘ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%&#39;%’</li>\n<li>_ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%_%’</li>\n<li>“ 이스케이핑 -&gt; select * from copang_main.test where sentence like ‘%&quot;&quot;%’</li>\n<li>대문자 제외 소문자 찾기 select * from copang_main.test where sentence like binary ‘%g%’</li>\n</ul>\n<h4 id=\"ANY\"><a href=\"#ANY\" class=\"headerlink\" title=\"ANY\"></a>ANY</h4><ul>\n<li><strong>ANY는 나올 수 있는 모든 조건에 OR 연산을 수행한것과 동일한 결과 반환</strong></li>\n<li>수량이 10개인 제품 전부 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ANY</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">=</span> <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>수량이 1000개 초과인 제품 전부 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ANY</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">&gt;</span> <span class=\"number\">1000</span>);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"ALL\"><a href=\"#ALL\" class=\"headerlink\" title=\"ALL\"></a>ALL</h4><ul>\n<li>조건을 모두 만족하는 행을 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> ProductName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Products</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> ProductID <span class=\"operator\">=</span> <span class=\"keyword\">ALL</span></span><br><span class=\"line\">  (<span class=\"keyword\">SELECT</span> ProductID</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> OrderDetails</span><br><span class=\"line\">  <span class=\"keyword\">WHERE</span> Quantity <span class=\"operator\">=</span> <span class=\"number\">10</span>);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"EXISTS\"><a href=\"#EXISTS\" class=\"headerlink\" title=\"EXISTS\"></a>EXISTS</h4><ul>\n<li>EXISTS는 행의 존재 여부를 확인하여 True&#x2F;False 값을 반환</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> SupplierName</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Suppliers</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> <span class=\"keyword\">EXISTS</span> (<span class=\"keyword\">SELECT</span> ProductName <span class=\"keyword\">FROM</span> Products <span class=\"keyword\">WHERE</span> Products.SupplierID <span class=\"operator\">=</span> Suppliers.supplierID <span class=\"keyword\">AND</span> Price <span class=\"operator\">=</span> <span class=\"number\">22</span>);</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li>SQL로 맛보는 데이터 전처리분석</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[SQL]WHERE절의 이해","path":"2022/06/13/DE-SQL-Where/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Data Engineering","tags":["SQL"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[SQL]간단한 CASE WHEN 용법","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**간단한 mysql case when 용법 정리**\n\n---\n\n### Case When\n\n- 기본적으로 파생변수(컬럼)를 생성하는데 사용한다.\n- 파생변수이기 때문에 Select절 에 들어간다.\n\n\n#### Case When 용법\n\n용법은 어렵지 않다. 파생변수 Select 절에 들어가는 것만 주의\n\n```sql\nSELECT player_name,\n       weight,\n       CASE WHEN weight > 250 THEN 'over 250'\n            WHEN weight > 200 THEN '201-250'\n            WHEN weight > 175 THEN '176-200'\n            ELSE '175 or under' END AS weight_group\n  FROM benn.college_football_players\n```\n\n- order by에 사용힐 컬럼 생성\n\n```sql\nSELECT CustomerName, City, Country\nFROM Customers\nORDER BY\n(CASE\n    WHEN City IS NULL THEN Country\n    ELSE City\nEND);\n```\n\n\n```sql\nSELECT animal_id, name,\ncase \n    when sex_upon_intake like \"Intact%\" then \"O\"\n    else \"X\" \nend as \"중성화\"\nfrom animal_ins\norder by animal_id\n```\n\n\n**References & annotation**\n---\n\n- https://www.w3schools.com/sql/sql_case.asp\n","source":"_posts/DE-SQL-case-when.md","raw":"---\ntitle: '[SQL]간단한 CASE WHEN 용법'\ncategories:\n    - Data Engineering\ntags:\n  - SQL\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**간단한 mysql case when 용법 정리**\n\n---\n\n### Case When\n\n- 기본적으로 파생변수(컬럼)를 생성하는데 사용한다.\n- 파생변수이기 때문에 Select절 에 들어간다.\n\n\n#### Case When 용법\n\n용법은 어렵지 않다. 파생변수 Select 절에 들어가는 것만 주의\n\n```sql\nSELECT player_name,\n       weight,\n       CASE WHEN weight > 250 THEN 'over 250'\n            WHEN weight > 200 THEN '201-250'\n            WHEN weight > 175 THEN '176-200'\n            ELSE '175 or under' END AS weight_group\n  FROM benn.college_football_players\n```\n\n- order by에 사용힐 컬럼 생성\n\n```sql\nSELECT CustomerName, City, Country\nFROM Customers\nORDER BY\n(CASE\n    WHEN City IS NULL THEN Country\n    ELSE City\nEND);\n```\n\n\n```sql\nSELECT animal_id, name,\ncase \n    when sex_upon_intake like \"Intact%\" then \"O\"\n    else \"X\" \nend as \"중성화\"\nfrom animal_ins\norder by animal_id\n```\n\n\n**References & annotation**\n---\n\n- https://www.w3schools.com/sql/sql_case.asp\n","slug":"DE-SQL-case-when","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsby0007b36q7oe446yb","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>간단한 mysql case when 용법 정리</strong></p>\n<hr>\n<h3 id=\"Case-When\"><a href=\"#Case-When\" class=\"headerlink\" title=\"Case When\"></a>Case When</h3><ul>\n<li>기본적으로 파생변수(컬럼)를 생성하는데 사용한다.</li>\n<li>파생변수이기 때문에 Select절 에 들어간다.</li>\n</ul>\n<h4 id=\"Case-When-용법\"><a href=\"#Case-When-용법\" class=\"headerlink\" title=\"Case When 용법\"></a>Case When 용법</h4><p>용법은 어렵지 않다. 파생변수 Select 절에 들어가는 것만 주의</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> player_name,</span><br><span class=\"line\">       weight,</span><br><span class=\"line\">       <span class=\"keyword\">CASE</span> <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">250</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;over 250&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">200</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;201-250&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">175</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;176-200&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">ELSE</span> <span class=\"string\">&#x27;175 or under&#x27;</span> <span class=\"keyword\">END</span> <span class=\"keyword\">AS</span> weight_group</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> benn.college_football_players</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>order by에 사용힐 컬럼 생성</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> CustomerName, City, Country</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Customers</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span></span><br><span class=\"line\">(<span class=\"keyword\">CASE</span></span><br><span class=\"line\">    <span class=\"keyword\">WHEN</span> City <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">THEN</span> Country</span><br><span class=\"line\">    <span class=\"keyword\">ELSE</span> City</span><br><span class=\"line\"><span class=\"keyword\">END</span>);</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> animal_id, name,</span><br><span class=\"line\"><span class=\"keyword\">case</span> </span><br><span class=\"line\">    <span class=\"keyword\">when</span> sex_upon_intake <span class=\"keyword\">like</span> &quot;Intact%&quot; <span class=\"keyword\">then</span> &quot;O&quot;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> &quot;X&quot; </span><br><span class=\"line\"><span class=\"keyword\">end</span> <span class=\"keyword\">as</span> &quot;중성화&quot;</span><br><span class=\"line\"><span class=\"keyword\">from</span> animal_ins</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> animal_id</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.w3schools.com/sql/sql_case.asp\">https://www.w3schools.com/sql/sql_case.asp</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>간단한 mysql case when 용법 정리</strong></p>\n<hr>\n<h3 id=\"Case-When\"><a href=\"#Case-When\" class=\"headerlink\" title=\"Case When\"></a>Case When</h3><ul>\n<li>기본적으로 파생변수(컬럼)를 생성하는데 사용한다.</li>\n<li>파생변수이기 때문에 Select절 에 들어간다.</li>\n</ul>\n<h4 id=\"Case-When-용법\"><a href=\"#Case-When-용법\" class=\"headerlink\" title=\"Case When 용법\"></a>Case When 용법</h4><p>용법은 어렵지 않다. 파생변수 Select 절에 들어가는 것만 주의</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> player_name,</span><br><span class=\"line\">       weight,</span><br><span class=\"line\">       <span class=\"keyword\">CASE</span> <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">250</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;over 250&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">200</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;201-250&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">WHEN</span> weight <span class=\"operator\">&gt;</span> <span class=\"number\">175</span> <span class=\"keyword\">THEN</span> <span class=\"string\">&#x27;176-200&#x27;</span></span><br><span class=\"line\">            <span class=\"keyword\">ELSE</span> <span class=\"string\">&#x27;175 or under&#x27;</span> <span class=\"keyword\">END</span> <span class=\"keyword\">AS</span> weight_group</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> benn.college_football_players</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>order by에 사용힐 컬럼 생성</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> CustomerName, City, Country</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> Customers</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span></span><br><span class=\"line\">(<span class=\"keyword\">CASE</span></span><br><span class=\"line\">    <span class=\"keyword\">WHEN</span> City <span class=\"keyword\">IS</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">THEN</span> Country</span><br><span class=\"line\">    <span class=\"keyword\">ELSE</span> City</span><br><span class=\"line\"><span class=\"keyword\">END</span>);</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> animal_id, name,</span><br><span class=\"line\"><span class=\"keyword\">case</span> </span><br><span class=\"line\">    <span class=\"keyword\">when</span> sex_upon_intake <span class=\"keyword\">like</span> &quot;Intact%&quot; <span class=\"keyword\">then</span> &quot;O&quot;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> &quot;X&quot; </span><br><span class=\"line\"><span class=\"keyword\">end</span> <span class=\"keyword\">as</span> &quot;중성화&quot;</span><br><span class=\"line\"><span class=\"keyword\">from</span> animal_ins</span><br><span class=\"line\"><span class=\"keyword\">order</span> <span class=\"keyword\">by</span> animal_id</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.w3schools.com/sql/sql_case.asp\">https://www.w3schools.com/sql/sql_case.asp</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[SQL]간단한 CASE WHEN 용법","path":"2022/06/13/DE-SQL-case-when/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Data Engineering","tags":["SQL"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[SQL]서브쿼리 정리","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**간단한 서브쿼리 종류와 용법 정리**\n---\n\n### Subquery\n\n---\n\n**_Concept_**\n\n\n- **서브쿼리** : 서브쿼리는 하나의 SQL쿼리 안에 포함된 다른 SQL쿼리를 말한다.\n\n---\n\n- **서브쿼리 사용상황**\n    - 가장 기본적으로 알려지지 않은 조건을 사용해서 조회해야할 때\n    - DB에 접근하는 속도를 향상시킬 때 \n\n- **사용시 주의점**\n    - 항상 괄호로 감싸서 사용할 것\n    - 서브쿼리의 결과가 2건 이상이라면(다중행) **반드시** 비교연산자와 함께 사용한다, \n    - 서브쿼리 내에서는 order by 사용 못함( order by는 쿼리에서 하나만 사용)\n    - 서브쿼리는 메인쿼리의 컬럼을 모두 사용할 수 있지만, 메인쿼리는 서브쿼리의 컬럼을 사용할 수 없다.\n    - 질의 결과에 서브쿼리 컬럼을 표시해야 한다면 조인 방식으로 변환하거나 함수, 스칼라 서브쿼리 등을 사용해야 한다.\n\n- **종류**\n  + 단일 행 서브쿼리 : 특정 행을 반환. 이 행을 조건절도도 사용가능\n    * ex) 평균값알아내는 서브쿼리를 통해 평균값 이상의 그룹을 출력\n  * 다중행 서브쿼리 : 결과가 2건이상 반환되는 서브쿼리. 반드시 비교연산자와 함께 사용. Where 절에 괄호로 들어간다.\n    - IN(서브쿼리) : 서브쿼리의 결과에 존재하는 값과 동일한 조건의미\n    - ALL(서브쿼리) : 모든 값을 만족하는 조건\n    - ANY(서브쿼리) : 비교연산자에 \">\" 를 썼다면 ANY가 어떤 하나라도 맞는지 조건이기 때문에 결과중에 가장 작은값보다 크면 만족한다.\n    - EXIST(서브쿼리) :  서브쿼리의 결과를 만족하는 값이 존재하는지 여부 확인. 존재유무만 확인하기에 1건만 찾으면 더 이상검색안함\n  * 다중 컬럼 서브쿼리 : 서브쿼리 결과로 **여러 개의 컬럼이 반환**되어 메인쿼리 조건과 동시에 비교되는 것.\n  \n  ```sql\n  -- 다둥컬럼 서브퉈리 예시\n\n  select ord_num, agent_code, ord_date, ord_amount\n  from orders\n  where(agent_code, ord_amount) IN\n  (SELECT agent_code, MIN(ord_amount)\n  FROM orders \n  GROUP BY agent_code);  \n\n  ```\n\n\n#### Where 절의 Subquery\n\n- 비교연산자 IN 사용\n```sql\n# 특정 행 반환\nSELECT \n    employee_id, first_name, last_name\nFROM\n    employees\nWHERE\n    department_id IN (SELECT \n            department_id\n        FROM\n            departments\n        WHERE\n            location_id = \"찾는 아이디\")\nORDER BY first_name , last_name;\n\n```\n\n- MAX나 MIN 사용\n\n```sql\n\n# where 절에서 서브쿼리로 정의한 조건을 select 절에 쓸 수 있다.\n\nSELECT \n    employee_id, first_name, last_name, salary\nFROM\n    employees\nWHERE\n    salary = (SELECT \n            MAX(salary)\n        FROM\n            employees)\nORDER BY first_name , last_name;\n\n```\n\n- AVG로 조건걸기\n\n\n```sql\n\nSELECT \n    employee_id, first_name, last_name, salary\nFROM\n    employees\nWHERE\n    salary > (SELECT \n            AVG(salary)\n        FROM\n            employees);    \n\n\n```\n\n- 서브쿼리 조건문처럼 사용하기\n\n```sql\nSELECT *\n  FROM tutorial.sf_crime_incidents_2014_01\n WHERE Date = (SELECT MIN(date)\n                 FROM tutorial.sf_crime_incidents_2014_01\n              )\n```\n\n\n\n#### FROM 절의 Subquery(Inline View)\n\n- SQL이 실행될 때만 동적으로 생성되는 Inline view \n\n\n```sql\nSELECT \n    MAX(items), \n    MIN(items), \n    FLOOR(AVG(items))\nFROM\n    (SELECT \n        orderNumber, COUNT(orderNumber) AS items\n    FROM\n        orderdetails\n    GROUP BY orderNumber) AS lineitems;\n```\n\n\n- 파생테이블은 반드시 alias를 가진다,\n\n```sql\nselect\n    substring(address,1,2) as region,\n    count(*) as review_count\nfrom review as r left outer join member as m\non r.mem_id = m.id\ngroup by substring(address,1,2)\nhaving region is not null\n    and region != '안드'\n\nselect avg(review_count),\n       max(review_count),\n       min(review_count)\nfrom\n(select\n    substring(address,1,2) as region,\n    count(*) as review_count\nfrom review as r left outer join member as m\non r.mem_id = m.id\ngroup by substring(address,1,2)\nhaving region is not null\n    and region != '안드') as review_count_summary #서브쿼리로 탄생한 파생테이블은 반드시 alias를 가져야 한다\n```\n\n\n-- Join과 서브쿼리 같이 사용하기\n\n```sql\nSELECT *\n  FROM tutorial.sf_crime_incidents_2014_01 incidents\n  JOIN ( SELECT date\n           FROM tutorial.sf_crime_incidents_2014_01\n          ORDER BY date\n          LIMIT 5\n       ) sub\n    ON incidents.date = sub.date\n\n```\n\n#### SELECT 절의 Subquery(Scala Subquery)\n\n- SELECT 절 안에 SELECT가 있을 경우 Scala 서브쿼리라 부르며 기본적으로 한 행만 리턴한다.\n\n```sql\nSELECT PLAYER, HEIGHT , (SELECT AVG(HEIGHT)\n                         FROM PLAYER X\n                         WHERE X.TEAM_ID = P.TEAM_ID)\nFROM PLAYER_P\n\n```\n\n- 기본적으로 outer join이 적용되어 있다.\n- 쿼리 수행 횟수를 최소화하기 위해서 입력값과 출력값을 내부 캐시에 저장한다.\n- 대용량 데이터를 처리할 경우 속도가 느려질 수 있다.\n\n```sql\n\nSELECT A.PKID\n    , A.TITLE\n    , NVL(B.NAME, '탈퇴한 회원'), B.NAME\n    , (SELECT COUNT(*) FROM REPLY WHERE P_PKID = B.PKID) AS COUNT1\nFROM BOARD B LEFT OUTER JOIN MEMBER M\n    ON B.MEM_NO = M.PKID\n\n```\n\n\n\n**References & annotation**\n---\n- https://mode.com/sql-tutorial/sql-sub-queries/\n- https://www.mysqltutorial.org/mysql-subquery/","source":"_posts/DE-SQL-subquery.md","raw":"---\ntitle: '[SQL]서브쿼리 정리'\ncategories:\n  - [Data Engineering]\ntags:\n  - SQL\n  - Subquery\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**간단한 서브쿼리 종류와 용법 정리**\n---\n\n### Subquery\n\n---\n\n**_Concept_**\n\n\n- **서브쿼리** : 서브쿼리는 하나의 SQL쿼리 안에 포함된 다른 SQL쿼리를 말한다.\n\n---\n\n- **서브쿼리 사용상황**\n    - 가장 기본적으로 알려지지 않은 조건을 사용해서 조회해야할 때\n    - DB에 접근하는 속도를 향상시킬 때 \n\n- **사용시 주의점**\n    - 항상 괄호로 감싸서 사용할 것\n    - 서브쿼리의 결과가 2건 이상이라면(다중행) **반드시** 비교연산자와 함께 사용한다, \n    - 서브쿼리 내에서는 order by 사용 못함( order by는 쿼리에서 하나만 사용)\n    - 서브쿼리는 메인쿼리의 컬럼을 모두 사용할 수 있지만, 메인쿼리는 서브쿼리의 컬럼을 사용할 수 없다.\n    - 질의 결과에 서브쿼리 컬럼을 표시해야 한다면 조인 방식으로 변환하거나 함수, 스칼라 서브쿼리 등을 사용해야 한다.\n\n- **종류**\n  + 단일 행 서브쿼리 : 특정 행을 반환. 이 행을 조건절도도 사용가능\n    * ex) 평균값알아내는 서브쿼리를 통해 평균값 이상의 그룹을 출력\n  * 다중행 서브쿼리 : 결과가 2건이상 반환되는 서브쿼리. 반드시 비교연산자와 함께 사용. Where 절에 괄호로 들어간다.\n    - IN(서브쿼리) : 서브쿼리의 결과에 존재하는 값과 동일한 조건의미\n    - ALL(서브쿼리) : 모든 값을 만족하는 조건\n    - ANY(서브쿼리) : 비교연산자에 \">\" 를 썼다면 ANY가 어떤 하나라도 맞는지 조건이기 때문에 결과중에 가장 작은값보다 크면 만족한다.\n    - EXIST(서브쿼리) :  서브쿼리의 결과를 만족하는 값이 존재하는지 여부 확인. 존재유무만 확인하기에 1건만 찾으면 더 이상검색안함\n  * 다중 컬럼 서브쿼리 : 서브쿼리 결과로 **여러 개의 컬럼이 반환**되어 메인쿼리 조건과 동시에 비교되는 것.\n  \n  ```sql\n  -- 다둥컬럼 서브퉈리 예시\n\n  select ord_num, agent_code, ord_date, ord_amount\n  from orders\n  where(agent_code, ord_amount) IN\n  (SELECT agent_code, MIN(ord_amount)\n  FROM orders \n  GROUP BY agent_code);  \n\n  ```\n\n\n#### Where 절의 Subquery\n\n- 비교연산자 IN 사용\n```sql\n# 특정 행 반환\nSELECT \n    employee_id, first_name, last_name\nFROM\n    employees\nWHERE\n    department_id IN (SELECT \n            department_id\n        FROM\n            departments\n        WHERE\n            location_id = \"찾는 아이디\")\nORDER BY first_name , last_name;\n\n```\n\n- MAX나 MIN 사용\n\n```sql\n\n# where 절에서 서브쿼리로 정의한 조건을 select 절에 쓸 수 있다.\n\nSELECT \n    employee_id, first_name, last_name, salary\nFROM\n    employees\nWHERE\n    salary = (SELECT \n            MAX(salary)\n        FROM\n            employees)\nORDER BY first_name , last_name;\n\n```\n\n- AVG로 조건걸기\n\n\n```sql\n\nSELECT \n    employee_id, first_name, last_name, salary\nFROM\n    employees\nWHERE\n    salary > (SELECT \n            AVG(salary)\n        FROM\n            employees);    \n\n\n```\n\n- 서브쿼리 조건문처럼 사용하기\n\n```sql\nSELECT *\n  FROM tutorial.sf_crime_incidents_2014_01\n WHERE Date = (SELECT MIN(date)\n                 FROM tutorial.sf_crime_incidents_2014_01\n              )\n```\n\n\n\n#### FROM 절의 Subquery(Inline View)\n\n- SQL이 실행될 때만 동적으로 생성되는 Inline view \n\n\n```sql\nSELECT \n    MAX(items), \n    MIN(items), \n    FLOOR(AVG(items))\nFROM\n    (SELECT \n        orderNumber, COUNT(orderNumber) AS items\n    FROM\n        orderdetails\n    GROUP BY orderNumber) AS lineitems;\n```\n\n\n- 파생테이블은 반드시 alias를 가진다,\n\n```sql\nselect\n    substring(address,1,2) as region,\n    count(*) as review_count\nfrom review as r left outer join member as m\non r.mem_id = m.id\ngroup by substring(address,1,2)\nhaving region is not null\n    and region != '안드'\n\nselect avg(review_count),\n       max(review_count),\n       min(review_count)\nfrom\n(select\n    substring(address,1,2) as region,\n    count(*) as review_count\nfrom review as r left outer join member as m\non r.mem_id = m.id\ngroup by substring(address,1,2)\nhaving region is not null\n    and region != '안드') as review_count_summary #서브쿼리로 탄생한 파생테이블은 반드시 alias를 가져야 한다\n```\n\n\n-- Join과 서브쿼리 같이 사용하기\n\n```sql\nSELECT *\n  FROM tutorial.sf_crime_incidents_2014_01 incidents\n  JOIN ( SELECT date\n           FROM tutorial.sf_crime_incidents_2014_01\n          ORDER BY date\n          LIMIT 5\n       ) sub\n    ON incidents.date = sub.date\n\n```\n\n#### SELECT 절의 Subquery(Scala Subquery)\n\n- SELECT 절 안에 SELECT가 있을 경우 Scala 서브쿼리라 부르며 기본적으로 한 행만 리턴한다.\n\n```sql\nSELECT PLAYER, HEIGHT , (SELECT AVG(HEIGHT)\n                         FROM PLAYER X\n                         WHERE X.TEAM_ID = P.TEAM_ID)\nFROM PLAYER_P\n\n```\n\n- 기본적으로 outer join이 적용되어 있다.\n- 쿼리 수행 횟수를 최소화하기 위해서 입력값과 출력값을 내부 캐시에 저장한다.\n- 대용량 데이터를 처리할 경우 속도가 느려질 수 있다.\n\n```sql\n\nSELECT A.PKID\n    , A.TITLE\n    , NVL(B.NAME, '탈퇴한 회원'), B.NAME\n    , (SELECT COUNT(*) FROM REPLY WHERE P_PKID = B.PKID) AS COUNT1\nFROM BOARD B LEFT OUTER JOIN MEMBER M\n    ON B.MEM_NO = M.PKID\n\n```\n\n\n\n**References & annotation**\n---\n- https://mode.com/sql-tutorial/sql-sub-queries/\n- https://www.mysqltutorial.org/mysql-subquery/","slug":"DE-SQL-subquery","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsbz0008b36q6db4gv4v","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"간단한-서브쿼리-종류와-용법-정리\"><a href=\"#간단한-서브쿼리-종류와-용법-정리\" class=\"headerlink\" title=\"간단한 서브쿼리 종류와 용법 정리\"></a><strong>간단한 서브쿼리 종류와 용법 정리</strong></h2><h3 id=\"Subquery\"><a href=\"#Subquery\" class=\"headerlink\" title=\"Subquery\"></a>Subquery</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>서브쿼리</strong> : 서브쿼리는 하나의 SQL쿼리 안에 포함된 다른 SQL쿼리를 말한다.</li>\n</ul>\n<hr>\n<ul>\n<li><p><strong>서브쿼리 사용상황</strong></p>\n<ul>\n<li>가장 기본적으로 알려지지 않은 조건을 사용해서 조회해야할 때</li>\n<li>DB에 접근하는 속도를 향상시킬 때</li>\n</ul>\n</li>\n<li><p><strong>사용시 주의점</strong></p>\n<ul>\n<li>항상 괄호로 감싸서 사용할 것</li>\n<li>서브쿼리의 결과가 2건 이상이라면(다중행) <strong>반드시</strong> 비교연산자와 함께 사용한다, </li>\n<li>서브쿼리 내에서는 order by 사용 못함( order by는 쿼리에서 하나만 사용)</li>\n<li>서브쿼리는 메인쿼리의 컬럼을 모두 사용할 수 있지만, 메인쿼리는 서브쿼리의 컬럼을 사용할 수 없다.</li>\n<li>질의 결과에 서브쿼리 컬럼을 표시해야 한다면 조인 방식으로 변환하거나 함수, 스칼라 서브쿼리 등을 사용해야 한다.</li>\n</ul>\n</li>\n<li><p><strong>종류</strong></p>\n<ul>\n<li>단일 행 서브쿼리 : 특정 행을 반환. 이 행을 조건절도도 사용가능<ul>\n<li>ex) 평균값알아내는 서브쿼리를 통해 평균값 이상의 그룹을 출력</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>다중행 서브쿼리 : 결과가 2건이상 반환되는 서브쿼리. 반드시 비교연산자와 함께 사용. Where 절에 괄호로 들어간다.<ul>\n<li>IN(서브쿼리) : 서브쿼리의 결과에 존재하는 값과 동일한 조건의미</li>\n<li>ALL(서브쿼리) : 모든 값을 만족하는 조건</li>\n<li>ANY(서브쿼리) : 비교연산자에 “&gt;” 를 썼다면 ANY가 어떤 하나라도 맞는지 조건이기 때문에 결과중에 가장 작은값보다 크면 만족한다.</li>\n<li>EXIST(서브쿼리) :  서브쿼리의 결과를 만족하는 값이 존재하는지 여부 확인. 존재유무만 확인하기에 1건만 찾으면 더 이상검색안함</li>\n</ul>\n</li>\n<li>다중 컬럼 서브쿼리 : 서브쿼리 결과로 <strong>여러 개의 컬럼이 반환</strong>되어 메인쿼리 조건과 동시에 비교되는 것.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- 다둥컬럼 서브퉈리 예시</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> ord_num, agent_code, ord_date, ord_amount</span><br><span class=\"line\"><span class=\"keyword\">from</span> orders</span><br><span class=\"line\"><span class=\"keyword\">where</span>(agent_code, ord_amount) <span class=\"keyword\">IN</span></span><br><span class=\"line\">(<span class=\"keyword\">SELECT</span> agent_code, <span class=\"built_in\">MIN</span>(ord_amount)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> orders </span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> agent_code);  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"Where-절의-Subquery\"><a href=\"#Where-절의-Subquery\" class=\"headerlink\" title=\"Where 절의 Subquery\"></a>Where 절의 Subquery</h4><ul>\n<li><p>비교연산자 IN 사용</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 특정 행 반환</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    department_id <span class=\"keyword\">IN</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            department_id</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            departments</span><br><span class=\"line\">        <span class=\"keyword\">WHERE</span></span><br><span class=\"line\">            location_id <span class=\"operator\">=</span> &quot;찾는 아이디&quot;)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> first_name , last_name;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>MAX나 MIN 사용</p>\n</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"># <span class=\"keyword\">where</span> 절에서 서브쿼리로 정의한 조건을 <span class=\"keyword\">select</span> 절에 쓸 수 있다.</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name, salary</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    salary <span class=\"operator\">=</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            <span class=\"built_in\">MAX</span>(salary)</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            employees)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> first_name , last_name;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>AVG로 조건걸기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name, salary</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    salary <span class=\"operator\">&gt;</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            <span class=\"built_in\">AVG</span>(salary)</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            employees);    </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>서브쿼리 조건문처럼 사용하기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\"> <span class=\"keyword\">WHERE</span> <span class=\"type\">Date</span> <span class=\"operator\">=</span> (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">MIN</span>(<span class=\"type\">date</span>)</span><br><span class=\"line\">                 <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\">              )</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"FROM-절의-Subquery-Inline-View\"><a href=\"#FROM-절의-Subquery-Inline-View\" class=\"headerlink\" title=\"FROM 절의 Subquery(Inline View)\"></a>FROM 절의 Subquery(Inline View)</h4><ul>\n<li>SQL이 실행될 때만 동적으로 생성되는 Inline view</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    <span class=\"built_in\">MAX</span>(items), </span><br><span class=\"line\">    <span class=\"built_in\">MIN</span>(items), </span><br><span class=\"line\">    <span class=\"built_in\">FLOOR</span>(<span class=\"built_in\">AVG</span>(items))</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">        orderNumber, <span class=\"built_in\">COUNT</span>(orderNumber) <span class=\"keyword\">AS</span> items</span><br><span class=\"line\">    <span class=\"keyword\">FROM</span></span><br><span class=\"line\">        orderdetails</span><br><span class=\"line\">    <span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> orderNumber) <span class=\"keyword\">AS</span> lineitems;</span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>파생테이블은 반드시 alias를 가진다,</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>) <span class=\"keyword\">as</span> region,</span><br><span class=\"line\">    <span class=\"built_in\">count</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">as</span> review_count</span><br><span class=\"line\"><span class=\"keyword\">from</span> review <span class=\"keyword\">as</span> r <span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span> <span class=\"keyword\">member</span> <span class=\"keyword\">as</span> m</span><br><span class=\"line\"><span class=\"keyword\">on</span> r.mem_id <span class=\"operator\">=</span> m.id</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"keyword\">having</span> region <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">null</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> region <span class=\"operator\">!=</span> <span class=\"string\">&#x27;안드&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"built_in\">avg</span>(review_count),</span><br><span class=\"line\">       <span class=\"built_in\">max</span>(review_count),</span><br><span class=\"line\">       <span class=\"built_in\">min</span>(review_count)</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span></span><br><span class=\"line\">    <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>) <span class=\"keyword\">as</span> region,</span><br><span class=\"line\">    <span class=\"built_in\">count</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">as</span> review_count</span><br><span class=\"line\"><span class=\"keyword\">from</span> review <span class=\"keyword\">as</span> r <span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span> <span class=\"keyword\">member</span> <span class=\"keyword\">as</span> m</span><br><span class=\"line\"><span class=\"keyword\">on</span> r.mem_id <span class=\"operator\">=</span> m.id</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"keyword\">having</span> region <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">null</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> region <span class=\"operator\">!=</span> <span class=\"string\">&#x27;안드&#x27;</span>) <span class=\"keyword\">as</span> review_count_summary #서브쿼리로 탄생한 파생테이블은 반드시 alias를 가져야 한다</span><br></pre></td></tr></table></figure>\n\n\n<p>– Join과 서브쿼리 같이 사용하기</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01 incidents</span><br><span class=\"line\">  <span class=\"keyword\">JOIN</span> ( <span class=\"keyword\">SELECT</span> <span class=\"type\">date</span></span><br><span class=\"line\">           <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\">          <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"type\">date</span></span><br><span class=\"line\">          LIMIT <span class=\"number\">5</span></span><br><span class=\"line\">       ) sub</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> incidents.date <span class=\"operator\">=</span> sub.date</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"SELECT-절의-Subquery-Scala-Subquery\"><a href=\"#SELECT-절의-Subquery-Scala-Subquery\" class=\"headerlink\" title=\"SELECT 절의 Subquery(Scala Subquery)\"></a>SELECT 절의 Subquery(Scala Subquery)</h4><ul>\n<li>SELECT 절 안에 SELECT가 있을 경우 Scala 서브쿼리라 부르며 기본적으로 한 행만 리턴한다.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> PLAYER, HEIGHT , (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">AVG</span>(HEIGHT)</span><br><span class=\"line\">                         <span class=\"keyword\">FROM</span> PLAYER X</span><br><span class=\"line\">                         <span class=\"keyword\">WHERE</span> X.TEAM_ID <span class=\"operator\">=</span> P.TEAM_ID)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> PLAYER_P</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>기본적으로 outer join이 적용되어 있다.</li>\n<li>쿼리 수행 횟수를 최소화하기 위해서 입력값과 출력값을 내부 캐시에 저장한다.</li>\n<li>대용량 데이터를 처리할 경우 속도가 느려질 수 있다.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> A.PKID</span><br><span class=\"line\">    , A.TITLE</span><br><span class=\"line\">    , NVL(B.NAME, <span class=\"string\">&#x27;탈퇴한 회원&#x27;</span>), B.NAME</span><br><span class=\"line\">    , (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">COUNT</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">FROM</span> REPLY <span class=\"keyword\">WHERE</span> P_PKID <span class=\"operator\">=</span> B.PKID) <span class=\"keyword\">AS</span> COUNT1</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> BOARD B <span class=\"keyword\">LEFT</span> <span class=\"keyword\">OUTER</span> <span class=\"keyword\">JOIN</span> <span class=\"keyword\">MEMBER</span> M</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> B.MEM_NO <span class=\"operator\">=</span> M.PKID</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://mode.com/sql-tutorial/sql-sub-queries/\">https://mode.com/sql-tutorial/sql-sub-queries/</a></li>\n<li><a href=\"https://www.mysqltutorial.org/mysql-subquery/\">https://www.mysqltutorial.org/mysql-subquery/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"간단한-서브쿼리-종류와-용법-정리\"><a href=\"#간단한-서브쿼리-종류와-용법-정리\" class=\"headerlink\" title=\"간단한 서브쿼리 종류와 용법 정리\"></a><strong>간단한 서브쿼리 종류와 용법 정리</strong></h2><h3 id=\"Subquery\"><a href=\"#Subquery\" class=\"headerlink\" title=\"Subquery\"></a>Subquery</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>서브쿼리</strong> : 서브쿼리는 하나의 SQL쿼리 안에 포함된 다른 SQL쿼리를 말한다.</li>\n</ul>\n<hr>\n<ul>\n<li><p><strong>서브쿼리 사용상황</strong></p>\n<ul>\n<li>가장 기본적으로 알려지지 않은 조건을 사용해서 조회해야할 때</li>\n<li>DB에 접근하는 속도를 향상시킬 때</li>\n</ul>\n</li>\n<li><p><strong>사용시 주의점</strong></p>\n<ul>\n<li>항상 괄호로 감싸서 사용할 것</li>\n<li>서브쿼리의 결과가 2건 이상이라면(다중행) <strong>반드시</strong> 비교연산자와 함께 사용한다, </li>\n<li>서브쿼리 내에서는 order by 사용 못함( order by는 쿼리에서 하나만 사용)</li>\n<li>서브쿼리는 메인쿼리의 컬럼을 모두 사용할 수 있지만, 메인쿼리는 서브쿼리의 컬럼을 사용할 수 없다.</li>\n<li>질의 결과에 서브쿼리 컬럼을 표시해야 한다면 조인 방식으로 변환하거나 함수, 스칼라 서브쿼리 등을 사용해야 한다.</li>\n</ul>\n</li>\n<li><p><strong>종류</strong></p>\n<ul>\n<li>단일 행 서브쿼리 : 특정 행을 반환. 이 행을 조건절도도 사용가능<ul>\n<li>ex) 평균값알아내는 서브쿼리를 통해 평균값 이상의 그룹을 출력</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>다중행 서브쿼리 : 결과가 2건이상 반환되는 서브쿼리. 반드시 비교연산자와 함께 사용. Where 절에 괄호로 들어간다.<ul>\n<li>IN(서브쿼리) : 서브쿼리의 결과에 존재하는 값과 동일한 조건의미</li>\n<li>ALL(서브쿼리) : 모든 값을 만족하는 조건</li>\n<li>ANY(서브쿼리) : 비교연산자에 “&gt;” 를 썼다면 ANY가 어떤 하나라도 맞는지 조건이기 때문에 결과중에 가장 작은값보다 크면 만족한다.</li>\n<li>EXIST(서브쿼리) :  서브쿼리의 결과를 만족하는 값이 존재하는지 여부 확인. 존재유무만 확인하기에 1건만 찾으면 더 이상검색안함</li>\n</ul>\n</li>\n<li>다중 컬럼 서브쿼리 : 서브쿼리 결과로 <strong>여러 개의 컬럼이 반환</strong>되어 메인쿼리 조건과 동시에 비교되는 것.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- 다둥컬럼 서브퉈리 예시</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> ord_num, agent_code, ord_date, ord_amount</span><br><span class=\"line\"><span class=\"keyword\">from</span> orders</span><br><span class=\"line\"><span class=\"keyword\">where</span>(agent_code, ord_amount) <span class=\"keyword\">IN</span></span><br><span class=\"line\">(<span class=\"keyword\">SELECT</span> agent_code, <span class=\"built_in\">MIN</span>(ord_amount)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> orders </span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> agent_code);  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"Where-절의-Subquery\"><a href=\"#Where-절의-Subquery\" class=\"headerlink\" title=\"Where 절의 Subquery\"></a>Where 절의 Subquery</h4><ul>\n<li><p>비교연산자 IN 사용</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 특정 행 반환</span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    department_id <span class=\"keyword\">IN</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            department_id</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            departments</span><br><span class=\"line\">        <span class=\"keyword\">WHERE</span></span><br><span class=\"line\">            location_id <span class=\"operator\">=</span> &quot;찾는 아이디&quot;)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> first_name , last_name;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>MAX나 MIN 사용</p>\n</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"># <span class=\"keyword\">where</span> 절에서 서브쿼리로 정의한 조건을 <span class=\"keyword\">select</span> 절에 쓸 수 있다.</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name, salary</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    salary <span class=\"operator\">=</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            <span class=\"built_in\">MAX</span>(salary)</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            employees)</span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> first_name , last_name;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>AVG로 조건걸기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    employee_id, first_name, last_name, salary</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    employees</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span></span><br><span class=\"line\">    salary <span class=\"operator\">&gt;</span> (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">            <span class=\"built_in\">AVG</span>(salary)</span><br><span class=\"line\">        <span class=\"keyword\">FROM</span></span><br><span class=\"line\">            employees);    </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>서브쿼리 조건문처럼 사용하기</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\"> <span class=\"keyword\">WHERE</span> <span class=\"type\">Date</span> <span class=\"operator\">=</span> (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">MIN</span>(<span class=\"type\">date</span>)</span><br><span class=\"line\">                 <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\">              )</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"FROM-절의-Subquery-Inline-View\"><a href=\"#FROM-절의-Subquery-Inline-View\" class=\"headerlink\" title=\"FROM 절의 Subquery(Inline View)\"></a>FROM 절의 Subquery(Inline View)</h4><ul>\n<li>SQL이 실행될 때만 동적으로 생성되는 Inline view</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">    <span class=\"built_in\">MAX</span>(items), </span><br><span class=\"line\">    <span class=\"built_in\">MIN</span>(items), </span><br><span class=\"line\">    <span class=\"built_in\">FLOOR</span>(<span class=\"built_in\">AVG</span>(items))</span><br><span class=\"line\"><span class=\"keyword\">FROM</span></span><br><span class=\"line\">    (<span class=\"keyword\">SELECT</span> </span><br><span class=\"line\">        orderNumber, <span class=\"built_in\">COUNT</span>(orderNumber) <span class=\"keyword\">AS</span> items</span><br><span class=\"line\">    <span class=\"keyword\">FROM</span></span><br><span class=\"line\">        orderdetails</span><br><span class=\"line\">    <span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> orderNumber) <span class=\"keyword\">AS</span> lineitems;</span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>파생테이블은 반드시 alias를 가진다,</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>) <span class=\"keyword\">as</span> region,</span><br><span class=\"line\">    <span class=\"built_in\">count</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">as</span> review_count</span><br><span class=\"line\"><span class=\"keyword\">from</span> review <span class=\"keyword\">as</span> r <span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span> <span class=\"keyword\">member</span> <span class=\"keyword\">as</span> m</span><br><span class=\"line\"><span class=\"keyword\">on</span> r.mem_id <span class=\"operator\">=</span> m.id</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"keyword\">having</span> region <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">null</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> region <span class=\"operator\">!=</span> <span class=\"string\">&#x27;안드&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"built_in\">avg</span>(review_count),</span><br><span class=\"line\">       <span class=\"built_in\">max</span>(review_count),</span><br><span class=\"line\">       <span class=\"built_in\">min</span>(review_count)</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(<span class=\"keyword\">select</span></span><br><span class=\"line\">    <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>) <span class=\"keyword\">as</span> region,</span><br><span class=\"line\">    <span class=\"built_in\">count</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">as</span> review_count</span><br><span class=\"line\"><span class=\"keyword\">from</span> review <span class=\"keyword\">as</span> r <span class=\"keyword\">left</span> <span class=\"keyword\">outer</span> <span class=\"keyword\">join</span> <span class=\"keyword\">member</span> <span class=\"keyword\">as</span> m</span><br><span class=\"line\"><span class=\"keyword\">on</span> r.mem_id <span class=\"operator\">=</span> m.id</span><br><span class=\"line\"><span class=\"keyword\">group</span> <span class=\"keyword\">by</span> <span class=\"built_in\">substring</span>(address,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"keyword\">having</span> region <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">null</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> region <span class=\"operator\">!=</span> <span class=\"string\">&#x27;안드&#x27;</span>) <span class=\"keyword\">as</span> review_count_summary #서브쿼리로 탄생한 파생테이블은 반드시 alias를 가져야 한다</span><br></pre></td></tr></table></figure>\n\n\n<p>– Join과 서브쿼리 같이 사용하기</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span></span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01 incidents</span><br><span class=\"line\">  <span class=\"keyword\">JOIN</span> ( <span class=\"keyword\">SELECT</span> <span class=\"type\">date</span></span><br><span class=\"line\">           <span class=\"keyword\">FROM</span> tutorial.sf_crime_incidents_2014_01</span><br><span class=\"line\">          <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"type\">date</span></span><br><span class=\"line\">          LIMIT <span class=\"number\">5</span></span><br><span class=\"line\">       ) sub</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> incidents.date <span class=\"operator\">=</span> sub.date</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"SELECT-절의-Subquery-Scala-Subquery\"><a href=\"#SELECT-절의-Subquery-Scala-Subquery\" class=\"headerlink\" title=\"SELECT 절의 Subquery(Scala Subquery)\"></a>SELECT 절의 Subquery(Scala Subquery)</h4><ul>\n<li>SELECT 절 안에 SELECT가 있을 경우 Scala 서브쿼리라 부르며 기본적으로 한 행만 리턴한다.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> PLAYER, HEIGHT , (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">AVG</span>(HEIGHT)</span><br><span class=\"line\">                         <span class=\"keyword\">FROM</span> PLAYER X</span><br><span class=\"line\">                         <span class=\"keyword\">WHERE</span> X.TEAM_ID <span class=\"operator\">=</span> P.TEAM_ID)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> PLAYER_P</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>기본적으로 outer join이 적용되어 있다.</li>\n<li>쿼리 수행 횟수를 최소화하기 위해서 입력값과 출력값을 내부 캐시에 저장한다.</li>\n<li>대용량 데이터를 처리할 경우 속도가 느려질 수 있다.</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> A.PKID</span><br><span class=\"line\">    , A.TITLE</span><br><span class=\"line\">    , NVL(B.NAME, <span class=\"string\">&#x27;탈퇴한 회원&#x27;</span>), B.NAME</span><br><span class=\"line\">    , (<span class=\"keyword\">SELECT</span> <span class=\"built_in\">COUNT</span>(<span class=\"operator\">*</span>) <span class=\"keyword\">FROM</span> REPLY <span class=\"keyword\">WHERE</span> P_PKID <span class=\"operator\">=</span> B.PKID) <span class=\"keyword\">AS</span> COUNT1</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> BOARD B <span class=\"keyword\">LEFT</span> <span class=\"keyword\">OUTER</span> <span class=\"keyword\">JOIN</span> <span class=\"keyword\">MEMBER</span> M</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> B.MEM_NO <span class=\"operator\">=</span> M.PKID</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://mode.com/sql-tutorial/sql-sub-queries/\">https://mode.com/sql-tutorial/sql-sub-queries/</a></li>\n<li><a href=\"https://www.mysqltutorial.org/mysql-subquery/\">https://www.mysqltutorial.org/mysql-subquery/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[SQL]서브쿼리 정리","path":"2022/06/13/DE-SQL-subquery/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Data Engineering","tags":["SQL","Subquery"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Neural Network]Recurrent Neural Network","date":"2022-06-13T14:23:32.340Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","source":"_posts/DL-RNN.md","raw":"---\ntitle: '[Neural Network]Recurrent Neural Network'\ncategories:\n  - - Neural Network\ntags:\n  - RNN\ndate: \nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","slug":"DL-RNN","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc0000bb36qht5d7kpu","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Neural Network]Recurrent Neural Network","path":"2022/06/13/DL-RNN/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.340Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.340Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":["RNN"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Neural Network]역전파 알고리즘(backpropagation)","date":"2021-09-28T07:59:23.000Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nhttps://edgeaiguru.com/Feedforward-and-Backpropagation\n# \n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면\n역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다. 간단히 역전파의 컨셉을 알아보자.**\n\n---\n## 역전파 알고리즘\n\n**순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면 역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다.**\n\n역전파 알고리즘에서 가중치를 업데이트 한다는 것은 가중치 매개변수의 기울기(Graidant)를 예측값을 바탕으로 다시 계산한다는 것이다.\n\n\n기본적으로 **타겟과 예측값의 차이를 줄이기 위해** 가중치를 업데이트한다.\n\n$$\n y = activiate(\\sum(\\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}) + bias)$$\n \n\n타겟과 예측값의 차이는 `loss function(cost function)`이라고 볼 수 있다.\n\n비용함수는 수식으로 나타내면 아래와 같다.\n\n$$J(\\theta) = y - h_\\theta(x)$$\n여기서 $h_\\theta(x)$ 는 가설함수(모형)이다.\n\n\n역전파(backward Propagation)는 예측값의 국소적 미분값을 순전파(Forward Propagation)의 반대방향으로 곱한 후 다음노드로 전달하는 것이다.\n\n비용함수의 편미분을 통해 기울기를 구하는 이유는 **기울기가 비용함수의 값을 최소화 하는 방향을 제시하기 때문이다.**\n\n![](https://i.imgur.com/Olxv64J.png)\n\n<center><b>그림 1. Gradiant Boosting을 통한 global optimum 찾기</b></center>\n\n\n만약 모형의 loss function이 MSE(Mear Sqared Error)일 경우, 비용함수는 아래와 같이 나타낼 수 있다. (m은 sample의 수를 의미)\n\n\n$$ J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$\n\n\n이를 미분할 경우 직접 계산하기 어렵거나 불가능하기 때문에 Chain Rule을 사용한 합성함수의 편미분을 통해 구한다.\n\n\n아래와 같은 신경망이 있고 output node의 활성화함수는 sigmoid라고 할 경우\n\n\n![](https://i.imgur.com/bGCvYVJ.png)\n\n<center><b>그림 2. 신경망으로 나타낸 backpropagation</b></center>\n\n하나의 가중치에 대한 Gradiant를 아래와 같이 나타날 수 있다.\n\n$$ Gradiant=\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}}=\\frac{\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)} {\\partial \\theta_{i}}$$\n\n이 때 분자는 가중치 $\\theta_i$에 대해 미분할 수 없기 때문에 아래와 같이 `Chain Rule` 을 사용해서 Gradiant를 도출한다.\n\n$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}} =\\frac{\\partial J(\\theta)}{\\partial z_{2}} \\times \\frac{\\partial z_{2}}{\\partial s_{2}} \\times \\frac{\\partial s_{2}}{\\partial \\theta_{i}}  $$\n\n**chain rule(연쇄법칙)**\nchain rule은 합성함수의 미분규칙이며 역전파과정에서 Gradiant를 도출할 때 사용된다.\n\n기본적으로 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.\n\n![](https://i.imgur.com/4eSVZW0.png)\n\n\n<center><b>그림 3. Chain Rule 예시</b></center>\n\n\n**가중치 업데이트**\n\n도출된 값을 learning rate와 곱해서 기존 가중치에서 빼주면 새로운 가중치는 다음과 같이 나타낼 수 있다.\n\n$$\\theta_{j}=\\theta_{j}-\\eta \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$$\n\n\n**결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 `학습`이라는 것을 알 수 있다.**\n\n\n### 신경망 학습 알고리즘 절차 정리\n\n퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.\n\n1. 학습할 신경망 구조를 선택\n    - 입력층 유닛의 수 = Feature 수 (input layer)\n    - 출력층 유닛의 수 = target class 수 (output layer)\n    - 은닉층 수, 각 은닉층의 노드 수 (hidden layer)\n      - hyperparameter의 영역이다. \n      - sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.\n  \n2. 가중치 초기화\n3. 순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산\n   - 입력벡터와 가중치벡터의 내적을 산출 \n   비용함수 $J(\\theta)$를 계산\n4. 역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산\n5. optimizer를 통해 loss function을 최소화\n6. 어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.\n   - 한번 학습할때의 sample의 size 를 `batch` 라고 한다.\n   - 전체 sample에 대해 2-5 의 과정을 반복한 것을 `epoch`라고 한다.\n   - `iteration`은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.\n\n\n### 머신러닝에서의 학습\n\n미분은 순간의 변화율을 구하는 것이다.\n**역전파는 모형에서 계산한 예측값과 실제값의 차이를 바탕으로 미분을 통해 가중치를 보정하는 것을 최대한 반복해서 수행하는 것이다.** \n구체적으로는 손실함수의 국소적 미분값(local deravitive)를 구해서 학습률과 곱한 값을 기존 가중치에서 빼주는 것을 손실함수가 최소가 될 때까지 반복하는 것이다.\n`손실함수의 각 피처에 대한 편미분을 벡터로 묶은 것을 그래디언트(Gradient)라고 부른다.`\n결국 역전파는 Gradiant를 조정하는 것을 반복하는 것이라고 볼 수 있다.\n결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 `학습`이라는 것을 알 수 있다.\n\n\n## 신경망 학습 알고리즘 절차 정리\n\n퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.\n\n0. 학습할 신경망 구조를 선택\n    - 입력층 유닛의 수 = Feature 수 (input layer)\n    - 출력층 유닛의 수 = target class 수 (output layer)\n    - 은닉층 수, 각 은닉층의 노드 수 (hidden layer)\n      - hyperparameter의 영역이다. \n      - sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.\n  \n1. 가중치 초기화\n2. 순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산\n   - 입력벡터와 가중치벡터의 내적을 산출 \n   비용함수 $J(\\theta)$를 계산\n4. 역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산\n5. optimizer를 통해 loss function을 최소화\n6. 어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.\n   - 한번 학습할때의 sample의 size 를 `batch` 라고 한다.\n   - 전체 sample에 대해 2-5 의 과정을 반복한 것을 `epoch`라고 한다.\n   - `iteration`은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.\n\n## References\n\n- https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n- https://edgeaiguru.com/Feedforward-and-Backpropagation","source":"_posts/DL-backpropagation.md","raw":"---\ntitle: '[Neural Network]역전파 알고리즘(backpropagation)'\ncategories:\n  - [Neural Network]\ntags:\ndate: 2021-09-28 16:59:23\nupdated:\nmathjax: true\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nhttps://edgeaiguru.com/Feedforward-and-Backpropagation\n# \n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면\n역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다. 간단히 역전파의 컨셉을 알아보자.**\n\n---\n## 역전파 알고리즘\n\n**순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면 역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다.**\n\n역전파 알고리즘에서 가중치를 업데이트 한다는 것은 가중치 매개변수의 기울기(Graidant)를 예측값을 바탕으로 다시 계산한다는 것이다.\n\n\n기본적으로 **타겟과 예측값의 차이를 줄이기 위해** 가중치를 업데이트한다.\n\n$$\n y = activiate(\\sum(\\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}) + bias)$$\n \n\n타겟과 예측값의 차이는 `loss function(cost function)`이라고 볼 수 있다.\n\n비용함수는 수식으로 나타내면 아래와 같다.\n\n$$J(\\theta) = y - h_\\theta(x)$$\n여기서 $h_\\theta(x)$ 는 가설함수(모형)이다.\n\n\n역전파(backward Propagation)는 예측값의 국소적 미분값을 순전파(Forward Propagation)의 반대방향으로 곱한 후 다음노드로 전달하는 것이다.\n\n비용함수의 편미분을 통해 기울기를 구하는 이유는 **기울기가 비용함수의 값을 최소화 하는 방향을 제시하기 때문이다.**\n\n![](https://i.imgur.com/Olxv64J.png)\n\n<center><b>그림 1. Gradiant Boosting을 통한 global optimum 찾기</b></center>\n\n\n만약 모형의 loss function이 MSE(Mear Sqared Error)일 경우, 비용함수는 아래와 같이 나타낼 수 있다. (m은 sample의 수를 의미)\n\n\n$$ J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$\n\n\n이를 미분할 경우 직접 계산하기 어렵거나 불가능하기 때문에 Chain Rule을 사용한 합성함수의 편미분을 통해 구한다.\n\n\n아래와 같은 신경망이 있고 output node의 활성화함수는 sigmoid라고 할 경우\n\n\n![](https://i.imgur.com/bGCvYVJ.png)\n\n<center><b>그림 2. 신경망으로 나타낸 backpropagation</b></center>\n\n하나의 가중치에 대한 Gradiant를 아래와 같이 나타날 수 있다.\n\n$$ Gradiant=\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}}=\\frac{\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)} {\\partial \\theta_{i}}$$\n\n이 때 분자는 가중치 $\\theta_i$에 대해 미분할 수 없기 때문에 아래와 같이 `Chain Rule` 을 사용해서 Gradiant를 도출한다.\n\n$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}} =\\frac{\\partial J(\\theta)}{\\partial z_{2}} \\times \\frac{\\partial z_{2}}{\\partial s_{2}} \\times \\frac{\\partial s_{2}}{\\partial \\theta_{i}}  $$\n\n**chain rule(연쇄법칙)**\nchain rule은 합성함수의 미분규칙이며 역전파과정에서 Gradiant를 도출할 때 사용된다.\n\n기본적으로 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.\n\n![](https://i.imgur.com/4eSVZW0.png)\n\n\n<center><b>그림 3. Chain Rule 예시</b></center>\n\n\n**가중치 업데이트**\n\n도출된 값을 learning rate와 곱해서 기존 가중치에서 빼주면 새로운 가중치는 다음과 같이 나타낼 수 있다.\n\n$$\\theta_{j}=\\theta_{j}-\\eta \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$$\n\n\n**결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 `학습`이라는 것을 알 수 있다.**\n\n\n### 신경망 학습 알고리즘 절차 정리\n\n퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.\n\n1. 학습할 신경망 구조를 선택\n    - 입력층 유닛의 수 = Feature 수 (input layer)\n    - 출력층 유닛의 수 = target class 수 (output layer)\n    - 은닉층 수, 각 은닉층의 노드 수 (hidden layer)\n      - hyperparameter의 영역이다. \n      - sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.\n  \n2. 가중치 초기화\n3. 순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산\n   - 입력벡터와 가중치벡터의 내적을 산출 \n   비용함수 $J(\\theta)$를 계산\n4. 역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산\n5. optimizer를 통해 loss function을 최소화\n6. 어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.\n   - 한번 학습할때의 sample의 size 를 `batch` 라고 한다.\n   - 전체 sample에 대해 2-5 의 과정을 반복한 것을 `epoch`라고 한다.\n   - `iteration`은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.\n\n\n### 머신러닝에서의 학습\n\n미분은 순간의 변화율을 구하는 것이다.\n**역전파는 모형에서 계산한 예측값과 실제값의 차이를 바탕으로 미분을 통해 가중치를 보정하는 것을 최대한 반복해서 수행하는 것이다.** \n구체적으로는 손실함수의 국소적 미분값(local deravitive)를 구해서 학습률과 곱한 값을 기존 가중치에서 빼주는 것을 손실함수가 최소가 될 때까지 반복하는 것이다.\n`손실함수의 각 피처에 대한 편미분을 벡터로 묶은 것을 그래디언트(Gradient)라고 부른다.`\n결국 역전파는 Gradiant를 조정하는 것을 반복하는 것이라고 볼 수 있다.\n결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 `학습`이라는 것을 알 수 있다.\n\n\n## 신경망 학습 알고리즘 절차 정리\n\n퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.\n\n0. 학습할 신경망 구조를 선택\n    - 입력층 유닛의 수 = Feature 수 (input layer)\n    - 출력층 유닛의 수 = target class 수 (output layer)\n    - 은닉층 수, 각 은닉층의 노드 수 (hidden layer)\n      - hyperparameter의 영역이다. \n      - sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.\n  \n1. 가중치 초기화\n2. 순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산\n   - 입력벡터와 가중치벡터의 내적을 산출 \n   비용함수 $J(\\theta)$를 계산\n4. 역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산\n5. optimizer를 통해 loss function을 최소화\n6. 어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.\n   - 한번 학습할때의 sample의 size 를 `batch` 라고 한다.\n   - 전체 sample에 대해 2-5 의 과정을 반복한 것을 `epoch`라고 한다.\n   - `iteration`은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.\n\n## References\n\n- https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\n- https://edgeaiguru.com/Feedforward-and-Backpropagation","slug":"DL-backpropagation","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc1000cb36q6zrj7x4w","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nhttps://edgeaiguru.com/Feedforward-and-Backpropagation\n# \n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면<br>역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다. 간단히 역전파의 컨셉을 알아보자.</strong></p>\n<hr>\n<h2 id=\"역전파-알고리즘\"><a href=\"#역전파-알고리즘\" class=\"headerlink\" title=\"역전파 알고리즘\"></a>역전파 알고리즘</h2><p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면 역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다.</strong></p>\n<p>역전파 알고리즘에서 가중치를 업데이트 한다는 것은 가중치 매개변수의 기울기(Graidant)를 예측값을 바탕으로 다시 계산한다는 것이다.</p>\n<p>기본적으로 <strong>타겟과 예측값의 차이를 줄이기 위해</strong> 가중치를 업데이트한다.</p>\n<p>$$<br> y &#x3D; activiate(\\sum(\\theta_{1}x_{1} + \\theta_{2}x_{2} + … + \\theta_{n}x_{n}) + bias)$$</p>\n<p>타겟과 예측값의 차이는 <code>loss function(cost function)</code>이라고 볼 수 있다.</p>\n<p>비용함수는 수식으로 나타내면 아래와 같다.</p>\n<p>$$J(\\theta) &#x3D; y - h_\\theta(x)$$<br>여기서 $h_\\theta(x)$ 는 가설함수(모형)이다.</p>\n<p>역전파(backward Propagation)는 예측값의 국소적 미분값을 순전파(Forward Propagation)의 반대방향으로 곱한 후 다음노드로 전달하는 것이다.</p>\n<p>비용함수의 편미분을 통해 기울기를 구하는 이유는 <strong>기울기가 비용함수의 값을 최소화 하는 방향을 제시하기 때문이다.</strong></p>\n<p><img src=\"https://i.imgur.com/Olxv64J.png\"></p>\n<center><b>그림 1. Gradiant Boosting을 통한 global optimum 찾기</b></center>\n\n\n<p>만약 모형의 loss function이 MSE(Mear Sqared Error)일 경우, 비용함수는 아래와 같이 나타낼 수 있다. (m은 sample의 수를 의미)</p>\n<p>$$ J(\\theta)&#x3D;\\frac{1}{2 m} \\sum_{i&#x3D;1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$</p>\n<p>이를 미분할 경우 직접 계산하기 어렵거나 불가능하기 때문에 Chain Rule을 사용한 합성함수의 편미분을 통해 구한다.</p>\n<p>아래와 같은 신경망이 있고 output node의 활성화함수는 sigmoid라고 할 경우</p>\n<p><img src=\"https://i.imgur.com/bGCvYVJ.png\"></p>\n<center><b>그림 2. 신경망으로 나타낸 backpropagation</b></center>\n\n<p>하나의 가중치에 대한 Gradiant를 아래와 같이 나타날 수 있다.</p>\n<p>$$ Gradiant&#x3D;\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}}&#x3D;\\frac{\\frac{1}{2 m} \\sum_{i&#x3D;1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)} {\\partial \\theta_{i}}$$</p>\n<p>이 때 분자는 가중치 $\\theta_i$에 대해 미분할 수 없기 때문에 아래와 같이 <code>Chain Rule</code> 을 사용해서 Gradiant를 도출한다.</p>\n<p>$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}} &#x3D;\\frac{\\partial J(\\theta)}{\\partial z_{2}} \\times \\frac{\\partial z_{2}}{\\partial s_{2}} \\times \\frac{\\partial s_{2}}{\\partial \\theta_{i}}  $$</p>\n<p><strong>chain rule(연쇄법칙)</strong><br>chain rule은 합성함수의 미분규칙이며 역전파과정에서 Gradiant를 도출할 때 사용된다.</p>\n<p>기본적으로 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.</p>\n<p><img src=\"https://i.imgur.com/4eSVZW0.png\"></p>\n<center><b>그림 3. Chain Rule 예시</b></center>\n\n\n<p><strong>가중치 업데이트</strong></p>\n<p>도출된 값을 learning rate와 곱해서 기존 가중치에서 빼주면 새로운 가중치는 다음과 같이 나타낼 수 있다.</p>\n<p>$$\\theta_{j}&#x3D;\\theta_{j}-\\eta \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$$</p>\n<p><strong>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</strong></p>\n<h3 id=\"신경망-학습-알고리즘-절차-정리\"><a href=\"#신경망-학습-알고리즘-절차-정리\" class=\"headerlink\" title=\"신경망 학습 알고리즘 절차 정리\"></a>신경망 학습 알고리즘 절차 정리</h3><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>\n<ol>\n<li><p>학습할 신경망 구조를 선택</p>\n<ul>\n<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>\n<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>\n<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>\n<li>hyperparameter의 영역이다. </li>\n<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>가중치 초기화</p>\n</li>\n<li><p>순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>\n<ul>\n<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\\theta)$를 계산</li>\n</ul>\n</li>\n<li><p>역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산</p>\n</li>\n<li><p>optimizer를 통해 loss function을 최소화</p>\n</li>\n<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>\n<ul>\n<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>\n<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>\n<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"머신러닝에서의-학습\"><a href=\"#머신러닝에서의-학습\" class=\"headerlink\" title=\"머신러닝에서의 학습\"></a>머신러닝에서의 학습</h3><p>미분은 순간의 변화율을 구하는 것이다.<br><strong>역전파는 모형에서 계산한 예측값과 실제값의 차이를 바탕으로 미분을 통해 가중치를 보정하는 것을 최대한 반복해서 수행하는 것이다.</strong><br>구체적으로는 손실함수의 국소적 미분값(local deravitive)를 구해서 학습률과 곱한 값을 기존 가중치에서 빼주는 것을 손실함수가 최소가 될 때까지 반복하는 것이다.<br><code>손실함수의 각 피처에 대한 편미분을 벡터로 묶은 것을 그래디언트(Gradient)라고 부른다.</code><br>결국 역전파는 Gradiant를 조정하는 것을 반복하는 것이라고 볼 수 있다.<br>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</p>\n<h2 id=\"신경망-학습-알고리즘-절차-정리-1\"><a href=\"#신경망-학습-알고리즘-절차-정리-1\" class=\"headerlink\" title=\"신경망 학습 알고리즘 절차 정리\"></a>신경망 학습 알고리즘 절차 정리</h2><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>\n<ol start=\"0\">\n<li><p>학습할 신경망 구조를 선택</p>\n<ul>\n<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>\n<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>\n<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>\n<li>hyperparameter의 영역이다. </li>\n<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>가중치 초기화</p>\n</li>\n<li><p>순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>\n<ul>\n<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\\theta)$를 계산</li>\n</ul>\n</li>\n<li><p>역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산</p>\n</li>\n<li><p>optimizer를 통해 loss function을 최소화</p>\n</li>\n<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>\n<ul>\n<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>\n<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>\n<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\">https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd</a></li>\n<li><a href=\"https://edgeaiguru.com/Feedforward-and-Backpropagation\">https://edgeaiguru.com/Feedforward-and-Backpropagation</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nhttps://edgeaiguru.com/Feedforward-and-Backpropagation\n# \n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면<br>역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다. 간단히 역전파의 컨셉을 알아보자.</strong></p>\n<hr>\n<h2 id=\"역전파-알고리즘\"><a href=\"#역전파-알고리즘\" class=\"headerlink\" title=\"역전파 알고리즘\"></a>역전파 알고리즘</h2><p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면 역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다.</strong></p>\n<p>역전파 알고리즘에서 가중치를 업데이트 한다는 것은 가중치 매개변수의 기울기(Graidant)를 예측값을 바탕으로 다시 계산한다는 것이다.</p>\n<p>기본적으로 <strong>타겟과 예측값의 차이를 줄이기 위해</strong> 가중치를 업데이트한다.</p>\n<p>$$<br> y &#x3D; activiate(\\sum(\\theta_{1}x_{1} + \\theta_{2}x_{2} + … + \\theta_{n}x_{n}) + bias)$$</p>\n<p>타겟과 예측값의 차이는 <code>loss function(cost function)</code>이라고 볼 수 있다.</p>\n<p>비용함수는 수식으로 나타내면 아래와 같다.</p>\n<p>$$J(\\theta) &#x3D; y - h_\\theta(x)$$<br>여기서 $h_\\theta(x)$ 는 가설함수(모형)이다.</p>\n<p>역전파(backward Propagation)는 예측값의 국소적 미분값을 순전파(Forward Propagation)의 반대방향으로 곱한 후 다음노드로 전달하는 것이다.</p>\n<p>비용함수의 편미분을 통해 기울기를 구하는 이유는 <strong>기울기가 비용함수의 값을 최소화 하는 방향을 제시하기 때문이다.</strong></p>\n<p><img src=\"https://i.imgur.com/Olxv64J.png\"></p>\n<center><b>그림 1. Gradiant Boosting을 통한 global optimum 찾기</b></center>\n\n\n<p>만약 모형의 loss function이 MSE(Mear Sqared Error)일 경우, 비용함수는 아래와 같이 나타낼 수 있다. (m은 sample의 수를 의미)</p>\n<p>$$ J(\\theta)&#x3D;\\frac{1}{2 m} \\sum_{i&#x3D;1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$</p>\n<p>이를 미분할 경우 직접 계산하기 어렵거나 불가능하기 때문에 Chain Rule을 사용한 합성함수의 편미분을 통해 구한다.</p>\n<p>아래와 같은 신경망이 있고 output node의 활성화함수는 sigmoid라고 할 경우</p>\n<p><img src=\"https://i.imgur.com/bGCvYVJ.png\"></p>\n<center><b>그림 2. 신경망으로 나타낸 backpropagation</b></center>\n\n<p>하나의 가중치에 대한 Gradiant를 아래와 같이 나타날 수 있다.</p>\n<p>$$ Gradiant&#x3D;\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}}&#x3D;\\frac{\\frac{1}{2 m} \\sum_{i&#x3D;1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)} {\\partial \\theta_{i}}$$</p>\n<p>이 때 분자는 가중치 $\\theta_i$에 대해 미분할 수 없기 때문에 아래와 같이 <code>Chain Rule</code> 을 사용해서 Gradiant를 도출한다.</p>\n<p>$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{i}} &#x3D;\\frac{\\partial J(\\theta)}{\\partial z_{2}} \\times \\frac{\\partial z_{2}}{\\partial s_{2}} \\times \\frac{\\partial s_{2}}{\\partial \\theta_{i}}  $$</p>\n<p><strong>chain rule(연쇄법칙)</strong><br>chain rule은 합성함수의 미분규칙이며 역전파과정에서 Gradiant를 도출할 때 사용된다.</p>\n<p>기본적으로 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.</p>\n<p><img src=\"https://i.imgur.com/4eSVZW0.png\"></p>\n<center><b>그림 3. Chain Rule 예시</b></center>\n\n\n<p><strong>가중치 업데이트</strong></p>\n<p>도출된 값을 learning rate와 곱해서 기존 가중치에서 빼주면 새로운 가중치는 다음과 같이 나타낼 수 있다.</p>\n<p>$$\\theta_{j}&#x3D;\\theta_{j}-\\eta \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)$$</p>\n<p><strong>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</strong></p>\n<h3 id=\"신경망-학습-알고리즘-절차-정리\"><a href=\"#신경망-학습-알고리즘-절차-정리\" class=\"headerlink\" title=\"신경망 학습 알고리즘 절차 정리\"></a>신경망 학습 알고리즘 절차 정리</h3><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>\n<ol>\n<li><p>학습할 신경망 구조를 선택</p>\n<ul>\n<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>\n<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>\n<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>\n<li>hyperparameter의 영역이다. </li>\n<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>가중치 초기화</p>\n</li>\n<li><p>순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>\n<ul>\n<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\\theta)$를 계산</li>\n</ul>\n</li>\n<li><p>역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산</p>\n</li>\n<li><p>optimizer를 통해 loss function을 최소화</p>\n</li>\n<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>\n<ul>\n<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>\n<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>\n<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"머신러닝에서의-학습\"><a href=\"#머신러닝에서의-학습\" class=\"headerlink\" title=\"머신러닝에서의 학습\"></a>머신러닝에서의 학습</h3><p>미분은 순간의 변화율을 구하는 것이다.<br><strong>역전파는 모형에서 계산한 예측값과 실제값의 차이를 바탕으로 미분을 통해 가중치를 보정하는 것을 최대한 반복해서 수행하는 것이다.</strong><br>구체적으로는 손실함수의 국소적 미분값(local deravitive)를 구해서 학습률과 곱한 값을 기존 가중치에서 빼주는 것을 손실함수가 최소가 될 때까지 반복하는 것이다.<br><code>손실함수의 각 피처에 대한 편미분을 벡터로 묶은 것을 그래디언트(Gradient)라고 부른다.</code><br>결국 역전파는 Gradiant를 조정하는 것을 반복하는 것이라고 볼 수 있다.<br>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</p>\n<h2 id=\"신경망-학습-알고리즘-절차-정리-1\"><a href=\"#신경망-학습-알고리즘-절차-정리-1\" class=\"headerlink\" title=\"신경망 학습 알고리즘 절차 정리\"></a>신경망 학습 알고리즘 절차 정리</h2><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>\n<ol start=\"0\">\n<li><p>학습할 신경망 구조를 선택</p>\n<ul>\n<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>\n<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>\n<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>\n<li>hyperparameter의 영역이다. </li>\n<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>가중치 초기화</p>\n</li>\n<li><p>순방향 전파를 통해 $h_{\\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>\n<ul>\n<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\\theta)$를 계산</li>\n</ul>\n</li>\n<li><p>역방향 전파를 통해 편미분 값들 $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$ 을 계산</p>\n</li>\n<li><p>optimizer를 통해 loss function을 최소화</p>\n</li>\n<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>\n<ul>\n<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>\n<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>\n<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd\">https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd</a></li>\n<li><a href=\"https://edgeaiguru.com/Feedforward-and-Backpropagation\">https://edgeaiguru.com/Feedforward-and-Backpropagation</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://i.imgur.com/Olxv64J.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Neural Network]역전파 알고리즘(backpropagation)","path":"2021/09/28/DL-backpropagation/","eyeCatchImage":"https://i.imgur.com/Olxv64J.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-09-28T07:59:23.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-09-28T07:59:23.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Neural Network]하이퍼파라미터","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**hyperparameter에 대해 알아보고 딥러닝에서 쓸 수 있는 hyperparameter tuning option에 대해 알아본다.**\n\n- training epoch\n- batch_size\n- learning rate\n- optimization algorithms\n- momentum\n- activate function\n\n---\n\n## References","source":"_posts/DL-hyperparameter.md","raw":"---\ntitle: '[Neural Network]하이퍼파라미터'\ncategories:\n  - Neural Network\ntags:\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**hyperparameter에 대해 알아보고 딥러닝에서 쓸 수 있는 hyperparameter tuning option에 대해 알아본다.**\n\n- training epoch\n- batch_size\n- learning rate\n- optimization algorithms\n- momentum\n- activate function\n\n---\n\n## References","slug":"DL-hyperparameter","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc2000gb36q3xefcx97","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>hyperparameter에 대해 알아보고 딥러닝에서 쓸 수 있는 hyperparameter tuning option에 대해 알아본다.</strong></p>\n<ul>\n<li>training epoch</li>\n<li>batch_size</li>\n<li>learning rate</li>\n<li>optimization algorithms</li>\n<li>momentum</li>\n<li>activate function</li>\n</ul>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>hyperparameter에 대해 알아보고 딥러닝에서 쓸 수 있는 hyperparameter tuning option에 대해 알아본다.</strong></p>\n<ul>\n<li>training epoch</li>\n<li>batch_size</li>\n<li>learning rate</li>\n<li>optimization algorithms</li>\n<li>momentum</li>\n<li>activate function</li>\n</ul>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Neural Network]하이퍼파라미터","path":"2022/06/13/DL-hyperparameter/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Deep Learning]Loss function","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n\n\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","source":"_posts/DL-lossfunction.md","raw":"---\ntitle: '[Deep Learning]Loss function'\ncategories:\n  - [Neural Network]\ntags:\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n\n\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","slug":"DL-lossfunction","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc3000ib36q3erihpe8","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n\n\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n\n\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Deep Learning]Loss function","path":"2022/06/13/DL-lossfunction/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Deep Learning]Optimizer","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\noptimizer란 \n\noptimizer 종류\n\noptimizer 선택 방법론\n\n구현\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","source":"_posts/DL-optimizer.md","raw":"---\ntitle: '[Deep Learning]Optimizer'\ncategories:\n  - Neural Network\ntags:\n  - Optimizer\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\noptimizer란 \n\noptimizer 종류\n\noptimizer 선택 방법론\n\n구현\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","slug":"DL-optimizer","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc4000nb36q6n427ulm","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\noptimizer란 \n\noptimizer 종류\n\noptimizer 선택 방법론\n\n구현\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\noptimizer란 \n\noptimizer 종류\n\noptimizer 선택 방법론\n\n구현\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Deep Learning]Optimizer","path":"2022/06/13/DL-optimizer/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":["Optimizer"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Neural Network]Perceptron의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n**Perceptron은 딥러닝의 기초가 되는 알고리즘이다. 개념은 간단하지만 추후 나올 다른 신경망의 기원이 되는 알고리즘인 만큼 정확히 알 필요가 있다. 여기서는 최적화 관점에서의 Perceptron을 다룬다.**\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**Perceptron은 딥러닝의 기초가 되는 중요한 알고리즘이지만 Percepton 자체는 그냥 인풋을 두 개의 클래스로 분류하는 이진분류에 속한다.\n최적화 관점에서의 Perceptron을 알아보자**\n\n- [Perceptron](#perceptron)\n  - [최적화 관점에서의 Perceptron](#최적화-관점에서의-perceptron)\n  - [Perception as Linear Binary Classifier](#perception-as-linear-binary-classifier)\n- [논리게이트](#논리게이트)\n- [활성화 함수](#활성화-함수)\n  - [Step Function](#step-function)\n  - [Sigmoid Function](#sigmoid-function)\n  - [RelU Function](#relu-function)\n- [Python으로 Perceptron 구현하기](#python으로-perceptron-구현하기)\n- [정리](#정리)\n- [References](#references)\n\n---\n\n## Perceptron\n\nPerceptron은 **여러 신호를 입력으로 받아 하나의 신호를 출력하는 일종의 뉴런**이다. 생물학에서 이야기하는 그 뉴런의 컨셉을 생각하면 이해가 쉽다.\n\n\n- 한개의 뉴런으로 여러 입력신호(x0, x1, ...)가 입력되면 각각 고유한 가중치(weights, w0, w1, ...)가 곱해지고 더해진다.\n- 편향(bias)\n- 가중치가 곱해진 값들은 모두 더해져 정해진 임계값(threshold)을 넘을 경우에만 다음 노드들이 있는 층(layer)으로 신호가 전해진다.\n\n\n아래 그림을 통해 기본적인 퍼셉트론 노드의 구조를 쉽게 이해할 수있다.\n\n\n![](https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png)\n\n  \n<center><b>그림1. Perceptron 구조</b></center>\n\n\n**Perceptron은 정확히 말하면 Perceptron Learning Algorithm인데 이름에서 보다시피 알고리즘의 일종이다.**\n\n\n기본적으로 SVM이나 Decision tree 처럼 학습을 하는 알고리즘이기 때문에 최적화 문제랑 같이보면 이해가 쉽다.\n\n\n![](2021-08-10-10-47-38.png)\n\n1. 가중치(w) 를 \n2. 가중치를 업데이트한다.\n   \n   가중치\n\n\n\\begin{align}\n y =activation(\\sum(w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n}) + bias)\n\\end{align}\n \n\n\n![](ML-DL-perceptron/perceptron_al.png)\n<center><b>그림2. Perceptron Learning Algorithm</b></center>\n\n\n### 최적화 관점에서의 Perceptron\n\n최적화 관점으로 생각하면 Perceptron 또한 비용함수인 $f(h)-y$ 를 최소화 하는 목적을 가지고 있다고 볼 수 있다.\n\n\nPerceptron 알고리즘에서의 비욯함수는 `0-1 loss`인데 이는 단순히 잘못된 예측에 대해 1의 패널티를 부여하고 제대로된 예측은 그대로 놔두는 것이다.\n\n이를 수식으로 나타내면 아래와 같다\n\n$L(\\hat{y}, y) = I(\\hat{y} \\ne y)$ \n\n(여기서 I 는 indicator 함수로 0아니면 1의 결과를 반환한다.)\n\n이러한 비용함수의 문제는 gradient descent 를 사용해 국소최적해(local optimum) 를 찾기 어렵다는 것이다.\n\n\n### Perception as Linear Binary Classifier\nPerceptron의 컨셉을 다시 살펴보면 \n\n\n## 논리게이트\n\n\n## 활성화 함수\n\n활성화 함수는 입력벡터와 가중치벡터의 가중합인 net input을 받아 출력신호를 도출해내는 함수이다.\n\n### Step Function\n\n### Sigmoid Function\n\n### RelU Function\n\n\n## Python으로 Perceptron 구현하기\n\n간단한 Perceptron을 Python으로 구현해보자\n\n```\n\n\n```\n\n## 정리\n\n- 퍼셉트론은 입출력을 가지는 일종의 알고리즘이다. 입력에 따라 정해진 규칙에 따른 값을 반환한다.\n- 퍼셉트론은 가중치와 편향을 매개변수로 지정한다.\n  - 가중치\n  - 편향\n- 퍼셉트론으로 논리회로를 표현할 수 있다.\n- XOR 게이트의 경우 단층 퍼셉트론을 사\n- 퍼셉트론은 샘플을 입력받아 가중치 w를 연결하여 net input을 계산한다. \n- net input은 입력벡터와 가중치 벡터의 내적이다. \n- net input은 activation 함수로 전달되어\n- \n## References\n\n- https://en.wikipedia.org/wiki/Perceptron\n- https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975","source":"_posts/DL-perceptron.md","raw":"---\ntitle: '[Neural Network]Perceptron의 이해'\ncategories:\n  - [Neural Network]\ntags:\ndate:\nupdated:\n---\n\n<!--\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n**Perceptron은 딥러닝의 기초가 되는 알고리즘이다. 개념은 간단하지만 추후 나올 다른 신경망의 기원이 되는 알고리즘인 만큼 정확히 알 필요가 있다. 여기서는 최적화 관점에서의 Perceptron을 다룬다.**\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**Perceptron은 딥러닝의 기초가 되는 중요한 알고리즘이지만 Percepton 자체는 그냥 인풋을 두 개의 클래스로 분류하는 이진분류에 속한다.\n최적화 관점에서의 Perceptron을 알아보자**\n\n- [Perceptron](#perceptron)\n  - [최적화 관점에서의 Perceptron](#최적화-관점에서의-perceptron)\n  - [Perception as Linear Binary Classifier](#perception-as-linear-binary-classifier)\n- [논리게이트](#논리게이트)\n- [활성화 함수](#활성화-함수)\n  - [Step Function](#step-function)\n  - [Sigmoid Function](#sigmoid-function)\n  - [RelU Function](#relu-function)\n- [Python으로 Perceptron 구현하기](#python으로-perceptron-구현하기)\n- [정리](#정리)\n- [References](#references)\n\n---\n\n## Perceptron\n\nPerceptron은 **여러 신호를 입력으로 받아 하나의 신호를 출력하는 일종의 뉴런**이다. 생물학에서 이야기하는 그 뉴런의 컨셉을 생각하면 이해가 쉽다.\n\n\n- 한개의 뉴런으로 여러 입력신호(x0, x1, ...)가 입력되면 각각 고유한 가중치(weights, w0, w1, ...)가 곱해지고 더해진다.\n- 편향(bias)\n- 가중치가 곱해진 값들은 모두 더해져 정해진 임계값(threshold)을 넘을 경우에만 다음 노드들이 있는 층(layer)으로 신호가 전해진다.\n\n\n아래 그림을 통해 기본적인 퍼셉트론 노드의 구조를 쉽게 이해할 수있다.\n\n\n![](https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png)\n\n  \n<center><b>그림1. Perceptron 구조</b></center>\n\n\n**Perceptron은 정확히 말하면 Perceptron Learning Algorithm인데 이름에서 보다시피 알고리즘의 일종이다.**\n\n\n기본적으로 SVM이나 Decision tree 처럼 학습을 하는 알고리즘이기 때문에 최적화 문제랑 같이보면 이해가 쉽다.\n\n\n![](2021-08-10-10-47-38.png)\n\n1. 가중치(w) 를 \n2. 가중치를 업데이트한다.\n   \n   가중치\n\n\n\\begin{align}\n y =activation(\\sum(w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n}) + bias)\n\\end{align}\n \n\n\n![](ML-DL-perceptron/perceptron_al.png)\n<center><b>그림2. Perceptron Learning Algorithm</b></center>\n\n\n### 최적화 관점에서의 Perceptron\n\n최적화 관점으로 생각하면 Perceptron 또한 비용함수인 $f(h)-y$ 를 최소화 하는 목적을 가지고 있다고 볼 수 있다.\n\n\nPerceptron 알고리즘에서의 비욯함수는 `0-1 loss`인데 이는 단순히 잘못된 예측에 대해 1의 패널티를 부여하고 제대로된 예측은 그대로 놔두는 것이다.\n\n이를 수식으로 나타내면 아래와 같다\n\n$L(\\hat{y}, y) = I(\\hat{y} \\ne y)$ \n\n(여기서 I 는 indicator 함수로 0아니면 1의 결과를 반환한다.)\n\n이러한 비용함수의 문제는 gradient descent 를 사용해 국소최적해(local optimum) 를 찾기 어렵다는 것이다.\n\n\n### Perception as Linear Binary Classifier\nPerceptron의 컨셉을 다시 살펴보면 \n\n\n## 논리게이트\n\n\n## 활성화 함수\n\n활성화 함수는 입력벡터와 가중치벡터의 가중합인 net input을 받아 출력신호를 도출해내는 함수이다.\n\n### Step Function\n\n### Sigmoid Function\n\n### RelU Function\n\n\n## Python으로 Perceptron 구현하기\n\n간단한 Perceptron을 Python으로 구현해보자\n\n```\n\n\n```\n\n## 정리\n\n- 퍼셉트론은 입출력을 가지는 일종의 알고리즘이다. 입력에 따라 정해진 규칙에 따른 값을 반환한다.\n- 퍼셉트론은 가중치와 편향을 매개변수로 지정한다.\n  - 가중치\n  - 편향\n- 퍼셉트론으로 논리회로를 표현할 수 있다.\n- XOR 게이트의 경우 단층 퍼셉트론을 사\n- 퍼셉트론은 샘플을 입력받아 가중치 w를 연결하여 net input을 계산한다. \n- net input은 입력벡터와 가중치 벡터의 내적이다. \n- net input은 activation 함수로 전달되어\n- \n## References\n\n- https://en.wikipedia.org/wiki/Perceptron\n- https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975","slug":"DL-perceptron","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc4000qb36q8tbc0zaq","content":"<!--\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n**Perceptron은 딥러닝의 기초가 되는 알고리즘이다. 개념은 간단하지만 추후 나올 다른 신경망의 기원이 되는 알고리즘인 만큼 정확히 알 필요가 있다. 여기서는 최적화 관점에서의 Perceptron을 다룬다.**\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>Perceptron은 딥러닝의 기초가 되는 중요한 알고리즘이지만 Percepton 자체는 그냥 인풋을 두 개의 클래스로 분류하는 이진분류에 속한다.<br>최적화 관점에서의 Perceptron을 알아보자</strong></p>\n<ul>\n<li><a href=\"#perceptron\">Perceptron</a><ul>\n<li><a href=\"#%EC%B5%9C%EC%A0%81%ED%99%94-%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%EC%9D%98-perceptron\">최적화 관점에서의 Perceptron</a></li>\n<li><a href=\"#perception-as-linear-binary-classifier\">Perception as Linear Binary Classifier</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%85%BC%EB%A6%AC%EA%B2%8C%EC%9D%B4%ED%8A%B8\">논리게이트</a></li>\n<li><a href=\"#%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98\">활성화 함수</a><ul>\n<li><a href=\"#step-function\">Step Function</a></li>\n<li><a href=\"#sigmoid-function\">Sigmoid Function</a></li>\n<li><a href=\"#relu-function\">RelU Function</a></li>\n</ul>\n</li>\n<li><a href=\"#python%EC%9C%BC%EB%A1%9C-perceptron-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0\">Python으로 Perceptron 구현하기</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n<hr>\n<h2 id=\"Perceptron\"><a href=\"#Perceptron\" class=\"headerlink\" title=\"Perceptron\"></a>Perceptron</h2><p>Perceptron은 <strong>여러 신호를 입력으로 받아 하나의 신호를 출력하는 일종의 뉴런</strong>이다. 생물학에서 이야기하는 그 뉴런의 컨셉을 생각하면 이해가 쉽다.</p>\n<ul>\n<li>한개의 뉴런으로 여러 입력신호(x0, x1, …)가 입력되면 각각 고유한 가중치(weights, w0, w1, …)가 곱해지고 더해진다.</li>\n<li>편향(bias)</li>\n<li>가중치가 곱해진 값들은 모두 더해져 정해진 임계값(threshold)을 넘을 경우에만 다음 노드들이 있는 층(layer)으로 신호가 전해진다.</li>\n</ul>\n<p>아래 그림을 통해 기본적인 퍼셉트론 노드의 구조를 쉽게 이해할 수있다.</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png\"></p>\n<center><b>그림1. Perceptron 구조</b></center>\n\n\n<p><strong>Perceptron은 정확히 말하면 Perceptron Learning Algorithm인데 이름에서 보다시피 알고리즘의 일종이다.</strong></p>\n<p>기본적으로 SVM이나 Decision tree 처럼 학습을 하는 알고리즘이기 때문에 최적화 문제랑 같이보면 이해가 쉽다.</p>\n<p><img src=\"/2021-08-10-10-47-38.png\"></p>\n<ol>\n<li><p>가중치(w) 를 </p>\n</li>\n<li><p>가중치를 업데이트한다.</p>\n<p>가중치</p>\n</li>\n</ol>\n<p>\\begin{align}<br> y &#x3D;activation(\\sum(w_{1}x_{1} + w_{2}x_{2} + … + w_{n}x_{n}) + bias)<br>\\end{align}</p>\n<p><img src=\"/ML-DL-perceptron/perceptron_al.png\"></p>\n<center><b>그림2. Perceptron Learning Algorithm</b></center>\n\n\n<h3 id=\"최적화-관점에서의-Perceptron\"><a href=\"#최적화-관점에서의-Perceptron\" class=\"headerlink\" title=\"최적화 관점에서의 Perceptron\"></a>최적화 관점에서의 Perceptron</h3><p>최적화 관점으로 생각하면 Perceptron 또한 비용함수인 $f(h)-y$ 를 최소화 하는 목적을 가지고 있다고 볼 수 있다.</p>\n<p>Perceptron 알고리즘에서의 비욯함수는 <code>0-1 loss</code>인데 이는 단순히 잘못된 예측에 대해 1의 패널티를 부여하고 제대로된 예측은 그대로 놔두는 것이다.</p>\n<p>이를 수식으로 나타내면 아래와 같다</p>\n<p>$L(\\hat{y}, y) &#x3D; I(\\hat{y} \\ne y)$ </p>\n<p>(여기서 I 는 indicator 함수로 0아니면 1의 결과를 반환한다.)</p>\n<p>이러한 비용함수의 문제는 gradient descent 를 사용해 국소최적해(local optimum) 를 찾기 어렵다는 것이다.</p>\n<h3 id=\"Perception-as-Linear-Binary-Classifier\"><a href=\"#Perception-as-Linear-Binary-Classifier\" class=\"headerlink\" title=\"Perception as Linear Binary Classifier\"></a>Perception as Linear Binary Classifier</h3><p>Perceptron의 컨셉을 다시 살펴보면 </p>\n<h2 id=\"논리게이트\"><a href=\"#논리게이트\" class=\"headerlink\" title=\"논리게이트\"></a>논리게이트</h2><h2 id=\"활성화-함수\"><a href=\"#활성화-함수\" class=\"headerlink\" title=\"활성화 함수\"></a>활성화 함수</h2><p>활성화 함수는 입력벡터와 가중치벡터의 가중합인 net input을 받아 출력신호를 도출해내는 함수이다.</p>\n<h3 id=\"Step-Function\"><a href=\"#Step-Function\" class=\"headerlink\" title=\"Step Function\"></a>Step Function</h3><h3 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h3><h3 id=\"RelU-Function\"><a href=\"#RelU-Function\" class=\"headerlink\" title=\"RelU Function\"></a>RelU Function</h3><h2 id=\"Python으로-Perceptron-구현하기\"><a href=\"#Python으로-Perceptron-구현하기\" class=\"headerlink\" title=\"Python으로 Perceptron 구현하기\"></a>Python으로 Perceptron 구현하기</h2><p>간단한 Perceptron을 Python으로 구현해보자</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"정리\"><a href=\"#정리\" class=\"headerlink\" title=\"정리\"></a>정리</h2><ul>\n<li><p>퍼셉트론은 입출력을 가지는 일종의 알고리즘이다. 입력에 따라 정해진 규칙에 따른 값을 반환한다.</p>\n</li>\n<li><p>퍼셉트론은 가중치와 편향을 매개변수로 지정한다.</p>\n<ul>\n<li>가중치</li>\n<li>편향</li>\n</ul>\n</li>\n<li><p>퍼셉트론으로 논리회로를 표현할 수 있다.</p>\n</li>\n<li><p>XOR 게이트의 경우 단층 퍼셉트론을 사</p>\n</li>\n<li><p>퍼셉트론은 샘플을 입력받아 가중치 w를 연결하여 net input을 계산한다. </p>\n</li>\n<li><p>net input은 입력벡터와 가중치 벡터의 내적이다. </p>\n</li>\n<li><p>net input은 activation 함수로 전달되어</p>\n</li>\n<li><h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2></li>\n<li><p><a href=\"https://en.wikipedia.org/wiki/Perceptron\">https://en.wikipedia.org/wiki/Perceptron</a></p>\n</li>\n<li><p><a href=\"https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\">https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975</a></p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n**Perceptron은 딥러닝의 기초가 되는 알고리즘이다. 개념은 간단하지만 추후 나올 다른 신경망의 기원이 되는 알고리즘인 만큼 정확히 알 필요가 있다. 여기서는 최적화 관점에서의 Perceptron을 다룬다.**\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>Perceptron은 딥러닝의 기초가 되는 중요한 알고리즘이지만 Percepton 자체는 그냥 인풋을 두 개의 클래스로 분류하는 이진분류에 속한다.<br>최적화 관점에서의 Perceptron을 알아보자</strong></p>\n<ul>\n<li><a href=\"#perceptron\">Perceptron</a><ul>\n<li><a href=\"#%EC%B5%9C%EC%A0%81%ED%99%94-%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%EC%9D%98-perceptron\">최적화 관점에서의 Perceptron</a></li>\n<li><a href=\"#perception-as-linear-binary-classifier\">Perception as Linear Binary Classifier</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%85%BC%EB%A6%AC%EA%B2%8C%EC%9D%B4%ED%8A%B8\">논리게이트</a></li>\n<li><a href=\"#%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98\">활성화 함수</a><ul>\n<li><a href=\"#step-function\">Step Function</a></li>\n<li><a href=\"#sigmoid-function\">Sigmoid Function</a></li>\n<li><a href=\"#relu-function\">RelU Function</a></li>\n</ul>\n</li>\n<li><a href=\"#python%EC%9C%BC%EB%A1%9C-perceptron-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0\">Python으로 Perceptron 구현하기</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n<hr>\n<h2 id=\"Perceptron\"><a href=\"#Perceptron\" class=\"headerlink\" title=\"Perceptron\"></a>Perceptron</h2><p>Perceptron은 <strong>여러 신호를 입력으로 받아 하나의 신호를 출력하는 일종의 뉴런</strong>이다. 생물학에서 이야기하는 그 뉴런의 컨셉을 생각하면 이해가 쉽다.</p>\n<ul>\n<li>한개의 뉴런으로 여러 입력신호(x0, x1, …)가 입력되면 각각 고유한 가중치(weights, w0, w1, …)가 곱해지고 더해진다.</li>\n<li>편향(bias)</li>\n<li>가중치가 곱해진 값들은 모두 더해져 정해진 임계값(threshold)을 넘을 경우에만 다음 노드들이 있는 층(layer)으로 신호가 전해진다.</li>\n</ul>\n<p>아래 그림을 통해 기본적인 퍼셉트론 노드의 구조를 쉽게 이해할 수있다.</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png\"></p>\n<center><b>그림1. Perceptron 구조</b></center>\n\n\n<p><strong>Perceptron은 정확히 말하면 Perceptron Learning Algorithm인데 이름에서 보다시피 알고리즘의 일종이다.</strong></p>\n<p>기본적으로 SVM이나 Decision tree 처럼 학습을 하는 알고리즘이기 때문에 최적화 문제랑 같이보면 이해가 쉽다.</p>\n<p><img src=\"/2021-08-10-10-47-38.png\"></p>\n<ol>\n<li><p>가중치(w) 를 </p>\n</li>\n<li><p>가중치를 업데이트한다.</p>\n<p>가중치</p>\n</li>\n</ol>\n<p>\\begin{align}<br> y &#x3D;activation(\\sum(w_{1}x_{1} + w_{2}x_{2} + … + w_{n}x_{n}) + bias)<br>\\end{align}</p>\n<p><img src=\"/ML-DL-perceptron/perceptron_al.png\"></p>\n<center><b>그림2. Perceptron Learning Algorithm</b></center>\n\n\n<h3 id=\"최적화-관점에서의-Perceptron\"><a href=\"#최적화-관점에서의-Perceptron\" class=\"headerlink\" title=\"최적화 관점에서의 Perceptron\"></a>최적화 관점에서의 Perceptron</h3><p>최적화 관점으로 생각하면 Perceptron 또한 비용함수인 $f(h)-y$ 를 최소화 하는 목적을 가지고 있다고 볼 수 있다.</p>\n<p>Perceptron 알고리즘에서의 비욯함수는 <code>0-1 loss</code>인데 이는 단순히 잘못된 예측에 대해 1의 패널티를 부여하고 제대로된 예측은 그대로 놔두는 것이다.</p>\n<p>이를 수식으로 나타내면 아래와 같다</p>\n<p>$L(\\hat{y}, y) &#x3D; I(\\hat{y} \\ne y)$ </p>\n<p>(여기서 I 는 indicator 함수로 0아니면 1의 결과를 반환한다.)</p>\n<p>이러한 비용함수의 문제는 gradient descent 를 사용해 국소최적해(local optimum) 를 찾기 어렵다는 것이다.</p>\n<h3 id=\"Perception-as-Linear-Binary-Classifier\"><a href=\"#Perception-as-Linear-Binary-Classifier\" class=\"headerlink\" title=\"Perception as Linear Binary Classifier\"></a>Perception as Linear Binary Classifier</h3><p>Perceptron의 컨셉을 다시 살펴보면 </p>\n<h2 id=\"논리게이트\"><a href=\"#논리게이트\" class=\"headerlink\" title=\"논리게이트\"></a>논리게이트</h2><h2 id=\"활성화-함수\"><a href=\"#활성화-함수\" class=\"headerlink\" title=\"활성화 함수\"></a>활성화 함수</h2><p>활성화 함수는 입력벡터와 가중치벡터의 가중합인 net input을 받아 출력신호를 도출해내는 함수이다.</p>\n<h3 id=\"Step-Function\"><a href=\"#Step-Function\" class=\"headerlink\" title=\"Step Function\"></a>Step Function</h3><h3 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h3><h3 id=\"RelU-Function\"><a href=\"#RelU-Function\" class=\"headerlink\" title=\"RelU Function\"></a>RelU Function</h3><h2 id=\"Python으로-Perceptron-구현하기\"><a href=\"#Python으로-Perceptron-구현하기\" class=\"headerlink\" title=\"Python으로 Perceptron 구현하기\"></a>Python으로 Perceptron 구현하기</h2><p>간단한 Perceptron을 Python으로 구현해보자</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"정리\"><a href=\"#정리\" class=\"headerlink\" title=\"정리\"></a>정리</h2><ul>\n<li><p>퍼셉트론은 입출력을 가지는 일종의 알고리즘이다. 입력에 따라 정해진 규칙에 따른 값을 반환한다.</p>\n</li>\n<li><p>퍼셉트론은 가중치와 편향을 매개변수로 지정한다.</p>\n<ul>\n<li>가중치</li>\n<li>편향</li>\n</ul>\n</li>\n<li><p>퍼셉트론으로 논리회로를 표현할 수 있다.</p>\n</li>\n<li><p>XOR 게이트의 경우 단층 퍼셉트론을 사</p>\n</li>\n<li><p>퍼셉트론은 샘플을 입력받아 가중치 w를 연결하여 net input을 계산한다. </p>\n</li>\n<li><p>net input은 입력벡터와 가중치 벡터의 내적이다. </p>\n</li>\n<li><p>net input은 activation 함수로 전달되어</p>\n</li>\n<li><h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2></li>\n<li><p><a href=\"https://en.wikipedia.org/wiki/Perceptron\">https://en.wikipedia.org/wiki/Perceptron</a></p>\n</li>\n<li><p><a href=\"https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\">https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975</a></p>\n</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Neural Network]Perceptron의 이해","path":"2022/06/13/DL-perceptron/","eyeCatchImage":"https://upload.wikimedia.org/wikipedia/commons/f/ff/Rosenblattperceptron.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Deep Leearing] 학습 규제하기(Handling Overfitting)","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**과적합 방지를 위한 기법을 규제라고 부른다.**\n\n- [Weight Decay](#weight-decay)\n- [Weight Constraint](#weight-constraint)\n- [Dropout](#dropout)\n- [[추가] 학습에 batch size가 미치는 영향](#추가-학습에-batch-size가-미치는-영향)\n- [References](#references)\n\n---\n\n## Weight Decay\n\n학습은 \n\n\n$$Loss = MSE + \\lambda \\times  * {w}^2$$\n\n\n왜 이런 조치를 하는 것이 overfitting을 줄여주는가?\n\n어딘가에 Global minimum이 존재\n\n하지만 이 Global minimum은 `학습한` training set에서만 적용\n\nRegularization 항을 추가할 경우 원점에 loss surface가 하나 더 생김\n\nGlobal minimum에 빠질 경우 overfitting 이 일어나는데 loss surface를 하나 더 만듦으로써 다른 training set에서도 해당 모형이 잘 적용될 수 있게끔하는 것이다.\n\n## Weight Constraint\n\n## Dropout\n\n##\n\n\n## [추가] 학습에 batch size가 미치는 영향\n\n## References\n\n- [batch size 관련](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e) \n- [overfitting in deeplearing models](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e)\n- [weight decay](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)\n\n<!--\n- [L1 & L2 Regularization](https://www.youtube.com/watch?v=_sz3KTyB9Lk&t=1063s)\n- [가중치 초기화 관련](https://youtu.be/ScWTYHQra5E)\n- [Dropout](https://www.youtube.com/watch?v=ajeliDMD86U)\n- [Ng 교수님의 하이퍼파라미터 설명](https://www.youtube.com/watch?v=wKkcBPp3F1Y)\n- [parameter와 hyperparameter 차이](https://youtu.be/Kh06wgGbi78?t=12)\n- [학습 규제 방식에 대한 설명 강의](https://youtu.be/_sz3KTyB9Lk?t=1005)\n- [L1/L2-regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n-->\n\n- [L1 & L2 용어정리](https://light-tree.tistory.com/125)\n","source":"_posts/DL-regularization.md","raw":"---\ntitle: '[Deep Leearing] 학습 규제하기(Handling Overfitting)'\ncategories:\n   - [Neural Network]\ntags:\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**과적합 방지를 위한 기법을 규제라고 부른다.**\n\n- [Weight Decay](#weight-decay)\n- [Weight Constraint](#weight-constraint)\n- [Dropout](#dropout)\n- [[추가] 학습에 batch size가 미치는 영향](#추가-학습에-batch-size가-미치는-영향)\n- [References](#references)\n\n---\n\n## Weight Decay\n\n학습은 \n\n\n$$Loss = MSE + \\lambda \\times  * {w}^2$$\n\n\n왜 이런 조치를 하는 것이 overfitting을 줄여주는가?\n\n어딘가에 Global minimum이 존재\n\n하지만 이 Global minimum은 `학습한` training set에서만 적용\n\nRegularization 항을 추가할 경우 원점에 loss surface가 하나 더 생김\n\nGlobal minimum에 빠질 경우 overfitting 이 일어나는데 loss surface를 하나 더 만듦으로써 다른 training set에서도 해당 모형이 잘 적용될 수 있게끔하는 것이다.\n\n## Weight Constraint\n\n## Dropout\n\n##\n\n\n## [추가] 학습에 batch size가 미치는 영향\n\n## References\n\n- [batch size 관련](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e) \n- [overfitting in deeplearing models](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e)\n- [weight decay](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)\n\n<!--\n- [L1 & L2 Regularization](https://www.youtube.com/watch?v=_sz3KTyB9Lk&t=1063s)\n- [가중치 초기화 관련](https://youtu.be/ScWTYHQra5E)\n- [Dropout](https://www.youtube.com/watch?v=ajeliDMD86U)\n- [Ng 교수님의 하이퍼파라미터 설명](https://www.youtube.com/watch?v=wKkcBPp3F1Y)\n- [parameter와 hyperparameter 차이](https://youtu.be/Kh06wgGbi78?t=12)\n- [학습 규제 방식에 대한 설명 강의](https://youtu.be/_sz3KTyB9Lk?t=1005)\n- [L1/L2-regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n-->\n\n- [L1 & L2 용어정리](https://light-tree.tistory.com/125)\n","slug":"DL-regularization","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc5000ub36qcvy7bcez","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>과적합 방지를 위한 기법을 규제라고 부른다.</strong></p>\n<ul>\n<li><a href=\"#weight-decay\">Weight Decay</a></li>\n<li><a href=\"#weight-constraint\">Weight Constraint</a></li>\n<li><a href=\"#dropout\">Dropout</a></li>\n<li><a href=\"#%EC%B6%94%EA%B0%80-%ED%95%99%EC%8A%B5%EC%97%90-batch-size%EA%B0%80-%EB%AF%B8%EC%B9%98%EB%8A%94-%EC%98%81%ED%96%A5\">[추가] 학습에 batch size가 미치는 영향</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n<hr>\n<h2 id=\"Weight-Decay\"><a href=\"#Weight-Decay\" class=\"headerlink\" title=\"Weight Decay\"></a>Weight Decay</h2><p>학습은 </p>\n<p>$$Loss &#x3D; MSE + \\lambda \\times  * {w}^2$$</p>\n<p>왜 이런 조치를 하는 것이 overfitting을 줄여주는가?</p>\n<p>어딘가에 Global minimum이 존재</p>\n<p>하지만 이 Global minimum은 <code>학습한</code> training set에서만 적용</p>\n<p>Regularization 항을 추가할 경우 원점에 loss surface가 하나 더 생김</p>\n<p>Global minimum에 빠질 경우 overfitting 이 일어나는데 loss surface를 하나 더 만듦으로써 다른 training set에서도 해당 모형이 잘 적용될 수 있게끔하는 것이다.</p>\n<h2 id=\"Weight-Constraint\"><a href=\"#Weight-Constraint\" class=\"headerlink\" title=\"Weight Constraint\"></a>Weight Constraint</h2><h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h2 id=\"추가-학습에-batch-size가-미치는-영향\"><a href=\"#추가-학습에-batch-size가-미치는-영향\" class=\"headerlink\" title=\"[추가] 학습에 batch size가 미치는 영향\"></a>[추가] 학습에 batch size가 미치는 영향</h2><h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\">batch size 관련</a> </li>\n<li><a href=\"https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\">overfitting in deeplearing models</a></li>\n<li><a href=\"https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab\">weight decay</a></li>\n</ul>\n<!--\n- [L1 & L2 Regularization](https://www.youtube.com/watch?v=_sz3KTyB9Lk&t=1063s)\n- [가중치 초기화 관련](https://youtu.be/ScWTYHQra5E)\n- [Dropout](https://www.youtube.com/watch?v=ajeliDMD86U)\n- [Ng 교수님의 하이퍼파라미터 설명](https://www.youtube.com/watch?v=wKkcBPp3F1Y)\n- [parameter와 hyperparameter 차이](https://youtu.be/Kh06wgGbi78?t=12)\n- [학습 규제 방식에 대한 설명 강의](https://youtu.be/_sz3KTyB9Lk?t=1005)\n- [L1/L2-regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n-->\n\n<ul>\n<li><a href=\"https://light-tree.tistory.com/125\">L1 &amp; L2 용어정리</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>과적합 방지를 위한 기법을 규제라고 부른다.</strong></p>\n<ul>\n<li><a href=\"#weight-decay\">Weight Decay</a></li>\n<li><a href=\"#weight-constraint\">Weight Constraint</a></li>\n<li><a href=\"#dropout\">Dropout</a></li>\n<li><a href=\"#%EC%B6%94%EA%B0%80-%ED%95%99%EC%8A%B5%EC%97%90-batch-size%EA%B0%80-%EB%AF%B8%EC%B9%98%EB%8A%94-%EC%98%81%ED%96%A5\">[추가] 학습에 batch size가 미치는 영향</a></li>\n<li><a href=\"#references\">References</a></li>\n</ul>\n<hr>\n<h2 id=\"Weight-Decay\"><a href=\"#Weight-Decay\" class=\"headerlink\" title=\"Weight Decay\"></a>Weight Decay</h2><p>학습은 </p>\n<p>$$Loss &#x3D; MSE + \\lambda \\times  * {w}^2$$</p>\n<p>왜 이런 조치를 하는 것이 overfitting을 줄여주는가?</p>\n<p>어딘가에 Global minimum이 존재</p>\n<p>하지만 이 Global minimum은 <code>학습한</code> training set에서만 적용</p>\n<p>Regularization 항을 추가할 경우 원점에 loss surface가 하나 더 생김</p>\n<p>Global minimum에 빠질 경우 overfitting 이 일어나는데 loss surface를 하나 더 만듦으로써 다른 training set에서도 해당 모형이 잘 적용될 수 있게끔하는 것이다.</p>\n<h2 id=\"Weight-Constraint\"><a href=\"#Weight-Constraint\" class=\"headerlink\" title=\"Weight Constraint\"></a>Weight Constraint</h2><h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><h2 id=\"추가-학습에-batch-size가-미치는-영향\"><a href=\"#추가-학습에-batch-size가-미치는-영향\" class=\"headerlink\" title=\"[추가] 학습에 batch size가 미치는 영향\"></a>[추가] 학습에 batch size가 미치는 영향</h2><h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\">batch size 관련</a> </li>\n<li><a href=\"https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\">overfitting in deeplearing models</a></li>\n<li><a href=\"https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab\">weight decay</a></li>\n</ul>\n<!--\n- [L1 & L2 Regularization](https://www.youtube.com/watch?v=_sz3KTyB9Lk&t=1063s)\n- [가중치 초기화 관련](https://youtu.be/ScWTYHQra5E)\n- [Dropout](https://www.youtube.com/watch?v=ajeliDMD86U)\n- [Ng 교수님의 하이퍼파라미터 설명](https://www.youtube.com/watch?v=wKkcBPp3F1Y)\n- [parameter와 hyperparameter 차이](https://youtu.be/Kh06wgGbi78?t=12)\n- [학습 규제 방식에 대한 설명 강의](https://youtu.be/_sz3KTyB9Lk?t=1005)\n- [L1/L2-regularization](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n-->\n\n<ul>\n<li><a href=\"https://light-tree.tistory.com/125\">L1 &amp; L2 용어정리</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Deep Leearing] 학습 규제하기(Handling Overfitting)","path":"2022/06/13/DL-regularization/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Neural Network","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Metrics]회귀모델의 평가지표","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**회귀모델의 평가지표 정리**\n\n### 회귀모델의 평가지표\n\n- $R^2$ 외에, MAE는 단위 유닛이 같으므로 보다 해석에 용이함.\n- MSE는 제곱을 하기 때문에 특이값에 보다 민감. \n- RMSE는 MSE를 실제값과 유사한 단위로 변화시켜줌.\n- 회귀문제에서 RMSE가 일반적으로 선호되는 방법이지만, 상황에 맞는 다른 방식을 사용. 특이값이 많은 경우에는 MAE를 사용.\n\n---\n**_Concept_**\n\n* MSE (Mean Squared Error) = \n$\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}$\n* MAE (Mean absolute error) = $\\frac{1}{n}\\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}} \\right |$\n* RMSE (Root Mean Squared Error) = \n$\\sqrt{MSE}$\n* R-squared (Coefficient of determination) = \n$1 - \\frac{\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}{\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}} = 1 - \\frac{SSE}{SST} = \\frac {SSR}{SST}$\n\n* MAPE = $\\frac { \\sum \\vert \\frac { y - \\hat y}{y} \\vert }{n}*100\\%$\n\n- 참고\n    - SSE(Sum of Squares `Error`, 관측치와 예측치 차이): $\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}$\n    - SSR(Sum of Squares due to `Regression`, 예측치와 평균 차이): $\\sum_{i=1}^{n}(\\hat{y_{i}} - \\bar{y_{i}})^{2}$\n    - SST(Sum of Squares `Total`, 관측치와 평균 차이): $\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}$ , SSE + SSR\n\n---\n\n#### MSE\n\n- 모델의 예측값과 실제값 차이의 면적의 합.\n- 특이값이 존재할 경우 수치가 많이 늘어남\n\n#### MAE\n\n- MSE 보다 특이치에 robust\n- 절대값을 취하기 때문에 매우 직관적\n\n#### RMSE\n\n- MSE의 제곱근. \n- 큰 오류값에 대해 패널티를 주기 때문에 보통 이걸 사용\n\n#### R-squared\n\n- 설명량\n- $R^2$ 값이 1에 가까울 수록 데이터를 잘 설명하는 모델이 됨\n\n#### MAPE\n\n```python\ndef MAPE(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n```\n- MAE를 퍼센트 변환한 것.\n- MAE와 마찬가지로 MSE보다 특이치에 robust하다.\n- 모델에 대한 편향이 존재.\n- 0 근처의 값에서는 사용하기 어렵습니다.\n\n#### MPE\n\n```python\ndef MAPE(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n```\n- MAPE를 퍼센트 변환한 것.\n- 절대값을 제외했기 때문에 overperformance인지 underperformance인지 쉽게 알 수 있다.\n\n\n## References\n\n\n- https://www.dataquest.io/blog/understanding-regression-error-metrics/\n- https://machinelearningmastery.com/regression-metrics-for-machine-learning/\n","source":"_posts/ML-Metrics-Regression.md","raw":"---\ntitle: '[Metrics]회귀모델의 평가지표'\ncategories:\n  - [Machine Learning]\ntags:\n  - Regression\n  - Metrics\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**회귀모델의 평가지표 정리**\n\n### 회귀모델의 평가지표\n\n- $R^2$ 외에, MAE는 단위 유닛이 같으므로 보다 해석에 용이함.\n- MSE는 제곱을 하기 때문에 특이값에 보다 민감. \n- RMSE는 MSE를 실제값과 유사한 단위로 변화시켜줌.\n- 회귀문제에서 RMSE가 일반적으로 선호되는 방법이지만, 상황에 맞는 다른 방식을 사용. 특이값이 많은 경우에는 MAE를 사용.\n\n---\n**_Concept_**\n\n* MSE (Mean Squared Error) = \n$\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}$\n* MAE (Mean absolute error) = $\\frac{1}{n}\\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}} \\right |$\n* RMSE (Root Mean Squared Error) = \n$\\sqrt{MSE}$\n* R-squared (Coefficient of determination) = \n$1 - \\frac{\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}{\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}} = 1 - \\frac{SSE}{SST} = \\frac {SSR}{SST}$\n\n* MAPE = $\\frac { \\sum \\vert \\frac { y - \\hat y}{y} \\vert }{n}*100\\%$\n\n- 참고\n    - SSE(Sum of Squares `Error`, 관측치와 예측치 차이): $\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}$\n    - SSR(Sum of Squares due to `Regression`, 예측치와 평균 차이): $\\sum_{i=1}^{n}(\\hat{y_{i}} - \\bar{y_{i}})^{2}$\n    - SST(Sum of Squares `Total`, 관측치와 평균 차이): $\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}$ , SSE + SSR\n\n---\n\n#### MSE\n\n- 모델의 예측값과 실제값 차이의 면적의 합.\n- 특이값이 존재할 경우 수치가 많이 늘어남\n\n#### MAE\n\n- MSE 보다 특이치에 robust\n- 절대값을 취하기 때문에 매우 직관적\n\n#### RMSE\n\n- MSE의 제곱근. \n- 큰 오류값에 대해 패널티를 주기 때문에 보통 이걸 사용\n\n#### R-squared\n\n- 설명량\n- $R^2$ 값이 1에 가까울 수록 데이터를 잘 설명하는 모델이 됨\n\n#### MAPE\n\n```python\ndef MAPE(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n```\n- MAE를 퍼센트 변환한 것.\n- MAE와 마찬가지로 MSE보다 특이치에 robust하다.\n- 모델에 대한 편향이 존재.\n- 0 근처의 값에서는 사용하기 어렵습니다.\n\n#### MPE\n\n```python\ndef MAPE(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n```\n- MAPE를 퍼센트 변환한 것.\n- 절대값을 제외했기 때문에 overperformance인지 underperformance인지 쉽게 알 수 있다.\n\n\n## References\n\n\n- https://www.dataquest.io/blog/understanding-regression-error-metrics/\n- https://machinelearningmastery.com/regression-metrics-for-machine-learning/\n","slug":"ML-Metrics-Regression","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc6000wb36q8hsebd7r","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>회귀모델의 평가지표 정리</strong></p>\n<h3 id=\"회귀모델의-평가지표\"><a href=\"#회귀모델의-평가지표\" class=\"headerlink\" title=\"회귀모델의 평가지표\"></a>회귀모델의 평가지표</h3><ul>\n<li>$R^2$ 외에, MAE는 단위 유닛이 같으므로 보다 해석에 용이함.</li>\n<li>MSE는 제곱을 하기 때문에 특이값에 보다 민감. </li>\n<li>RMSE는 MSE를 실제값과 유사한 단위로 변화시켜줌.</li>\n<li>회귀문제에서 RMSE가 일반적으로 선호되는 방법이지만, 상황에 맞는 다른 방식을 사용. 특이값이 많은 경우에는 MAE를 사용.</li>\n</ul>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><p>MSE (Mean Squared Error) &#x3D;<br>$\\frac{1}{n}\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}$</p>\n</li>\n<li><p>MAE (Mean absolute error) &#x3D; $\\frac{1}{n}\\sum_{i&#x3D;1}^{n}\\left | y_{i} - \\hat{y_{i}} \\right |$</p>\n</li>\n<li><p>RMSE (Root Mean Squared Error) &#x3D;<br>$\\sqrt{MSE}$</p>\n</li>\n<li><p>R-squared (Coefficient of determination) &#x3D;<br>$1 - \\frac{\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}}{\\sum_{i&#x3D;1}^{n}(y_{i} - \\bar{y_{i}})^{2}} &#x3D; 1 - \\frac{SSE}{SST} &#x3D; \\frac {SSR}{SST}$</p>\n</li>\n<li><p>MAPE &#x3D; $\\frac { \\sum \\vert \\frac { y - \\hat y}{y} \\vert }{n}*100%$</p>\n</li>\n</ul>\n<ul>\n<li>참고<ul>\n<li>SSE(Sum of Squares <code>Error</code>, 관측치와 예측치 차이): $\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}$</li>\n<li>SSR(Sum of Squares due to <code>Regression</code>, 예측치와 평균 차이): $\\sum_{i&#x3D;1}^{n}(\\hat{y_{i}} - \\bar{y_{i}})^{2}$</li>\n<li>SST(Sum of Squares <code>Total</code>, 관측치와 평균 차이): $\\sum_{i&#x3D;1}^{n}(y_{i} - \\bar{y_{i}})^{2}$ , SSE + SSR</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h4><ul>\n<li>모델의 예측값과 실제값 차이의 면적의 합.</li>\n<li>특이값이 존재할 경우 수치가 많이 늘어남</li>\n</ul>\n<h4 id=\"MAE\"><a href=\"#MAE\" class=\"headerlink\" title=\"MAE\"></a>MAE</h4><ul>\n<li>MSE 보다 특이치에 robust</li>\n<li>절대값을 취하기 때문에 매우 직관적</li>\n</ul>\n<h4 id=\"RMSE\"><a href=\"#RMSE\" class=\"headerlink\" title=\"RMSE\"></a>RMSE</h4><ul>\n<li>MSE의 제곱근. </li>\n<li>큰 오류값에 대해 패널티를 주기 때문에 보통 이걸 사용</li>\n</ul>\n<h4 id=\"R-squared\"><a href=\"#R-squared\" class=\"headerlink\" title=\"R-squared\"></a>R-squared</h4><ul>\n<li>설명량</li>\n<li>$R^2$ 값이 1에 가까울 수록 데이터를 잘 설명하는 모델이 됨</li>\n</ul>\n<h4 id=\"MAPE\"><a href=\"#MAPE\" class=\"headerlink\" title=\"MAPE\"></a>MAPE</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">MAPE</span>(<span class=\"params\">y_true, y_pred</span>): </span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.mean(np.<span class=\"built_in\">abs</span>((y_true - y_pred) / y_true)) * <span class=\"number\">100</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>MAE를 퍼센트 변환한 것.</li>\n<li>MAE와 마찬가지로 MSE보다 특이치에 robust하다.</li>\n<li>모델에 대한 편향이 존재.</li>\n<li>0 근처의 값에서는 사용하기 어렵습니다.</li>\n</ul>\n<h4 id=\"MPE\"><a href=\"#MPE\" class=\"headerlink\" title=\"MPE\"></a>MPE</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">MAPE</span>(<span class=\"params\">y_true, y_pred</span>): </span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.mean(np.<span class=\"built_in\">abs</span>((y_true - y_pred) / y_true)) * <span class=\"number\">100</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>MAPE를 퍼센트 변환한 것.</li>\n<li>절대값을 제외했기 때문에 overperformance인지 underperformance인지 쉽게 알 수 있다.</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.dataquest.io/blog/understanding-regression-error-metrics/\">https://www.dataquest.io/blog/understanding-regression-error-metrics/</a></li>\n<li><a href=\"https://machinelearningmastery.com/regression-metrics-for-machine-learning/\">https://machinelearningmastery.com/regression-metrics-for-machine-learning/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>회귀모델의 평가지표 정리</strong></p>\n<h3 id=\"회귀모델의-평가지표\"><a href=\"#회귀모델의-평가지표\" class=\"headerlink\" title=\"회귀모델의 평가지표\"></a>회귀모델의 평가지표</h3><ul>\n<li>$R^2$ 외에, MAE는 단위 유닛이 같으므로 보다 해석에 용이함.</li>\n<li>MSE는 제곱을 하기 때문에 특이값에 보다 민감. </li>\n<li>RMSE는 MSE를 실제값과 유사한 단위로 변화시켜줌.</li>\n<li>회귀문제에서 RMSE가 일반적으로 선호되는 방법이지만, 상황에 맞는 다른 방식을 사용. 특이값이 많은 경우에는 MAE를 사용.</li>\n</ul>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><p>MSE (Mean Squared Error) &#x3D;<br>$\\frac{1}{n}\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}$</p>\n</li>\n<li><p>MAE (Mean absolute error) &#x3D; $\\frac{1}{n}\\sum_{i&#x3D;1}^{n}\\left | y_{i} - \\hat{y_{i}} \\right |$</p>\n</li>\n<li><p>RMSE (Root Mean Squared Error) &#x3D;<br>$\\sqrt{MSE}$</p>\n</li>\n<li><p>R-squared (Coefficient of determination) &#x3D;<br>$1 - \\frac{\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}}{\\sum_{i&#x3D;1}^{n}(y_{i} - \\bar{y_{i}})^{2}} &#x3D; 1 - \\frac{SSE}{SST} &#x3D; \\frac {SSR}{SST}$</p>\n</li>\n<li><p>MAPE &#x3D; $\\frac { \\sum \\vert \\frac { y - \\hat y}{y} \\vert }{n}*100%$</p>\n</li>\n</ul>\n<ul>\n<li>참고<ul>\n<li>SSE(Sum of Squares <code>Error</code>, 관측치와 예측치 차이): $\\sum_{i&#x3D;1}^{n}(y_{i} - \\hat{y_{i}})^{2}$</li>\n<li>SSR(Sum of Squares due to <code>Regression</code>, 예측치와 평균 차이): $\\sum_{i&#x3D;1}^{n}(\\hat{y_{i}} - \\bar{y_{i}})^{2}$</li>\n<li>SST(Sum of Squares <code>Total</code>, 관측치와 평균 차이): $\\sum_{i&#x3D;1}^{n}(y_{i} - \\bar{y_{i}})^{2}$ , SSE + SSR</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"MSE\"><a href=\"#MSE\" class=\"headerlink\" title=\"MSE\"></a>MSE</h4><ul>\n<li>모델의 예측값과 실제값 차이의 면적의 합.</li>\n<li>특이값이 존재할 경우 수치가 많이 늘어남</li>\n</ul>\n<h4 id=\"MAE\"><a href=\"#MAE\" class=\"headerlink\" title=\"MAE\"></a>MAE</h4><ul>\n<li>MSE 보다 특이치에 robust</li>\n<li>절대값을 취하기 때문에 매우 직관적</li>\n</ul>\n<h4 id=\"RMSE\"><a href=\"#RMSE\" class=\"headerlink\" title=\"RMSE\"></a>RMSE</h4><ul>\n<li>MSE의 제곱근. </li>\n<li>큰 오류값에 대해 패널티를 주기 때문에 보통 이걸 사용</li>\n</ul>\n<h4 id=\"R-squared\"><a href=\"#R-squared\" class=\"headerlink\" title=\"R-squared\"></a>R-squared</h4><ul>\n<li>설명량</li>\n<li>$R^2$ 값이 1에 가까울 수록 데이터를 잘 설명하는 모델이 됨</li>\n</ul>\n<h4 id=\"MAPE\"><a href=\"#MAPE\" class=\"headerlink\" title=\"MAPE\"></a>MAPE</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">MAPE</span>(<span class=\"params\">y_true, y_pred</span>): </span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.mean(np.<span class=\"built_in\">abs</span>((y_true - y_pred) / y_true)) * <span class=\"number\">100</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>MAE를 퍼센트 변환한 것.</li>\n<li>MAE와 마찬가지로 MSE보다 특이치에 robust하다.</li>\n<li>모델에 대한 편향이 존재.</li>\n<li>0 근처의 값에서는 사용하기 어렵습니다.</li>\n</ul>\n<h4 id=\"MPE\"><a href=\"#MPE\" class=\"headerlink\" title=\"MPE\"></a>MPE</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">MAPE</span>(<span class=\"params\">y_true, y_pred</span>): </span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.mean(np.<span class=\"built_in\">abs</span>((y_true - y_pred) / y_true)) * <span class=\"number\">100</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>MAPE를 퍼센트 변환한 것.</li>\n<li>절대값을 제외했기 때문에 overperformance인지 underperformance인지 쉽게 알 수 있다.</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.dataquest.io/blog/understanding-regression-error-metrics/\">https://www.dataquest.io/blog/understanding-regression-error-metrics/</a></li>\n<li><a href=\"https://machinelearningmastery.com/regression-metrics-for-machine-learning/\">https://machinelearningmastery.com/regression-metrics-for-machine-learning/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Metrics]회귀모델의 평가지표","path":"2022/06/13/ML-Metrics-Regression/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Regression","Metrics"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Tree]Decision Tree의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Decision Tree의 이해\n\n---\n**_Concept_**\n- **Decision Tree(결정트리)**: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.\n- **threshold** : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,\n- **full tree** : 모든 학습데이터에 대해 분기한 상태.\n- **Entropy** : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치\n- **지니 불순도** : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)\n- **정보 이득** : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.\n- **Root Node** : 초기노드. 데이터셋 혹은 샘플 전체. \n- **Leaf Node(Terminal Node)** : 자식이 없는 노드.하위노드가 없다.\n- **Pure Node** : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.\n- **Branch** : sub-section of an entire tree.\n- **Splitting** : 특정 노드를 나눠 하위노드를 생성하는 것.\n- **Pruning** : 특정 노드의 하위노드를 날리는 것(삭제).\n- **Pre-prune**: When you stop growing DT branches when information becomes unreliable.\n- **Post-prune**: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.\n\n![](https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png)\n\n---\n\n### Note\n---\n\n- SVM처럼 **분기점(threshold)을 학습한다.**\n- 기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.\n- **분기의 기준이 정보이득이라는 것이 핵심이다.**\n- 과적합을 방지하기 위해 pruning이 필요하다.\n- 선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.\n- 특성을 해석하기 좋아 많이 쓰임\n- **샘플에 민감해 트리 고저가 자주 바뀐다.**\n- 앙상블 방법의 기초가 된다.\n- **결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.**\n- 학습 데이터셋에 과대적합되는 경향이 있다.\n- 결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)\n- 따라서 사전/사후 가지치기를 통해 과대적합을 방지한다.\n- 알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.\n\n### 불순도 지표\n---\n\n#### Entropy\n---\n\n[엔트로피 중요개념](https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/)\n\n[매우중요](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)\n\n- Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)\n- 0~1의 값을 가진다.\n    + 클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. \n    + 데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.\n- 트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -> 엔트로피가 최소화되는 방향으로 학습을 진행한다.\n\n<p align=\"center\">\n<img src=\"https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png\" alt=\"drawing\" width=\"400\"/>\n</p>\n\n- **정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)**\n- N은 범주의 개수\n- $p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.\n\n$$\\text { Entropy }(p)=-\\sum_{i=1}^{N} p_{i} \\log _{2} p_{i}$$\n\n\n#### 지니불순도\n---\n\n- **잘못 분류될 확률을 최소화하기 위한 기준이다.**\n  - 정확히는 `데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)`\n  - 기본적으로 Single Node에 대해 계산한 값이다,\n- 클래스의 비율이 완벽히 균등할 때 최대가 된다.\n- 기본적으로 노드가 중요할수록 불순도가 크게 감소한다.\n- 범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.\n- **Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.**\n- **지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.**(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)\n  - 불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.\n- 최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.\n- Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.\n- Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.\n\n\n$$\\text{Gini Impurity}=\\sum_{i=1}^{N} p(i) *(1-p(i))$$\n\n\n#### information Gain\n---\n\n\n- leaf의 결과는 기본적으로 majority 를 반환한다.\n- **정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.**\n  - 이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.\n- Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.\n\n$$IG(Parent,Children) = E(Parent) - E(Parent | Children)$$\n\n- **자식 노드의 불순도가 낮을수록 정보 이득이 커진다.** \n- 보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.\n\n\n$$\\text {E(parent)} - [\\text {weighted average}] * E(children)$$\n\n![](https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png)\n\n- 엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.\n- 엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.\n\n\n\n### 결정트리의 최적화 문제\n---\n\n\n- [최적화 원리와 코드](https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a)\n\n**Training algorithm**\n\n- **기본적으로 Best Threshold를 찾는 문제이다**\n- Start at the top node and at each node select the best split based o the best information gain\n- Greedy Search : Loop over all features and over all thresholds (**all possible feature values**)\n- Save the best split features and split threshold at each node\n- Build the tree recursively\n- Apply some stopping criteria to stop growing\n    + maximum depth\n    + minimum samples\n    + etc..\n\n- When we have a leaf node, store the most common class label of this node\n\n\n**Predict := Traverse tree**\n- Traverse the tree recursively.\n- At each node look at the best split feature of the test feature vector x and go left or right **depending on x[feature idx] <= threshold**\n- When we reach the leaf node we return the stored most common class label\n\n\n### Pruning\n---\n\n**Put limits in How trees grow**\n\n#### PrePruning\n---\n\n- 트리의 최대 깊이 제한하기(max_depth)\n- 리프의 최대 개수 제한하기\n- 노드가 분할하기 위한 데이터 포인트의 최소 개수 지정\n\n- sklearn에서 제공하는 관련 Hyperparameter\n    + max_depth : 일반화 성능관련. 트리의 최대깊이\n        * min_sample_splite\n        * max_feature : 최대 피처 사용수\n        * random_state : random state\n        * class_weight : 가중치 balance 맟추기\n\n#### PostPruning\n---\n\nPost-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.\n\n\n\n### Feature Importance in Decision Tree\n---\n\n\n### More to learn\n---\n\n- Pruning\n- Handling missing data\n- Building Trees for regression\n- Using trees to explore datasets\n\n**more**\n\n- Gini-Index is providing us with the highest accuracy with max depth = 6.\n- Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.\n- With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.\n\nTherefore with taking the criteria as Gini and max_depth = 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.\n\n\n**결정트리의 장점**\n\n- 설명가능성\n\n\n**결정트리의 단점**\n- 과적합\n\n### 구현\n---\n\n- numpy로 구현\n- **기본적으로 Best Split Threshold를 찾는 것이 목적이다.**\n\n\n```python\n\nimport numpy as np\nfrom collections import Counter\n\n\ndef entropy(y):\n    \"\"\"\n    Compute the entropy of a label vector\n    :param y: label vector\n    :return: entropy\n    \"\"\"\n    \n    hist = np.bincount(y) # class distribution # 0부터 max까지 class label의 빈도\n    ps = hist / len(y) # probability of each class\n    \n    return -np.sum([p * np.log2(p) for p in ps if p != 0]) # 음수에 대해서는 정의하지 않음\n\n\nclass Node:\n    def __init__(self,feature=None,threshold=None,left=None,right=None,*,value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n        \n    def is_leaf(self):\n        return self.value is not None # leaf node의 경우 value가 있다.\n    \nclass DecisionTree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats = None):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n        \n    def fit(self, X, y):\n        # grow tree\n        # X.shape[1] : feature의 개수\n        \n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        # if not self.n_feats -> n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) \n        # input의 feature 수보다 n_feats기 커지지 않게끔하는 \n        self.root = self._grow_tree(X, y)\n        \n    def _grow_tree(self, X, y, depth=0):\n        n_sample, n_feats = X.shape\n        n_labels = len(np.unique(y))\n        \n        # stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우\n        \n        if (\n            depth >= self.max_depth \n            or n_labels == 1 \n            or n_sample < self.min_samples_split\n        ):\n            leaf_value = self._most_common_label(y)\n            return Node(value=leaf_value)\n        \n        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=False)\n        \n        # calculate information gain\n        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)\n        \n        # grow the children that result from splitting on the best feature\n        \n        # 정보이득을 계산한 best_feature와 best threshold 기준으로 분할\n        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)\n        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+1) # depth+1\n        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+1)\n        \n        return Node(best_feat, best_threshold, left, right) \n        \n    def _best_criteria(self,X,y,feat_idxs):\n        \"\"\"\n        Find the best criteria to split the data\n        :param X: input data\n        :param y: label\n        :param feat_idxs: indices of features to consider\n        :return: best feature index, best threshold\n        \"\"\"\n        best_gain = -1\n        \n        split_idx, split_threshold = None, None\n        \n        for feat_idx in feat_idxs:\n            X_col = X[:,feat_idx] # X의 각 feature\n            thresholds = np.unique(X_col) # 각 feature의 cardianlity\n            for threshold in thresholds:\n                gain = self._information_gain(y,X_col,threshold) # 각 feuture의 모든 threshold에 대해서 gain을 계산\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feat_idx\n                    split_threshold = threshold\n                    \n        return split_idx, split_threshold\n    \n    \n    def _information_gain(self,y,X_column,split_threshold):\n        \"\"\"\n        Calculate information gain\n        E(parent) - [weight average] * E(Children)\n        \"\"\"\n        \n        # parent entropy\n        \n        parent_entropy = entropy(y)\n        \n        \n        # generate split\n        left_idxs, right_idxs = self._split(X_column, split_threshold)\n        \n        # 더이상 분할이 안될 경우 정보이득이 0\n        if (len(left_idxs) == 0) or (len(right_idxs)) == 0:\n            return 0\n\n        # compute the weighted avg. of the loss for the children\n        n = len(y)\n        n_l, n_r = len(left_idxs), len(right_idxs)\n        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n\n        # information gain is difference in loss before vs. after split\n        ig = parent_entropy - child_entropy\n        return ig\n        \n    def _split(self, X_column, split_threshold):\n        \"\"\"\n        Split data according to the threshold\n        :param X_column: input data\n        :param split_threshold: threshold to split\n        :return: left and right indices\n        \"\"\"\n        # np.argwhere을 사용 조건에 해당하는 인덱스 반환.\n        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n        return left_idxs, right_idxs\n    \n    def _most_common_label(self, y):\n        \"\"\"\n        Find the most common label in the dataset\n        :param y: labels\n        :return: most common label\n        \"\"\"\n        counter = Counter(y)\n        # counter.most_common(1) -> [(label, count)] # 리스트 안에 튜플\n        most_common = counter.most_common(1)[0][0]\n        return most_common # Counter(y) : Counter({0: 2, 1: 2}) #value과 count중 value만 반환 \n\n    def predict(self, X):\n        # traverse the tree\n        return np.array([self._traverse_tree(x,self.root) for x in X]) # X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환\n    \n    \n    def _traverse_tree(self, x, node):\n        \"\"\"\n        Traverse the tree from the root\n        :param x: input data\n        :param node: root node\n        :return: label\n        \"\"\"\n        if node.is_leaf(): # check if leaf node\n            return node.value\n        \n        if x[node.feature] <= node.threshold:\n            return self._traverse_tree(x, node.left)\n        else:\n            return self._traverse_tree(x, node.right)\n    \n    \nif __name__ == '__main__':\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n    \n    def accuracy(y,y_pred):\n        acc = np.sum(y == y_pred) / len(y)\n        return acc\n    \n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    clf = DecisionTree(max_depth=10)\n    clf.fit(X_train, y_train)\n    \n    y_pred = clf.predict(X_test)\n    acc = accuracy(y_test, y_pred)\n    \n    print(f\"Accuracy : {acc}\")\n```\n\n**References & annotation**\n---\n\n- [결정트리의 최적화 문제](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)\n- [정보이득](https://machinelearningmastery.com/information-gain-and-mutual-information/)\n- [지니불순도](https://victorzhou.com/blog/gini-impurity/)\n- [불순도 지표들](https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/)\n- [Post_Pruning](-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b)","source":"_posts/ML-SP-Decision_Tree.md","raw":"---\ntitle: '[Tree]Decision Tree의 이해'\ncategories:\n  - Machine Learning\ntags:\n  - Supervised Learning\n  - Decision Tree\ndate:\nupdated:\nmathjax: true\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Decision Tree의 이해\n\n---\n**_Concept_**\n- **Decision Tree(결정트리)**: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.\n- **threshold** : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,\n- **full tree** : 모든 학습데이터에 대해 분기한 상태.\n- **Entropy** : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치\n- **지니 불순도** : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)\n- **정보 이득** : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.\n- **Root Node** : 초기노드. 데이터셋 혹은 샘플 전체. \n- **Leaf Node(Terminal Node)** : 자식이 없는 노드.하위노드가 없다.\n- **Pure Node** : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.\n- **Branch** : sub-section of an entire tree.\n- **Splitting** : 특정 노드를 나눠 하위노드를 생성하는 것.\n- **Pruning** : 특정 노드의 하위노드를 날리는 것(삭제).\n- **Pre-prune**: When you stop growing DT branches when information becomes unreliable.\n- **Post-prune**: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.\n\n![](https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png)\n\n---\n\n### Note\n---\n\n- SVM처럼 **분기점(threshold)을 학습한다.**\n- 기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.\n- **분기의 기준이 정보이득이라는 것이 핵심이다.**\n- 과적합을 방지하기 위해 pruning이 필요하다.\n- 선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.\n- 특성을 해석하기 좋아 많이 쓰임\n- **샘플에 민감해 트리 고저가 자주 바뀐다.**\n- 앙상블 방법의 기초가 된다.\n- **결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.**\n- 학습 데이터셋에 과대적합되는 경향이 있다.\n- 결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)\n- 따라서 사전/사후 가지치기를 통해 과대적합을 방지한다.\n- 알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.\n\n### 불순도 지표\n---\n\n#### Entropy\n---\n\n[엔트로피 중요개념](https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/)\n\n[매우중요](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)\n\n- Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)\n- 0~1의 값을 가진다.\n    + 클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. \n    + 데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.\n- 트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -> 엔트로피가 최소화되는 방향으로 학습을 진행한다.\n\n<p align=\"center\">\n<img src=\"https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png\" alt=\"drawing\" width=\"400\"/>\n</p>\n\n- **정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)**\n- N은 범주의 개수\n- $p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.\n\n$$\\text { Entropy }(p)=-\\sum_{i=1}^{N} p_{i} \\log _{2} p_{i}$$\n\n\n#### 지니불순도\n---\n\n- **잘못 분류될 확률을 최소화하기 위한 기준이다.**\n  - 정확히는 `데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)`\n  - 기본적으로 Single Node에 대해 계산한 값이다,\n- 클래스의 비율이 완벽히 균등할 때 최대가 된다.\n- 기본적으로 노드가 중요할수록 불순도가 크게 감소한다.\n- 범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.\n- **Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.**\n- **지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.**(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)\n  - 불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.\n- 최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.\n- Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.\n- Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.\n\n\n$$\\text{Gini Impurity}=\\sum_{i=1}^{N} p(i) *(1-p(i))$$\n\n\n#### information Gain\n---\n\n\n- leaf의 결과는 기본적으로 majority 를 반환한다.\n- **정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.**\n  - 이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.\n- Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.\n\n$$IG(Parent,Children) = E(Parent) - E(Parent | Children)$$\n\n- **자식 노드의 불순도가 낮을수록 정보 이득이 커진다.** \n- 보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.\n\n\n$$\\text {E(parent)} - [\\text {weighted average}] * E(children)$$\n\n![](https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png)\n\n- 엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.\n- 엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.\n\n\n\n### 결정트리의 최적화 문제\n---\n\n\n- [최적화 원리와 코드](https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a)\n\n**Training algorithm**\n\n- **기본적으로 Best Threshold를 찾는 문제이다**\n- Start at the top node and at each node select the best split based o the best information gain\n- Greedy Search : Loop over all features and over all thresholds (**all possible feature values**)\n- Save the best split features and split threshold at each node\n- Build the tree recursively\n- Apply some stopping criteria to stop growing\n    + maximum depth\n    + minimum samples\n    + etc..\n\n- When we have a leaf node, store the most common class label of this node\n\n\n**Predict := Traverse tree**\n- Traverse the tree recursively.\n- At each node look at the best split feature of the test feature vector x and go left or right **depending on x[feature idx] <= threshold**\n- When we reach the leaf node we return the stored most common class label\n\n\n### Pruning\n---\n\n**Put limits in How trees grow**\n\n#### PrePruning\n---\n\n- 트리의 최대 깊이 제한하기(max_depth)\n- 리프의 최대 개수 제한하기\n- 노드가 분할하기 위한 데이터 포인트의 최소 개수 지정\n\n- sklearn에서 제공하는 관련 Hyperparameter\n    + max_depth : 일반화 성능관련. 트리의 최대깊이\n        * min_sample_splite\n        * max_feature : 최대 피처 사용수\n        * random_state : random state\n        * class_weight : 가중치 balance 맟추기\n\n#### PostPruning\n---\n\nPost-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.\n\n\n\n### Feature Importance in Decision Tree\n---\n\n\n### More to learn\n---\n\n- Pruning\n- Handling missing data\n- Building Trees for regression\n- Using trees to explore datasets\n\n**more**\n\n- Gini-Index is providing us with the highest accuracy with max depth = 6.\n- Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.\n- With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.\n\nTherefore with taking the criteria as Gini and max_depth = 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.\n\n\n**결정트리의 장점**\n\n- 설명가능성\n\n\n**결정트리의 단점**\n- 과적합\n\n### 구현\n---\n\n- numpy로 구현\n- **기본적으로 Best Split Threshold를 찾는 것이 목적이다.**\n\n\n```python\n\nimport numpy as np\nfrom collections import Counter\n\n\ndef entropy(y):\n    \"\"\"\n    Compute the entropy of a label vector\n    :param y: label vector\n    :return: entropy\n    \"\"\"\n    \n    hist = np.bincount(y) # class distribution # 0부터 max까지 class label의 빈도\n    ps = hist / len(y) # probability of each class\n    \n    return -np.sum([p * np.log2(p) for p in ps if p != 0]) # 음수에 대해서는 정의하지 않음\n\n\nclass Node:\n    def __init__(self,feature=None,threshold=None,left=None,right=None,*,value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n        \n    def is_leaf(self):\n        return self.value is not None # leaf node의 경우 value가 있다.\n    \nclass DecisionTree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats = None):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n        \n    def fit(self, X, y):\n        # grow tree\n        # X.shape[1] : feature의 개수\n        \n        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n        # if not self.n_feats -> n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) \n        # input의 feature 수보다 n_feats기 커지지 않게끔하는 \n        self.root = self._grow_tree(X, y)\n        \n    def _grow_tree(self, X, y, depth=0):\n        n_sample, n_feats = X.shape\n        n_labels = len(np.unique(y))\n        \n        # stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우\n        \n        if (\n            depth >= self.max_depth \n            or n_labels == 1 \n            or n_sample < self.min_samples_split\n        ):\n            leaf_value = self._most_common_label(y)\n            return Node(value=leaf_value)\n        \n        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=False)\n        \n        # calculate information gain\n        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)\n        \n        # grow the children that result from splitting on the best feature\n        \n        # 정보이득을 계산한 best_feature와 best threshold 기준으로 분할\n        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)\n        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+1) # depth+1\n        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+1)\n        \n        return Node(best_feat, best_threshold, left, right) \n        \n    def _best_criteria(self,X,y,feat_idxs):\n        \"\"\"\n        Find the best criteria to split the data\n        :param X: input data\n        :param y: label\n        :param feat_idxs: indices of features to consider\n        :return: best feature index, best threshold\n        \"\"\"\n        best_gain = -1\n        \n        split_idx, split_threshold = None, None\n        \n        for feat_idx in feat_idxs:\n            X_col = X[:,feat_idx] # X의 각 feature\n            thresholds = np.unique(X_col) # 각 feature의 cardianlity\n            for threshold in thresholds:\n                gain = self._information_gain(y,X_col,threshold) # 각 feuture의 모든 threshold에 대해서 gain을 계산\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feat_idx\n                    split_threshold = threshold\n                    \n        return split_idx, split_threshold\n    \n    \n    def _information_gain(self,y,X_column,split_threshold):\n        \"\"\"\n        Calculate information gain\n        E(parent) - [weight average] * E(Children)\n        \"\"\"\n        \n        # parent entropy\n        \n        parent_entropy = entropy(y)\n        \n        \n        # generate split\n        left_idxs, right_idxs = self._split(X_column, split_threshold)\n        \n        # 더이상 분할이 안될 경우 정보이득이 0\n        if (len(left_idxs) == 0) or (len(right_idxs)) == 0:\n            return 0\n\n        # compute the weighted avg. of the loss for the children\n        n = len(y)\n        n_l, n_r = len(left_idxs), len(right_idxs)\n        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n\n        # information gain is difference in loss before vs. after split\n        ig = parent_entropy - child_entropy\n        return ig\n        \n    def _split(self, X_column, split_threshold):\n        \"\"\"\n        Split data according to the threshold\n        :param X_column: input data\n        :param split_threshold: threshold to split\n        :return: left and right indices\n        \"\"\"\n        # np.argwhere을 사용 조건에 해당하는 인덱스 반환.\n        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n        return left_idxs, right_idxs\n    \n    def _most_common_label(self, y):\n        \"\"\"\n        Find the most common label in the dataset\n        :param y: labels\n        :return: most common label\n        \"\"\"\n        counter = Counter(y)\n        # counter.most_common(1) -> [(label, count)] # 리스트 안에 튜플\n        most_common = counter.most_common(1)[0][0]\n        return most_common # Counter(y) : Counter({0: 2, 1: 2}) #value과 count중 value만 반환 \n\n    def predict(self, X):\n        # traverse the tree\n        return np.array([self._traverse_tree(x,self.root) for x in X]) # X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환\n    \n    \n    def _traverse_tree(self, x, node):\n        \"\"\"\n        Traverse the tree from the root\n        :param x: input data\n        :param node: root node\n        :return: label\n        \"\"\"\n        if node.is_leaf(): # check if leaf node\n            return node.value\n        \n        if x[node.feature] <= node.threshold:\n            return self._traverse_tree(x, node.left)\n        else:\n            return self._traverse_tree(x, node.right)\n    \n    \nif __name__ == '__main__':\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n    \n    def accuracy(y,y_pred):\n        acc = np.sum(y == y_pred) / len(y)\n        return acc\n    \n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    clf = DecisionTree(max_depth=10)\n    clf.fit(X_train, y_train)\n    \n    y_pred = clf.predict(X_test)\n    acc = accuracy(y_test, y_pred)\n    \n    print(f\"Accuracy : {acc}\")\n```\n\n**References & annotation**\n---\n\n- [결정트리의 최적화 문제](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)\n- [정보이득](https://machinelearningmastery.com/information-gain-and-mutual-information/)\n- [지니불순도](https://victorzhou.com/blog/gini-impurity/)\n- [불순도 지표들](https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/)\n- [Post_Pruning](-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b)","slug":"ML-SP-Decision_Tree","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc6000zb36q7ceaebpt","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Decision-Tree의-이해\"><a href=\"#Decision-Tree의-이해\" class=\"headerlink\" title=\"Decision Tree의 이해\"></a>Decision Tree의 이해</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Decision Tree(결정트리)</strong>: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.</li>\n<li><strong>threshold</strong> : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,</li>\n<li><strong>full tree</strong> : 모든 학습데이터에 대해 분기한 상태.</li>\n<li><strong>Entropy</strong> : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치</li>\n<li><strong>지니 불순도</strong> : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</li>\n<li><strong>정보 이득</strong> : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.</li>\n<li><strong>Root Node</strong> : 초기노드. 데이터셋 혹은 샘플 전체. </li>\n<li><strong>Leaf Node(Terminal Node)</strong> : 자식이 없는 노드.하위노드가 없다.</li>\n<li><strong>Pure Node</strong> : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.</li>\n<li><strong>Branch</strong> : sub-section of an entire tree.</li>\n<li><strong>Splitting</strong> : 특정 노드를 나눠 하위노드를 생성하는 것.</li>\n<li><strong>Pruning</strong> : 특정 노드의 하위노드를 날리는 것(삭제).</li>\n<li><strong>Pre-prune</strong>: When you stop growing DT branches when information becomes unreliable.</li>\n<li><strong>Post-prune</strong>: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png\"></p>\n<hr>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h3><hr>\n<ul>\n<li>SVM처럼 <strong>분기점(threshold)을 학습한다.</strong></li>\n<li>기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.</li>\n<li><strong>분기의 기준이 정보이득이라는 것이 핵심이다.</strong></li>\n<li>과적합을 방지하기 위해 pruning이 필요하다.</li>\n<li>선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.</li>\n<li>특성을 해석하기 좋아 많이 쓰임</li>\n<li><strong>샘플에 민감해 트리 고저가 자주 바뀐다.</strong></li>\n<li>앙상블 방법의 기초가 된다.</li>\n<li><strong>결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예&#x2F;아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.</strong></li>\n<li>학습 데이터셋에 과대적합되는 경향이 있다.</li>\n<li>결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)</li>\n<li>따라서 사전&#x2F;사후 가지치기를 통해 과대적합을 방지한다.</li>\n<li>알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.</li>\n</ul>\n<h3 id=\"불순도-지표\"><a href=\"#불순도-지표\" class=\"headerlink\" title=\"불순도 지표\"></a>불순도 지표</h3><hr>\n<h4 id=\"Entropy\"><a href=\"#Entropy\" class=\"headerlink\" title=\"Entropy\"></a>Entropy</h4><hr>\n<p><a href=\"https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/\">엔트로피 중요개념</a></p>\n<p><a href=\"https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\">매우중요</a></p>\n<ul>\n<li>Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)</li>\n<li>0~1의 값을 가진다.<ul>\n<li>클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. </li>\n<li>데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.</li>\n</ul>\n</li>\n<li>트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -&gt; 엔트로피가 최소화되는 방향으로 학습을 진행한다.</li>\n</ul>\n<p align=\"center\">\n<img src=\"https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png\" alt=\"drawing\" width=\"400\"/>\n</p>\n\n<ul>\n<li><strong>정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)</strong></li>\n<li>N은 범주의 개수</li>\n<li>$p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.</li>\n</ul>\n<p>$$\\text { Entropy }(p)&#x3D;-\\sum_{i&#x3D;1}^{N} p_{i} \\log <em>{2} p</em>{i}$$</p>\n<h4 id=\"지니불순도\"><a href=\"#지니불순도\" class=\"headerlink\" title=\"지니불순도\"></a>지니불순도</h4><hr>\n<ul>\n<li><strong>잘못 분류될 확률을 최소화하기 위한 기준이다.</strong><ul>\n<li>정확히는 <code>데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</code></li>\n<li>기본적으로 Single Node에 대해 계산한 값이다,</li>\n</ul>\n</li>\n<li>클래스의 비율이 완벽히 균등할 때 최대가 된다.</li>\n<li>기본적으로 노드가 중요할수록 불순도가 크게 감소한다.</li>\n<li>범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.</li>\n<li><strong>Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.</strong></li>\n<li><strong>지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.</strong>(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)<ul>\n<li>불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.</li>\n</ul>\n</li>\n<li>최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.</li>\n<li>Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.</li>\n<li>Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.</li>\n</ul>\n<p>$$\\text{Gini Impurity}&#x3D;\\sum_{i&#x3D;1}^{N} p(i) *(1-p(i))$$</p>\n<h4 id=\"information-Gain\"><a href=\"#information-Gain\" class=\"headerlink\" title=\"information Gain\"></a>information Gain</h4><hr>\n<ul>\n<li>leaf의 결과는 기본적으로 majority 를 반환한다.</li>\n<li><strong>정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.</strong><ul>\n<li>이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.</li>\n</ul>\n</li>\n<li>Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.</li>\n</ul>\n<p>$$IG(Parent,Children) &#x3D; E(Parent) - E(Parent | Children)$$</p>\n<ul>\n<li><strong>자식 노드의 불순도가 낮을수록 정보 이득이 커진다.</strong> </li>\n<li>보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.</li>\n</ul>\n<p>$$\\text {E(parent)} - [\\text {weighted average}] * E(children)$$</p>\n<p><img src=\"https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png\"></p>\n<ul>\n<li>엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.</li>\n<li>엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.</li>\n</ul>\n<h3 id=\"결정트리의-최적화-문제\"><a href=\"#결정트리의-최적화-문제\" class=\"headerlink\" title=\"결정트리의 최적화 문제\"></a>결정트리의 최적화 문제</h3><hr>\n<ul>\n<li><a href=\"https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a\">최적화 원리와 코드</a></li>\n</ul>\n<p><strong>Training algorithm</strong></p>\n<ul>\n<li><p><strong>기본적으로 Best Threshold를 찾는 문제이다</strong></p>\n</li>\n<li><p>Start at the top node and at each node select the best split based o the best information gain</p>\n</li>\n<li><p>Greedy Search : Loop over all features and over all thresholds (<strong>all possible feature values</strong>)</p>\n</li>\n<li><p>Save the best split features and split threshold at each node</p>\n</li>\n<li><p>Build the tree recursively</p>\n</li>\n<li><p>Apply some stopping criteria to stop growing</p>\n<ul>\n<li>maximum depth</li>\n<li>minimum samples</li>\n<li>etc..</li>\n</ul>\n</li>\n<li><p>When we have a leaf node, store the most common class label of this node</p>\n</li>\n</ul>\n<p><strong>Predict :&#x3D; Traverse tree</strong></p>\n<ul>\n<li>Traverse the tree recursively.</li>\n<li>At each node look at the best split feature of the test feature vector x and go left or right <strong>depending on x[feature idx] &lt;&#x3D; threshold</strong></li>\n<li>When we reach the leaf node we return the stored most common class label</li>\n</ul>\n<h3 id=\"Pruning\"><a href=\"#Pruning\" class=\"headerlink\" title=\"Pruning\"></a>Pruning</h3><hr>\n<p><strong>Put limits in How trees grow</strong></p>\n<h4 id=\"PrePruning\"><a href=\"#PrePruning\" class=\"headerlink\" title=\"PrePruning\"></a>PrePruning</h4><hr>\n<ul>\n<li><p>트리의 최대 깊이 제한하기(max_depth)</p>\n</li>\n<li><p>리프의 최대 개수 제한하기</p>\n</li>\n<li><p>노드가 분할하기 위한 데이터 포인트의 최소 개수 지정</p>\n</li>\n<li><p>sklearn에서 제공하는 관련 Hyperparameter</p>\n<ul>\n<li>max_depth : 일반화 성능관련. 트리의 최대깊이<ul>\n<li>min_sample_splite</li>\n<li>max_feature : 최대 피처 사용수</li>\n<li>random_state : random state</li>\n<li>class_weight : 가중치 balance 맟추기</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"PostPruning\"><a href=\"#PostPruning\" class=\"headerlink\" title=\"PostPruning\"></a>PostPruning</h4><hr>\n<p>Post-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.</p>\n<h3 id=\"Feature-Importance-in-Decision-Tree\"><a href=\"#Feature-Importance-in-Decision-Tree\" class=\"headerlink\" title=\"Feature Importance in Decision Tree\"></a>Feature Importance in Decision Tree</h3><hr>\n<h3 id=\"More-to-learn\"><a href=\"#More-to-learn\" class=\"headerlink\" title=\"More to learn\"></a>More to learn</h3><hr>\n<ul>\n<li>Pruning</li>\n<li>Handling missing data</li>\n<li>Building Trees for regression</li>\n<li>Using trees to explore datasets</li>\n</ul>\n<p><strong>more</strong></p>\n<ul>\n<li>Gini-Index is providing us with the highest accuracy with max depth &#x3D; 6.</li>\n<li>Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.</li>\n<li>With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.</li>\n</ul>\n<p>Therefore with taking the criteria as Gini and max_depth &#x3D; 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.</p>\n<p><strong>결정트리의 장점</strong></p>\n<ul>\n<li>설명가능성</li>\n</ul>\n<p><strong>결정트리의 단점</strong></p>\n<ul>\n<li>과적합</li>\n</ul>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><hr>\n<ul>\n<li>numpy로 구현</li>\n<li><strong>기본적으로 Best Split Threshold를 찾는 것이 목적이다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">entropy</span>(<span class=\"params\">y</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Compute the entropy of a label vector</span></span><br><span class=\"line\"><span class=\"string\">    :param y: label vector</span></span><br><span class=\"line\"><span class=\"string\">    :return: entropy</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    hist = np.bincount(y) <span class=\"comment\"># class distribution # 0부터 max까지 class label의 빈도</span></span><br><span class=\"line\">    ps = hist / <span class=\"built_in\">len</span>(y) <span class=\"comment\"># probability of each class</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> -np.<span class=\"built_in\">sum</span>([p * np.log2(p) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> ps <span class=\"keyword\">if</span> p != <span class=\"number\">0</span>]) <span class=\"comment\"># 음수에 대해서는 정의하지 않음</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Node</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,feature=<span class=\"literal\">None</span>,threshold=<span class=\"literal\">None</span>,left=<span class=\"literal\">None</span>,right=<span class=\"literal\">None</span>,*,value=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.feature = feature</span><br><span class=\"line\">        self.threshold = threshold</span><br><span class=\"line\">        self.left = left</span><br><span class=\"line\">        self.right = right</span><br><span class=\"line\">        self.value = value</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">is_leaf</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.value <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"comment\"># leaf node의 경우 value가 있다.</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DecisionTree</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, min_samples_split=<span class=\"number\">2</span>, max_depth=<span class=\"number\">100</span>, n_feats = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.min_samples_split = min_samples_split</span><br><span class=\"line\">        self.max_depth = max_depth</span><br><span class=\"line\">        self.n_feats = n_feats</span><br><span class=\"line\">        self.root = <span class=\"literal\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>):</span><br><span class=\"line\">        <span class=\"comment\"># grow tree</span></span><br><span class=\"line\">        <span class=\"comment\"># X.shape[1] : feature의 개수</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        self.n_feats = X.shape[<span class=\"number\">1</span>] <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.n_feats <span class=\"keyword\">else</span> <span class=\"built_in\">min</span>(self.n_feats, X.shape[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># if not self.n_feats -&gt; n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) </span></span><br><span class=\"line\">        <span class=\"comment\"># input의 feature 수보다 n_feats기 커지지 않게끔하는 </span></span><br><span class=\"line\">        self.root = self._grow_tree(X, y)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_grow_tree</span>(<span class=\"params\">self, X, y, depth=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        n_sample, n_feats = X.shape</span><br><span class=\"line\">        n_labels = <span class=\"built_in\">len</span>(np.unique(y))</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (</span><br><span class=\"line\">            depth &gt;= self.max_depth </span><br><span class=\"line\">            <span class=\"keyword\">or</span> n_labels == <span class=\"number\">1</span> </span><br><span class=\"line\">            <span class=\"keyword\">or</span> n_sample &lt; self.min_samples_split</span><br><span class=\"line\">        ):</span><br><span class=\"line\">            leaf_value = self._most_common_label(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> Node(value=leaf_value)</span><br><span class=\"line\">        </span><br><span class=\"line\">        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># calculate information gain</span></span><br><span class=\"line\">        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># grow the children that result from splitting on the best feature</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 정보이득을 계산한 best_feature와 best threshold 기준으로 분할</span></span><br><span class=\"line\">        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)</span><br><span class=\"line\">        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+<span class=\"number\">1</span>) <span class=\"comment\"># depth+1</span></span><br><span class=\"line\">        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+<span class=\"number\">1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> Node(best_feat, best_threshold, left, right) </span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_best_criteria</span>(<span class=\"params\">self,X,y,feat_idxs</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Find the best criteria to split the data</span></span><br><span class=\"line\"><span class=\"string\">        :param X: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param y: label</span></span><br><span class=\"line\"><span class=\"string\">        :param feat_idxs: indices of features to consider</span></span><br><span class=\"line\"><span class=\"string\">        :return: best feature index, best threshold</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        best_gain = -<span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        split_idx, split_threshold = <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> feat_idx <span class=\"keyword\">in</span> feat_idxs:</span><br><span class=\"line\">            X_col = X[:,feat_idx] <span class=\"comment\"># X의 각 feature</span></span><br><span class=\"line\">            thresholds = np.unique(X_col) <span class=\"comment\"># 각 feature의 cardianlity</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> threshold <span class=\"keyword\">in</span> thresholds:</span><br><span class=\"line\">                gain = self._information_gain(y,X_col,threshold) <span class=\"comment\"># 각 feuture의 모든 threshold에 대해서 gain을 계산</span></span><br><span class=\"line\">                </span><br><span class=\"line\">                <span class=\"keyword\">if</span> gain &gt; best_gain:</span><br><span class=\"line\">                    best_gain = gain</span><br><span class=\"line\">                    split_idx = feat_idx</span><br><span class=\"line\">                    split_threshold = threshold</span><br><span class=\"line\">                    </span><br><span class=\"line\">        <span class=\"keyword\">return</span> split_idx, split_threshold</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_information_gain</span>(<span class=\"params\">self,y,X_column,split_threshold</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Calculate information gain</span></span><br><span class=\"line\"><span class=\"string\">        E(parent) - [weight average] * E(Children)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># parent entropy</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        parent_entropy = entropy(y)</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># generate split</span></span><br><span class=\"line\">        left_idxs, right_idxs = self._split(X_column, split_threshold)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 더이상 분할이 안될 경우 정보이득이 0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">len</span>(left_idxs) == <span class=\"number\">0</span>) <span class=\"keyword\">or</span> (<span class=\"built_in\">len</span>(right_idxs)) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># compute the weighted avg. of the loss for the children</span></span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        n_l, n_r = <span class=\"built_in\">len</span>(left_idxs), <span class=\"built_in\">len</span>(right_idxs)</span><br><span class=\"line\">        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])</span><br><span class=\"line\">        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># information gain is difference in loss before vs. after split</span></span><br><span class=\"line\">        ig = parent_entropy - child_entropy</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ig</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_split</span>(<span class=\"params\">self, X_column, split_threshold</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Split data according to the threshold</span></span><br><span class=\"line\"><span class=\"string\">        :param X_column: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param split_threshold: threshold to split</span></span><br><span class=\"line\"><span class=\"string\">        :return: left and right indices</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># np.argwhere을 사용 조건에 해당하는 인덱스 반환.</span></span><br><span class=\"line\">        left_idxs = np.argwhere(X_column &lt;= split_threshold).flatten()</span><br><span class=\"line\">        right_idxs = np.argwhere(X_column &gt; split_threshold).flatten()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> left_idxs, right_idxs</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_most_common_label</span>(<span class=\"params\">self, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Find the most common label in the dataset</span></span><br><span class=\"line\"><span class=\"string\">        :param y: labels</span></span><br><span class=\"line\"><span class=\"string\">        :return: most common label</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        counter = Counter(y)</span><br><span class=\"line\">        <span class=\"comment\"># counter.most_common(1) -&gt; [(label, count)] # 리스트 안에 튜플</span></span><br><span class=\"line\">        most_common = counter.most_common(<span class=\"number\">1</span>)[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> most_common <span class=\"comment\"># Counter(y) : Counter(&#123;0: 2, 1: 2&#125;) #value과 count중 value만 반환 </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># traverse the tree</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array([self._traverse_tree(x,self.root) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]) <span class=\"comment\"># X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_traverse_tree</span>(<span class=\"params\">self, x, node</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Traverse the tree from the root</span></span><br><span class=\"line\"><span class=\"string\">        :param x: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param node: root node</span></span><br><span class=\"line\"><span class=\"string\">        :return: label</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.is_leaf(): <span class=\"comment\"># check if leaf node</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> node.value</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> x[node.feature] &lt;= node.threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self._traverse_tree(x, node.left)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self._traverse_tree(x, node.right)</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\">    <span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y,y_pred</span>):</span><br><span class=\"line\">        acc = np.<span class=\"built_in\">sum</span>(y == y_pred) / <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> acc</span><br><span class=\"line\">    </span><br><span class=\"line\">    data = datasets.load_breast_cancer()</span><br><span class=\"line\">    X, y = data.data, data.target</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    clf = DecisionTree(max_depth=<span class=\"number\">10</span>)</span><br><span class=\"line\">    clf.fit(X_train, y_train)</span><br><span class=\"line\">    </span><br><span class=\"line\">    y_pred = clf.predict(X_test)</span><br><span class=\"line\">    acc = accuracy(y_test, y_pred)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Accuracy : <span class=\"subst\">&#123;acc&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html\">결정트리의 최적화 문제</a></li>\n<li><a href=\"https://machinelearningmastery.com/information-gain-and-mutual-information/\">정보이득</a></li>\n<li><a href=\"https://victorzhou.com/blog/gini-impurity/\">지니불순도</a></li>\n<li><a href=\"https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/\">불순도 지표들</a></li>\n<li><a href=\"-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b\">Post_Pruning</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Decision-Tree의-이해\"><a href=\"#Decision-Tree의-이해\" class=\"headerlink\" title=\"Decision Tree의 이해\"></a>Decision Tree의 이해</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Decision Tree(결정트리)</strong>: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.</li>\n<li><strong>threshold</strong> : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,</li>\n<li><strong>full tree</strong> : 모든 학습데이터에 대해 분기한 상태.</li>\n<li><strong>Entropy</strong> : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치</li>\n<li><strong>지니 불순도</strong> : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</li>\n<li><strong>정보 이득</strong> : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.</li>\n<li><strong>Root Node</strong> : 초기노드. 데이터셋 혹은 샘플 전체. </li>\n<li><strong>Leaf Node(Terminal Node)</strong> : 자식이 없는 노드.하위노드가 없다.</li>\n<li><strong>Pure Node</strong> : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.</li>\n<li><strong>Branch</strong> : sub-section of an entire tree.</li>\n<li><strong>Splitting</strong> : 특정 노드를 나눠 하위노드를 생성하는 것.</li>\n<li><strong>Pruning</strong> : 특정 노드의 하위노드를 날리는 것(삭제).</li>\n<li><strong>Pre-prune</strong>: When you stop growing DT branches when information becomes unreliable.</li>\n<li><strong>Post-prune</strong>: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png\"></p>\n<hr>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h3><hr>\n<ul>\n<li>SVM처럼 <strong>분기점(threshold)을 학습한다.</strong></li>\n<li>기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.</li>\n<li><strong>분기의 기준이 정보이득이라는 것이 핵심이다.</strong></li>\n<li>과적합을 방지하기 위해 pruning이 필요하다.</li>\n<li>선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.</li>\n<li>특성을 해석하기 좋아 많이 쓰임</li>\n<li><strong>샘플에 민감해 트리 고저가 자주 바뀐다.</strong></li>\n<li>앙상블 방법의 기초가 된다.</li>\n<li><strong>결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예&#x2F;아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.</strong></li>\n<li>학습 데이터셋에 과대적합되는 경향이 있다.</li>\n<li>결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)</li>\n<li>따라서 사전&#x2F;사후 가지치기를 통해 과대적합을 방지한다.</li>\n<li>알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.</li>\n</ul>\n<h3 id=\"불순도-지표\"><a href=\"#불순도-지표\" class=\"headerlink\" title=\"불순도 지표\"></a>불순도 지표</h3><hr>\n<h4 id=\"Entropy\"><a href=\"#Entropy\" class=\"headerlink\" title=\"Entropy\"></a>Entropy</h4><hr>\n<p><a href=\"https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/\">엔트로피 중요개념</a></p>\n<p><a href=\"https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\">매우중요</a></p>\n<ul>\n<li>Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)</li>\n<li>0~1의 값을 가진다.<ul>\n<li>클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. </li>\n<li>데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.</li>\n</ul>\n</li>\n<li>트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -&gt; 엔트로피가 최소화되는 방향으로 학습을 진행한다.</li>\n</ul>\n<p align=\"center\">\n<img src=\"https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png\" alt=\"drawing\" width=\"400\"/>\n</p>\n\n<ul>\n<li><strong>정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)</strong></li>\n<li>N은 범주의 개수</li>\n<li>$p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.</li>\n</ul>\n<p>$$\\text { Entropy }(p)&#x3D;-\\sum_{i&#x3D;1}^{N} p_{i} \\log <em>{2} p</em>{i}$$</p>\n<h4 id=\"지니불순도\"><a href=\"#지니불순도\" class=\"headerlink\" title=\"지니불순도\"></a>지니불순도</h4><hr>\n<ul>\n<li><strong>잘못 분류될 확률을 최소화하기 위한 기준이다.</strong><ul>\n<li>정확히는 <code>데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</code></li>\n<li>기본적으로 Single Node에 대해 계산한 값이다,</li>\n</ul>\n</li>\n<li>클래스의 비율이 완벽히 균등할 때 최대가 된다.</li>\n<li>기본적으로 노드가 중요할수록 불순도가 크게 감소한다.</li>\n<li>범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.</li>\n<li><strong>Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.</strong></li>\n<li><strong>지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.</strong>(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)<ul>\n<li>불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.</li>\n</ul>\n</li>\n<li>최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.</li>\n<li>Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.</li>\n<li>Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.</li>\n</ul>\n<p>$$\\text{Gini Impurity}&#x3D;\\sum_{i&#x3D;1}^{N} p(i) *(1-p(i))$$</p>\n<h4 id=\"information-Gain\"><a href=\"#information-Gain\" class=\"headerlink\" title=\"information Gain\"></a>information Gain</h4><hr>\n<ul>\n<li>leaf의 결과는 기본적으로 majority 를 반환한다.</li>\n<li><strong>정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.</strong><ul>\n<li>이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.</li>\n</ul>\n</li>\n<li>Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.</li>\n</ul>\n<p>$$IG(Parent,Children) &#x3D; E(Parent) - E(Parent | Children)$$</p>\n<ul>\n<li><strong>자식 노드의 불순도가 낮을수록 정보 이득이 커진다.</strong> </li>\n<li>보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.</li>\n</ul>\n<p>$$\\text {E(parent)} - [\\text {weighted average}] * E(children)$$</p>\n<p><img src=\"https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png\"></p>\n<ul>\n<li>엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.</li>\n<li>엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.</li>\n</ul>\n<h3 id=\"결정트리의-최적화-문제\"><a href=\"#결정트리의-최적화-문제\" class=\"headerlink\" title=\"결정트리의 최적화 문제\"></a>결정트리의 최적화 문제</h3><hr>\n<ul>\n<li><a href=\"https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a\">최적화 원리와 코드</a></li>\n</ul>\n<p><strong>Training algorithm</strong></p>\n<ul>\n<li><p><strong>기본적으로 Best Threshold를 찾는 문제이다</strong></p>\n</li>\n<li><p>Start at the top node and at each node select the best split based o the best information gain</p>\n</li>\n<li><p>Greedy Search : Loop over all features and over all thresholds (<strong>all possible feature values</strong>)</p>\n</li>\n<li><p>Save the best split features and split threshold at each node</p>\n</li>\n<li><p>Build the tree recursively</p>\n</li>\n<li><p>Apply some stopping criteria to stop growing</p>\n<ul>\n<li>maximum depth</li>\n<li>minimum samples</li>\n<li>etc..</li>\n</ul>\n</li>\n<li><p>When we have a leaf node, store the most common class label of this node</p>\n</li>\n</ul>\n<p><strong>Predict :&#x3D; Traverse tree</strong></p>\n<ul>\n<li>Traverse the tree recursively.</li>\n<li>At each node look at the best split feature of the test feature vector x and go left or right <strong>depending on x[feature idx] &lt;&#x3D; threshold</strong></li>\n<li>When we reach the leaf node we return the stored most common class label</li>\n</ul>\n<h3 id=\"Pruning\"><a href=\"#Pruning\" class=\"headerlink\" title=\"Pruning\"></a>Pruning</h3><hr>\n<p><strong>Put limits in How trees grow</strong></p>\n<h4 id=\"PrePruning\"><a href=\"#PrePruning\" class=\"headerlink\" title=\"PrePruning\"></a>PrePruning</h4><hr>\n<ul>\n<li><p>트리의 최대 깊이 제한하기(max_depth)</p>\n</li>\n<li><p>리프의 최대 개수 제한하기</p>\n</li>\n<li><p>노드가 분할하기 위한 데이터 포인트의 최소 개수 지정</p>\n</li>\n<li><p>sklearn에서 제공하는 관련 Hyperparameter</p>\n<ul>\n<li>max_depth : 일반화 성능관련. 트리의 최대깊이<ul>\n<li>min_sample_splite</li>\n<li>max_feature : 최대 피처 사용수</li>\n<li>random_state : random state</li>\n<li>class_weight : 가중치 balance 맟추기</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"PostPruning\"><a href=\"#PostPruning\" class=\"headerlink\" title=\"PostPruning\"></a>PostPruning</h4><hr>\n<p>Post-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.</p>\n<h3 id=\"Feature-Importance-in-Decision-Tree\"><a href=\"#Feature-Importance-in-Decision-Tree\" class=\"headerlink\" title=\"Feature Importance in Decision Tree\"></a>Feature Importance in Decision Tree</h3><hr>\n<h3 id=\"More-to-learn\"><a href=\"#More-to-learn\" class=\"headerlink\" title=\"More to learn\"></a>More to learn</h3><hr>\n<ul>\n<li>Pruning</li>\n<li>Handling missing data</li>\n<li>Building Trees for regression</li>\n<li>Using trees to explore datasets</li>\n</ul>\n<p><strong>more</strong></p>\n<ul>\n<li>Gini-Index is providing us with the highest accuracy with max depth &#x3D; 6.</li>\n<li>Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.</li>\n<li>With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.</li>\n</ul>\n<p>Therefore with taking the criteria as Gini and max_depth &#x3D; 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.</p>\n<p><strong>결정트리의 장점</strong></p>\n<ul>\n<li>설명가능성</li>\n</ul>\n<p><strong>결정트리의 단점</strong></p>\n<ul>\n<li>과적합</li>\n</ul>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><hr>\n<ul>\n<li>numpy로 구현</li>\n<li><strong>기본적으로 Best Split Threshold를 찾는 것이 목적이다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">entropy</span>(<span class=\"params\">y</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Compute the entropy of a label vector</span></span><br><span class=\"line\"><span class=\"string\">    :param y: label vector</span></span><br><span class=\"line\"><span class=\"string\">    :return: entropy</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    hist = np.bincount(y) <span class=\"comment\"># class distribution # 0부터 max까지 class label의 빈도</span></span><br><span class=\"line\">    ps = hist / <span class=\"built_in\">len</span>(y) <span class=\"comment\"># probability of each class</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> -np.<span class=\"built_in\">sum</span>([p * np.log2(p) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> ps <span class=\"keyword\">if</span> p != <span class=\"number\">0</span>]) <span class=\"comment\"># 음수에 대해서는 정의하지 않음</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Node</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,feature=<span class=\"literal\">None</span>,threshold=<span class=\"literal\">None</span>,left=<span class=\"literal\">None</span>,right=<span class=\"literal\">None</span>,*,value=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.feature = feature</span><br><span class=\"line\">        self.threshold = threshold</span><br><span class=\"line\">        self.left = left</span><br><span class=\"line\">        self.right = right</span><br><span class=\"line\">        self.value = value</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">is_leaf</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.value <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"comment\"># leaf node의 경우 value가 있다.</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DecisionTree</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, min_samples_split=<span class=\"number\">2</span>, max_depth=<span class=\"number\">100</span>, n_feats = <span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.min_samples_split = min_samples_split</span><br><span class=\"line\">        self.max_depth = max_depth</span><br><span class=\"line\">        self.n_feats = n_feats</span><br><span class=\"line\">        self.root = <span class=\"literal\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>):</span><br><span class=\"line\">        <span class=\"comment\"># grow tree</span></span><br><span class=\"line\">        <span class=\"comment\"># X.shape[1] : feature의 개수</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        self.n_feats = X.shape[<span class=\"number\">1</span>] <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.n_feats <span class=\"keyword\">else</span> <span class=\"built_in\">min</span>(self.n_feats, X.shape[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># if not self.n_feats -&gt; n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) </span></span><br><span class=\"line\">        <span class=\"comment\"># input의 feature 수보다 n_feats기 커지지 않게끔하는 </span></span><br><span class=\"line\">        self.root = self._grow_tree(X, y)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_grow_tree</span>(<span class=\"params\">self, X, y, depth=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        n_sample, n_feats = X.shape</span><br><span class=\"line\">        n_labels = <span class=\"built_in\">len</span>(np.unique(y))</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (</span><br><span class=\"line\">            depth &gt;= self.max_depth </span><br><span class=\"line\">            <span class=\"keyword\">or</span> n_labels == <span class=\"number\">1</span> </span><br><span class=\"line\">            <span class=\"keyword\">or</span> n_sample &lt; self.min_samples_split</span><br><span class=\"line\">        ):</span><br><span class=\"line\">            leaf_value = self._most_common_label(y)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> Node(value=leaf_value)</span><br><span class=\"line\">        </span><br><span class=\"line\">        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># calculate information gain</span></span><br><span class=\"line\">        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># grow the children that result from splitting on the best feature</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 정보이득을 계산한 best_feature와 best threshold 기준으로 분할</span></span><br><span class=\"line\">        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)</span><br><span class=\"line\">        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+<span class=\"number\">1</span>) <span class=\"comment\"># depth+1</span></span><br><span class=\"line\">        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+<span class=\"number\">1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> Node(best_feat, best_threshold, left, right) </span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_best_criteria</span>(<span class=\"params\">self,X,y,feat_idxs</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Find the best criteria to split the data</span></span><br><span class=\"line\"><span class=\"string\">        :param X: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param y: label</span></span><br><span class=\"line\"><span class=\"string\">        :param feat_idxs: indices of features to consider</span></span><br><span class=\"line\"><span class=\"string\">        :return: best feature index, best threshold</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        best_gain = -<span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        split_idx, split_threshold = <span class=\"literal\">None</span>, <span class=\"literal\">None</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> feat_idx <span class=\"keyword\">in</span> feat_idxs:</span><br><span class=\"line\">            X_col = X[:,feat_idx] <span class=\"comment\"># X의 각 feature</span></span><br><span class=\"line\">            thresholds = np.unique(X_col) <span class=\"comment\"># 각 feature의 cardianlity</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> threshold <span class=\"keyword\">in</span> thresholds:</span><br><span class=\"line\">                gain = self._information_gain(y,X_col,threshold) <span class=\"comment\"># 각 feuture의 모든 threshold에 대해서 gain을 계산</span></span><br><span class=\"line\">                </span><br><span class=\"line\">                <span class=\"keyword\">if</span> gain &gt; best_gain:</span><br><span class=\"line\">                    best_gain = gain</span><br><span class=\"line\">                    split_idx = feat_idx</span><br><span class=\"line\">                    split_threshold = threshold</span><br><span class=\"line\">                    </span><br><span class=\"line\">        <span class=\"keyword\">return</span> split_idx, split_threshold</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_information_gain</span>(<span class=\"params\">self,y,X_column,split_threshold</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Calculate information gain</span></span><br><span class=\"line\"><span class=\"string\">        E(parent) - [weight average] * E(Children)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># parent entropy</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        parent_entropy = entropy(y)</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># generate split</span></span><br><span class=\"line\">        left_idxs, right_idxs = self._split(X_column, split_threshold)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 더이상 분할이 안될 경우 정보이득이 0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">len</span>(left_idxs) == <span class=\"number\">0</span>) <span class=\"keyword\">or</span> (<span class=\"built_in\">len</span>(right_idxs)) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># compute the weighted avg. of the loss for the children</span></span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        n_l, n_r = <span class=\"built_in\">len</span>(left_idxs), <span class=\"built_in\">len</span>(right_idxs)</span><br><span class=\"line\">        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])</span><br><span class=\"line\">        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># information gain is difference in loss before vs. after split</span></span><br><span class=\"line\">        ig = parent_entropy - child_entropy</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ig</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_split</span>(<span class=\"params\">self, X_column, split_threshold</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Split data according to the threshold</span></span><br><span class=\"line\"><span class=\"string\">        :param X_column: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param split_threshold: threshold to split</span></span><br><span class=\"line\"><span class=\"string\">        :return: left and right indices</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># np.argwhere을 사용 조건에 해당하는 인덱스 반환.</span></span><br><span class=\"line\">        left_idxs = np.argwhere(X_column &lt;= split_threshold).flatten()</span><br><span class=\"line\">        right_idxs = np.argwhere(X_column &gt; split_threshold).flatten()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> left_idxs, right_idxs</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_most_common_label</span>(<span class=\"params\">self, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Find the most common label in the dataset</span></span><br><span class=\"line\"><span class=\"string\">        :param y: labels</span></span><br><span class=\"line\"><span class=\"string\">        :return: most common label</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        counter = Counter(y)</span><br><span class=\"line\">        <span class=\"comment\"># counter.most_common(1) -&gt; [(label, count)] # 리스트 안에 튜플</span></span><br><span class=\"line\">        most_common = counter.most_common(<span class=\"number\">1</span>)[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> most_common <span class=\"comment\"># Counter(y) : Counter(&#123;0: 2, 1: 2&#125;) #value과 count중 value만 반환 </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"comment\"># traverse the tree</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array([self._traverse_tree(x,self.root) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]) <span class=\"comment\"># X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_traverse_tree</span>(<span class=\"params\">self, x, node</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Traverse the tree from the root</span></span><br><span class=\"line\"><span class=\"string\">        :param x: input data</span></span><br><span class=\"line\"><span class=\"string\">        :param node: root node</span></span><br><span class=\"line\"><span class=\"string\">        :return: label</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.is_leaf(): <span class=\"comment\"># check if leaf node</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> node.value</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> x[node.feature] &lt;= node.threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self._traverse_tree(x, node.left)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self._traverse_tree(x, node.right)</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\">    <span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y,y_pred</span>):</span><br><span class=\"line\">        acc = np.<span class=\"built_in\">sum</span>(y == y_pred) / <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> acc</span><br><span class=\"line\">    </span><br><span class=\"line\">    data = datasets.load_breast_cancer()</span><br><span class=\"line\">    X, y = data.data, data.target</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    clf = DecisionTree(max_depth=<span class=\"number\">10</span>)</span><br><span class=\"line\">    clf.fit(X_train, y_train)</span><br><span class=\"line\">    </span><br><span class=\"line\">    y_pred = clf.predict(X_test)</span><br><span class=\"line\">    acc = accuracy(y_test, y_pred)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Accuracy : <span class=\"subst\">&#123;acc&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html\">결정트리의 최적화 문제</a></li>\n<li><a href=\"https://machinelearningmastery.com/information-gain-and-mutual-information/\">정보이득</a></li>\n<li><a href=\"https://victorzhou.com/blog/gini-impurity/\">지니불순도</a></li>\n<li><a href=\"https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/\">불순도 지표들</a></li>\n<li><a href=\"-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b\">Post_Pruning</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Tree]Decision Tree의 이해","path":"2022/06/13/ML-SP-Decision_Tree/","eyeCatchImage":"https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Decision Tree"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Tree]주요 Decision Tree 알고리즘","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.\n\n---\n\n## Main Decision Tree Algorithms\n\n### **CHAID**\n\nThe Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.\n**In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.**\n\n### **CART**\nCART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.\n\n**Prepare Data for CART**\n- The **splitting of numerical features** can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!\n\n- Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization/manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.\n\n- In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.\n\n- If height or depth of the tree is exactly one then such a tree is called as a decision stump.\n\n- Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.\n\n- Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.\n- Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.\n\n- Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.\n\n\n### **ID3**\n\nThe Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).\n\n### **C4.5**\nC4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.\n\nUnlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.\nAdditionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.\n\n\n## References\n\n\n- https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\n","source":"_posts/ML-SP-Main-Decision-Tree-Algorithms.md","raw":"---\ntitle: '[Tree]주요 Decision Tree 알고리즘'\ncategories:\n  -  Machine Learning\ntags:\n  - Decision Tree\n  - Supervised Learning\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.\n\n---\n\n## Main Decision Tree Algorithms\n\n### **CHAID**\n\nThe Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.\n**In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.**\n\n### **CART**\nCART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.\n\n**Prepare Data for CART**\n- The **splitting of numerical features** can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!\n\n- Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization/manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.\n\n- In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.\n\n- If height or depth of the tree is exactly one then such a tree is called as a decision stump.\n\n- Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.\n\n- Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.\n- Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.\n\n- Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.\n\n\n### **ID3**\n\nThe Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).\n\n### **C4.5**\nC4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.\n\nUnlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.\nAdditionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.\n\n\n## References\n\n\n- https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\n","slug":"ML-SP-Main-Decision-Tree-Algorithms","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc70012b36qd3is4fiq","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.</p>\n<hr>\n<h2 id=\"Main-Decision-Tree-Algorithms\"><a href=\"#Main-Decision-Tree-Algorithms\" class=\"headerlink\" title=\"Main Decision Tree Algorithms\"></a>Main Decision Tree Algorithms</h2><h3 id=\"CHAID\"><a href=\"#CHAID\" class=\"headerlink\" title=\"CHAID\"></a><strong>CHAID</strong></h3><p>The Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.<br><strong>In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.</strong></p>\n<h3 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a><strong>CART</strong></h3><p>CART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.</p>\n<p><strong>Prepare Data for CART</strong></p>\n<ul>\n<li><p>The <strong>splitting of numerical features</strong> can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!</p>\n</li>\n<li><p>Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization&#x2F;manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.</p>\n</li>\n<li><p>In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.</p>\n</li>\n<li><p>If height or depth of the tree is exactly one then such a tree is called as a decision stump.</p>\n</li>\n<li><p>Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.</p>\n</li>\n<li><p>Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.</p>\n</li>\n<li><p>Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.</p>\n</li>\n<li><p>Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.</p>\n</li>\n</ul>\n<h3 id=\"ID3\"><a href=\"#ID3\" class=\"headerlink\" title=\"ID3\"></a><strong>ID3</strong></h3><p>The Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a><strong>C4.5</strong></h3><p>C4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.</p>\n<p>Unlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.<br>Additionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\">https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.</p>\n<hr>\n<h2 id=\"Main-Decision-Tree-Algorithms\"><a href=\"#Main-Decision-Tree-Algorithms\" class=\"headerlink\" title=\"Main Decision Tree Algorithms\"></a>Main Decision Tree Algorithms</h2><h3 id=\"CHAID\"><a href=\"#CHAID\" class=\"headerlink\" title=\"CHAID\"></a><strong>CHAID</strong></h3><p>The Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.<br><strong>In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.</strong></p>\n<h3 id=\"CART\"><a href=\"#CART\" class=\"headerlink\" title=\"CART\"></a><strong>CART</strong></h3><p>CART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.</p>\n<p><strong>Prepare Data for CART</strong></p>\n<ul>\n<li><p>The <strong>splitting of numerical features</strong> can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!</p>\n</li>\n<li><p>Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization&#x2F;manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.</p>\n</li>\n<li><p>In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.</p>\n</li>\n<li><p>If height or depth of the tree is exactly one then such a tree is called as a decision stump.</p>\n</li>\n<li><p>Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.</p>\n</li>\n<li><p>Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.</p>\n</li>\n<li><p>Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.</p>\n</li>\n<li><p>Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.</p>\n</li>\n</ul>\n<h3 id=\"ID3\"><a href=\"#ID3\" class=\"headerlink\" title=\"ID3\"></a><strong>ID3</strong></h3><p>The Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).</p>\n<h3 id=\"C4-5\"><a href=\"#C4-5\" class=\"headerlink\" title=\"C4.5\"></a><strong>C4.5</strong></h3><p>C4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.</p>\n<p>Unlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.<br>Additionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\">https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Tree]주요 Decision Tree 알고리즘","path":"2022/06/13/ML-SP-Main-Decision-Tree-Algorithms/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Decision Tree"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Tree]Random Forest의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Random Forest\n\n---\n**_Concept_**\n\n- **Bagging** : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. \n- **weak learner** : 서로 독립적으로 만들어지며 **상관이 낮은** 약한 분류기.\n- **Random Subspace Method** : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용\n- **Random Forest** : 여러 `week learner`들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.\n- **Bootstrap** : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.\n- **OOB** : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.\n---\n\n> A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n\n**랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.**\n\n### Bagging\n\n<img src=\"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png\" width=\"700\" />\n\n배깅의 핵심적인 목표는 **의사결정 트리 사이의 분산을 줄이는 것이다.** . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 `부트스트래핑`을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 `랜덤 포레스트`이다. `랜덤포레스트`는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.\n\n### Random Subspace method\n\n- Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.\n- train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.\n- **feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 `weak learner`에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.**\n- 기본모델 생성시 **특성 m개 중 일부분 k개의 특성을 선택(sampling)한다** \n- k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.\n- $\\sqrt{m}$을 k로 활용할 수도 있다.\n- k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.\n- k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.\n- k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 \n- 서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**\n\n- 트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.\n![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcaLvgA%2FbtraSXTXHT3%2FT0aBmdkrhHd3FCKGsSWN9k%2Fimg.png)\n\n**배깅과 Random subspave method의 비교**\n\n- bagging: it is better when the training samples are sparse(결측값이 많은 경우)\n- Random subspace method: it is better when the classes are compact and the boundaries are smooth.\n\n### Random Forest 알고리즘\n\n- m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.\n- m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. \n- 각각의 약한 분류기로 결과를 예측한다.\n- 각 분류기의 예측결과를 모아 최종결과를 도출한다.\n  + 분류 문제일 경우 다수결을 통해 최종 결과를 도출한다\n  + 회귀 문제일 경우 평균을 통해 최종결과를 도출한다.\n\n### Random Forest 주요 hyperparameter\n\n[Randomforest Hyperparameter](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/)\nsklearn에서 제공하는 하이퍼파라미터 기준으로 정리\n\n- max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.\n- n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.\n- min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.\n- oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.\n\n```python\n# 보통 고려하는 것들\n{'bootstrap': True,\n 'criterion': 'mse',\n 'max_depth': 3, # depth가 3일때까지만 split\n 'max_features': 'auto',\n 'max_leaf_nodes': 4, # leaf node가 4개일때까지만 split\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 3, # 생성될 노드들의 샘플 수가 3개 이상이여만 split \n 'min_samples_split': 5, # 5개 이상의 샘플만 split\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 10,\n 'n_jobs': 1,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n```\n\n### Random Forest 장단점\n\n**장점**\n\n- 과적합에 강하다.\n- 이상치에 크게 영향받지 않는다.\n- Scaling이 필요가 없다.\n- 결측값에 크게 영향받지 않는다.\n\n**단점**\n\n- 고차원의 희소한 데이터에 대해 성능이 저하된다.\n- training 속도 느림(메모리 소모)\n- 개별 트리 분석이 어럽다.\n\n\n\n### Random Forest 구현\n\n- sklearn을 활용한 구현\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4,\n                            n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\nRandomForestClassifier(...)\nprint(clf.predict([[0, 0, 0, 0]]))\n\n```\n\n\n- numpy를 활용한 구현\n\n```python\nfrom collections import Counter\n\nimport numpy as np\n\nfrom .decision_tree import DecisionTree\n\n\ndef bootstrap_sample(X, y):\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, n_samples, replace=True)\n    return X[idxs], y[idxs]\n\n\ndef most_common_label(y):\n    counter = Counter(y)\n    most_common = counter.most_common(1)[0][0]\n    return most_common\n\n\nclass RandomForest:\n    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n        self.n_trees = n_trees\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            tree = DecisionTree(\n                min_samples_split=self.min_samples_split,\n                max_depth=self.max_depth,\n                n_feats=self.n_feats,\n            )\n            X_samp, y_samp = bootstrap_sample(X, y)\n            tree.fit(X_samp, y_samp)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        tree_preds = np.swapaxes(tree_preds, 0, 1)\n        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n        return np.array(y_pred)\n\n\n```\n\n\n## References\n\n\n- https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","source":"_posts/ML-SP-Random_Forest.md","raw":"---\ntitle: '[Tree]Random Forest의 이해'\ncategories:\n  - Machine Learning\ntags:\n  - Random Forest\n  - Supervised Learning\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Random Forest\n\n---\n**_Concept_**\n\n- **Bagging** : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. \n- **weak learner** : 서로 독립적으로 만들어지며 **상관이 낮은** 약한 분류기.\n- **Random Subspace Method** : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용\n- **Random Forest** : 여러 `week learner`들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.\n- **Bootstrap** : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.\n- **OOB** : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.\n---\n\n> A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n\n**랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.**\n\n### Bagging\n\n<img src=\"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png\" width=\"700\" />\n\n배깅의 핵심적인 목표는 **의사결정 트리 사이의 분산을 줄이는 것이다.** . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 `부트스트래핑`을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 `랜덤 포레스트`이다. `랜덤포레스트`는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.\n\n### Random Subspace method\n\n- Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.\n- train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.\n- **feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 `weak learner`에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.**\n- 기본모델 생성시 **특성 m개 중 일부분 k개의 특성을 선택(sampling)한다** \n- k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.\n- $\\sqrt{m}$을 k로 활용할 수도 있다.\n- k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.\n- k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.\n- k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 \n- 서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**\n\n- 트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.\n![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcaLvgA%2FbtraSXTXHT3%2FT0aBmdkrhHd3FCKGsSWN9k%2Fimg.png)\n\n**배깅과 Random subspave method의 비교**\n\n- bagging: it is better when the training samples are sparse(결측값이 많은 경우)\n- Random subspace method: it is better when the classes are compact and the boundaries are smooth.\n\n### Random Forest 알고리즘\n\n- m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.\n- m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. \n- 각각의 약한 분류기로 결과를 예측한다.\n- 각 분류기의 예측결과를 모아 최종결과를 도출한다.\n  + 분류 문제일 경우 다수결을 통해 최종 결과를 도출한다\n  + 회귀 문제일 경우 평균을 통해 최종결과를 도출한다.\n\n### Random Forest 주요 hyperparameter\n\n[Randomforest Hyperparameter](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/)\nsklearn에서 제공하는 하이퍼파라미터 기준으로 정리\n\n- max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.\n- n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.\n- min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.\n- oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.\n\n```python\n# 보통 고려하는 것들\n{'bootstrap': True,\n 'criterion': 'mse',\n 'max_depth': 3, # depth가 3일때까지만 split\n 'max_features': 'auto',\n 'max_leaf_nodes': 4, # leaf node가 4개일때까지만 split\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 3, # 생성될 노드들의 샘플 수가 3개 이상이여만 split \n 'min_samples_split': 5, # 5개 이상의 샘플만 split\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 10,\n 'n_jobs': 1,\n 'oob_score': False,\n 'random_state': 42,\n 'verbose': 0,\n 'warm_start': False}\n```\n\n### Random Forest 장단점\n\n**장점**\n\n- 과적합에 강하다.\n- 이상치에 크게 영향받지 않는다.\n- Scaling이 필요가 없다.\n- 결측값에 크게 영향받지 않는다.\n\n**단점**\n\n- 고차원의 희소한 데이터에 대해 성능이 저하된다.\n- training 속도 느림(메모리 소모)\n- 개별 트리 분석이 어럽다.\n\n\n\n### Random Forest 구현\n\n- sklearn을 활용한 구현\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4,\n                            n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\nRandomForestClassifier(...)\nprint(clf.predict([[0, 0, 0, 0]]))\n\n```\n\n\n- numpy를 활용한 구현\n\n```python\nfrom collections import Counter\n\nimport numpy as np\n\nfrom .decision_tree import DecisionTree\n\n\ndef bootstrap_sample(X, y):\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, n_samples, replace=True)\n    return X[idxs], y[idxs]\n\n\ndef most_common_label(y):\n    counter = Counter(y)\n    most_common = counter.most_common(1)[0][0]\n    return most_common\n\n\nclass RandomForest:\n    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n        self.n_trees = n_trees\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            tree = DecisionTree(\n                min_samples_split=self.min_samples_split,\n                max_depth=self.max_depth,\n                n_feats=self.n_feats,\n            )\n            X_samp, y_samp = bootstrap_sample(X, y)\n            tree.fit(X_samp, y_samp)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        tree_preds = np.swapaxes(tree_preds, 0, 1)\n        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n        return np.array(y_pred)\n\n\n```\n\n\n## References\n\n\n- https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","slug":"ML-SP-Random_Forest","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc80016b36qc3lfayrx","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Random-Forest\"><a href=\"#Random-Forest\" class=\"headerlink\" title=\"Random Forest\"></a>Random Forest</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Bagging</strong> : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. </li>\n<li><strong>weak learner</strong> : 서로 독립적으로 만들어지며 <strong>상관이 낮은</strong> 약한 분류기.</li>\n<li><strong>Random Subspace Method</strong> : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용</li>\n<li><strong>Random Forest</strong> : 여러 <code>week learner</code>들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.</li>\n<li><strong>Bootstrap</strong> : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.</li>\n<li><strong>OOB</strong> : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.</li>\n</ul>\n<hr>\n<blockquote>\n<p>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</p>\n</blockquote>\n<p><strong>랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.</strong></p>\n<h3 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h3><img src=\"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png\" width=\"700\" />\n\n<p>배깅의 핵심적인 목표는 <strong>의사결정 트리 사이의 분산을 줄이는 것이다.</strong> . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 <code>부트스트래핑</code>을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 <code>랜덤 포레스트</code>이다. <code>랜덤포레스트</code>는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.</p>\n<h3 id=\"Random-Subspace-method\"><a href=\"#Random-Subspace-method\" class=\"headerlink\" title=\"Random Subspace method\"></a>Random Subspace method</h3><ul>\n<li><p>Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.</p>\n</li>\n<li><p>train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.</p>\n</li>\n<li><p><strong>feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 <code>weak learner</code>에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.</strong></p>\n</li>\n<li><p>기본모델 생성시 <strong>특성 m개 중 일부분 k개의 특성을 선택(sampling)한다</strong> </p>\n</li>\n<li><p>k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.</p>\n</li>\n<li><p>$\\sqrt{m}$을 k로 활용할 수도 있다.</p>\n</li>\n<li><p>k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.</p>\n</li>\n<li><p>k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.</p>\n</li>\n<li><p>k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 </p>\n</li>\n<li><p>서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**</p>\n</li>\n<li><p>트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.<br><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/caLvgA/btraSXTXHT3/T0aBmdkrhHd3FCKGsSWN9k/img.png\"></p>\n</li>\n</ul>\n<p><strong>배깅과 Random subspave method의 비교</strong></p>\n<ul>\n<li>bagging: it is better when the training samples are sparse(결측값이 많은 경우)</li>\n<li>Random subspace method: it is better when the classes are compact and the boundaries are smooth.</li>\n</ul>\n<h3 id=\"Random-Forest-알고리즘\"><a href=\"#Random-Forest-알고리즘\" class=\"headerlink\" title=\"Random Forest 알고리즘\"></a>Random Forest 알고리즘</h3><ul>\n<li>m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.</li>\n<li>m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. </li>\n<li>각각의 약한 분류기로 결과를 예측한다.</li>\n<li>각 분류기의 예측결과를 모아 최종결과를 도출한다.<ul>\n<li>분류 문제일 경우 다수결을 통해 최종 결과를 도출한다</li>\n<li>회귀 문제일 경우 평균을 통해 최종결과를 도출한다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Random-Forest-주요-hyperparameter\"><a href=\"#Random-Forest-주요-hyperparameter\" class=\"headerlink\" title=\"Random Forest 주요 hyperparameter\"></a>Random Forest 주요 hyperparameter</h3><p><a href=\"https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\">Randomforest Hyperparameter</a><br>sklearn에서 제공하는 하이퍼파라미터 기준으로 정리</p>\n<ul>\n<li>max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.</li>\n<li>n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.</li>\n<li>min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.</li>\n<li>oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 보통 고려하는 것들</span></span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;bootstrap&#x27;</span>: <span class=\"literal\">True</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;criterion&#x27;</span>: <span class=\"string\">&#x27;mse&#x27;</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;max_depth&#x27;</span>: <span class=\"number\">3</span>, <span class=\"comment\"># depth가 3일때까지만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;max_features&#x27;</span>: <span class=\"string\">&#x27;auto&#x27;</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;max_leaf_nodes&#x27;</span>: <span class=\"number\">4</span>, <span class=\"comment\"># leaf node가 4개일때까지만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_impurity_decrease&#x27;</span>: <span class=\"number\">0.0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;min_impurity_split&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;min_samples_leaf&#x27;</span>: <span class=\"number\">3</span>, <span class=\"comment\"># 생성될 노드들의 샘플 수가 3개 이상이여만 split </span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_samples_split&#x27;</span>: <span class=\"number\">5</span>, <span class=\"comment\"># 5개 이상의 샘플만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_weight_fraction_leaf&#x27;</span>: <span class=\"number\">0.0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;n_estimators&#x27;</span>: <span class=\"number\">10</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;n_jobs&#x27;</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;oob_score&#x27;</span>: <span class=\"literal\">False</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;random_state&#x27;</span>: <span class=\"number\">42</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;verbose&#x27;</span>: <span class=\"number\">0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;warm_start&#x27;</span>: <span class=\"literal\">False</span>&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Random-Forest-장단점\"><a href=\"#Random-Forest-장단점\" class=\"headerlink\" title=\"Random Forest 장단점\"></a>Random Forest 장단점</h3><p><strong>장점</strong></p>\n<ul>\n<li>과적합에 강하다.</li>\n<li>이상치에 크게 영향받지 않는다.</li>\n<li>Scaling이 필요가 없다.</li>\n<li>결측값에 크게 영향받지 않는다.</li>\n</ul>\n<p><strong>단점</strong></p>\n<ul>\n<li>고차원의 희소한 데이터에 대해 성능이 저하된다.</li>\n<li>training 속도 느림(메모리 소모)</li>\n<li>개별 트리 분석이 어럽다.</li>\n</ul>\n<h3 id=\"Random-Forest-구현\"><a href=\"#Random-Forest-구현\" class=\"headerlink\" title=\"Random Forest 구현\"></a>Random Forest 구현</h3><ul>\n<li>sklearn을 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\">X, y = make_classification(n_samples=<span class=\"number\">1000</span>, n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                            n_informative=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                            random_state=<span class=\"number\">0</span>, shuffle=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">clf = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X, y)</span><br><span class=\"line\">RandomForestClassifier(...)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(clf.predict([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]]))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>numpy를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> .decision_tree <span class=\"keyword\">import</span> DecisionTree</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bootstrap_sample</span>(<span class=\"params\">X, y</span>):</span><br><span class=\"line\">    n_samples = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    idxs = np.random.choice(n_samples, n_samples, replace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X[idxs], y[idxs]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">most_common_label</span>(<span class=\"params\">y</span>):</span><br><span class=\"line\">    counter = Counter(y)</span><br><span class=\"line\">    most_common = counter.most_common(<span class=\"number\">1</span>)[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> most_common</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RandomForest</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n_trees=<span class=\"number\">10</span>, min_samples_split=<span class=\"number\">2</span>, max_depth=<span class=\"number\">100</span>, n_feats=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.n_trees = n_trees</span><br><span class=\"line\">        self.min_samples_split = min_samples_split</span><br><span class=\"line\">        self.max_depth = max_depth</span><br><span class=\"line\">        self.n_feats = n_feats</span><br><span class=\"line\">        self.trees = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>):</span><br><span class=\"line\">        self.trees = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_trees):</span><br><span class=\"line\">            tree = DecisionTree(</span><br><span class=\"line\">                min_samples_split=self.min_samples_split,</span><br><span class=\"line\">                max_depth=self.max_depth,</span><br><span class=\"line\">                n_feats=self.n_feats,</span><br><span class=\"line\">            )</span><br><span class=\"line\">            X_samp, y_samp = bootstrap_sample(X, y)</span><br><span class=\"line\">            tree.fit(X_samp, y_samp)</span><br><span class=\"line\">            self.trees.append(tree)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        tree_preds = np.array([tree.predict(X) <span class=\"keyword\">for</span> tree <span class=\"keyword\">in</span> self.trees])</span><br><span class=\"line\">        tree_preds = np.swapaxes(tree_preds, <span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        y_pred = [most_common_label(tree_pred) <span class=\"keyword\">for</span> tree_pred <span class=\"keyword\">in</span> tree_preds]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array(y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Random-Forest\"><a href=\"#Random-Forest\" class=\"headerlink\" title=\"Random Forest\"></a>Random Forest</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Bagging</strong> : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. </li>\n<li><strong>weak learner</strong> : 서로 독립적으로 만들어지며 <strong>상관이 낮은</strong> 약한 분류기.</li>\n<li><strong>Random Subspace Method</strong> : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용</li>\n<li><strong>Random Forest</strong> : 여러 <code>week learner</code>들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.</li>\n<li><strong>Bootstrap</strong> : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.</li>\n<li><strong>OOB</strong> : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.</li>\n</ul>\n<hr>\n<blockquote>\n<p>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</p>\n</blockquote>\n<p><strong>랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.</strong></p>\n<h3 id=\"Bagging\"><a href=\"#Bagging\" class=\"headerlink\" title=\"Bagging\"></a>Bagging</h3><img src=\"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png\" width=\"700\" />\n\n<p>배깅의 핵심적인 목표는 <strong>의사결정 트리 사이의 분산을 줄이는 것이다.</strong> . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 <code>부트스트래핑</code>을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 <code>랜덤 포레스트</code>이다. <code>랜덤포레스트</code>는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.</p>\n<h3 id=\"Random-Subspace-method\"><a href=\"#Random-Subspace-method\" class=\"headerlink\" title=\"Random Subspace method\"></a>Random Subspace method</h3><ul>\n<li><p>Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.</p>\n</li>\n<li><p>train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.</p>\n</li>\n<li><p><strong>feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 <code>weak learner</code>에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.</strong></p>\n</li>\n<li><p>기본모델 생성시 <strong>특성 m개 중 일부분 k개의 특성을 선택(sampling)한다</strong> </p>\n</li>\n<li><p>k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.</p>\n</li>\n<li><p>$\\sqrt{m}$을 k로 활용할 수도 있다.</p>\n</li>\n<li><p>k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.</p>\n</li>\n<li><p>k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.</p>\n</li>\n<li><p>k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 </p>\n</li>\n<li><p>서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**</p>\n</li>\n<li><p>트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.<br><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/caLvgA/btraSXTXHT3/T0aBmdkrhHd3FCKGsSWN9k/img.png\"></p>\n</li>\n</ul>\n<p><strong>배깅과 Random subspave method의 비교</strong></p>\n<ul>\n<li>bagging: it is better when the training samples are sparse(결측값이 많은 경우)</li>\n<li>Random subspace method: it is better when the classes are compact and the boundaries are smooth.</li>\n</ul>\n<h3 id=\"Random-Forest-알고리즘\"><a href=\"#Random-Forest-알고리즘\" class=\"headerlink\" title=\"Random Forest 알고리즘\"></a>Random Forest 알고리즘</h3><ul>\n<li>m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.</li>\n<li>m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. </li>\n<li>각각의 약한 분류기로 결과를 예측한다.</li>\n<li>각 분류기의 예측결과를 모아 최종결과를 도출한다.<ul>\n<li>분류 문제일 경우 다수결을 통해 최종 결과를 도출한다</li>\n<li>회귀 문제일 경우 평균을 통해 최종결과를 도출한다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Random-Forest-주요-hyperparameter\"><a href=\"#Random-Forest-주요-hyperparameter\" class=\"headerlink\" title=\"Random Forest 주요 hyperparameter\"></a>Random Forest 주요 hyperparameter</h3><p><a href=\"https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\">Randomforest Hyperparameter</a><br>sklearn에서 제공하는 하이퍼파라미터 기준으로 정리</p>\n<ul>\n<li>max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.</li>\n<li>n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.</li>\n<li>min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.</li>\n<li>oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 보통 고려하는 것들</span></span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;bootstrap&#x27;</span>: <span class=\"literal\">True</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;criterion&#x27;</span>: <span class=\"string\">&#x27;mse&#x27;</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;max_depth&#x27;</span>: <span class=\"number\">3</span>, <span class=\"comment\"># depth가 3일때까지만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;max_features&#x27;</span>: <span class=\"string\">&#x27;auto&#x27;</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;max_leaf_nodes&#x27;</span>: <span class=\"number\">4</span>, <span class=\"comment\"># leaf node가 4개일때까지만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_impurity_decrease&#x27;</span>: <span class=\"number\">0.0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;min_impurity_split&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;min_samples_leaf&#x27;</span>: <span class=\"number\">3</span>, <span class=\"comment\"># 생성될 노드들의 샘플 수가 3개 이상이여만 split </span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_samples_split&#x27;</span>: <span class=\"number\">5</span>, <span class=\"comment\"># 5개 이상의 샘플만 split</span></span><br><span class=\"line\"> <span class=\"string\">&#x27;min_weight_fraction_leaf&#x27;</span>: <span class=\"number\">0.0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;n_estimators&#x27;</span>: <span class=\"number\">10</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;n_jobs&#x27;</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;oob_score&#x27;</span>: <span class=\"literal\">False</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;random_state&#x27;</span>: <span class=\"number\">42</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;verbose&#x27;</span>: <span class=\"number\">0</span>,</span><br><span class=\"line\"> <span class=\"string\">&#x27;warm_start&#x27;</span>: <span class=\"literal\">False</span>&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Random-Forest-장단점\"><a href=\"#Random-Forest-장단점\" class=\"headerlink\" title=\"Random Forest 장단점\"></a>Random Forest 장단점</h3><p><strong>장점</strong></p>\n<ul>\n<li>과적합에 강하다.</li>\n<li>이상치에 크게 영향받지 않는다.</li>\n<li>Scaling이 필요가 없다.</li>\n<li>결측값에 크게 영향받지 않는다.</li>\n</ul>\n<p><strong>단점</strong></p>\n<ul>\n<li>고차원의 희소한 데이터에 대해 성능이 저하된다.</li>\n<li>training 속도 느림(메모리 소모)</li>\n<li>개별 트리 분석이 어럽다.</li>\n</ul>\n<h3 id=\"Random-Forest-구현\"><a href=\"#Random-Forest-구현\" class=\"headerlink\" title=\"Random Forest 구현\"></a>Random Forest 구현</h3><ul>\n<li>sklearn을 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\">X, y = make_classification(n_samples=<span class=\"number\">1000</span>, n_features=<span class=\"number\">4</span>,</span><br><span class=\"line\">                            n_informative=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>,</span><br><span class=\"line\">                            random_state=<span class=\"number\">0</span>, shuffle=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">clf = RandomForestClassifier(max_depth=<span class=\"number\">2</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf.fit(X, y)</span><br><span class=\"line\">RandomForestClassifier(...)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(clf.predict([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]]))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>numpy를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> .decision_tree <span class=\"keyword\">import</span> DecisionTree</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bootstrap_sample</span>(<span class=\"params\">X, y</span>):</span><br><span class=\"line\">    n_samples = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    idxs = np.random.choice(n_samples, n_samples, replace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X[idxs], y[idxs]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">most_common_label</span>(<span class=\"params\">y</span>):</span><br><span class=\"line\">    counter = Counter(y)</span><br><span class=\"line\">    most_common = counter.most_common(<span class=\"number\">1</span>)[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> most_common</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">RandomForest</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n_trees=<span class=\"number\">10</span>, min_samples_split=<span class=\"number\">2</span>, max_depth=<span class=\"number\">100</span>, n_feats=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        self.n_trees = n_trees</span><br><span class=\"line\">        self.min_samples_split = min_samples_split</span><br><span class=\"line\">        self.max_depth = max_depth</span><br><span class=\"line\">        self.n_feats = n_feats</span><br><span class=\"line\">        self.trees = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>):</span><br><span class=\"line\">        self.trees = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_trees):</span><br><span class=\"line\">            tree = DecisionTree(</span><br><span class=\"line\">                min_samples_split=self.min_samples_split,</span><br><span class=\"line\">                max_depth=self.max_depth,</span><br><span class=\"line\">                n_feats=self.n_feats,</span><br><span class=\"line\">            )</span><br><span class=\"line\">            X_samp, y_samp = bootstrap_sample(X, y)</span><br><span class=\"line\">            tree.fit(X_samp, y_samp)</span><br><span class=\"line\">            self.trees.append(tree)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        tree_preds = np.array([tree.predict(X) <span class=\"keyword\">for</span> tree <span class=\"keyword\">in</span> self.trees])</span><br><span class=\"line\">        tree_preds = np.swapaxes(tree_preds, <span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        y_pred = [most_common_label(tree_pred) <span class=\"keyword\">for</span> tree_pred <span class=\"keyword\">in</span> tree_preds]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array(y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Tree]Random Forest의 이해","path":"2022/06/13/ML-SP-Random_Forest/","eyeCatchImage":"https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Random Forest"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[SVM]서포트벡터머신의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<!--\n진짜 ref\nhttps://excelsior-cjh.tistory.com/165\n\n\n진짜 가장중요한 ref\nhttps://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin\n\n-->\n\n---\n**_Concept_**\n\n- **결정 경계**: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.\n- **초평면(hyperplane)** : flat affine subspace of p-1 (p는 데이터의 차원) \n- **Support Vector** : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트\n- **Margin** : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)\n- **Support Vector Machine** : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.\n  + **Soft Margin** : **Allow misclassification**. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.\n  + **Hard Margin**: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.\n---\n\n### Note\n---\n- 데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.\n- 기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.\n- margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)\n- 마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,\n- 패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.\n  + maximizing the margin and minimizing the loss\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png)\n\nmargin이 최대화 하려면 결정경계에 해당하는 wx+b=0이 되게끔 하는 w를 찾아야 한다.\n이는 `wx+b=0`에 수직인 벡터(법선벡터)인 $\\frac{2}{\\|\\boldsymbol{w}\\|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)\n따라서 $\\frac{2}{\\|\\boldsymbol{w}\\|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.\nGraidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{\\|\\boldsymbol{w}\\|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.\n\n$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$\n\nclass label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.\n\n- **양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.**\n\n이를 모두 만족하는 제약식은 아래와 같다.\n\n$\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$\n\n\n따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.\n\n\n$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$\n\n$$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$\n\n### Soft Margin\n---\n\n소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. \n\nhyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.\n\n반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.\n\n소프트 마진 SVM의 최적화 함수는 다음과 같다.\n\n아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0>인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.\n\n$$\\min \\frac{1}{2}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{m} \\zeta_{i}$$\n\n{% raw %}\n\n$$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$\n\n{% endraw %}\n\n### Hinge Loss\n---\n\nmax(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.\n\nSVM의 loss function은 `hinge loss` 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.\n\nSVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.\n\n결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.\n\n결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.\n\n![](https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png)\n\n### 구현\n\n- iris data set에 대해 soft margin 구현\n\n사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.\n```python\nfrom sklearn.svm import svc\nlinear_svm = SVC(kernel='linear',C=1.0, random_state=42)\n\nlinear_svm.fit(X_train,y_train)\n```\n\n- numpy로 직접구현\n\n\n```python\n# numpy로 svm구현\n\nimport numpy as np\n\nclass SVM:\n\n    def __init__(self,learning_rate=0.0001,lambda_param =0.01,n_iter =1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self,X,y):\n        y_ = np.where(y<=0 ,-1, 1)\n        n_samples = X.shape\n\n        self.w = np.zeros(n_features) # 가중치 초기화\n        self.b = 0 # 편향 초기화\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                \"\"\"\n                current index, data point\n                \"\"\"\n                condition = y_[idx] * (np.dot(x_i,self.w)) >= 1 # 제약조건 구현\n\n                # 가중치 업데이트(hinge loss의 gradient update)\n\n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i,y_[idx]))\n\n\n    def predict(self,X):\n        linear_output = np.dot(X,self.w) - self.b\n        return np.sign(linear_output) # numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환\n\n# weight가 주어졌을 경우 SVM을 시각화하는 함수\ndef visualize_svm():\n    def get_hyperplane_value(x, w, b, offset):\n        return (-w[0] * x + b + offset) / w[1]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n\n    x0_1 = np.amin(X[:, 0])\n    x0_2 = np.amax(X[:, 0])\n\n    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n    ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n\n    x1_min = np.amin(X[:, 1])\n    x1_max = np.amax(X[:, 1])\n    ax.set_ylim([x1_min - 3, x1_max + 3])\n\n    plt.show()\n\n\n```\n\n\n\n**Reference & Annotaion**\n\n- https://youtu.be/efR1C6CvhmE\n- https://en.wikipedia.org/wiki/Support-vector_machine\n- https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1\n- 데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.\n- 법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.","source":"_posts/ML-SP-SVM.md","raw":"---\ntitle: '[SVM]서포트벡터머신의 이해'\ncategories:\n  - - Machine Learning\n    - Supervised Learning\ndate:\nupdated:\ntags:\n  - SVM\n  - hyperplane\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<!--\n진짜 ref\nhttps://excelsior-cjh.tistory.com/165\n\n\n진짜 가장중요한 ref\nhttps://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin\n\n-->\n\n---\n**_Concept_**\n\n- **결정 경계**: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.\n- **초평면(hyperplane)** : flat affine subspace of p-1 (p는 데이터의 차원) \n- **Support Vector** : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트\n- **Margin** : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)\n- **Support Vector Machine** : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.\n  + **Soft Margin** : **Allow misclassification**. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.\n  + **Hard Margin**: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.\n---\n\n### Note\n---\n- 데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.\n- 기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.\n- margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)\n- 마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,\n- 패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.\n  + maximizing the margin and minimizing the loss\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png)\n\nmargin이 최대화 하려면 결정경계에 해당하는 wx+b=0이 되게끔 하는 w를 찾아야 한다.\n이는 `wx+b=0`에 수직인 벡터(법선벡터)인 $\\frac{2}{\\|\\boldsymbol{w}\\|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)\n따라서 $\\frac{2}{\\|\\boldsymbol{w}\\|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.\nGraidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{\\|\\boldsymbol{w}\\|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.\n\n$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$\n\nclass label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.\n\n- **양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.**\n\n이를 모두 만족하는 제약식은 아래와 같다.\n\n$\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$\n\n\n따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.\n\n\n$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$\n\n$$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$\n\n### Soft Margin\n---\n\n소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. \n\nhyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.\n\n반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.\n\n소프트 마진 SVM의 최적화 함수는 다음과 같다.\n\n아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0>인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.\n\n$$\\min \\frac{1}{2}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{m} \\zeta_{i}$$\n\n{% raw %}\n\n$$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$\n\n{% endraw %}\n\n### Hinge Loss\n---\n\nmax(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.\n\nSVM의 loss function은 `hinge loss` 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.\n\nSVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.\n\n결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.\n\n결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.\n\n![](https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png)\n\n### 구현\n\n- iris data set에 대해 soft margin 구현\n\n사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.\n```python\nfrom sklearn.svm import svc\nlinear_svm = SVC(kernel='linear',C=1.0, random_state=42)\n\nlinear_svm.fit(X_train,y_train)\n```\n\n- numpy로 직접구현\n\n\n```python\n# numpy로 svm구현\n\nimport numpy as np\n\nclass SVM:\n\n    def __init__(self,learning_rate=0.0001,lambda_param =0.01,n_iter =1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self,X,y):\n        y_ = np.where(y<=0 ,-1, 1)\n        n_samples = X.shape\n\n        self.w = np.zeros(n_features) # 가중치 초기화\n        self.b = 0 # 편향 초기화\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                \"\"\"\n                current index, data point\n                \"\"\"\n                condition = y_[idx] * (np.dot(x_i,self.w)) >= 1 # 제약조건 구현\n\n                # 가중치 업데이트(hinge loss의 gradient update)\n\n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i,y_[idx]))\n\n\n    def predict(self,X):\n        linear_output = np.dot(X,self.w) - self.b\n        return np.sign(linear_output) # numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환\n\n# weight가 주어졌을 경우 SVM을 시각화하는 함수\ndef visualize_svm():\n    def get_hyperplane_value(x, w, b, offset):\n        return (-w[0] * x + b + offset) / w[1]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n\n    x0_1 = np.amin(X[:, 0])\n    x0_2 = np.amax(X[:, 0])\n\n    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n    ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n\n    x1_min = np.amin(X[:, 1])\n    x1_max = np.amax(X[:, 1])\n    ax.set_ylim([x1_min - 3, x1_max + 3])\n\n    plt.show()\n\n\n```\n\n\n\n**Reference & Annotaion**\n\n- https://youtu.be/efR1C6CvhmE\n- https://en.wikipedia.org/wiki/Support-vector_machine\n- https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1\n- 데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.\n- 법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.","slug":"ML-SP-SVM","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc80018b36qb7312dam","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<!--\n진짜 ref\nhttps://excelsior-cjh.tistory.com/165\n\n\n진짜 가장중요한 ref\nhttps://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin\n\n-->\n\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>결정 경계</strong>: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.</li>\n<li><strong>초평면(hyperplane)</strong> : flat affine subspace of p-1 (p는 데이터의 차원) </li>\n<li><strong>Support Vector</strong> : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트</li>\n<li><strong>Margin</strong> : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)</li>\n<li><strong>Support Vector Machine</strong> : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.<ul>\n<li><strong>Soft Margin</strong> : <strong>Allow misclassification</strong>. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.</li>\n<li><strong>Hard Margin</strong>: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h3><hr>\n<ul>\n<li>데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.</li>\n<li>기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.</li>\n<li>margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)</li>\n<li>마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,</li>\n<li>패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.<ul>\n<li>maximizing the margin and minimizing the loss</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png\"></p>\n<p>margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.<br>이는 <code>wx+b=0</code>에 수직인 벡터(법선벡터)인 $\\frac{2}{|\\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)<br>따라서 $\\frac{2}{|\\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.<br>Graidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{|\\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}|\\boldsymbol{w}|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.</p>\n<ul>\n<li><strong>양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.</strong></li>\n</ul>\n<p>이를 모두 만족하는 제약식은 아래와 같다.</p>\n<p>$\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$</p>\n<p>따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>$$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$</p>\n<h3 id=\"Soft-Margin\"><a href=\"#Soft-Margin\" class=\"headerlink\" title=\"Soft Margin\"></a>Soft Margin</h3><hr>\n<p>소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. </p>\n<p>hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.</p>\n<p>반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.</p>\n<p>소프트 마진 SVM의 최적화 함수는 다음과 같다.</p>\n<p>아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.</p>\n<p>$$\\min \\frac{1}{2}|\\mathbf{w}|^{2}+C \\sum_{i&#x3D;1}^{m} \\zeta_{i}$$</p>\n\n\n$$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$\n\n\n\n<h3 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h3><hr>\n<p>max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.</p>\n<p>SVM의 loss function은 <code>hinge loss</code> 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.</p>\n<p>SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.</p>\n<p>결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.</p>\n<p>결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.</p>\n<p><img src=\"https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png\"></p>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><ul>\n<li>iris data set에 대해 soft margin 구현</li>\n</ul>\n<p>사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> svc</span><br><span class=\"line\">linear_svm = SVC(kernel=<span class=\"string\">&#x27;linear&#x27;</span>,C=<span class=\"number\">1.0</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">linear_svm.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>numpy로 직접구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># numpy로 svm구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SVM</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,learning_rate=<span class=\"number\">0.0001</span>,lambda_param =<span class=\"number\">0.01</span>,n_iter =<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">        self.lr = learning_rate</span><br><span class=\"line\">        self.lambda_param = lambda_param</span><br><span class=\"line\">        self.n_iters = n_iters</span><br><span class=\"line\">        self.w = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.b = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">        y_ = np.where(y&lt;=<span class=\"number\">0</span> ,-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        n_samples = X.shape</span><br><span class=\"line\"></span><br><span class=\"line\">        self.w = np.zeros(n_features) <span class=\"comment\"># 가중치 초기화</span></span><br><span class=\"line\">        self.b = <span class=\"number\">0</span> <span class=\"comment\"># 편향 초기화</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> idx, x_i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(X):</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">                current index, data point</span></span><br><span class=\"line\"><span class=\"string\">                &quot;&quot;&quot;</span></span><br><span class=\"line\">                condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= <span class=\"number\">1</span> <span class=\"comment\"># 제약조건 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># 가중치 업데이트(hinge loss의 gradient update)</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> condition:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w - np.dot(x_i,y_[idx]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">        linear_output = np.dot(X,self.w) - self.b</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.sign(linear_output) <span class=\"comment\"># numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weight가 주어졌을 경우 SVM을 시각화하는 함수</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">visualize_svm</span>():</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">get_hyperplane_value</span>(<span class=\"params\">x, w, b, offset</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (-w[<span class=\"number\">0</span>] * x + b + offset) / w[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    fig = plt.figure()</span><br><span class=\"line\">    ax = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&quot;o&quot;</span>, c=y)</span><br><span class=\"line\"></span><br><span class=\"line\">    x0_1 = np.amin(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    x0_2 = np.amax(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\">    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1, x1_2], <span class=\"string\">&quot;y--&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_min = np.amin(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1_max = np.amax(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    ax.set_ylim([x1_min - <span class=\"number\">3</span>, x1_max + <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>Reference &amp; Annotaion</strong></p>\n<ul>\n<li><a href=\"https://youtu.be/efR1C6CvhmE\">https://youtu.be/efR1C6CvhmE</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>\n<li><a href=\"https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1\">https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1</a></li>\n<li>데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.</li>\n<li>법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<!--\n진짜 ref\nhttps://excelsior-cjh.tistory.com/165\n\n\n진짜 가장중요한 ref\nhttps://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin\n\n-->\n\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>결정 경계</strong>: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.</li>\n<li><strong>초평면(hyperplane)</strong> : flat affine subspace of p-1 (p는 데이터의 차원) </li>\n<li><strong>Support Vector</strong> : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트</li>\n<li><strong>Margin</strong> : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)</li>\n<li><strong>Support Vector Machine</strong> : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.<ul>\n<li><strong>Soft Margin</strong> : <strong>Allow misclassification</strong>. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.</li>\n<li><strong>Hard Margin</strong>: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h3><hr>\n<ul>\n<li>데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.</li>\n<li>기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.</li>\n<li>margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)</li>\n<li>마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,</li>\n<li>패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.<ul>\n<li>maximizing the margin and minimizing the loss</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png\"></p>\n<p>margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.<br>이는 <code>wx+b=0</code>에 수직인 벡터(법선벡터)인 $\\frac{2}{|\\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)<br>따라서 $\\frac{2}{|\\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.<br>Graidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{|\\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}|\\boldsymbol{w}|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.</p>\n<ul>\n<li><strong>양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.</strong></li>\n</ul>\n<p>이를 모두 만족하는 제약식은 아래와 같다.</p>\n<p>$\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$</p>\n<p>따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>$$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$</p>\n<h3 id=\"Soft-Margin\"><a href=\"#Soft-Margin\" class=\"headerlink\" title=\"Soft Margin\"></a>Soft Margin</h3><hr>\n<p>소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. </p>\n<p>hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.</p>\n<p>반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.</p>\n<p>소프트 마진 SVM의 최적화 함수는 다음과 같다.</p>\n<p>아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.</p>\n<p>$$\\min \\frac{1}{2}|\\mathbf{w}|^{2}+C \\sum_{i&#x3D;1}^{m} \\zeta_{i}$$</p>\n\n\n$$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$\n\n\n\n<h3 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h3><hr>\n<p>max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.</p>\n<p>SVM의 loss function은 <code>hinge loss</code> 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.</p>\n<p>SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.</p>\n<p>결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.</p>\n<p>결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.</p>\n<p><img src=\"https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png\"></p>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><ul>\n<li>iris data set에 대해 soft margin 구현</li>\n</ul>\n<p>사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> svc</span><br><span class=\"line\">linear_svm = SVC(kernel=<span class=\"string\">&#x27;linear&#x27;</span>,C=<span class=\"number\">1.0</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">linear_svm.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>numpy로 직접구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># numpy로 svm구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SVM</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,learning_rate=<span class=\"number\">0.0001</span>,lambda_param =<span class=\"number\">0.01</span>,n_iter =<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">        self.lr = learning_rate</span><br><span class=\"line\">        self.lambda_param = lambda_param</span><br><span class=\"line\">        self.n_iters = n_iters</span><br><span class=\"line\">        self.w = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.b = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">        y_ = np.where(y&lt;=<span class=\"number\">0</span> ,-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        n_samples = X.shape</span><br><span class=\"line\"></span><br><span class=\"line\">        self.w = np.zeros(n_features) <span class=\"comment\"># 가중치 초기화</span></span><br><span class=\"line\">        self.b = <span class=\"number\">0</span> <span class=\"comment\"># 편향 초기화</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> idx, x_i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(X):</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">                current index, data point</span></span><br><span class=\"line\"><span class=\"string\">                &quot;&quot;&quot;</span></span><br><span class=\"line\">                condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= <span class=\"number\">1</span> <span class=\"comment\"># 제약조건 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># 가중치 업데이트(hinge loss의 gradient update)</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> condition:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w - np.dot(x_i,y_[idx]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">        linear_output = np.dot(X,self.w) - self.b</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.sign(linear_output) <span class=\"comment\"># numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weight가 주어졌을 경우 SVM을 시각화하는 함수</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">visualize_svm</span>():</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">get_hyperplane_value</span>(<span class=\"params\">x, w, b, offset</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (-w[<span class=\"number\">0</span>] * x + b + offset) / w[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    fig = plt.figure()</span><br><span class=\"line\">    ax = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&quot;o&quot;</span>, c=y)</span><br><span class=\"line\"></span><br><span class=\"line\">    x0_1 = np.amin(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    x0_2 = np.amax(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\">    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1, x1_2], <span class=\"string\">&quot;y--&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_min = np.amin(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1_max = np.amax(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    ax.set_ylim([x1_min - <span class=\"number\">3</span>, x1_max + <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>Reference &amp; Annotaion</strong></p>\n<ul>\n<li><a href=\"https://youtu.be/efR1C6CvhmE\">https://youtu.be/efR1C6CvhmE</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>\n<li><a href=\"https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1\">https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1</a></li>\n<li>데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.</li>\n<li>법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[SVM]서포트벡터머신의 이해","path":"2022/06/13/ML-SP-SVM/","eyeCatchImage":"https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning > Supervised Learning","tags":["SVM","hyperplane"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Classification]로지스틱 회귀와 크로스엔트로피","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center >Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Deep Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n## Logistic Regression\n\n\n**로지스틱회귀의 파라미터 추정은 Feature X에 대한 선형회귀모델을 X에 대한 target의 log odds에 Fitting하는 것이다.**\n\n**Fitting의 방식**\n\nMaximum Likelyhood를 최대화하는 것 = 이항편차(binary deviance)를 줄이는 것 = cross entropy loss 를 줄이는 것\n\nMLE(Maximum Likelyhood Estimation)를 통해 이해할 수도 있지만 여기서는 Cross Entropy를 통한 최적화 관점에서의 Logistic 회귀를 주로 다룬다.\n\n---\n**_Concept_**\n\n- **Cross Entropy** :두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량\n  + **Cross Entropy** 는 기본적으로 **추정된 분포가 실제의 분포와 열마나 가까운지** 를 결정한다. \n- **sigmoid** : input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종\n- **softmax** : 로지스틱 회귀를 다중 클래스 분류로 확장할때 사용하는 활성화 함수.분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.\n- **모수(Parameter)** : 머신러닝에서 파라미터는 **모델의 형태를 결정하는 값**으로 정의할 수 있다. 예를 들어 y = wx+b라는 모델이 주어졌을 때 w,b가 모델의 모수가 된다.\n- **우도** : 확률 분포의 모수가, 어떤 확률변수의 표집값과 일관되는 정도를 나타내는 값. **관측값이 고정되어있을 때** 그 관측치가 어떤 확률분포에서 나왔는가의 문제 \n- **최대우도추정(Maxium Likelyhood Estimation)** :얻어진 데이터를 토대로 그 확률변수의 모수를 구하는 방법이다. 어떤 모수가 주어졌을 때, 원하는 값들이 나올 우도함수를 최대로 만드는 **모수를 선택하는 방법**이다.\n  + **관측값(데이터) 가 주어진 상태에서 그 관측값이 나올 우도함수 **\n  + – MLE관점에서 볼때 로지스틱회귀는 x와 w가 주어졌을 때 y의 확률. y의 확률이 나올 수 있는 w의 최대값을 구하는 문제이다.\n- 우도함수 :  가능도함수는 모수가 $\\theta$일 때, 특정 표본 x 가 나타날 함수.\n---\n\n\n**정리**\n\n- Linear regression 에서는 연속적인 값을 출력하는 반면 Logistic regression에서 기본적으로 기대되는 output은 확률이다.\n- **오차함수의 기본적인 검증 방식이 0이나 1로부터 예측값이 얼마나 떨어져 있는 지를 측정하는 것이다**\n- 확률을 도출하기 위해 **선형모델에 sigmoid 함수를 적용**한다.\n- 기본적으로 L2규제를 적욯하기 때문에 규제를 강하게 하면 계수가 0에 가깝게 되지만 완전히 0이 되지는 않는다.\n- 설명하기 쉬운 모델을 원한다면 특성의 개수를 제한하는 L1규제를 사용한다. \n  + L1규제는 0이 많은 sparse data에 적합하다.\n- Hyperparameter C를 통해 규제의 강도를 결정한다.\n- C는 Ridge나 Lasso에서의 규제강도인 alpha($\\lambda$)의 inverse이다. 다시말해 **C 값이 높을 수록 규제가 감소하고 C값이 낮으면 계수 벡터가 0에 가까워진다**.(피쳐의 영향이 줄어든다.)\n- 이는 C의 값이 낮다면 데이터 포인트를 다수에 맞추려고 하는 반면 C의 값이 높다면 개개의 데이터 포인트를 명확하게 분류하고자 하는 알고리즘으로 볼 수 있다.\n\n\n#### Cost function in Logistic Regression (Cross Entropy)\n\n- **Cross Entropy는 두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량이다**. 즉, 서로 다른 두 확률분포에 대해 같은 사건이 가지는 정보량을 계산한 것이다.\n- 기본적으로 **추정된 분포가 실제의 분포와 열마나 가까운지** 를 결정한다.\n  + p(x)는 true label의 분포를 one-hot encoding 형식으로 나타낸 것이다.\n  + q(x)는 현재 예측모델의 추정값의 분포이다.\n- 모형이 예측한 확률분포들 중 정답에 해당하는 위치의 뉴런에 -log를 취한 것이 출력값이 된다.\n- `-log` 를 취하는 이유는 출력값이 0,1 사이의 확률값으로 나와하 하기때문이다.\n\n![크로스 엔트로피 수식](https://i.stack.imgur.com/gNip2.png)\n\n- MSE을 비용함수로 사용할 경우 국소 최소값에 빠질 가능성이 있어 크로스 엔트로피 함수를 사용한다.\n- **정답에 해당하는 뉴런값의 오차만 계산에 들어간다는 것이 특징이다.**\n\n- 정답에 해당하는 위치의 뉴런이 0에 가까워 질수록 y값이 exponential하게 증가하게 된다.\n- Best case는 모델이 예측한 분포와 타겟의 분포가 같은 경우. 이 경우 오차가 0이된다.\n- worst case는 target 위치의 뉴런 값이 0인 경우이며 이 때 Cross Entropy 오차는 무한히 증가한다. \n\n![크로스 엔트로피 오차](https://ml-cheatsheet.readthedocs.io/en/latest/_images/y1andy2_logistic_function.png)\n\n- 크로스 엔트로피 구현\n\n```python\nimport numpy as np\n\np = np.array([0, 1, 0])             # True probability (one-hot)\nq = np.array([0.228, 0.619, 0.153]) # Predicted probability\n\ncross_entropy_loss = -np.sum(p * np.log(q))\nprint(cross_entropy_loss)\n# 0.47965000629754095\n```\n\n- 크로스 엔트로피 비용함수를 통해 로지스틱 회귀 모형의 목적함수를 정의할 수 있다.\n  + $\\lambda \\rVert W \\rVert_2$ 는 l2 규제항이다.\n  + $\\lambda$ 가 0이 되면 규제항이 없는 단순 로지스틱회귀가 된다. \n  + $\\lambda$가 커질수록 W가 줄어든다.($\\lambda$가 무한대로 가면 가중치는 0으로 수렴)\n  + $\\lambda$ 수치를 조정함으로서  fit과 magnitude 사이에서 균형을 맞출 수 있다.\n  + C는 $\\lambda$ 역수로 sklearn에서 hyperparameter로 쓰인다.\n\n$$J(w) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]+\\lambda \\rVert W \\rVert_2$$\n\n아래와 같이 보다 간소화 해서 작성할 수 있다.\n\n$$J(w)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{Cost}\\left(h\\left(x^{(i)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} w_{j}^{2}$$\n\n\n이 경우 업데이트 시 **Gradiant를** 아래아 같이 작성할 수 있다.\n\n$$\\frac{\\partial}{\\partial w_{i}} J(w)=\\frac{1}{m}\\left[\\sum_{j=1}^{m}\\left(h\\left(x^{(j)}\\right)-y^{(j)}\\right) x_{i}^{(j)}+\\lambda w_{i}\\right]$$\n\n#### Sigmoid Function\n\n\n- input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종.\n- 기준점인 0.5를 기준으로 출력값을 결정한다.\n- w가 커질수록 기울기가 커진다.\n- b의 크기에 따라 함수 자체가 이동한다.\n\n```python\n# 시그모이드 구현\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# sigmoid non-convex logistic least squares cost function\n# convex 한것은 비용함수가 구불구불해서 국소 최저점이 존재하는 것\n\ndef sigmoid_least_squares(w):\n    cost = 0\n    for p in range(y.size):\n        x_p = x[:,p]\n        y_p = y[:,p]\n        cost += (sigmoid(w[0] + w[1]*x_p) - y_p)**2\n    return cost/y.size\n```\n\n![](https://jermwatt.github.io/machine_learning_refined/mlrefined_images/superlearn_images/sigmoid.png)\n\n\n### Softmax\n\n- 로지스틱 회귀를 다중클래스 분류로 확장한 것.\n- 분류하고자 하는 클래스가 n개일 때, n차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다.\n- 소프트맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.\n\n![](https://www.gstatic.com/education/formulas2/397133473/en/softmax_function.svg)\n\n- **확률의 총합이 1이다.**\n\n- 식 자체는 단순하게 `probability = exp(value) / sum v in list exp(v)` 로 나타낼 수 있다. `n번째 일 확률 / 전체 확률` 로 생각하면 된다. \n- [지수함수(exp)가 식에 포함된 이유](https://gooopy.tistory.com/53)\n\n- softmax 구현\n```python\nIn [3]: from numpy import exp\n   ...:\n   ...: \n   ...: def softmax(vector):\n   ...:   e = exp(vector)\n   ...:   return e / e.sum()\n   ...:\n   ...: \n   ...: data = [1, 3, 2]\n   ...: # convert list of numbers to a list of probabilities\n   ...: result = softmax(data)\n   ...: # report the probabilities\n   ...: print(result)\n   ...: # report the sum of the probabilities\n   ...: print(sum(result))\n[0.09003057 0.66524096 0.24472847]\n1.0\n\n```\n\n### 구현\n\n- numpy를 활용한 구현\n```python\n# 정확도를 측정하기 위한 accuracy 함수\ndef accuracy(y_true,y_pred):\n  accuracy = np.sum(y_true==y_pred)/len(y_true)\n\n  return accuracy\n\n\nimport numpy as np\n\nclass LogisticRegression:\n\n  def __init__(self,lr=0.001,n_iters=1000):\n    self.lr = lr\n    self.n_iters = n_iters\n    self.weigts = None\n    self.bias = None\n\n\n  def fit(self,X,y):\n    # init parameters\n\n    n_samples, n_features = X.shape\n\n    self.weigts = np.zeros(n_features)\n    self.bias =0\n\n    # gradient descent\n\n    for _ in range(self.n_iters):\n      linear_model = np.dot(X,self.weigts)+self.bias\n\n      y_pred = self._sigmoid(linear_model) # 선형모델에 sigmoid 적용\n\n\n      dw = (1/n_samples) * np.dot(X.T,(y_pred-y)) # W에 대해 편미분\n      db = (1/n_samples) * np.sum(y_pred-y)\n\n\n      self.weigts = -= self.lr * dw\n      self.bias = -= self.lr*db\n\n\n  def predict(self,X):\n    linear_model = np.dot(X,self.weigts)+self.bias\n    y_pred = self._sigmoid(linear_model) # 0~1 사이의 float을 반환\n\n    y_pred_class = [1 if i > 0.5 else 0 for i in y_pred] # 값을 0,1로 고정\n\n    return y_pred_class\n\n\n  def _sigmoid(self,x):\n    return 1/(1+np.exp(-x))\n```\n\n- sklearn을 활용한 구현\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(random_state=0).fit(X, y)\nclf.predict(X[:2, :])\n\nclf.predict_proba(X[:2, :]) # 확률 출력\n\n\nclf.score(X, y)\n```\n\n- tensorflow를 활용한 구현\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import optimizers\n\n\nx = np.array([0.42264594, 0.4524148 , 0.93797131, 0.36534474, 0.40276151,0.29153749, 0.05982402, 0.24713247, 0.91650771, 0.45207763])\ny = np.array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) \n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=1, activation='sigmoid'))\n\nsgd = optimizers.SGD(lr=0.001)\nmodel.compile(optimizer=sgd ,loss='binary_crossentropy', metrics=['binary_accuracy'])\n\nmodel.fit(x, y, epochs=200)\n```\n\n\n## References\n\n- [Cross Entropy](https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932)\n- [Logistic Regression](https://youtu.be/yIYKR4sgzI8?list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe)\n- [로지스틱 회귀 구현](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)\n- [소프트맥스](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)\n","source":"_posts/ML-SP-logistic_regression.md","raw":"---\ntitle: '[Classification]로지스틱 회귀와 크로스엔트로피'\ncategories:\n  - [Machine Learning]\ntags:\n  - Logistic Regression\n  - Cross Entropy\n  - Supervised Learning\ndate:\nupdated:\n---\n\n<!--\n\n<center >Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Deep Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n## Logistic Regression\n\n\n**로지스틱회귀의 파라미터 추정은 Feature X에 대한 선형회귀모델을 X에 대한 target의 log odds에 Fitting하는 것이다.**\n\n**Fitting의 방식**\n\nMaximum Likelyhood를 최대화하는 것 = 이항편차(binary deviance)를 줄이는 것 = cross entropy loss 를 줄이는 것\n\nMLE(Maximum Likelyhood Estimation)를 통해 이해할 수도 있지만 여기서는 Cross Entropy를 통한 최적화 관점에서의 Logistic 회귀를 주로 다룬다.\n\n---\n**_Concept_**\n\n- **Cross Entropy** :두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량\n  + **Cross Entropy** 는 기본적으로 **추정된 분포가 실제의 분포와 열마나 가까운지** 를 결정한다. \n- **sigmoid** : input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종\n- **softmax** : 로지스틱 회귀를 다중 클래스 분류로 확장할때 사용하는 활성화 함수.분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.\n- **모수(Parameter)** : 머신러닝에서 파라미터는 **모델의 형태를 결정하는 값**으로 정의할 수 있다. 예를 들어 y = wx+b라는 모델이 주어졌을 때 w,b가 모델의 모수가 된다.\n- **우도** : 확률 분포의 모수가, 어떤 확률변수의 표집값과 일관되는 정도를 나타내는 값. **관측값이 고정되어있을 때** 그 관측치가 어떤 확률분포에서 나왔는가의 문제 \n- **최대우도추정(Maxium Likelyhood Estimation)** :얻어진 데이터를 토대로 그 확률변수의 모수를 구하는 방법이다. 어떤 모수가 주어졌을 때, 원하는 값들이 나올 우도함수를 최대로 만드는 **모수를 선택하는 방법**이다.\n  + **관측값(데이터) 가 주어진 상태에서 그 관측값이 나올 우도함수 **\n  + – MLE관점에서 볼때 로지스틱회귀는 x와 w가 주어졌을 때 y의 확률. y의 확률이 나올 수 있는 w의 최대값을 구하는 문제이다.\n- 우도함수 :  가능도함수는 모수가 $\\theta$일 때, 특정 표본 x 가 나타날 함수.\n---\n\n\n**정리**\n\n- Linear regression 에서는 연속적인 값을 출력하는 반면 Logistic regression에서 기본적으로 기대되는 output은 확률이다.\n- **오차함수의 기본적인 검증 방식이 0이나 1로부터 예측값이 얼마나 떨어져 있는 지를 측정하는 것이다**\n- 확률을 도출하기 위해 **선형모델에 sigmoid 함수를 적용**한다.\n- 기본적으로 L2규제를 적욯하기 때문에 규제를 강하게 하면 계수가 0에 가깝게 되지만 완전히 0이 되지는 않는다.\n- 설명하기 쉬운 모델을 원한다면 특성의 개수를 제한하는 L1규제를 사용한다. \n  + L1규제는 0이 많은 sparse data에 적합하다.\n- Hyperparameter C를 통해 규제의 강도를 결정한다.\n- C는 Ridge나 Lasso에서의 규제강도인 alpha($\\lambda$)의 inverse이다. 다시말해 **C 값이 높을 수록 규제가 감소하고 C값이 낮으면 계수 벡터가 0에 가까워진다**.(피쳐의 영향이 줄어든다.)\n- 이는 C의 값이 낮다면 데이터 포인트를 다수에 맞추려고 하는 반면 C의 값이 높다면 개개의 데이터 포인트를 명확하게 분류하고자 하는 알고리즘으로 볼 수 있다.\n\n\n#### Cost function in Logistic Regression (Cross Entropy)\n\n- **Cross Entropy는 두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량이다**. 즉, 서로 다른 두 확률분포에 대해 같은 사건이 가지는 정보량을 계산한 것이다.\n- 기본적으로 **추정된 분포가 실제의 분포와 열마나 가까운지** 를 결정한다.\n  + p(x)는 true label의 분포를 one-hot encoding 형식으로 나타낸 것이다.\n  + q(x)는 현재 예측모델의 추정값의 분포이다.\n- 모형이 예측한 확률분포들 중 정답에 해당하는 위치의 뉴런에 -log를 취한 것이 출력값이 된다.\n- `-log` 를 취하는 이유는 출력값이 0,1 사이의 확률값으로 나와하 하기때문이다.\n\n![크로스 엔트로피 수식](https://i.stack.imgur.com/gNip2.png)\n\n- MSE을 비용함수로 사용할 경우 국소 최소값에 빠질 가능성이 있어 크로스 엔트로피 함수를 사용한다.\n- **정답에 해당하는 뉴런값의 오차만 계산에 들어간다는 것이 특징이다.**\n\n- 정답에 해당하는 위치의 뉴런이 0에 가까워 질수록 y값이 exponential하게 증가하게 된다.\n- Best case는 모델이 예측한 분포와 타겟의 분포가 같은 경우. 이 경우 오차가 0이된다.\n- worst case는 target 위치의 뉴런 값이 0인 경우이며 이 때 Cross Entropy 오차는 무한히 증가한다. \n\n![크로스 엔트로피 오차](https://ml-cheatsheet.readthedocs.io/en/latest/_images/y1andy2_logistic_function.png)\n\n- 크로스 엔트로피 구현\n\n```python\nimport numpy as np\n\np = np.array([0, 1, 0])             # True probability (one-hot)\nq = np.array([0.228, 0.619, 0.153]) # Predicted probability\n\ncross_entropy_loss = -np.sum(p * np.log(q))\nprint(cross_entropy_loss)\n# 0.47965000629754095\n```\n\n- 크로스 엔트로피 비용함수를 통해 로지스틱 회귀 모형의 목적함수를 정의할 수 있다.\n  + $\\lambda \\rVert W \\rVert_2$ 는 l2 규제항이다.\n  + $\\lambda$ 가 0이 되면 규제항이 없는 단순 로지스틱회귀가 된다. \n  + $\\lambda$가 커질수록 W가 줄어든다.($\\lambda$가 무한대로 가면 가중치는 0으로 수렴)\n  + $\\lambda$ 수치를 조정함으로서  fit과 magnitude 사이에서 균형을 맞출 수 있다.\n  + C는 $\\lambda$ 역수로 sklearn에서 hyperparameter로 쓰인다.\n\n$$J(w) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]+\\lambda \\rVert W \\rVert_2$$\n\n아래와 같이 보다 간소화 해서 작성할 수 있다.\n\n$$J(w)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{Cost}\\left(h\\left(x^{(i)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2 m} \\sum_{j=1}^{n} w_{j}^{2}$$\n\n\n이 경우 업데이트 시 **Gradiant를** 아래아 같이 작성할 수 있다.\n\n$$\\frac{\\partial}{\\partial w_{i}} J(w)=\\frac{1}{m}\\left[\\sum_{j=1}^{m}\\left(h\\left(x^{(j)}\\right)-y^{(j)}\\right) x_{i}^{(j)}+\\lambda w_{i}\\right]$$\n\n#### Sigmoid Function\n\n\n- input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종.\n- 기준점인 0.5를 기준으로 출력값을 결정한다.\n- w가 커질수록 기울기가 커진다.\n- b의 크기에 따라 함수 자체가 이동한다.\n\n```python\n# 시그모이드 구현\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# sigmoid non-convex logistic least squares cost function\n# convex 한것은 비용함수가 구불구불해서 국소 최저점이 존재하는 것\n\ndef sigmoid_least_squares(w):\n    cost = 0\n    for p in range(y.size):\n        x_p = x[:,p]\n        y_p = y[:,p]\n        cost += (sigmoid(w[0] + w[1]*x_p) - y_p)**2\n    return cost/y.size\n```\n\n![](https://jermwatt.github.io/machine_learning_refined/mlrefined_images/superlearn_images/sigmoid.png)\n\n\n### Softmax\n\n- 로지스틱 회귀를 다중클래스 분류로 확장한 것.\n- 분류하고자 하는 클래스가 n개일 때, n차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다.\n- 소프트맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.\n\n![](https://www.gstatic.com/education/formulas2/397133473/en/softmax_function.svg)\n\n- **확률의 총합이 1이다.**\n\n- 식 자체는 단순하게 `probability = exp(value) / sum v in list exp(v)` 로 나타낼 수 있다. `n번째 일 확률 / 전체 확률` 로 생각하면 된다. \n- [지수함수(exp)가 식에 포함된 이유](https://gooopy.tistory.com/53)\n\n- softmax 구현\n```python\nIn [3]: from numpy import exp\n   ...:\n   ...: \n   ...: def softmax(vector):\n   ...:   e = exp(vector)\n   ...:   return e / e.sum()\n   ...:\n   ...: \n   ...: data = [1, 3, 2]\n   ...: # convert list of numbers to a list of probabilities\n   ...: result = softmax(data)\n   ...: # report the probabilities\n   ...: print(result)\n   ...: # report the sum of the probabilities\n   ...: print(sum(result))\n[0.09003057 0.66524096 0.24472847]\n1.0\n\n```\n\n### 구현\n\n- numpy를 활용한 구현\n```python\n# 정확도를 측정하기 위한 accuracy 함수\ndef accuracy(y_true,y_pred):\n  accuracy = np.sum(y_true==y_pred)/len(y_true)\n\n  return accuracy\n\n\nimport numpy as np\n\nclass LogisticRegression:\n\n  def __init__(self,lr=0.001,n_iters=1000):\n    self.lr = lr\n    self.n_iters = n_iters\n    self.weigts = None\n    self.bias = None\n\n\n  def fit(self,X,y):\n    # init parameters\n\n    n_samples, n_features = X.shape\n\n    self.weigts = np.zeros(n_features)\n    self.bias =0\n\n    # gradient descent\n\n    for _ in range(self.n_iters):\n      linear_model = np.dot(X,self.weigts)+self.bias\n\n      y_pred = self._sigmoid(linear_model) # 선형모델에 sigmoid 적용\n\n\n      dw = (1/n_samples) * np.dot(X.T,(y_pred-y)) # W에 대해 편미분\n      db = (1/n_samples) * np.sum(y_pred-y)\n\n\n      self.weigts = -= self.lr * dw\n      self.bias = -= self.lr*db\n\n\n  def predict(self,X):\n    linear_model = np.dot(X,self.weigts)+self.bias\n    y_pred = self._sigmoid(linear_model) # 0~1 사이의 float을 반환\n\n    y_pred_class = [1 if i > 0.5 else 0 for i in y_pred] # 값을 0,1로 고정\n\n    return y_pred_class\n\n\n  def _sigmoid(self,x):\n    return 1/(1+np.exp(-x))\n```\n\n- sklearn을 활용한 구현\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(random_state=0).fit(X, y)\nclf.predict(X[:2, :])\n\nclf.predict_proba(X[:2, :]) # 확률 출력\n\n\nclf.score(X, y)\n```\n\n- tensorflow를 활용한 구현\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import optimizers\n\n\nx = np.array([0.42264594, 0.4524148 , 0.93797131, 0.36534474, 0.40276151,0.29153749, 0.05982402, 0.24713247, 0.91650771, 0.45207763])\ny = np.array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) \n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=1, activation='sigmoid'))\n\nsgd = optimizers.SGD(lr=0.001)\nmodel.compile(optimizer=sgd ,loss='binary_crossentropy', metrics=['binary_accuracy'])\n\nmodel.fit(x, y, epochs=200)\n```\n\n\n## References\n\n- [Cross Entropy](https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932)\n- [Logistic Regression](https://youtu.be/yIYKR4sgzI8?list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe)\n- [로지스틱 회귀 구현](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)\n- [소프트맥스](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)\n","slug":"ML-SP-logistic_regression","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsc9001cb36qcefp4dga","content":"<!--\n\n<center >Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Deep Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><p><strong>로지스틱회귀의 파라미터 추정은 Feature X에 대한 선형회귀모델을 X에 대한 target의 log odds에 Fitting하는 것이다.</strong></p>\n<p><strong>Fitting의 방식</strong></p>\n<p>Maximum Likelyhood를 최대화하는 것 &#x3D; 이항편차(binary deviance)를 줄이는 것 &#x3D; cross entropy loss 를 줄이는 것</p>\n<p>MLE(Maximum Likelyhood Estimation)를 통해 이해할 수도 있지만 여기서는 Cross Entropy를 통한 최적화 관점에서의 Logistic 회귀를 주로 다룬다.</p>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Cross Entropy</strong> :두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량<ul>\n<li><strong>Cross Entropy</strong> 는 기본적으로 <strong>추정된 분포가 실제의 분포와 열마나 가까운지</strong> 를 결정한다.</li>\n</ul>\n</li>\n<li><strong>sigmoid</strong> : input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종</li>\n<li><strong>softmax</strong> : 로지스틱 회귀를 다중 클래스 분류로 확장할때 사용하는 활성화 함수.분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.</li>\n<li><strong>모수(Parameter)</strong> : 머신러닝에서 파라미터는 <strong>모델의 형태를 결정하는 값</strong>으로 정의할 수 있다. 예를 들어 y &#x3D; wx+b라는 모델이 주어졌을 때 w,b가 모델의 모수가 된다.</li>\n<li><strong>우도</strong> : 확률 분포의 모수가, 어떤 확률변수의 표집값과 일관되는 정도를 나타내는 값. <strong>관측값이 고정되어있을 때</strong> 그 관측치가 어떤 확률분포에서 나왔는가의 문제 </li>\n<li><strong>최대우도추정(Maxium Likelyhood Estimation)</strong> :얻어진 데이터를 토대로 그 확률변수의 모수를 구하는 방법이다. 어떤 모수가 주어졌을 때, 원하는 값들이 나올 우도함수를 최대로 만드는 <strong>모수를 선택하는 방법</strong>이다.<ul>\n<li>**관측값(데이터) 가 주어진 상태에서 그 관측값이 나올 우도함수 **</li>\n<li>– MLE관점에서 볼때 로지스틱회귀는 x와 w가 주어졌을 때 y의 확률. y의 확률이 나올 수 있는 w의 최대값을 구하는 문제이다.</li>\n</ul>\n</li>\n<li>우도함수 :  가능도함수는 모수가 $\\theta$일 때, 특정 표본 x 가 나타날 함수.</li>\n</ul>\n<hr>\n<p><strong>정리</strong></p>\n<ul>\n<li>Linear regression 에서는 연속적인 값을 출력하는 반면 Logistic regression에서 기본적으로 기대되는 output은 확률이다.</li>\n<li><strong>오차함수의 기본적인 검증 방식이 0이나 1로부터 예측값이 얼마나 떨어져 있는 지를 측정하는 것이다</strong></li>\n<li>확률을 도출하기 위해 <strong>선형모델에 sigmoid 함수를 적용</strong>한다.</li>\n<li>기본적으로 L2규제를 적욯하기 때문에 규제를 강하게 하면 계수가 0에 가깝게 되지만 완전히 0이 되지는 않는다.</li>\n<li>설명하기 쉬운 모델을 원한다면 특성의 개수를 제한하는 L1규제를 사용한다. <ul>\n<li>L1규제는 0이 많은 sparse data에 적합하다.</li>\n</ul>\n</li>\n<li>Hyperparameter C를 통해 규제의 강도를 결정한다.</li>\n<li>C는 Ridge나 Lasso에서의 규제강도인 alpha($\\lambda$)의 inverse이다. 다시말해 <strong>C 값이 높을 수록 규제가 감소하고 C값이 낮으면 계수 벡터가 0에 가까워진다</strong>.(피쳐의 영향이 줄어든다.)</li>\n<li>이는 C의 값이 낮다면 데이터 포인트를 다수에 맞추려고 하는 반면 C의 값이 높다면 개개의 데이터 포인트를 명확하게 분류하고자 하는 알고리즘으로 볼 수 있다.</li>\n</ul>\n<h4 id=\"Cost-function-in-Logistic-Regression-Cross-Entropy\"><a href=\"#Cost-function-in-Logistic-Regression-Cross-Entropy\" class=\"headerlink\" title=\"Cost function in Logistic Regression (Cross Entropy)\"></a>Cost function in Logistic Regression (Cross Entropy)</h4><ul>\n<li><strong>Cross Entropy는 두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량이다</strong>. 즉, 서로 다른 두 확률분포에 대해 같은 사건이 가지는 정보량을 계산한 것이다.</li>\n<li>기본적으로 <strong>추정된 분포가 실제의 분포와 열마나 가까운지</strong> 를 결정한다.<ul>\n<li>p(x)는 true label의 분포를 one-hot encoding 형식으로 나타낸 것이다.</li>\n<li>q(x)는 현재 예측모델의 추정값의 분포이다.</li>\n</ul>\n</li>\n<li>모형이 예측한 확률분포들 중 정답에 해당하는 위치의 뉴런에 -log를 취한 것이 출력값이 된다.</li>\n<li><code>-log</code> 를 취하는 이유는 출력값이 0,1 사이의 확률값으로 나와하 하기때문이다.</li>\n</ul>\n<p><img src=\"https://i.stack.imgur.com/gNip2.png\" alt=\"크로스 엔트로피 수식\"></p>\n<ul>\n<li><p>MSE을 비용함수로 사용할 경우 국소 최소값에 빠질 가능성이 있어 크로스 엔트로피 함수를 사용한다.</p>\n</li>\n<li><p><strong>정답에 해당하는 뉴런값의 오차만 계산에 들어간다는 것이 특징이다.</strong></p>\n</li>\n<li><p>정답에 해당하는 위치의 뉴런이 0에 가까워 질수록 y값이 exponential하게 증가하게 된다.</p>\n</li>\n<li><p>Best case는 모델이 예측한 분포와 타겟의 분포가 같은 경우. 이 경우 오차가 0이된다.</p>\n</li>\n<li><p>worst case는 target 위치의 뉴런 값이 0인 경우이며 이 때 Cross Entropy 오차는 무한히 증가한다.</p>\n</li>\n</ul>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/y1andy2_logistic_function.png\" alt=\"크로스 엔트로피 오차\"></p>\n<ul>\n<li>크로스 엔트로피 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">p = np.array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>])             <span class=\"comment\"># True probability (one-hot)</span></span><br><span class=\"line\">q = np.array([<span class=\"number\">0.228</span>, <span class=\"number\">0.619</span>, <span class=\"number\">0.153</span>]) <span class=\"comment\"># Predicted probability</span></span><br><span class=\"line\"></span><br><span class=\"line\">cross_entropy_loss = -np.<span class=\"built_in\">sum</span>(p * np.log(q))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(cross_entropy_loss)</span><br><span class=\"line\"><span class=\"comment\"># 0.47965000629754095</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>크로스 엔트로피 비용함수를 통해 로지스틱 회귀 모형의 목적함수를 정의할 수 있다.<ul>\n<li>$\\lambda \\rVert W \\rVert_2$ 는 l2 규제항이다.</li>\n<li>$\\lambda$ 가 0이 되면 규제항이 없는 단순 로지스틱회귀가 된다. </li>\n<li>$\\lambda$가 커질수록 W가 줄어든다.($\\lambda$가 무한대로 가면 가중치는 0으로 수렴)</li>\n<li>$\\lambda$ 수치를 조정함으로서  fit과 magnitude 사이에서 균형을 맞출 수 있다.</li>\n<li>C는 $\\lambda$ 역수로 sklearn에서 hyperparameter로 쓰인다.</li>\n</ul>\n</li>\n</ul>\n<p>$$J(w) &#x3D; -\\frac{1}{m} \\sum_{i&#x3D;1}^{m} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]+\\lambda \\rVert W \\rVert_2$$</p>\n<p>아래와 같이 보다 간소화 해서 작성할 수 있다.</p>\n<p>$$J(w)&#x3D;\\frac{1}{m} \\sum_{i&#x3D;1}^{m} \\operatorname{Cost}\\left(h\\left(x^{(i)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2 m} \\sum_{j&#x3D;1}^{n} w_{j}^{2}$$</p>\n<p>이 경우 업데이트 시 <strong>Gradiant를</strong> 아래아 같이 작성할 수 있다.</p>\n<p>$$\\frac{\\partial}{\\partial w_{i}} J(w)&#x3D;\\frac{1}{m}\\left[\\sum_{j&#x3D;1}^{m}\\left(h\\left(x^{(j)}\\right)-y^{(j)}\\right) x_{i}^{(j)}+\\lambda w_{i}\\right]$$</p>\n<h4 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h4><ul>\n<li>input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종.</li>\n<li>기준점인 0.5를 기준으로 출력값을 결정한다.</li>\n<li>w가 커질수록 기울기가 커진다.</li>\n<li>b의 크기에 따라 함수 자체가 이동한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 시그모이드 구현</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sigmoid non-convex logistic least squares cost function</span></span><br><span class=\"line\"><span class=\"comment\"># convex 한것은 비용함수가 구불구불해서 국소 최저점이 존재하는 것</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid_least_squares</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    cost = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(y.size):</span><br><span class=\"line\">        x_p = x[:,p]</span><br><span class=\"line\">        y_p = y[:,p]</span><br><span class=\"line\">        cost += (sigmoid(w[<span class=\"number\">0</span>] + w[<span class=\"number\">1</span>]*x_p) - y_p)**<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost/y.size</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://jermwatt.github.io/machine_learning_refined/mlrefined_images/superlearn_images/sigmoid.png\"></p>\n<h3 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h3><ul>\n<li>로지스틱 회귀를 다중클래스 분류로 확장한 것.</li>\n<li>분류하고자 하는 클래스가 n개일 때, n차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다.</li>\n<li>소프트맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.</li>\n</ul>\n<p><img src=\"https://www.gstatic.com/education/formulas2/397133473/en/softmax_function.svg\"></p>\n<ul>\n<li><p><strong>확률의 총합이 1이다.</strong></p>\n</li>\n<li><p>식 자체는 단순하게 <code>probability = exp(value) / sum v in list exp(v)</code> 로 나타낼 수 있다. <code>n번째 일 확률 / 전체 확률</code> 로 생각하면 된다. </p>\n</li>\n<li><p><a href=\"https://gooopy.tistory.com/53\">지수함수(exp)가 식에 포함된 이유</a></p>\n</li>\n<li><p>softmax 구현</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"keyword\">from</span> numpy <span class=\"keyword\">import</span> exp</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: </span><br><span class=\"line\">   ...: <span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">vector</span>):</span><br><span class=\"line\">   ...:   e = exp(vector)</span><br><span class=\"line\">   ...:   <span class=\"keyword\">return</span> e / e.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: </span><br><span class=\"line\">   ...: data = [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">   ...: <span class=\"comment\"># convert list of numbers to a list of probabilities</span></span><br><span class=\"line\">   ...: result = softmax(data)</span><br><span class=\"line\">   ...: <span class=\"comment\"># report the probabilities</span></span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(result)</span><br><span class=\"line\">   ...: <span class=\"comment\"># report the sum of the probabilities</span></span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"built_in\">sum</span>(result))</span><br><span class=\"line\">[<span class=\"number\">0.09003057</span> <span class=\"number\">0.66524096</span> <span class=\"number\">0.24472847</span>]</span><br><span class=\"line\"><span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><ul>\n<li><p>numpy를 활용한 구현</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 정확도를 측정하기 위한 accuracy 함수</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_true,y_pred</span>):</span><br><span class=\"line\">  accuracy = np.<span class=\"built_in\">sum</span>(y_true==y_pred)/<span class=\"built_in\">len</span>(y_true)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LogisticRegression</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,lr=<span class=\"number\">0.001</span>,n_iters=<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">    self.lr = lr</span><br><span class=\"line\">    self.n_iters = n_iters</span><br><span class=\"line\">    self.weigts = <span class=\"literal\">None</span></span><br><span class=\"line\">    self.bias = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">    <span class=\"comment\"># init parameters</span></span><br><span class=\"line\"></span><br><span class=\"line\">    n_samples, n_features = X.shape</span><br><span class=\"line\"></span><br><span class=\"line\">    self.weigts = np.zeros(n_features)</span><br><span class=\"line\">    self.bias =<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># gradient descent</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">      linear_model = np.dot(X,self.weigts)+self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">      y_pred = self._sigmoid(linear_model) <span class=\"comment\"># 선형모델에 sigmoid 적용</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      dw = (<span class=\"number\">1</span>/n_samples) * np.dot(X.T,(y_pred-y)) <span class=\"comment\"># W에 대해 편미분</span></span><br><span class=\"line\">      db = (<span class=\"number\">1</span>/n_samples) * np.<span class=\"built_in\">sum</span>(y_pred-y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      self.weigts = -= self.lr * dw</span><br><span class=\"line\">      self.bias = -= self.lr*db</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">    linear_model = np.dot(X,self.weigts)+self.bias</span><br><span class=\"line\">    y_pred = self._sigmoid(linear_model) <span class=\"comment\"># 0~1 사이의 float을 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\">    y_pred_class = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> i &gt; <span class=\"number\">0.5</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y_pred] <span class=\"comment\"># 값을 0,1로 고정</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_pred_class</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">_sigmoid</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sklearn을 활용한 구현</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\">X, y = load_iris(return_X_y=<span class=\"literal\">True</span>)</span><br><span class=\"line\">clf = LogisticRegression(random_state=<span class=\"number\">0</span>).fit(X, y)</span><br><span class=\"line\">clf.predict(X[:<span class=\"number\">2</span>, :])</span><br><span class=\"line\"></span><br><span class=\"line\">clf.predict_proba(X[:<span class=\"number\">2</span>, :]) <span class=\"comment\"># 확률 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf.score(X, y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tensorflow를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.models <span class=\"keyword\">import</span> Sequential</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> Dense</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras <span class=\"keyword\">import</span> optimizers</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([<span class=\"number\">0.42264594</span>, <span class=\"number\">0.4524148</span> , <span class=\"number\">0.93797131</span>, <span class=\"number\">0.36534474</span>, <span class=\"number\">0.40276151</span>,<span class=\"number\">0.29153749</span>, <span class=\"number\">0.05982402</span>, <span class=\"number\">0.24713247</span>, <span class=\"number\">0.91650771</span>, <span class=\"number\">0.45207763</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]) </span><br><span class=\"line\"></span><br><span class=\"line\">model = Sequential()</span><br><span class=\"line\">model.add(Dense(<span class=\"number\">1</span>, input_dim=<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">sgd = optimizers.SGD(lr=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=sgd ,loss=<span class=\"string\">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class=\"string\">&#x27;binary_accuracy&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">model.fit(x, y, epochs=<span class=\"number\">200</span>)</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932\">Cross Entropy</a></li>\n<li><a href=\"https://youtu.be/yIYKR4sgzI8?list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe\">Logistic Regression</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\">로지스틱 회귀 구현</a></li>\n<li><a href=\"https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\">소프트맥스</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center >Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Deep Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h2 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h2><p><strong>로지스틱회귀의 파라미터 추정은 Feature X에 대한 선형회귀모델을 X에 대한 target의 log odds에 Fitting하는 것이다.</strong></p>\n<p><strong>Fitting의 방식</strong></p>\n<p>Maximum Likelyhood를 최대화하는 것 &#x3D; 이항편차(binary deviance)를 줄이는 것 &#x3D; cross entropy loss 를 줄이는 것</p>\n<p>MLE(Maximum Likelyhood Estimation)를 통해 이해할 수도 있지만 여기서는 Cross Entropy를 통한 최적화 관점에서의 Logistic 회귀를 주로 다룬다.</p>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Cross Entropy</strong> :두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량<ul>\n<li><strong>Cross Entropy</strong> 는 기본적으로 <strong>추정된 분포가 실제의 분포와 열마나 가까운지</strong> 를 결정한다.</li>\n</ul>\n</li>\n<li><strong>sigmoid</strong> : input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종</li>\n<li><strong>softmax</strong> : 로지스틱 회귀를 다중 클래스 분류로 확장할때 사용하는 활성화 함수.분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.</li>\n<li><strong>모수(Parameter)</strong> : 머신러닝에서 파라미터는 <strong>모델의 형태를 결정하는 값</strong>으로 정의할 수 있다. 예를 들어 y &#x3D; wx+b라는 모델이 주어졌을 때 w,b가 모델의 모수가 된다.</li>\n<li><strong>우도</strong> : 확률 분포의 모수가, 어떤 확률변수의 표집값과 일관되는 정도를 나타내는 값. <strong>관측값이 고정되어있을 때</strong> 그 관측치가 어떤 확률분포에서 나왔는가의 문제 </li>\n<li><strong>최대우도추정(Maxium Likelyhood Estimation)</strong> :얻어진 데이터를 토대로 그 확률변수의 모수를 구하는 방법이다. 어떤 모수가 주어졌을 때, 원하는 값들이 나올 우도함수를 최대로 만드는 <strong>모수를 선택하는 방법</strong>이다.<ul>\n<li>**관측값(데이터) 가 주어진 상태에서 그 관측값이 나올 우도함수 **</li>\n<li>– MLE관점에서 볼때 로지스틱회귀는 x와 w가 주어졌을 때 y의 확률. y의 확률이 나올 수 있는 w의 최대값을 구하는 문제이다.</li>\n</ul>\n</li>\n<li>우도함수 :  가능도함수는 모수가 $\\theta$일 때, 특정 표본 x 가 나타날 함수.</li>\n</ul>\n<hr>\n<p><strong>정리</strong></p>\n<ul>\n<li>Linear regression 에서는 연속적인 값을 출력하는 반면 Logistic regression에서 기본적으로 기대되는 output은 확률이다.</li>\n<li><strong>오차함수의 기본적인 검증 방식이 0이나 1로부터 예측값이 얼마나 떨어져 있는 지를 측정하는 것이다</strong></li>\n<li>확률을 도출하기 위해 <strong>선형모델에 sigmoid 함수를 적용</strong>한다.</li>\n<li>기본적으로 L2규제를 적욯하기 때문에 규제를 강하게 하면 계수가 0에 가깝게 되지만 완전히 0이 되지는 않는다.</li>\n<li>설명하기 쉬운 모델을 원한다면 특성의 개수를 제한하는 L1규제를 사용한다. <ul>\n<li>L1규제는 0이 많은 sparse data에 적합하다.</li>\n</ul>\n</li>\n<li>Hyperparameter C를 통해 규제의 강도를 결정한다.</li>\n<li>C는 Ridge나 Lasso에서의 규제강도인 alpha($\\lambda$)의 inverse이다. 다시말해 <strong>C 값이 높을 수록 규제가 감소하고 C값이 낮으면 계수 벡터가 0에 가까워진다</strong>.(피쳐의 영향이 줄어든다.)</li>\n<li>이는 C의 값이 낮다면 데이터 포인트를 다수에 맞추려고 하는 반면 C의 값이 높다면 개개의 데이터 포인트를 명확하게 분류하고자 하는 알고리즘으로 볼 수 있다.</li>\n</ul>\n<h4 id=\"Cost-function-in-Logistic-Regression-Cross-Entropy\"><a href=\"#Cost-function-in-Logistic-Regression-Cross-Entropy\" class=\"headerlink\" title=\"Cost function in Logistic Regression (Cross Entropy)\"></a>Cost function in Logistic Regression (Cross Entropy)</h4><ul>\n<li><strong>Cross Entropy는 두 개의 확률분포 p와 q에 대해 하나의 사건 X가 갖는 정보량이다</strong>. 즉, 서로 다른 두 확률분포에 대해 같은 사건이 가지는 정보량을 계산한 것이다.</li>\n<li>기본적으로 <strong>추정된 분포가 실제의 분포와 열마나 가까운지</strong> 를 결정한다.<ul>\n<li>p(x)는 true label의 분포를 one-hot encoding 형식으로 나타낸 것이다.</li>\n<li>q(x)는 현재 예측모델의 추정값의 분포이다.</li>\n</ul>\n</li>\n<li>모형이 예측한 확률분포들 중 정답에 해당하는 위치의 뉴런에 -log를 취한 것이 출력값이 된다.</li>\n<li><code>-log</code> 를 취하는 이유는 출력값이 0,1 사이의 확률값으로 나와하 하기때문이다.</li>\n</ul>\n<p><img src=\"https://i.stack.imgur.com/gNip2.png\" alt=\"크로스 엔트로피 수식\"></p>\n<ul>\n<li><p>MSE을 비용함수로 사용할 경우 국소 최소값에 빠질 가능성이 있어 크로스 엔트로피 함수를 사용한다.</p>\n</li>\n<li><p><strong>정답에 해당하는 뉴런값의 오차만 계산에 들어간다는 것이 특징이다.</strong></p>\n</li>\n<li><p>정답에 해당하는 위치의 뉴런이 0에 가까워 질수록 y값이 exponential하게 증가하게 된다.</p>\n</li>\n<li><p>Best case는 모델이 예측한 분포와 타겟의 분포가 같은 경우. 이 경우 오차가 0이된다.</p>\n</li>\n<li><p>worst case는 target 위치의 뉴런 값이 0인 경우이며 이 때 Cross Entropy 오차는 무한히 증가한다.</p>\n</li>\n</ul>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/y1andy2_logistic_function.png\" alt=\"크로스 엔트로피 오차\"></p>\n<ul>\n<li>크로스 엔트로피 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">p = np.array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>])             <span class=\"comment\"># True probability (one-hot)</span></span><br><span class=\"line\">q = np.array([<span class=\"number\">0.228</span>, <span class=\"number\">0.619</span>, <span class=\"number\">0.153</span>]) <span class=\"comment\"># Predicted probability</span></span><br><span class=\"line\"></span><br><span class=\"line\">cross_entropy_loss = -np.<span class=\"built_in\">sum</span>(p * np.log(q))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(cross_entropy_loss)</span><br><span class=\"line\"><span class=\"comment\"># 0.47965000629754095</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>크로스 엔트로피 비용함수를 통해 로지스틱 회귀 모형의 목적함수를 정의할 수 있다.<ul>\n<li>$\\lambda \\rVert W \\rVert_2$ 는 l2 규제항이다.</li>\n<li>$\\lambda$ 가 0이 되면 규제항이 없는 단순 로지스틱회귀가 된다. </li>\n<li>$\\lambda$가 커질수록 W가 줄어든다.($\\lambda$가 무한대로 가면 가중치는 0으로 수렴)</li>\n<li>$\\lambda$ 수치를 조정함으로서  fit과 magnitude 사이에서 균형을 맞출 수 있다.</li>\n<li>C는 $\\lambda$ 역수로 sklearn에서 hyperparameter로 쓰인다.</li>\n</ul>\n</li>\n</ul>\n<p>$$J(w) &#x3D; -\\frac{1}{m} \\sum_{i&#x3D;1}^{m} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]+\\lambda \\rVert W \\rVert_2$$</p>\n<p>아래와 같이 보다 간소화 해서 작성할 수 있다.</p>\n<p>$$J(w)&#x3D;\\frac{1}{m} \\sum_{i&#x3D;1}^{m} \\operatorname{Cost}\\left(h\\left(x^{(i)}\\right), y^{(i)}\\right)+\\frac{\\lambda}{2 m} \\sum_{j&#x3D;1}^{n} w_{j}^{2}$$</p>\n<p>이 경우 업데이트 시 <strong>Gradiant를</strong> 아래아 같이 작성할 수 있다.</p>\n<p>$$\\frac{\\partial}{\\partial w_{i}} J(w)&#x3D;\\frac{1}{m}\\left[\\sum_{j&#x3D;1}^{m}\\left(h\\left(x^{(j)}\\right)-y^{(j)}\\right) x_{i}^{(j)}+\\lambda w_{i}\\right]$$</p>\n<h4 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h4><ul>\n<li>input을 0과 1사이로 조정하여 반환하는 활성화 함수의 일종.</li>\n<li>기준점인 0.5를 기준으로 출력값을 결정한다.</li>\n<li>w가 커질수록 기울기가 커진다.</li>\n<li>b의 크기에 따라 함수 자체가 이동한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 시그모이드 구현</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sigmoid non-convex logistic least squares cost function</span></span><br><span class=\"line\"><span class=\"comment\"># convex 한것은 비용함수가 구불구불해서 국소 최저점이 존재하는 것</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid_least_squares</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    cost = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(y.size):</span><br><span class=\"line\">        x_p = x[:,p]</span><br><span class=\"line\">        y_p = y[:,p]</span><br><span class=\"line\">        cost += (sigmoid(w[<span class=\"number\">0</span>] + w[<span class=\"number\">1</span>]*x_p) - y_p)**<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost/y.size</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://jermwatt.github.io/machine_learning_refined/mlrefined_images/superlearn_images/sigmoid.png\"></p>\n<h3 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h3><ul>\n<li>로지스틱 회귀를 다중클래스 분류로 확장한 것.</li>\n<li>분류하고자 하는 클래스가 n개일 때, n차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다.</li>\n<li>소프트맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다.</li>\n</ul>\n<p><img src=\"https://www.gstatic.com/education/formulas2/397133473/en/softmax_function.svg\"></p>\n<ul>\n<li><p><strong>확률의 총합이 1이다.</strong></p>\n</li>\n<li><p>식 자체는 단순하게 <code>probability = exp(value) / sum v in list exp(v)</code> 로 나타낼 수 있다. <code>n번째 일 확률 / 전체 확률</code> 로 생각하면 된다. </p>\n</li>\n<li><p><a href=\"https://gooopy.tistory.com/53\">지수함수(exp)가 식에 포함된 이유</a></p>\n</li>\n<li><p>softmax 구현</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"keyword\">from</span> numpy <span class=\"keyword\">import</span> exp</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: </span><br><span class=\"line\">   ...: <span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">vector</span>):</span><br><span class=\"line\">   ...:   e = exp(vector)</span><br><span class=\"line\">   ...:   <span class=\"keyword\">return</span> e / e.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: </span><br><span class=\"line\">   ...: data = [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">   ...: <span class=\"comment\"># convert list of numbers to a list of probabilities</span></span><br><span class=\"line\">   ...: result = softmax(data)</span><br><span class=\"line\">   ...: <span class=\"comment\"># report the probabilities</span></span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(result)</span><br><span class=\"line\">   ...: <span class=\"comment\"># report the sum of the probabilities</span></span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"built_in\">sum</span>(result))</span><br><span class=\"line\">[<span class=\"number\">0.09003057</span> <span class=\"number\">0.66524096</span> <span class=\"number\">0.24472847</span>]</span><br><span class=\"line\"><span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><ul>\n<li><p>numpy를 활용한 구현</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 정확도를 측정하기 위한 accuracy 함수</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_true,y_pred</span>):</span><br><span class=\"line\">  accuracy = np.<span class=\"built_in\">sum</span>(y_true==y_pred)/<span class=\"built_in\">len</span>(y_true)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LogisticRegression</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,lr=<span class=\"number\">0.001</span>,n_iters=<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">    self.lr = lr</span><br><span class=\"line\">    self.n_iters = n_iters</span><br><span class=\"line\">    self.weigts = <span class=\"literal\">None</span></span><br><span class=\"line\">    self.bias = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">    <span class=\"comment\"># init parameters</span></span><br><span class=\"line\"></span><br><span class=\"line\">    n_samples, n_features = X.shape</span><br><span class=\"line\"></span><br><span class=\"line\">    self.weigts = np.zeros(n_features)</span><br><span class=\"line\">    self.bias =<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># gradient descent</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">      linear_model = np.dot(X,self.weigts)+self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">      y_pred = self._sigmoid(linear_model) <span class=\"comment\"># 선형모델에 sigmoid 적용</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      dw = (<span class=\"number\">1</span>/n_samples) * np.dot(X.T,(y_pred-y)) <span class=\"comment\"># W에 대해 편미분</span></span><br><span class=\"line\">      db = (<span class=\"number\">1</span>/n_samples) * np.<span class=\"built_in\">sum</span>(y_pred-y)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      self.weigts = -= self.lr * dw</span><br><span class=\"line\">      self.bias = -= self.lr*db</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">    linear_model = np.dot(X,self.weigts)+self.bias</span><br><span class=\"line\">    y_pred = self._sigmoid(linear_model) <span class=\"comment\"># 0~1 사이의 float을 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\">    y_pred_class = [<span class=\"number\">1</span> <span class=\"keyword\">if</span> i &gt; <span class=\"number\">0.5</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y_pred] <span class=\"comment\"># 값을 0,1로 고정</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_pred_class</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">_sigmoid</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sklearn을 활용한 구현</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\">X, y = load_iris(return_X_y=<span class=\"literal\">True</span>)</span><br><span class=\"line\">clf = LogisticRegression(random_state=<span class=\"number\">0</span>).fit(X, y)</span><br><span class=\"line\">clf.predict(X[:<span class=\"number\">2</span>, :])</span><br><span class=\"line\"></span><br><span class=\"line\">clf.predict_proba(X[:<span class=\"number\">2</span>, :]) <span class=\"comment\"># 확률 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">clf.score(X, y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>tensorflow를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.models <span class=\"keyword\">import</span> Sequential</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> Dense</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras <span class=\"keyword\">import</span> optimizers</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([<span class=\"number\">0.42264594</span>, <span class=\"number\">0.4524148</span> , <span class=\"number\">0.93797131</span>, <span class=\"number\">0.36534474</span>, <span class=\"number\">0.40276151</span>,<span class=\"number\">0.29153749</span>, <span class=\"number\">0.05982402</span>, <span class=\"number\">0.24713247</span>, <span class=\"number\">0.91650771</span>, <span class=\"number\">0.45207763</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]) </span><br><span class=\"line\"></span><br><span class=\"line\">model = Sequential()</span><br><span class=\"line\">model.add(Dense(<span class=\"number\">1</span>, input_dim=<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">sgd = optimizers.SGD(lr=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=sgd ,loss=<span class=\"string\">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class=\"string\">&#x27;binary_accuracy&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">model.fit(x, y, epochs=<span class=\"number\">200</span>)</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932\">Cross Entropy</a></li>\n<li><a href=\"https://youtu.be/yIYKR4sgzI8?list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe\">Logistic Regression</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\">로지스틱 회귀 구현</a></li>\n<li><a href=\"https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\">소프트맥스</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://i.stack.imgur.com/gNip2.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Classification]로지스틱 회귀와 크로스엔트로피","path":"2022/06/13/ML-SP-logistic_regression/","eyeCatchImage":"https://i.stack.imgur.com/gNip2.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Logistic Regression","Cross Entropy"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Regression]머신러닝 관정에서의 회귀","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n### 머신러닝 관점에서의 회귀\n\n회귀분석에서 MSE은 비용함수이다.\n비용함수를 최소화 하는 최적화 관점에서 머신러닝을 볼 수 있다.\n기울기 업데이트를 통해 비용함수(MSE)의 최소값을 찾는다.\n\n---\n**_Concept_**\n\n- **Gradient Descent** : 함수의 기울기(즉, gradient)를 이용해 x의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법. 함수값을 최소화 하는 독립변수를 찾는 방법\n- **learning rate** : 학습을 한 내용을 다음 학습에 얼마나 반영할지의 문제. 정확히는 Loss 값을 각각의 가중치로 편미분하여 얻어낸 값에 얼마나 수정을 해야 할 지를 결정하는 하이퍼파라미터 \n  - learning rate가 너무 크다면 최적점에 도달하지 못하고 모델이 발산할 수 있다.\n  - learning rate가 너무 작다면 최적점에 도달하지 못하고 학습이 끝날 수 있다.\n- **iteration** : 학습(가중치 업데이트)의 반복횟수\n- **weight** : 경사하강법을 통해 업데이트 되는 feature의 가중치\n- **bias** : 활성함수에서 활성화가 잘 될지 안될지를 조절하는 hypterparameter의 일종.기본적으로 function curve 자체를 조정한다.(선형 비선형 상관없이)\n\n---\n\n#### Cost function of Linear Regression\n\n- 이는 가설함수-실제 target 인 오차 제곱합에 대해 평균을 취한 것이다.\n- 비용을 최소화 하는 w와 b를 찾는 것이 머신러닝에서의 학습의 목적이 된다.\n- 이를 아래와 같이 일반화 할 수 있다.\n\n$$cost(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$$\n\n- 단순선형회귀의 경우 아래와 같다\n\n$$ f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n\n\n#### Gradiant Descent\n\n- 경사하강법은 비용함수를 최소화하는 최적화 알고리즘의 일종이다.\n- 오차가 낮아지는 방향으로 이동할 목적으로 현재 위치를 미분한다.\n- **경사하강법의 원리는 반복적인 미분을 통한 w값의 업데이트를 통해 w, cost 지점의 경사(기울기)가 0이 되도록 만드는 것이다.** \n\n**경사하강법의 원리**\n![](https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg)\n\n일단 비용함수인 MSE부터 시작한다.\n\n$$f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n\n미분할 경우 아래와 같이 변하며\n\n$$(y_i - (mx_i + b))^2 = A(B(m,b))$$\n\n\n$$A(x) = x^2 $$\n\n$$\\frac{df}{dx} = A'(x) = 2x$$\n\n따라서 다음와 같이 미분할 수 있다.\n\n\n$$B(m,b) = y_i - (mx_i + b) = y_i - mx_i - b \\\\~\\\\ $$\n\n$$\\frac{dx}{dm} = B'(m) = 0 - x_i - 0 = -x_i \\\\~\\\\ $$\n\n$$\\frac{dx}{db} = B'(b) = 0 - 0 - 1 = -1 $$\n\n\n미분의 `Chain Rule` 을 활용하여 가중치와 편향의 미분값을 구할 수 있다.\n\n\n \n$$\\frac{df}{dm} = \\frac{df}{dx} \\frac{dx}{dm} \\\\~\\\\$$\n\n$$ \\frac{df}{db} = \\frac{df}{dx} \\frac{dx}{db} $$\n\n\n가중치와 절편에 Chain Rule을 적용해 미분을 하면 다음과 같다.\n\n$$\\frac{df}{dm} = A'(B(m,f)) B'(m) = 2(y_i - (mx_i + b)) \\cdot -x_i \\\\~\\\\$$\n\n$$\\frac{df}{db} = A'(B(m,f)) B'(b) = 2(y_i - (mx_i + b)) \\cdot -1$$\n\n따라서 비용함수(MSE)의 Gradiant를 아래과 같이 유도할 수 있다.\n\n$$\n  \\begin{align}\n  f'(m,b) =\n    \\begin{bmatrix}\n      \\frac{df}{dm}\\\\\n      \\frac{df}{db}\\\\\n    \\end{bmatrix}\n  &=\n    \\begin{bmatrix}\n      \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\\\\n      \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\\\\\n  &=\n    \\begin{bmatrix}\n       \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n       \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\n  \\end{align}\n$$\n\n**최적의 비용함수는 Learning Rate(학습률)와 기울기(Gradient)를 곱한 값을 기존 가중치에서 빼서 새로운 가중치로 설정하는 것을 반복하는 방식으로 구한다.**\n\n**따라서 최적화하고자 하는 함수 f(x)에 대해 아래와 같이 정리할 수 있다.**\n\n$$x_{i+1} = x_i - \\alpha \\frac{df}{dx}(x_i)$$\n\n**기본적으로 반복횟수가 많아질수록 오차가 줄어들어야 한다.**\n\n![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_regression_training_cost.png)\n\n#### Gradiant Descent를 활용한 선형회귀 구현\n\n- dw는 비용함수인 MSE를 가중치 W에 대하여 편미분한 것이다.\n- db는 비용함수인 MSE를 편향 b에 대하여 편미분한 것이다.\n\n\n```python\nimport numpy as np\n\n\ndef r2_score(y_true, y_pred):\n    corr_matrix = np.corrcoef(y_true, y_pred)\n    corr = corr_matrix[0, 1]\n    return corr ** 2\n\n\nclass LinearRegression:\n\n  def __init__(self, lr = 0.001, n_iters = 1000):\n    self.lr = lr\n    self.n_iters = n_iters\n    self.weigts = None\n    self.bias = None\n\n\n  def fit(self,X,y):\n    \n    # init paremeters : 시작지점을 초기화 한다.\n    n_samples , n_features = X.shape\n    self.weigts = np.zeros(n_features)\n    self.bias = 0\n\n    for _ in range(self.n_iters):\n      y_pred = np.dot(X,self.weigts) + self.bias\n\n      dw = (1/n_samples) * np.dot(X.T,(y_pred - y)) # 가중치의 기울기(Gradiant)(미분값)\n      db = (1/n_samples) * np.sum(y_pred - y) # 편향의 기울기\n\n      self.weigts -= self.lr * dw # 기울기 업데이트\n      self.bias -= self.lr * db # 편향 업데이트\n\n\n\n  def predict(self,X):\n    y_pred = np.dot(X,self.weigts) + self.bias\n\n    return y_pred\n\n```\n\n\n## References\n\n\n- [경사하강법과 회귀](https://angeloyeo.github.io/2020/08/24/linear_regression.html)\n- [ML from scratch](https://youtu.be/4swNt7PiamQ?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E)\n","source":"_posts/ML-SP-optimization.md","raw":"---\ntitle: \"[Regression]머신러닝 관정에서의 회귀\"\ntags:\n  - Python\n  - Supervised Learning\ncategories:\n  - [Machine Learning]\ndate: \nupdated:\n---\n\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n### 머신러닝 관점에서의 회귀\n\n회귀분석에서 MSE은 비용함수이다.\n비용함수를 최소화 하는 최적화 관점에서 머신러닝을 볼 수 있다.\n기울기 업데이트를 통해 비용함수(MSE)의 최소값을 찾는다.\n\n---\n**_Concept_**\n\n- **Gradient Descent** : 함수의 기울기(즉, gradient)를 이용해 x의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법. 함수값을 최소화 하는 독립변수를 찾는 방법\n- **learning rate** : 학습을 한 내용을 다음 학습에 얼마나 반영할지의 문제. 정확히는 Loss 값을 각각의 가중치로 편미분하여 얻어낸 값에 얼마나 수정을 해야 할 지를 결정하는 하이퍼파라미터 \n  - learning rate가 너무 크다면 최적점에 도달하지 못하고 모델이 발산할 수 있다.\n  - learning rate가 너무 작다면 최적점에 도달하지 못하고 학습이 끝날 수 있다.\n- **iteration** : 학습(가중치 업데이트)의 반복횟수\n- **weight** : 경사하강법을 통해 업데이트 되는 feature의 가중치\n- **bias** : 활성함수에서 활성화가 잘 될지 안될지를 조절하는 hypterparameter의 일종.기본적으로 function curve 자체를 조정한다.(선형 비선형 상관없이)\n\n---\n\n#### Cost function of Linear Regression\n\n- 이는 가설함수-실제 target 인 오차 제곱합에 대해 평균을 취한 것이다.\n- 비용을 최소화 하는 w와 b를 찾는 것이 머신러닝에서의 학습의 목적이 된다.\n- 이를 아래와 같이 일반화 할 수 있다.\n\n$$cost(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$$\n\n- 단순선형회귀의 경우 아래와 같다\n\n$$ f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n\n\n#### Gradiant Descent\n\n- 경사하강법은 비용함수를 최소화하는 최적화 알고리즘의 일종이다.\n- 오차가 낮아지는 방향으로 이동할 목적으로 현재 위치를 미분한다.\n- **경사하강법의 원리는 반복적인 미분을 통한 w값의 업데이트를 통해 w, cost 지점의 경사(기울기)가 0이 되도록 만드는 것이다.** \n\n**경사하강법의 원리**\n![](https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg)\n\n일단 비용함수인 MSE부터 시작한다.\n\n$$f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n\n미분할 경우 아래와 같이 변하며\n\n$$(y_i - (mx_i + b))^2 = A(B(m,b))$$\n\n\n$$A(x) = x^2 $$\n\n$$\\frac{df}{dx} = A'(x) = 2x$$\n\n따라서 다음와 같이 미분할 수 있다.\n\n\n$$B(m,b) = y_i - (mx_i + b) = y_i - mx_i - b \\\\~\\\\ $$\n\n$$\\frac{dx}{dm} = B'(m) = 0 - x_i - 0 = -x_i \\\\~\\\\ $$\n\n$$\\frac{dx}{db} = B'(b) = 0 - 0 - 1 = -1 $$\n\n\n미분의 `Chain Rule` 을 활용하여 가중치와 편향의 미분값을 구할 수 있다.\n\n\n \n$$\\frac{df}{dm} = \\frac{df}{dx} \\frac{dx}{dm} \\\\~\\\\$$\n\n$$ \\frac{df}{db} = \\frac{df}{dx} \\frac{dx}{db} $$\n\n\n가중치와 절편에 Chain Rule을 적용해 미분을 하면 다음과 같다.\n\n$$\\frac{df}{dm} = A'(B(m,f)) B'(m) = 2(y_i - (mx_i + b)) \\cdot -x_i \\\\~\\\\$$\n\n$$\\frac{df}{db} = A'(B(m,f)) B'(b) = 2(y_i - (mx_i + b)) \\cdot -1$$\n\n따라서 비용함수(MSE)의 Gradiant를 아래과 같이 유도할 수 있다.\n\n$$\n  \\begin{align}\n  f'(m,b) =\n    \\begin{bmatrix}\n      \\frac{df}{dm}\\\\\n      \\frac{df}{db}\\\\\n    \\end{bmatrix}\n  &=\n    \\begin{bmatrix}\n      \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\\\\n      \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\\\\\n  &=\n    \\begin{bmatrix}\n       \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n       \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\n  \\end{align}\n$$\n\n**최적의 비용함수는 Learning Rate(학습률)와 기울기(Gradient)를 곱한 값을 기존 가중치에서 빼서 새로운 가중치로 설정하는 것을 반복하는 방식으로 구한다.**\n\n**따라서 최적화하고자 하는 함수 f(x)에 대해 아래와 같이 정리할 수 있다.**\n\n$$x_{i+1} = x_i - \\alpha \\frac{df}{dx}(x_i)$$\n\n**기본적으로 반복횟수가 많아질수록 오차가 줄어들어야 한다.**\n\n![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_regression_training_cost.png)\n\n#### Gradiant Descent를 활용한 선형회귀 구현\n\n- dw는 비용함수인 MSE를 가중치 W에 대하여 편미분한 것이다.\n- db는 비용함수인 MSE를 편향 b에 대하여 편미분한 것이다.\n\n\n```python\nimport numpy as np\n\n\ndef r2_score(y_true, y_pred):\n    corr_matrix = np.corrcoef(y_true, y_pred)\n    corr = corr_matrix[0, 1]\n    return corr ** 2\n\n\nclass LinearRegression:\n\n  def __init__(self, lr = 0.001, n_iters = 1000):\n    self.lr = lr\n    self.n_iters = n_iters\n    self.weigts = None\n    self.bias = None\n\n\n  def fit(self,X,y):\n    \n    # init paremeters : 시작지점을 초기화 한다.\n    n_samples , n_features = X.shape\n    self.weigts = np.zeros(n_features)\n    self.bias = 0\n\n    for _ in range(self.n_iters):\n      y_pred = np.dot(X,self.weigts) + self.bias\n\n      dw = (1/n_samples) * np.dot(X.T,(y_pred - y)) # 가중치의 기울기(Gradiant)(미분값)\n      db = (1/n_samples) * np.sum(y_pred - y) # 편향의 기울기\n\n      self.weigts -= self.lr * dw # 기울기 업데이트\n      self.bias -= self.lr * db # 편향 업데이트\n\n\n\n  def predict(self,X):\n    y_pred = np.dot(X,self.weigts) + self.bias\n\n    return y_pred\n\n```\n\n\n## References\n\n\n- [경사하강법과 회귀](https://angeloyeo.github.io/2020/08/24/linear_regression.html)\n- [ML from scratch](https://youtu.be/4swNt7PiamQ?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E)\n","slug":"ML-SP-optimization","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsca001fb36qecumfic7","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h3 id=\"머신러닝-관점에서의-회귀\"><a href=\"#머신러닝-관점에서의-회귀\" class=\"headerlink\" title=\"머신러닝 관점에서의 회귀\"></a>머신러닝 관점에서의 회귀</h3><p>회귀분석에서 MSE은 비용함수이다.<br>비용함수를 최소화 하는 최적화 관점에서 머신러닝을 볼 수 있다.<br>기울기 업데이트를 통해 비용함수(MSE)의 최소값을 찾는다.</p>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Gradient Descent</strong> : 함수의 기울기(즉, gradient)를 이용해 x의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법. 함수값을 최소화 하는 독립변수를 찾는 방법</li>\n<li><strong>learning rate</strong> : 학습을 한 내용을 다음 학습에 얼마나 반영할지의 문제. 정확히는 Loss 값을 각각의 가중치로 편미분하여 얻어낸 값에 얼마나 수정을 해야 할 지를 결정하는 하이퍼파라미터 <ul>\n<li>learning rate가 너무 크다면 최적점에 도달하지 못하고 모델이 발산할 수 있다.</li>\n<li>learning rate가 너무 작다면 최적점에 도달하지 못하고 학습이 끝날 수 있다.</li>\n</ul>\n</li>\n<li><strong>iteration</strong> : 학습(가중치 업데이트)의 반복횟수</li>\n<li><strong>weight</strong> : 경사하강법을 통해 업데이트 되는 feature의 가중치</li>\n<li><strong>bias</strong> : 활성함수에서 활성화가 잘 될지 안될지를 조절하는 hypterparameter의 일종.기본적으로 function curve 자체를 조정한다.(선형 비선형 상관없이)</li>\n</ul>\n<hr>\n<h4 id=\"Cost-function-of-Linear-Regression\"><a href=\"#Cost-function-of-Linear-Regression\" class=\"headerlink\" title=\"Cost function of Linear Regression\"></a>Cost function of Linear Regression</h4><ul>\n<li>이는 가설함수-실제 target 인 오차 제곱합에 대해 평균을 취한 것이다.</li>\n<li>비용을 최소화 하는 w와 b를 찾는 것이 머신러닝에서의 학습의 목적이 된다.</li>\n<li>이를 아래와 같이 일반화 할 수 있다.</li>\n</ul>\n<p>$$cost(w, b) &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$$</p>\n<ul>\n<li>단순선형회귀의 경우 아래와 같다</li>\n</ul>\n<p>$$ f(m,b) &#x3D;  \\frac{1}{N} \\sum_{i&#x3D;1}^{n} (y_i - (mx_i + b))^2$$</p>\n<h4 id=\"Gradiant-Descent\"><a href=\"#Gradiant-Descent\" class=\"headerlink\" title=\"Gradiant Descent\"></a>Gradiant Descent</h4><ul>\n<li>경사하강법은 비용함수를 최소화하는 최적화 알고리즘의 일종이다.</li>\n<li>오차가 낮아지는 방향으로 이동할 목적으로 현재 위치를 미분한다.</li>\n<li><strong>경사하강법의 원리는 반복적인 미분을 통한 w값의 업데이트를 통해 w, cost 지점의 경사(기울기)가 0이 되도록 만드는 것이다.</strong></li>\n</ul>\n<p><strong>경사하강법의 원리</strong><br><img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\"></p>\n<p>일단 비용함수인 MSE부터 시작한다.</p>\n<p>$$f(m,b) &#x3D;  \\frac{1}{N} \\sum_{i&#x3D;1}^{n} (y_i - (mx_i + b))^2$$</p>\n<p>미분할 경우 아래와 같이 변하며</p>\n<p>$$(y_i - (mx_i + b))^2 &#x3D; A(B(m,b))$$</p>\n<p>$$A(x) &#x3D; x^2 $$</p>\n<p>$$\\frac{df}{dx} &#x3D; A’(x) &#x3D; 2x$$</p>\n<p>따라서 다음와 같이 미분할 수 있다.</p>\n<p>$$B(m,b) &#x3D; y_i - (mx_i + b) &#x3D; y_i - mx_i - b \\~\\ $$</p>\n<p>$$\\frac{dx}{dm} &#x3D; B’(m) &#x3D; 0 - x_i - 0 &#x3D; -x_i \\~\\ $$</p>\n<p>$$\\frac{dx}{db} &#x3D; B’(b) &#x3D; 0 - 0 - 1 &#x3D; -1 $$</p>\n<p>미분의 <code>Chain Rule</code> 을 활용하여 가중치와 편향의 미분값을 구할 수 있다.</p>\n<p>$$\\frac{df}{dm} &#x3D; \\frac{df}{dx} \\frac{dx}{dm} \\~\\$$</p>\n<p>$$ \\frac{df}{db} &#x3D; \\frac{df}{dx} \\frac{dx}{db} $$</p>\n<p>가중치와 절편에 Chain Rule을 적용해 미분을 하면 다음과 같다.</p>\n<p>$$\\frac{df}{dm} &#x3D; A’(B(m,f)) B’(m) &#x3D; 2(y_i - (mx_i + b)) \\cdot -x_i \\~\\$$</p>\n<p>$$\\frac{df}{db} &#x3D; A’(B(m,f)) B’(b) &#x3D; 2(y_i - (mx_i + b)) \\cdot -1$$</p>\n<p>따라서 비용함수(MSE)의 Gradiant를 아래과 같이 유도할 수 있다.</p>\n<p>$$<br>  \\begin{align}<br>  f’(m,b) &#x3D;<br>    \\begin{bmatrix}<br>      \\frac{df}{dm}\\<br>      \\frac{df}{db}\\<br>    \\end{bmatrix}<br>  &amp;&#x3D;<br>    \\begin{bmatrix}<br>      \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\<br>      \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\<br>    \\end{bmatrix}\\<br>  &amp;&#x3D;<br>    \\begin{bmatrix}<br>       \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\<br>       \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\<br>    \\end{bmatrix}<br>  \\end{align}<br>$$</p>\n<p><strong>최적의 비용함수는 Learning Rate(학습률)와 기울기(Gradient)를 곱한 값을 기존 가중치에서 빼서 새로운 가중치로 설정하는 것을 반복하는 방식으로 구한다.</strong></p>\n<p><strong>따라서 최적화하고자 하는 함수 f(x)에 대해 아래와 같이 정리할 수 있다.</strong></p>\n<p>$$x_{i+1} &#x3D; x_i - \\alpha \\frac{df}{dx}(x_i)$$</p>\n<p><strong>기본적으로 반복횟수가 많아질수록 오차가 줄어들어야 한다.</strong></p>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_regression_training_cost.png\"></p>\n<h4 id=\"Gradiant-Descent를-활용한-선형회귀-구현\"><a href=\"#Gradiant-Descent를-활용한-선형회귀-구현\" class=\"headerlink\" title=\"Gradiant Descent를 활용한 선형회귀 구현\"></a>Gradiant Descent를 활용한 선형회귀 구현</h4><ul>\n<li>dw는 비용함수인 MSE를 가중치 W에 대하여 편미분한 것이다.</li>\n<li>db는 비용함수인 MSE를 편향 b에 대하여 편미분한 것이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">r2_score</span>(<span class=\"params\">y_true, y_pred</span>):</span><br><span class=\"line\">    corr_matrix = np.corrcoef(y_true, y_pred)</span><br><span class=\"line\">    corr = corr_matrix[<span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> corr ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LinearRegression</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr = <span class=\"number\">0.001</span>, n_iters = <span class=\"number\">1000</span></span>):</span><br><span class=\"line\">    self.lr = lr</span><br><span class=\"line\">    self.n_iters = n_iters</span><br><span class=\"line\">    self.weigts = <span class=\"literal\">None</span></span><br><span class=\"line\">    self.bias = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># init paremeters : 시작지점을 초기화 한다.</span></span><br><span class=\"line\">    n_samples , n_features = X.shape</span><br><span class=\"line\">    self.weigts = np.zeros(n_features)</span><br><span class=\"line\">    self.bias = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">      y_pred = np.dot(X,self.weigts) + self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">      dw = (<span class=\"number\">1</span>/n_samples) * np.dot(X.T,(y_pred - y)) <span class=\"comment\"># 가중치의 기울기(Gradiant)(미분값)</span></span><br><span class=\"line\">      db = (<span class=\"number\">1</span>/n_samples) * np.<span class=\"built_in\">sum</span>(y_pred - y) <span class=\"comment\"># 편향의 기울기</span></span><br><span class=\"line\"></span><br><span class=\"line\">      self.weigts -= self.lr * dw <span class=\"comment\"># 기울기 업데이트</span></span><br><span class=\"line\">      self.bias -= self.lr * db <span class=\"comment\"># 편향 업데이트</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">    y_pred = np.dot(X,self.weigts) + self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_pred</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://angeloyeo.github.io/2020/08/24/linear_regression.html\">경사하강법과 회귀</a></li>\n<li><a href=\"https://youtu.be/4swNt7PiamQ?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E\">ML from scratch</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h3 id=\"머신러닝-관점에서의-회귀\"><a href=\"#머신러닝-관점에서의-회귀\" class=\"headerlink\" title=\"머신러닝 관점에서의 회귀\"></a>머신러닝 관점에서의 회귀</h3><p>회귀분석에서 MSE은 비용함수이다.<br>비용함수를 최소화 하는 최적화 관점에서 머신러닝을 볼 수 있다.<br>기울기 업데이트를 통해 비용함수(MSE)의 최소값을 찾는다.</p>\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>Gradient Descent</strong> : 함수의 기울기(즉, gradient)를 이용해 x의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법. 함수값을 최소화 하는 독립변수를 찾는 방법</li>\n<li><strong>learning rate</strong> : 학습을 한 내용을 다음 학습에 얼마나 반영할지의 문제. 정확히는 Loss 값을 각각의 가중치로 편미분하여 얻어낸 값에 얼마나 수정을 해야 할 지를 결정하는 하이퍼파라미터 <ul>\n<li>learning rate가 너무 크다면 최적점에 도달하지 못하고 모델이 발산할 수 있다.</li>\n<li>learning rate가 너무 작다면 최적점에 도달하지 못하고 학습이 끝날 수 있다.</li>\n</ul>\n</li>\n<li><strong>iteration</strong> : 학습(가중치 업데이트)의 반복횟수</li>\n<li><strong>weight</strong> : 경사하강법을 통해 업데이트 되는 feature의 가중치</li>\n<li><strong>bias</strong> : 활성함수에서 활성화가 잘 될지 안될지를 조절하는 hypterparameter의 일종.기본적으로 function curve 자체를 조정한다.(선형 비선형 상관없이)</li>\n</ul>\n<hr>\n<h4 id=\"Cost-function-of-Linear-Regression\"><a href=\"#Cost-function-of-Linear-Regression\" class=\"headerlink\" title=\"Cost function of Linear Regression\"></a>Cost function of Linear Regression</h4><ul>\n<li>이는 가설함수-실제 target 인 오차 제곱합에 대해 평균을 취한 것이다.</li>\n<li>비용을 최소화 하는 w와 b를 찾는 것이 머신러닝에서의 학습의 목적이 된다.</li>\n<li>이를 아래와 같이 일반화 할 수 있다.</li>\n</ul>\n<p>$$cost(w, b) &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$$</p>\n<ul>\n<li>단순선형회귀의 경우 아래와 같다</li>\n</ul>\n<p>$$ f(m,b) &#x3D;  \\frac{1}{N} \\sum_{i&#x3D;1}^{n} (y_i - (mx_i + b))^2$$</p>\n<h4 id=\"Gradiant-Descent\"><a href=\"#Gradiant-Descent\" class=\"headerlink\" title=\"Gradiant Descent\"></a>Gradiant Descent</h4><ul>\n<li>경사하강법은 비용함수를 최소화하는 최적화 알고리즘의 일종이다.</li>\n<li>오차가 낮아지는 방향으로 이동할 목적으로 현재 위치를 미분한다.</li>\n<li><strong>경사하강법의 원리는 반복적인 미분을 통한 w값의 업데이트를 통해 w, cost 지점의 경사(기울기)가 0이 되도록 만드는 것이다.</strong></li>\n</ul>\n<p><strong>경사하강법의 원리</strong><br><img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\"></p>\n<p>일단 비용함수인 MSE부터 시작한다.</p>\n<p>$$f(m,b) &#x3D;  \\frac{1}{N} \\sum_{i&#x3D;1}^{n} (y_i - (mx_i + b))^2$$</p>\n<p>미분할 경우 아래와 같이 변하며</p>\n<p>$$(y_i - (mx_i + b))^2 &#x3D; A(B(m,b))$$</p>\n<p>$$A(x) &#x3D; x^2 $$</p>\n<p>$$\\frac{df}{dx} &#x3D; A’(x) &#x3D; 2x$$</p>\n<p>따라서 다음와 같이 미분할 수 있다.</p>\n<p>$$B(m,b) &#x3D; y_i - (mx_i + b) &#x3D; y_i - mx_i - b \\~\\ $$</p>\n<p>$$\\frac{dx}{dm} &#x3D; B’(m) &#x3D; 0 - x_i - 0 &#x3D; -x_i \\~\\ $$</p>\n<p>$$\\frac{dx}{db} &#x3D; B’(b) &#x3D; 0 - 0 - 1 &#x3D; -1 $$</p>\n<p>미분의 <code>Chain Rule</code> 을 활용하여 가중치와 편향의 미분값을 구할 수 있다.</p>\n<p>$$\\frac{df}{dm} &#x3D; \\frac{df}{dx} \\frac{dx}{dm} \\~\\$$</p>\n<p>$$ \\frac{df}{db} &#x3D; \\frac{df}{dx} \\frac{dx}{db} $$</p>\n<p>가중치와 절편에 Chain Rule을 적용해 미분을 하면 다음과 같다.</p>\n<p>$$\\frac{df}{dm} &#x3D; A’(B(m,f)) B’(m) &#x3D; 2(y_i - (mx_i + b)) \\cdot -x_i \\~\\$$</p>\n<p>$$\\frac{df}{db} &#x3D; A’(B(m,f)) B’(b) &#x3D; 2(y_i - (mx_i + b)) \\cdot -1$$</p>\n<p>따라서 비용함수(MSE)의 Gradiant를 아래과 같이 유도할 수 있다.</p>\n<p>$$<br>  \\begin{align}<br>  f’(m,b) &#x3D;<br>    \\begin{bmatrix}<br>      \\frac{df}{dm}\\<br>      \\frac{df}{db}\\<br>    \\end{bmatrix}<br>  &amp;&#x3D;<br>    \\begin{bmatrix}<br>      \\frac{1}{N} \\sum -x_i \\cdot 2(y_i - (mx_i + b)) \\<br>      \\frac{1}{N} \\sum -1 \\cdot 2(y_i - (mx_i + b)) \\<br>    \\end{bmatrix}\\<br>  &amp;&#x3D;<br>    \\begin{bmatrix}<br>       \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\<br>       \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\<br>    \\end{bmatrix}<br>  \\end{align}<br>$$</p>\n<p><strong>최적의 비용함수는 Learning Rate(학습률)와 기울기(Gradient)를 곱한 값을 기존 가중치에서 빼서 새로운 가중치로 설정하는 것을 반복하는 방식으로 구한다.</strong></p>\n<p><strong>따라서 최적화하고자 하는 함수 f(x)에 대해 아래와 같이 정리할 수 있다.</strong></p>\n<p>$$x_{i+1} &#x3D; x_i - \\alpha \\frac{df}{dx}(x_i)$$</p>\n<p><strong>기본적으로 반복횟수가 많아질수록 오차가 줄어들어야 한다.</strong></p>\n<p><img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_regression_training_cost.png\"></p>\n<h4 id=\"Gradiant-Descent를-활용한-선형회귀-구현\"><a href=\"#Gradiant-Descent를-활용한-선형회귀-구현\" class=\"headerlink\" title=\"Gradiant Descent를 활용한 선형회귀 구현\"></a>Gradiant Descent를 활용한 선형회귀 구현</h4><ul>\n<li>dw는 비용함수인 MSE를 가중치 W에 대하여 편미분한 것이다.</li>\n<li>db는 비용함수인 MSE를 편향 b에 대하여 편미분한 것이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">r2_score</span>(<span class=\"params\">y_true, y_pred</span>):</span><br><span class=\"line\">    corr_matrix = np.corrcoef(y_true, y_pred)</span><br><span class=\"line\">    corr = corr_matrix[<span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> corr ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LinearRegression</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr = <span class=\"number\">0.001</span>, n_iters = <span class=\"number\">1000</span></span>):</span><br><span class=\"line\">    self.lr = lr</span><br><span class=\"line\">    self.n_iters = n_iters</span><br><span class=\"line\">    self.weigts = <span class=\"literal\">None</span></span><br><span class=\"line\">    self.bias = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># init paremeters : 시작지점을 초기화 한다.</span></span><br><span class=\"line\">    n_samples , n_features = X.shape</span><br><span class=\"line\">    self.weigts = np.zeros(n_features)</span><br><span class=\"line\">    self.bias = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">      y_pred = np.dot(X,self.weigts) + self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">      dw = (<span class=\"number\">1</span>/n_samples) * np.dot(X.T,(y_pred - y)) <span class=\"comment\"># 가중치의 기울기(Gradiant)(미분값)</span></span><br><span class=\"line\">      db = (<span class=\"number\">1</span>/n_samples) * np.<span class=\"built_in\">sum</span>(y_pred - y) <span class=\"comment\"># 편향의 기울기</span></span><br><span class=\"line\"></span><br><span class=\"line\">      self.weigts -= self.lr * dw <span class=\"comment\"># 기울기 업데이트</span></span><br><span class=\"line\">      self.bias -= self.lr * db <span class=\"comment\"># 편향 업데이트</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">    y_pred = np.dot(X,self.weigts) + self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_pred</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://angeloyeo.github.io/2020/08/24/linear_regression.html\">경사하강법과 회귀</a></li>\n<li><a href=\"https://youtu.be/4swNt7PiamQ?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E\">ML from scratch</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Regression]머신러닝 관정에서의 회귀","path":"2022/06/13/ML-SP-optimization/","eyeCatchImage":"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]단순선형회귀 구현하기(Simple Regression)","date":"2021-06-08T10:28:01.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n## Intro\n- Python 선형회귀 연습하기\n- nbconvert를 활용해 ipynb파일을 마크다운 파일로 변환\n\n## scikit-learn을 활용한 단순선형회귀\n\n### 데이터 불러오기 및 전처리\n\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n```\n\n```python\ndf = pd.read_csv('../data/kc_house_data.csv')\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>...</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1180.0</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2170.0</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>770.0</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1050.0</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>1680.0</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n\n\n\n\n```python\n# * 가 파이썬에서 뭘 의미하는지 찾아보자\n# https://mingrammer.com/understanding-the-asterisk-of-python/\n# https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\nuse_col = df.columns.tolist()[3:]\nprint(*use_col, sep = '\\n')\n```\n\n    bedrooms\n    bathrooms\n    sqft_living\n    sqft_lot\n    floors\n    waterfront\n    view\n    condition\n    grade\n    sqft_above\n    sqft_basement\n    yr_built\n    yr_renovated\n    zipcode\n    lat\n    long\n    sqft_living15\n    sqft_lot15\n    \n\n\n```python\n# 상관행렬 구하기\ncorrMatrix = df.corr()\n# 상관행렬에서 타겟변수 추출\nprice_col = corrMatrix['price'].to_frame().reset_index()\n# 상관계수높은 순으로 정렬\nprice_col.sort_values('price',ascending=False,inplace=True)\nprice_col.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>price</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqft_living</td>\n      <td>0.702035</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>grade</td>\n      <td>0.667434</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sqft_above</td>\n      <td>0.605567</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>sqft_living15</td>\n      <td>0.585379</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 가장 상관관계가 높은 변수 시각화\nsns.set(style='whitegrid', context='notebook')\ng=sns.pairplot(df[['price','sqft_living']])\nfor ax in g.axes.flatten(): # y축 한정 과학적 표기법을 일반 표기법으로 바꿔주기\n    ax.ticklabel_format(style='plain', scilimits=(0,0), axis='y')\n#plt.ticklabel_format(style='plain', axis='y')\nplt.show()\n```\n\n\n    \n![png](ML-SP-simple_regression/1_simple-regression_6_0.png)\n    \n\n\n### Baseline Model 만들기\n\n\n```python\n# get baseline model\nbaseline = df['price'].mean()\nerrors = df['price'] - baseline\nmae = errors.abs().mean()\n\nsns.lineplot(x=df['grade'], y=baseline, color='red')\nsns.scatterplot(data=df, x=\"grade\", y=\"price\")\nplt.ticklabel_format(style='plain', axis='y')\n```\n\n\n    \n![png](ML-SP-simple_regression/1_simple-regression_8_0.png)\n    \n\n\n\n```python\n\n#기본적으로 모델이 다중회귀를 가정하기 때문에 Feature의 선언을 배열로 해줘야 한다.\n\nX = df[['sqft_living']] \ny = df['price']\nm = LinearRegression()\n```\n\n\n```python\nm.fit(X,y)\n```\n\n\n\n\n    LinearRegression()\n\n\n\n\n```python\nX_test = [[6000]]\ny_pred = m.predict(X_test)\nprint(f'sqft_living이 {X_test[0][0]}인 경우의 예상 가격은 {int(y_pred)}로 나타남')\n```\n\n    sqft_living이 6000인 경우의 예상 가격은 1640160로 나타남\n    \n\n\n```python\n# 통계량 확인\nprint(\"coefficient-score : \",m.coef_)\nprint(\"intercept : \" , m.intercept_)\nprint(\"coefficient of determination: {:.2f}\".format(m.score(X,y)))\n```\n\n    coefficient-score :  [280.6235679]\n    intercept :  -43580.74309447361\n    coefficient of determination: 0.49\n    \n\n## References\n\n- https://scikit-learn.org/stable/modules/linear_model.html\n- https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n","source":"_posts/ML-SP-simple_regression.md","raw":"---\ntitle: \"[Python]단순선형회귀 구현하기(Simple Regression)\"\ntags:\n  - Python\n  - Supervised Learning\ncategories:\n  - Machine Learning\ndate: 2021-06-08 19:28:01\nupdated:\n---\n\n## Intro\n- Python 선형회귀 연습하기\n- nbconvert를 활용해 ipynb파일을 마크다운 파일로 변환\n\n## scikit-learn을 활용한 단순선형회귀\n\n### 데이터 불러오기 및 전처리\n\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n```\n\n```python\ndf = pd.read_csv('../data/kc_house_data.csv')\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>...</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1180.0</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2170.0</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>770.0</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1050.0</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>1680.0</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n\n\n\n\n```python\n# * 가 파이썬에서 뭘 의미하는지 찾아보자\n# https://mingrammer.com/understanding-the-asterisk-of-python/\n# https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\nuse_col = df.columns.tolist()[3:]\nprint(*use_col, sep = '\\n')\n```\n\n    bedrooms\n    bathrooms\n    sqft_living\n    sqft_lot\n    floors\n    waterfront\n    view\n    condition\n    grade\n    sqft_above\n    sqft_basement\n    yr_built\n    yr_renovated\n    zipcode\n    lat\n    long\n    sqft_living15\n    sqft_lot15\n    \n\n\n```python\n# 상관행렬 구하기\ncorrMatrix = df.corr()\n# 상관행렬에서 타겟변수 추출\nprice_col = corrMatrix['price'].to_frame().reset_index()\n# 상관계수높은 순으로 정렬\nprice_col.sort_values('price',ascending=False,inplace=True)\nprice_col.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>price</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqft_living</td>\n      <td>0.702035</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>grade</td>\n      <td>0.667434</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sqft_above</td>\n      <td>0.605567</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>sqft_living15</td>\n      <td>0.585379</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 가장 상관관계가 높은 변수 시각화\nsns.set(style='whitegrid', context='notebook')\ng=sns.pairplot(df[['price','sqft_living']])\nfor ax in g.axes.flatten(): # y축 한정 과학적 표기법을 일반 표기법으로 바꿔주기\n    ax.ticklabel_format(style='plain', scilimits=(0,0), axis='y')\n#plt.ticklabel_format(style='plain', axis='y')\nplt.show()\n```\n\n\n    \n![png](ML-SP-simple_regression/1_simple-regression_6_0.png)\n    \n\n\n### Baseline Model 만들기\n\n\n```python\n# get baseline model\nbaseline = df['price'].mean()\nerrors = df['price'] - baseline\nmae = errors.abs().mean()\n\nsns.lineplot(x=df['grade'], y=baseline, color='red')\nsns.scatterplot(data=df, x=\"grade\", y=\"price\")\nplt.ticklabel_format(style='plain', axis='y')\n```\n\n\n    \n![png](ML-SP-simple_regression/1_simple-regression_8_0.png)\n    \n\n\n\n```python\n\n#기본적으로 모델이 다중회귀를 가정하기 때문에 Feature의 선언을 배열로 해줘야 한다.\n\nX = df[['sqft_living']] \ny = df['price']\nm = LinearRegression()\n```\n\n\n```python\nm.fit(X,y)\n```\n\n\n\n\n    LinearRegression()\n\n\n\n\n```python\nX_test = [[6000]]\ny_pred = m.predict(X_test)\nprint(f'sqft_living이 {X_test[0][0]}인 경우의 예상 가격은 {int(y_pred)}로 나타남')\n```\n\n    sqft_living이 6000인 경우의 예상 가격은 1640160로 나타남\n    \n\n\n```python\n# 통계량 확인\nprint(\"coefficient-score : \",m.coef_)\nprint(\"intercept : \" , m.intercept_)\nprint(\"coefficient of determination: {:.2f}\".format(m.score(X,y)))\n```\n\n    coefficient-score :  [280.6235679]\n    intercept :  -43580.74309447361\n    coefficient of determination: 0.49\n    \n\n## References\n\n- https://scikit-learn.org/stable/modules/linear_model.html\n- https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n","slug":"ML-SP-simple_regression","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsca001jb36qeles5pz4","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><ul>\n<li>Python 선형회귀 연습하기</li>\n<li>nbconvert를 활용해 ipynb파일을 마크다운 파일로 변환</li>\n</ul>\n<h2 id=\"scikit-learn을-활용한-단순선형회귀\"><a href=\"#scikit-learn을-활용한-단순선형회귀\" class=\"headerlink\" title=\"scikit-learn을 활용한 단순선형회귀\"></a>scikit-learn을 활용한 단순선형회귀</h2><h3 id=\"데이터-불러오기-및-전처리\"><a href=\"#데이터-불러오기-및-전처리\" class=\"headerlink\" title=\"데이터 불러오기 및 전처리\"></a>데이터 불러오기 및 전처리</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;../data/kc_house_data.csv&#x27;</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>...</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1180.0</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2170.0</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>770.0</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1050.0</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>1680.0</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># * 가 파이썬에서 뭘 의미하는지 찾아보자</span></span><br><span class=\"line\"><span class=\"comment\"># https://mingrammer.com/understanding-the-asterisk-of-python/</span></span><br><span class=\"line\"><span class=\"comment\"># https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/</span></span><br><span class=\"line\">use_col = df.columns.tolist()[<span class=\"number\">3</span>:]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*use_col, sep = <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>bedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 상관행렬 구하기</span></span><br><span class=\"line\">corrMatrix = df.corr()</span><br><span class=\"line\"><span class=\"comment\"># 상관행렬에서 타겟변수 추출</span></span><br><span class=\"line\">price_col = corrMatrix[<span class=\"string\">&#x27;price&#x27;</span>].to_frame().reset_index()</span><br><span class=\"line\"><span class=\"comment\"># 상관계수높은 순으로 정렬</span></span><br><span class=\"line\">price_col.sort_values(<span class=\"string\">&#x27;price&#x27;</span>,ascending=<span class=\"literal\">False</span>,inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">price_col.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>price</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqft_living</td>\n      <td>0.702035</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>grade</td>\n      <td>0.667434</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sqft_above</td>\n      <td>0.605567</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>sqft_living15</td>\n      <td>0.585379</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 가장 상관관계가 높은 변수 시각화</span></span><br><span class=\"line\">sns.<span class=\"built_in\">set</span>(style=<span class=\"string\">&#x27;whitegrid&#x27;</span>, context=<span class=\"string\">&#x27;notebook&#x27;</span>)</span><br><span class=\"line\">g=sns.pairplot(df[[<span class=\"string\">&#x27;price&#x27;</span>,<span class=\"string\">&#x27;sqft_living&#x27;</span>]])</span><br><span class=\"line\"><span class=\"keyword\">for</span> ax <span class=\"keyword\">in</span> g.axes.flatten(): <span class=\"comment\"># y축 한정 과학적 표기법을 일반 표기법으로 바꿔주기</span></span><br><span class=\"line\">    ax.ticklabel_format(style=<span class=\"string\">&#x27;plain&#x27;</span>, scilimits=(<span class=\"number\">0</span>,<span class=\"number\">0</span>), axis=<span class=\"string\">&#x27;y&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.ticklabel_format(style=&#x27;plain&#x27;, axis=&#x27;y&#x27;)</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/ML-SP-simple_regression/1_simple-regression_6_0.png\" alt=\"png\"></p>\n<h3 id=\"Baseline-Model-만들기\"><a href=\"#Baseline-Model-만들기\" class=\"headerlink\" title=\"Baseline Model 만들기\"></a>Baseline Model 만들기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># get baseline model</span></span><br><span class=\"line\">baseline = df[<span class=\"string\">&#x27;price&#x27;</span>].mean()</span><br><span class=\"line\">errors = df[<span class=\"string\">&#x27;price&#x27;</span>] - baseline</span><br><span class=\"line\">mae = errors.<span class=\"built_in\">abs</span>().mean()</span><br><span class=\"line\"></span><br><span class=\"line\">sns.lineplot(x=df[<span class=\"string\">&#x27;grade&#x27;</span>], y=baseline, color=<span class=\"string\">&#x27;red&#x27;</span>)</span><br><span class=\"line\">sns.scatterplot(data=df, x=<span class=\"string\">&quot;grade&quot;</span>, y=<span class=\"string\">&quot;price&quot;</span>)</span><br><span class=\"line\">plt.ticklabel_format(style=<span class=\"string\">&#x27;plain&#x27;</span>, axis=<span class=\"string\">&#x27;y&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/ML-SP-simple_regression/1_simple-regression_8_0.png\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#기본적으로 모델이 다중회귀를 가정하기 때문에 Feature의 선언을 배열로 해줘야 한다.</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = df[[<span class=\"string\">&#x27;sqft_living&#x27;</span>]] </span><br><span class=\"line\">y = df[<span class=\"string\">&#x27;price&#x27;</span>]</span><br><span class=\"line\">m = LinearRegression()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m.fit(X,y)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>LinearRegression()\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_test = [[<span class=\"number\">6000</span>]]</span><br><span class=\"line\">y_pred = m.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;sqft_living이 <span class=\"subst\">&#123;X_test[<span class=\"number\">0</span>][<span class=\"number\">0</span>]&#125;</span>인 경우의 예상 가격은 <span class=\"subst\">&#123;<span class=\"built_in\">int</span>(y_pred)&#125;</span>로 나타남&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>sqft_living이 6000인 경우의 예상 가격은 1640160로 나타남\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 통계량 확인</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;coefficient-score : &quot;</span>,m.coef_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;intercept : &quot;</span> , m.intercept_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;coefficient of determination: &#123;:.2f&#125;&quot;</span>.<span class=\"built_in\">format</span>(m.score(X,y)))</span><br></pre></td></tr></table></figure>\n\n<pre><code>coefficient-score :  [280.6235679]\nintercept :  -43580.74309447361\ncoefficient of determination: 0.49\n</code></pre>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">https://scikit-learn.org/stable/modules/linear_model.html</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\">https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><ul>\n<li>Python 선형회귀 연습하기</li>\n<li>nbconvert를 활용해 ipynb파일을 마크다운 파일로 변환</li>\n</ul>\n<h2 id=\"scikit-learn을-활용한-단순선형회귀\"><a href=\"#scikit-learn을-활용한-단순선형회귀\" class=\"headerlink\" title=\"scikit-learn을 활용한 단순선형회귀\"></a>scikit-learn을 활용한 단순선형회귀</h2><h3 id=\"데이터-불러오기-및-전처리\"><a href=\"#데이터-불러오기-및-전처리\" class=\"headerlink\" title=\"데이터 불러오기 및 전처리\"></a>데이터 불러오기 및 전처리</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;../data/kc_house_data.csv&#x27;</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>...</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1180.0</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2170.0</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>770.0</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>1050.0</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>1680.0</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># * 가 파이썬에서 뭘 의미하는지 찾아보자</span></span><br><span class=\"line\"><span class=\"comment\"># https://mingrammer.com/understanding-the-asterisk-of-python/</span></span><br><span class=\"line\"><span class=\"comment\"># https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/</span></span><br><span class=\"line\">use_col = df.columns.tolist()[<span class=\"number\">3</span>:]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*use_col, sep = <span class=\"string\">&#x27;\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>bedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 상관행렬 구하기</span></span><br><span class=\"line\">corrMatrix = df.corr()</span><br><span class=\"line\"><span class=\"comment\"># 상관행렬에서 타겟변수 추출</span></span><br><span class=\"line\">price_col = corrMatrix[<span class=\"string\">&#x27;price&#x27;</span>].to_frame().reset_index()</span><br><span class=\"line\"><span class=\"comment\"># 상관계수높은 순으로 정렬</span></span><br><span class=\"line\">price_col.sort_values(<span class=\"string\">&#x27;price&#x27;</span>,ascending=<span class=\"literal\">False</span>,inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">price_col.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>price</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqft_living</td>\n      <td>0.702035</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>grade</td>\n      <td>0.667434</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sqft_above</td>\n      <td>0.605567</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>sqft_living15</td>\n      <td>0.585379</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 가장 상관관계가 높은 변수 시각화</span></span><br><span class=\"line\">sns.<span class=\"built_in\">set</span>(style=<span class=\"string\">&#x27;whitegrid&#x27;</span>, context=<span class=\"string\">&#x27;notebook&#x27;</span>)</span><br><span class=\"line\">g=sns.pairplot(df[[<span class=\"string\">&#x27;price&#x27;</span>,<span class=\"string\">&#x27;sqft_living&#x27;</span>]])</span><br><span class=\"line\"><span class=\"keyword\">for</span> ax <span class=\"keyword\">in</span> g.axes.flatten(): <span class=\"comment\"># y축 한정 과학적 표기법을 일반 표기법으로 바꿔주기</span></span><br><span class=\"line\">    ax.ticklabel_format(style=<span class=\"string\">&#x27;plain&#x27;</span>, scilimits=(<span class=\"number\">0</span>,<span class=\"number\">0</span>), axis=<span class=\"string\">&#x27;y&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\">#plt.ticklabel_format(style=&#x27;plain&#x27;, axis=&#x27;y&#x27;)</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/ML-SP-simple_regression/1_simple-regression_6_0.png\" alt=\"png\"></p>\n<h3 id=\"Baseline-Model-만들기\"><a href=\"#Baseline-Model-만들기\" class=\"headerlink\" title=\"Baseline Model 만들기\"></a>Baseline Model 만들기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># get baseline model</span></span><br><span class=\"line\">baseline = df[<span class=\"string\">&#x27;price&#x27;</span>].mean()</span><br><span class=\"line\">errors = df[<span class=\"string\">&#x27;price&#x27;</span>] - baseline</span><br><span class=\"line\">mae = errors.<span class=\"built_in\">abs</span>().mean()</span><br><span class=\"line\"></span><br><span class=\"line\">sns.lineplot(x=df[<span class=\"string\">&#x27;grade&#x27;</span>], y=baseline, color=<span class=\"string\">&#x27;red&#x27;</span>)</span><br><span class=\"line\">sns.scatterplot(data=df, x=<span class=\"string\">&quot;grade&quot;</span>, y=<span class=\"string\">&quot;price&quot;</span>)</span><br><span class=\"line\">plt.ticklabel_format(style=<span class=\"string\">&#x27;plain&#x27;</span>, axis=<span class=\"string\">&#x27;y&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/ML-SP-simple_regression/1_simple-regression_8_0.png\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#기본적으로 모델이 다중회귀를 가정하기 때문에 Feature의 선언을 배열로 해줘야 한다.</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = df[[<span class=\"string\">&#x27;sqft_living&#x27;</span>]] </span><br><span class=\"line\">y = df[<span class=\"string\">&#x27;price&#x27;</span>]</span><br><span class=\"line\">m = LinearRegression()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m.fit(X,y)</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>LinearRegression()\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_test = [[<span class=\"number\">6000</span>]]</span><br><span class=\"line\">y_pred = m.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;sqft_living이 <span class=\"subst\">&#123;X_test[<span class=\"number\">0</span>][<span class=\"number\">0</span>]&#125;</span>인 경우의 예상 가격은 <span class=\"subst\">&#123;<span class=\"built_in\">int</span>(y_pred)&#125;</span>로 나타남&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>sqft_living이 6000인 경우의 예상 가격은 1640160로 나타남\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 통계량 확인</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;coefficient-score : &quot;</span>,m.coef_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;intercept : &quot;</span> , m.intercept_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;coefficient of determination: &#123;:.2f&#125;&quot;</span>.<span class=\"built_in\">format</span>(m.score(X,y)))</span><br></pre></td></tr></table></figure>\n\n<pre><code>coefficient-score :  [280.6235679]\nintercept :  -43580.74309447361\ncoefficient of determination: 0.49\n</code></pre>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">https://scikit-learn.org/stable/modules/linear_model.html</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\">https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"/ML-SP-simple_regression/1_simple-regression_6_0.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]단순선형회귀 구현하기(Simple Regression)","path":"2021/06/08/ML-SP-simple_regression/","eyeCatchImage":"/ML-SP-simple_regression/1_simple-regression_6_0.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-06-08T10:28:01.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-06-08T10:28:01.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Supervised Learning","Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Unsupervised Learning]주성분분석(PCA)의 이해","date":"2021-05-29T07:59:23.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n- **비지도학습은 라벨이 달린 데이터를 이용해 데이터를 학습하는 과정 없이 데이터로부터 의미를 추출하는 것이다.**\n- **비지도학습의 목적은 크게 차원축소와 클러스터링 두 가지로 나눌 수 있다.**\n---\n\n- **비지도학습**\n  - **차원축소(PCA)** : 데이터의 변수를 관리 가능한 수준으로 차원을 줄이는 것, 변수와 레코드의 수가 아주 큰 상황이라면 차원축소를을 EDA의 연장으로 볼 수있다. \n    - PCA\n    - LDA\n    - SVD\n    - 요인분석\n    - etc\n  - **클러스터링**: 라벨이 정해진 응답변수가 없는 상황에서 예측 규칙을 만드는 것(그룹화)\n    - K평균 클러스터링\n    - 계층적 클러스터링\n    - 모델기반 클러스터링\n    - etc\n\n## 주성분분석(PCA)\n\n>**주성분분석은 데이터의 차원을 줄이기 위해, 공분산 행렬에서 고유 벡터/고유값을 구하고 가장 분산이 큰 방향을 가진 고유벡터에 입력데이터를 선형변환하는 것이다.**\n\n위 말을 이해하기 위해서는 차원축소를 하는 이유와 선형대수 관련 개념을 조금 알아야 한다.\n\n### 차원을 줄이는 이유\n차원을 줄이는 이유는 크게 두 가지가 있는데 하나는 메모리 소모를 줄이기 위해서고 다른 하나는 차원의 저주 때문이다.\n\n**메모리 소모 문제**\nR을 생각해보자. R을 써본 사람은 알겠지만 정말 개같이 느린 언어이다. 이는 R이 물리적 메모리를 저장하는 방식을 사용하기 때문이다.물론 파이썬도 그렇게 빠른 언어는 아니다. 넘파이가 빠른거지 파이썬이 빠른게 아니다. 그리고 넘파이는 C랑 포트란으로 짜여졌다.\n이처럼 수치해석을 하기에는 너무나도 느린 언어들로 큰 데이터를 분석하다보면 시간은 시간대로 날려먹고 제대로된 결과를 뽑지 못할 가능성이 높다. 따라서 데이터양이(정확히는 데이터 n값과 predictor의 수의 곱이) 너무 많을 경우 PCA등의 차원축소기법을 사용해 데이터의 차원을 줄여 분석에 쓰이는 연산량을 줄이는 방법을 사용한다.\n\n**차원의 저주(curse of demention)**\n데이터의 수보다 데이터의 차원이 더 큰 경우를 차원의 저주라고 하는데 이경우 모델링이 복잡해지고 예측력이 낮아진다. 이러면 모델링 자체가 쓸모없어지기에 차원이 너무 많은 경우 Feature Selection이나 Feature Extraction 등을 통해 사전 처리를 해줄 필요가 있다. \n\n### **공분산행렬**\n공분산행렬만 이해하면 PCA는 사실 그렇게 어렵지는 않다. PCA는 단순히 공분산행렬의 고유벡터를 기저벡터로 바꾼 것에 불과하다.일단 공분산이 뭔지 알아야 한다.\n공분산은 교차곱편차의 평균이다. 이는 각 변수들의 편차들의 곱들을 모두 더해 n으로 나눈 값을 의미한다.\n\n데이터 매트릭스 X에 대해서 공분산행렬을 아래와 같이 나타낼 수 있다.(자유도때문에 n-1로 나눠줌)\n\n$$Cov(X) = \\frac{1}{n-1}X^TX$$\n\n공분산은 **기본적으로 변수들이 함께 변화하는 정도**이다.\n\n공분산행렬은 **대각성분을 설명변수의 분산으로 채우고 나머지를 공분산으로 채운 행렬이다.**\n\n![](covariance_matrix.png)\n**그림1. 한번에 이해하는 공분산 행렬**\n\n### 선형변환\n- **행렬은 선형변환을 나타낸다.**\n- 선형은 곱하고 더하는 것으로만 이루어져 있다.(문과식 이해의 한계)\n- 벡터 `[a,b]` 의 와 행렬 M을 곱해 벡터 `[c,d]`가 나왔을 경우 \n- `[c,d]`는 유닛 벡터 `[x,y]` 의 의 a배와 b 배의 합으로 이해할 수 있다. \n- 이처럼 선형변환은 기본적으로 행렬과 벡터의 곱의 형태로 나타낼 수잇다.\n  \n다시 생각해 보면 다음과 같다.\n\n- `f` 라는 어떤 매핑을 사용하여 \n- 임의의 벡터 `[a, b]`에 대해서, \n- `[2a + b, a -2b ]`로 바꾼다는 것은 아래와 같이 나타낼 수 있다.\n\n$$\\begin{align}\nf(\\begin{bmatrix}a \\\\ b \\end{bmatrix}) = \\begin{bmatrix} 2a + b \\\\ a -2b \\\\  \\end{bmatrix}\n\\end{align}$$\n\n여기서 `[2a + b, a -2b ]`는 `[c,d]` 이며 특정 유닛벡터 (여기서는 `[2+b/a, a/b-2]`) 에 각각 a배, b배 한 값이 된다.\n\n**여기서 `f`를 행렬을 곱하는 것으로 생각하면 놀랍게도 행렬과 벡터의 곱이 벡터의 선형변환과 동일하다는 것을 알 수 있다.**\n\n### **Eigenvalue,Eigenvector**\n- 일단 행렬이 선형변환이라는 것을 알아야 한다. [진짜 설명 오지는 레퍼런스](https://angeloyeo.github.io/2019/07/15/Matrix_as_Linear_Transformation.html)를 읽어보자.\n- Eigenvector 는 특정 벡터에 대해 선형변환을 했을 때 크기만 바뀌고 방향은 바뀌지 않는 벡터(축)이다.(행렬의 방향성을 유지하는 선형변환의 주축)\n- Eigenvector 는 행렬이 벡터에 작용하는 주축이다.\n- **Eigenvector 는 어떤 행렬이 벡터에 작용하는 힘의 방향을 나타낸다.**\n  - 만약 그 행렬이 공분산 행렬일 경우 그 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지, 즉 어떤 방향으로 힘이 작용하는 지 나타낸다.-> 공분산행렬의 Eigenvector가 원데이터의 분산이 최대가 되는 축이다.\n- **Eigenvalue는 Eigenvector의 방향으로 크기가 얼마나 변화하는 지를 나타내는 값이다.**\n\n<!--\n$$ T \\cdot v = v' = \\lambda \\cdot v $$\n\n\n\n-->\n\n$$A = cov(v)$$\n\n$$Ax = \\lambda x$$\n\n- A는 행렬 v의 공분산행렬이다.\n- 여기서의 $\\lambda$가 Eigenvalue이다.\n- x가 Eigenvector이다.\n\nx가 0이 아니여야 식이 성립하기 때문에 행렬식을 활용해 다음을 만족하는 벡터를 찾는다.\n\n\n$$(A-\\lambda I)\\vec{x}=0$$\n\n\n$$det(A−\\lambda I)=0$$\n\n\n$$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} ax+by \\\\ cx+dy \\end{bmatrix} = \\lambda \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n$$\n\n### **주성분**\n\n결국 PCA에서 하고자 하는 것은 차원을 축소하면서 데이터 벡터를 **어떤 벡터**에 내적하는 것이 가능한 정보량(분산)을 유지하는 것인지 알아내는 것이다. 차원을 축소하면서 정보손실을 적게 하려면 원데이터의 분산이 최대가 되는 축을 찾아야 한다. 여기서 축이 되는 **어떤 벡터**가 주성분이며 공분산행렬의 eigenvector이다. \n\n**linear combination**\n주성분은 변수들의 선형 결합으로 표현된다. 이는 어떤 주성분 PC1이 있다고 했을때 이 PC1는 마치 다중회귀식마냥 기존 변수들의 조합으로 표현된다는 것이다.\n\n![](PCA_lc.png)\n**그림2. 선형결합으로서의 주성분**\n### **Feature Selection & Extraction**\n차원축소는 크게 변수선택과 변수추출로 나뉘며 PCA는 그 중 변수 추출에 속한다.\n- **변수선택**\n  - 덜 중요한 피처를 제거하는 방식으로 차원을 축소하는 방식\n  - ex) `Lasso`,`Generic algorithm`\n- **변수추출**\n  - 기존변수를 조합해서 새 변수를 만드는 방식\n  - 변수간 연관성을 고려할 수있다\n  - 해석이 어렵다.\n\n![](FS_FE.png)\n**그림3 변수선택과 변수추출의 직관적 이해**\n\n### **정리**\n- **선형변환**\n  - 행렬은 선형변환이다.\n  - 즉, 임의의 $\\mathbb{R}^2$ 벡터를 다른 $\\mathbb{R}^2$ 내부의 벡터로 변환 하는 과정은 특정 $M$라는 매트릭스를 곱하는 것과 같다.\n\n- **공분산행렬**\n  - 공분산은 교차곱편차의 평균이며 각 변수들이 얼마나 함께 변화하는 지를 나타낸다.\n  - 공분산을 표준화하면 상관계수이다. \n  - 공분산행렬은 각 feature들의 변동이 얼마나 유사한지를 나타낸다.\n  - 공분산행렬 자체는 단순히 정사각행렬의 대각성분을 각 변수의 분산으로 채우고 나머지를 공분산으로 채운 것이다.\n  - 공분산행렬의 서로다른 eigenvector 끼리는 직교한다.따라서 해당 eigenvector를 기저벡터로 하게끔 좌표축이 바뀐 데이터는 서로 상관이 없게 된다.\n- **고윳값/고유벡터**\n  - 고윳값은 벡터의 크기이다\n  - 고유벡터는 특정 벡터에 선형변환을 했을 때 크기만 바뀌고 뱡향은 바뀌지않는 벡터이다. 기본적으로 방향이다.\n  - 공분산행렬의 고유벡터의 열벡터가 주성분이다.\n  - 고유값은 기존벡터에서 고유벡터로 정사영했을 때의 분산이다.따라서 고유벡터로 사영할 경우 분산이 최대가 된다.\n- **주성분분석**\n  - 주성분은 수치형 예측변수의 선형결합(linear combination)이다.\n  - 주성분은 공분산행렬의 eigenvector이다.\n  - 주성분은 서로간의 상관관계가 최소화 되어 중복성이 줄어들도록 한다 \n  - 차원의 수만큼 주성분이 존재하며 모든 차원의 주성분의 설명량은 1이다.\n  - 주성분의 일부만으로도 전체 분산의 대부분을 설명할 수 있다.\n  - 첫번째 주성분이 가장 큰 분산을 가지고 있고 두번째 주성분은 첫번째 주성분과 직교한다는 전제 하에 두번째로 큰 분산을 가지고 있다.\n  - 부하(loading)는 예측변수들을 주성분으로 변동할때 쓰이는 가중치이다. 계수이다.\n  - Screeplot은 성분들의 변동을 나타낸다. 정확히는 성분의 수에 따른 분산의 변화량을 나타낸다. \n  - 다음의 두 가지 경우 PCA의 적용이 어렵다\n    - 데이터의 분포가 정규성을 띄지 않는 경우 적용 어려움\n      - 이 경우 커널 PCA 사용\n    - 분류 / 예측 문제에 대해서 데이터의 라벨을 고려하지 않기 때문에 효과적 분리가 어려움\n      - 이 경우 PLS 사용\n\n## 구현하기(R, Python)\n> 사실 이해하는게 어렵지 다른 대부분의 분석들과 마찬가지로 구현 자체는 매우 쉽다.\n\n### Eigenvalue, Eigenvector 구하기\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```r\n\n# eigen()을 사용해 매우 쉽게 구할 수 있다\n\n> M <- as.matrix(data.frame(c(1,-1,0),c(1,2,1),c(-2,1,-1)))\n> M\n     c.1...1..0. c.1..2..1. c..2..1...1.\n[1,]           1          1           -2\n[2,]          -1          2            1\n[3,]           0          1           -1\n> eigen(M)\neigen() decomposition\n$values\n[1]  2  1 -1\n\n$vectors\n          [,1]       [,2]          [,3]\n[1,] 0.3015113 -0.8017837  7.071068e-01\n[2,] 0.9045340 -0.5345225 -1.922963e-16\n[3,] 0.3015113 -0.2672612  7.071068e-01\n\n\n```\n### 공분산행렬 구하기\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```r\n```\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```python\n\n\n```\n\n### PCA구현하기\n\n- R을 활용한 pca\n```r\ndf <- data[,c('a','b')]\npca<-princomp(df)\n\npca$loadings # 주성분 부하량 확인\n\n```\n\n- python을 활용한 pca\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2) # n_components로 주성분의 수를 지정할 수 있다.\npca_values = pca.fit_transform(data_use) # 설명량\n\nprint('누적설명량 {0}'.format(sum(pca.explained_variance_ratio_)))\nprint('각 차원의 설명량 {0}'.format(pca.explained_variance_ratio_))\n\n```\n\n### PCA 시각화 관련 참고\n- seaborn을 활용한 시각화\n```python\n# seaborn을 활용한 시각화\n\nsns.scatterplot(pca_v[0],pca_v[1],data=pca_v,hue = 'target',\nstyle = 'target',\ns = 100)\n\n```\n- matplotlib을 활용한 시각화(1)\n```python\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\n\ndf = px.data.iris()\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n\npca = PCA()\ncomponents = pca.fit_transform(df[features])\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(4),\n    color=df[\"species\"]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n```\n- matplotlib을 활용한 시각화(2)\n```python\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\n```\n## References\n- 책\n  - Practical Statistics for Data Science\n- 블로그\n  - https://wiserloner.tistory.com/1297 # 고유값과 고유벡터\n  - https://huidea.tistory.com/126 #기술면접 참고\n  - https://rfriend.tistory.com/380 #python 선형대수 함수\n  - https://rfriend.tistory.com/181 #고유값과 고유벡터 구현\n  - https://youtu.be/jNwf-JUGWgg # 공분산행렬의 이해\n  - https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/ # 행렬연산과 공분산행렬\n  - 시각화\n    - https://plotly.com/python/pca-visualization/\n    - https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60","source":"_posts/ML-US-PCA.md","raw":"---\ntitle: \"[Unsupervised Learning]주성분분석(PCA)의 이해\"\ntags:\n  - Unsupervised Learning\n  - PCA\ncategories:\n  - [Machine Learning]\n  - [PCA]\ndate: 2021-05-29 16:59:23\nupdated:\n---\n\n- **비지도학습은 라벨이 달린 데이터를 이용해 데이터를 학습하는 과정 없이 데이터로부터 의미를 추출하는 것이다.**\n- **비지도학습의 목적은 크게 차원축소와 클러스터링 두 가지로 나눌 수 있다.**\n---\n\n- **비지도학습**\n  - **차원축소(PCA)** : 데이터의 변수를 관리 가능한 수준으로 차원을 줄이는 것, 변수와 레코드의 수가 아주 큰 상황이라면 차원축소를을 EDA의 연장으로 볼 수있다. \n    - PCA\n    - LDA\n    - SVD\n    - 요인분석\n    - etc\n  - **클러스터링**: 라벨이 정해진 응답변수가 없는 상황에서 예측 규칙을 만드는 것(그룹화)\n    - K평균 클러스터링\n    - 계층적 클러스터링\n    - 모델기반 클러스터링\n    - etc\n\n## 주성분분석(PCA)\n\n>**주성분분석은 데이터의 차원을 줄이기 위해, 공분산 행렬에서 고유 벡터/고유값을 구하고 가장 분산이 큰 방향을 가진 고유벡터에 입력데이터를 선형변환하는 것이다.**\n\n위 말을 이해하기 위해서는 차원축소를 하는 이유와 선형대수 관련 개념을 조금 알아야 한다.\n\n### 차원을 줄이는 이유\n차원을 줄이는 이유는 크게 두 가지가 있는데 하나는 메모리 소모를 줄이기 위해서고 다른 하나는 차원의 저주 때문이다.\n\n**메모리 소모 문제**\nR을 생각해보자. R을 써본 사람은 알겠지만 정말 개같이 느린 언어이다. 이는 R이 물리적 메모리를 저장하는 방식을 사용하기 때문이다.물론 파이썬도 그렇게 빠른 언어는 아니다. 넘파이가 빠른거지 파이썬이 빠른게 아니다. 그리고 넘파이는 C랑 포트란으로 짜여졌다.\n이처럼 수치해석을 하기에는 너무나도 느린 언어들로 큰 데이터를 분석하다보면 시간은 시간대로 날려먹고 제대로된 결과를 뽑지 못할 가능성이 높다. 따라서 데이터양이(정확히는 데이터 n값과 predictor의 수의 곱이) 너무 많을 경우 PCA등의 차원축소기법을 사용해 데이터의 차원을 줄여 분석에 쓰이는 연산량을 줄이는 방법을 사용한다.\n\n**차원의 저주(curse of demention)**\n데이터의 수보다 데이터의 차원이 더 큰 경우를 차원의 저주라고 하는데 이경우 모델링이 복잡해지고 예측력이 낮아진다. 이러면 모델링 자체가 쓸모없어지기에 차원이 너무 많은 경우 Feature Selection이나 Feature Extraction 등을 통해 사전 처리를 해줄 필요가 있다. \n\n### **공분산행렬**\n공분산행렬만 이해하면 PCA는 사실 그렇게 어렵지는 않다. PCA는 단순히 공분산행렬의 고유벡터를 기저벡터로 바꾼 것에 불과하다.일단 공분산이 뭔지 알아야 한다.\n공분산은 교차곱편차의 평균이다. 이는 각 변수들의 편차들의 곱들을 모두 더해 n으로 나눈 값을 의미한다.\n\n데이터 매트릭스 X에 대해서 공분산행렬을 아래와 같이 나타낼 수 있다.(자유도때문에 n-1로 나눠줌)\n\n$$Cov(X) = \\frac{1}{n-1}X^TX$$\n\n공분산은 **기본적으로 변수들이 함께 변화하는 정도**이다.\n\n공분산행렬은 **대각성분을 설명변수의 분산으로 채우고 나머지를 공분산으로 채운 행렬이다.**\n\n![](covariance_matrix.png)\n**그림1. 한번에 이해하는 공분산 행렬**\n\n### 선형변환\n- **행렬은 선형변환을 나타낸다.**\n- 선형은 곱하고 더하는 것으로만 이루어져 있다.(문과식 이해의 한계)\n- 벡터 `[a,b]` 의 와 행렬 M을 곱해 벡터 `[c,d]`가 나왔을 경우 \n- `[c,d]`는 유닛 벡터 `[x,y]` 의 의 a배와 b 배의 합으로 이해할 수 있다. \n- 이처럼 선형변환은 기본적으로 행렬과 벡터의 곱의 형태로 나타낼 수잇다.\n  \n다시 생각해 보면 다음과 같다.\n\n- `f` 라는 어떤 매핑을 사용하여 \n- 임의의 벡터 `[a, b]`에 대해서, \n- `[2a + b, a -2b ]`로 바꾼다는 것은 아래와 같이 나타낼 수 있다.\n\n$$\\begin{align}\nf(\\begin{bmatrix}a \\\\ b \\end{bmatrix}) = \\begin{bmatrix} 2a + b \\\\ a -2b \\\\  \\end{bmatrix}\n\\end{align}$$\n\n여기서 `[2a + b, a -2b ]`는 `[c,d]` 이며 특정 유닛벡터 (여기서는 `[2+b/a, a/b-2]`) 에 각각 a배, b배 한 값이 된다.\n\n**여기서 `f`를 행렬을 곱하는 것으로 생각하면 놀랍게도 행렬과 벡터의 곱이 벡터의 선형변환과 동일하다는 것을 알 수 있다.**\n\n### **Eigenvalue,Eigenvector**\n- 일단 행렬이 선형변환이라는 것을 알아야 한다. [진짜 설명 오지는 레퍼런스](https://angeloyeo.github.io/2019/07/15/Matrix_as_Linear_Transformation.html)를 읽어보자.\n- Eigenvector 는 특정 벡터에 대해 선형변환을 했을 때 크기만 바뀌고 방향은 바뀌지 않는 벡터(축)이다.(행렬의 방향성을 유지하는 선형변환의 주축)\n- Eigenvector 는 행렬이 벡터에 작용하는 주축이다.\n- **Eigenvector 는 어떤 행렬이 벡터에 작용하는 힘의 방향을 나타낸다.**\n  - 만약 그 행렬이 공분산 행렬일 경우 그 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지, 즉 어떤 방향으로 힘이 작용하는 지 나타낸다.-> 공분산행렬의 Eigenvector가 원데이터의 분산이 최대가 되는 축이다.\n- **Eigenvalue는 Eigenvector의 방향으로 크기가 얼마나 변화하는 지를 나타내는 값이다.**\n\n<!--\n$$ T \\cdot v = v' = \\lambda \\cdot v $$\n\n\n\n-->\n\n$$A = cov(v)$$\n\n$$Ax = \\lambda x$$\n\n- A는 행렬 v의 공분산행렬이다.\n- 여기서의 $\\lambda$가 Eigenvalue이다.\n- x가 Eigenvector이다.\n\nx가 0이 아니여야 식이 성립하기 때문에 행렬식을 활용해 다음을 만족하는 벡터를 찾는다.\n\n\n$$(A-\\lambda I)\\vec{x}=0$$\n\n\n$$det(A−\\lambda I)=0$$\n\n\n$$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} ax+by \\\\ cx+dy \\end{bmatrix} = \\lambda \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n$$\n\n### **주성분**\n\n결국 PCA에서 하고자 하는 것은 차원을 축소하면서 데이터 벡터를 **어떤 벡터**에 내적하는 것이 가능한 정보량(분산)을 유지하는 것인지 알아내는 것이다. 차원을 축소하면서 정보손실을 적게 하려면 원데이터의 분산이 최대가 되는 축을 찾아야 한다. 여기서 축이 되는 **어떤 벡터**가 주성분이며 공분산행렬의 eigenvector이다. \n\n**linear combination**\n주성분은 변수들의 선형 결합으로 표현된다. 이는 어떤 주성분 PC1이 있다고 했을때 이 PC1는 마치 다중회귀식마냥 기존 변수들의 조합으로 표현된다는 것이다.\n\n![](PCA_lc.png)\n**그림2. 선형결합으로서의 주성분**\n### **Feature Selection & Extraction**\n차원축소는 크게 변수선택과 변수추출로 나뉘며 PCA는 그 중 변수 추출에 속한다.\n- **변수선택**\n  - 덜 중요한 피처를 제거하는 방식으로 차원을 축소하는 방식\n  - ex) `Lasso`,`Generic algorithm`\n- **변수추출**\n  - 기존변수를 조합해서 새 변수를 만드는 방식\n  - 변수간 연관성을 고려할 수있다\n  - 해석이 어렵다.\n\n![](FS_FE.png)\n**그림3 변수선택과 변수추출의 직관적 이해**\n\n### **정리**\n- **선형변환**\n  - 행렬은 선형변환이다.\n  - 즉, 임의의 $\\mathbb{R}^2$ 벡터를 다른 $\\mathbb{R}^2$ 내부의 벡터로 변환 하는 과정은 특정 $M$라는 매트릭스를 곱하는 것과 같다.\n\n- **공분산행렬**\n  - 공분산은 교차곱편차의 평균이며 각 변수들이 얼마나 함께 변화하는 지를 나타낸다.\n  - 공분산을 표준화하면 상관계수이다. \n  - 공분산행렬은 각 feature들의 변동이 얼마나 유사한지를 나타낸다.\n  - 공분산행렬 자체는 단순히 정사각행렬의 대각성분을 각 변수의 분산으로 채우고 나머지를 공분산으로 채운 것이다.\n  - 공분산행렬의 서로다른 eigenvector 끼리는 직교한다.따라서 해당 eigenvector를 기저벡터로 하게끔 좌표축이 바뀐 데이터는 서로 상관이 없게 된다.\n- **고윳값/고유벡터**\n  - 고윳값은 벡터의 크기이다\n  - 고유벡터는 특정 벡터에 선형변환을 했을 때 크기만 바뀌고 뱡향은 바뀌지않는 벡터이다. 기본적으로 방향이다.\n  - 공분산행렬의 고유벡터의 열벡터가 주성분이다.\n  - 고유값은 기존벡터에서 고유벡터로 정사영했을 때의 분산이다.따라서 고유벡터로 사영할 경우 분산이 최대가 된다.\n- **주성분분석**\n  - 주성분은 수치형 예측변수의 선형결합(linear combination)이다.\n  - 주성분은 공분산행렬의 eigenvector이다.\n  - 주성분은 서로간의 상관관계가 최소화 되어 중복성이 줄어들도록 한다 \n  - 차원의 수만큼 주성분이 존재하며 모든 차원의 주성분의 설명량은 1이다.\n  - 주성분의 일부만으로도 전체 분산의 대부분을 설명할 수 있다.\n  - 첫번째 주성분이 가장 큰 분산을 가지고 있고 두번째 주성분은 첫번째 주성분과 직교한다는 전제 하에 두번째로 큰 분산을 가지고 있다.\n  - 부하(loading)는 예측변수들을 주성분으로 변동할때 쓰이는 가중치이다. 계수이다.\n  - Screeplot은 성분들의 변동을 나타낸다. 정확히는 성분의 수에 따른 분산의 변화량을 나타낸다. \n  - 다음의 두 가지 경우 PCA의 적용이 어렵다\n    - 데이터의 분포가 정규성을 띄지 않는 경우 적용 어려움\n      - 이 경우 커널 PCA 사용\n    - 분류 / 예측 문제에 대해서 데이터의 라벨을 고려하지 않기 때문에 효과적 분리가 어려움\n      - 이 경우 PLS 사용\n\n## 구현하기(R, Python)\n> 사실 이해하는게 어렵지 다른 대부분의 분석들과 마찬가지로 구현 자체는 매우 쉽다.\n\n### Eigenvalue, Eigenvector 구하기\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```r\n\n# eigen()을 사용해 매우 쉽게 구할 수 있다\n\n> M <- as.matrix(data.frame(c(1,-1,0),c(1,2,1),c(-2,1,-1)))\n> M\n     c.1...1..0. c.1..2..1. c..2..1...1.\n[1,]           1          1           -2\n[2,]          -1          2            1\n[3,]           0          1           -1\n> eigen(M)\neigen() decomposition\n$values\n[1]  2  1 -1\n\n$vectors\n          [,1]       [,2]          [,3]\n[1,] 0.3015113 -0.8017837  7.071068e-01\n[2,] 0.9045340 -0.5345225 -1.922963e-16\n[3,] 0.3015113 -0.2672612  7.071068e-01\n\n\n```\n### 공분산행렬 구하기\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```r\n```\n- R을 활용한 Eigenvalue, Eigenvector 계산 \n```python\n\n\n```\n\n### PCA구현하기\n\n- R을 활용한 pca\n```r\ndf <- data[,c('a','b')]\npca<-princomp(df)\n\npca$loadings # 주성분 부하량 확인\n\n```\n\n- python을 활용한 pca\n```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2) # n_components로 주성분의 수를 지정할 수 있다.\npca_values = pca.fit_transform(data_use) # 설명량\n\nprint('누적설명량 {0}'.format(sum(pca.explained_variance_ratio_)))\nprint('각 차원의 설명량 {0}'.format(pca.explained_variance_ratio_))\n\n```\n\n### PCA 시각화 관련 참고\n- seaborn을 활용한 시각화\n```python\n# seaborn을 활용한 시각화\n\nsns.scatterplot(pca_v[0],pca_v[1],data=pca_v,hue = 'target',\nstyle = 'target',\ns = 100)\n\n```\n- matplotlib을 활용한 시각화(1)\n```python\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\n\ndf = px.data.iris()\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n\npca = PCA()\ncomponents = pca.fit_transform(df[features])\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(4),\n    color=df[\"species\"]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n```\n- matplotlib을 활용한 시각화(2)\n```python\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\n```\n## References\n- 책\n  - Practical Statistics for Data Science\n- 블로그\n  - https://wiserloner.tistory.com/1297 # 고유값과 고유벡터\n  - https://huidea.tistory.com/126 #기술면접 참고\n  - https://rfriend.tistory.com/380 #python 선형대수 함수\n  - https://rfriend.tistory.com/181 #고유값과 고유벡터 구현\n  - https://youtu.be/jNwf-JUGWgg # 공분산행렬의 이해\n  - https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/ # 행렬연산과 공분산행렬\n  - 시각화\n    - https://plotly.com/python/pca-visualization/\n    - https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60","slug":"ML-US-PCA","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscb001mb36q9rr59rsy","content":"<ul>\n<li><strong>비지도학습은 라벨이 달린 데이터를 이용해 데이터를 학습하는 과정 없이 데이터로부터 의미를 추출하는 것이다.</strong></li>\n<li><strong>비지도학습의 목적은 크게 차원축소와 클러스터링 두 가지로 나눌 수 있다.</strong></li>\n</ul>\n<hr>\n<ul>\n<li><strong>비지도학습</strong><ul>\n<li><strong>차원축소(PCA)</strong> : 데이터의 변수를 관리 가능한 수준으로 차원을 줄이는 것, 변수와 레코드의 수가 아주 큰 상황이라면 차원축소를을 EDA의 연장으로 볼 수있다. <ul>\n<li>PCA</li>\n<li>LDA</li>\n<li>SVD</li>\n<li>요인분석</li>\n<li>etc</li>\n</ul>\n</li>\n<li><strong>클러스터링</strong>: 라벨이 정해진 응답변수가 없는 상황에서 예측 규칙을 만드는 것(그룹화)<ul>\n<li>K평균 클러스터링</li>\n<li>계층적 클러스터링</li>\n<li>모델기반 클러스터링</li>\n<li>etc</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"주성분분석-PCA\"><a href=\"#주성분분석-PCA\" class=\"headerlink\" title=\"주성분분석(PCA)\"></a>주성분분석(PCA)</h2><blockquote>\n<p><strong>주성분분석은 데이터의 차원을 줄이기 위해, 공분산 행렬에서 고유 벡터&#x2F;고유값을 구하고 가장 분산이 큰 방향을 가진 고유벡터에 입력데이터를 선형변환하는 것이다.</strong></p>\n</blockquote>\n<p>위 말을 이해하기 위해서는 차원축소를 하는 이유와 선형대수 관련 개념을 조금 알아야 한다.</p>\n<h3 id=\"차원을-줄이는-이유\"><a href=\"#차원을-줄이는-이유\" class=\"headerlink\" title=\"차원을 줄이는 이유\"></a>차원을 줄이는 이유</h3><p>차원을 줄이는 이유는 크게 두 가지가 있는데 하나는 메모리 소모를 줄이기 위해서고 다른 하나는 차원의 저주 때문이다.</p>\n<p><strong>메모리 소모 문제</strong><br>R을 생각해보자. R을 써본 사람은 알겠지만 정말 개같이 느린 언어이다. 이는 R이 물리적 메모리를 저장하는 방식을 사용하기 때문이다.물론 파이썬도 그렇게 빠른 언어는 아니다. 넘파이가 빠른거지 파이썬이 빠른게 아니다. 그리고 넘파이는 C랑 포트란으로 짜여졌다.<br>이처럼 수치해석을 하기에는 너무나도 느린 언어들로 큰 데이터를 분석하다보면 시간은 시간대로 날려먹고 제대로된 결과를 뽑지 못할 가능성이 높다. 따라서 데이터양이(정확히는 데이터 n값과 predictor의 수의 곱이) 너무 많을 경우 PCA등의 차원축소기법을 사용해 데이터의 차원을 줄여 분석에 쓰이는 연산량을 줄이는 방법을 사용한다.</p>\n<p><strong>차원의 저주(curse of demention)</strong><br>데이터의 수보다 데이터의 차원이 더 큰 경우를 차원의 저주라고 하는데 이경우 모델링이 복잡해지고 예측력이 낮아진다. 이러면 모델링 자체가 쓸모없어지기에 차원이 너무 많은 경우 Feature Selection이나 Feature Extraction 등을 통해 사전 처리를 해줄 필요가 있다. </p>\n<h3 id=\"공분산행렬\"><a href=\"#공분산행렬\" class=\"headerlink\" title=\"공분산행렬\"></a><strong>공분산행렬</strong></h3><p>공분산행렬만 이해하면 PCA는 사실 그렇게 어렵지는 않다. PCA는 단순히 공분산행렬의 고유벡터를 기저벡터로 바꾼 것에 불과하다.일단 공분산이 뭔지 알아야 한다.<br>공분산은 교차곱편차의 평균이다. 이는 각 변수들의 편차들의 곱들을 모두 더해 n으로 나눈 값을 의미한다.</p>\n<p>데이터 매트릭스 X에 대해서 공분산행렬을 아래와 같이 나타낼 수 있다.(자유도때문에 n-1로 나눠줌)</p>\n<p>$$Cov(X) &#x3D; \\frac{1}{n-1}X^TX$$</p>\n<p>공분산은 <strong>기본적으로 변수들이 함께 변화하는 정도</strong>이다.</p>\n<p>공분산행렬은 <strong>대각성분을 설명변수의 분산으로 채우고 나머지를 공분산으로 채운 행렬이다.</strong></p>\n<p><img src=\"/covariance_matrix.png\"><br><strong>그림1. 한번에 이해하는 공분산 행렬</strong></p>\n<h3 id=\"선형변환\"><a href=\"#선형변환\" class=\"headerlink\" title=\"선형변환\"></a>선형변환</h3><ul>\n<li><strong>행렬은 선형변환을 나타낸다.</strong></li>\n<li>선형은 곱하고 더하는 것으로만 이루어져 있다.(문과식 이해의 한계)</li>\n<li>벡터 <code>[a,b]</code> 의 와 행렬 M을 곱해 벡터 <code>[c,d]</code>가 나왔을 경우 </li>\n<li><code>[c,d]</code>는 유닛 벡터 <code>[x,y]</code> 의 의 a배와 b 배의 합으로 이해할 수 있다. </li>\n<li>이처럼 선형변환은 기본적으로 행렬과 벡터의 곱의 형태로 나타낼 수잇다.</li>\n</ul>\n<p>다시 생각해 보면 다음과 같다.</p>\n<ul>\n<li><code>f</code> 라는 어떤 매핑을 사용하여 </li>\n<li>임의의 벡터 <code>[a, b]</code>에 대해서, </li>\n<li><code>[2a + b, a -2b ]</code>로 바꾼다는 것은 아래와 같이 나타낼 수 있다.</li>\n</ul>\n<p>$$\\begin{align}<br>f(\\begin{bmatrix}a \\ b \\end{bmatrix}) &#x3D; \\begin{bmatrix} 2a + b \\ a -2b \\  \\end{bmatrix}<br>\\end{align}$$</p>\n<p>여기서 <code>[2a + b, a -2b ]</code>는 <code>[c,d]</code> 이며 특정 유닛벡터 (여기서는 <code>[2+b/a, a/b-2]</code>) 에 각각 a배, b배 한 값이 된다.</p>\n<p><strong>여기서 <code>f</code>를 행렬을 곱하는 것으로 생각하면 놀랍게도 행렬과 벡터의 곱이 벡터의 선형변환과 동일하다는 것을 알 수 있다.</strong></p>\n<h3 id=\"Eigenvalue-Eigenvector\"><a href=\"#Eigenvalue-Eigenvector\" class=\"headerlink\" title=\"Eigenvalue,Eigenvector\"></a><strong>Eigenvalue,Eigenvector</strong></h3><ul>\n<li>일단 행렬이 선형변환이라는 것을 알아야 한다. <a href=\"https://angeloyeo.github.io/2019/07/15/Matrix_as_Linear_Transformation.html\">진짜 설명 오지는 레퍼런스</a>를 읽어보자.</li>\n<li>Eigenvector 는 특정 벡터에 대해 선형변환을 했을 때 크기만 바뀌고 방향은 바뀌지 않는 벡터(축)이다.(행렬의 방향성을 유지하는 선형변환의 주축)</li>\n<li>Eigenvector 는 행렬이 벡터에 작용하는 주축이다.</li>\n<li><strong>Eigenvector 는 어떤 행렬이 벡터에 작용하는 힘의 방향을 나타낸다.</strong><ul>\n<li>만약 그 행렬이 공분산 행렬일 경우 그 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지, 즉 어떤 방향으로 힘이 작용하는 지 나타낸다.-&gt; 공분산행렬의 Eigenvector가 원데이터의 분산이 최대가 되는 축이다.</li>\n</ul>\n</li>\n<li><strong>Eigenvalue는 Eigenvector의 방향으로 크기가 얼마나 변화하는 지를 나타내는 값이다.</strong></li>\n</ul>\n<!--\n$$ T \\cdot v = v' = \\lambda \\cdot v $$\n\n\n\n-->\n\n<p>$$A &#x3D; cov(v)$$</p>\n<p>$$Ax &#x3D; \\lambda x$$</p>\n<ul>\n<li>A는 행렬 v의 공분산행렬이다.</li>\n<li>여기서의 $\\lambda$가 Eigenvalue이다.</li>\n<li>x가 Eigenvector이다.</li>\n</ul>\n<p>x가 0이 아니여야 식이 성립하기 때문에 행렬식을 활용해 다음을 만족하는 벡터를 찾는다.</p>\n<p>$$(A-\\lambda I)\\vec{x}&#x3D;0$$</p>\n<p>$$det(A−\\lambda I)&#x3D;0$$</p>\n<p>$$\\begin{bmatrix} a &amp; b \\ c &amp; d \\end{bmatrix}\\begin{bmatrix} x \\ y \\end{bmatrix} &#x3D; \\begin{bmatrix} ax+by \\ cx+dy \\end{bmatrix} &#x3D; \\lambda \\begin{bmatrix} x \\ y \\end{bmatrix}<br>$$</p>\n<h3 id=\"주성분\"><a href=\"#주성분\" class=\"headerlink\" title=\"주성분\"></a><strong>주성분</strong></h3><p>결국 PCA에서 하고자 하는 것은 차원을 축소하면서 데이터 벡터를 <strong>어떤 벡터</strong>에 내적하는 것이 가능한 정보량(분산)을 유지하는 것인지 알아내는 것이다. 차원을 축소하면서 정보손실을 적게 하려면 원데이터의 분산이 최대가 되는 축을 찾아야 한다. 여기서 축이 되는 <strong>어떤 벡터</strong>가 주성분이며 공분산행렬의 eigenvector이다. </p>\n<p><strong>linear combination</strong><br>주성분은 변수들의 선형 결합으로 표현된다. 이는 어떤 주성분 PC1이 있다고 했을때 이 PC1는 마치 다중회귀식마냥 기존 변수들의 조합으로 표현된다는 것이다.</p>\n<p><img src=\"/PCA_lc.png\"><br><strong>그림2. 선형결합으로서의 주성분</strong></p>\n<h3 id=\"Feature-Selection-amp-Extraction\"><a href=\"#Feature-Selection-amp-Extraction\" class=\"headerlink\" title=\"Feature Selection &amp; Extraction\"></a><strong>Feature Selection &amp; Extraction</strong></h3><p>차원축소는 크게 변수선택과 변수추출로 나뉘며 PCA는 그 중 변수 추출에 속한다.</p>\n<ul>\n<li><strong>변수선택</strong><ul>\n<li>덜 중요한 피처를 제거하는 방식으로 차원을 축소하는 방식</li>\n<li>ex) <code>Lasso</code>,<code>Generic algorithm</code></li>\n</ul>\n</li>\n<li><strong>변수추출</strong><ul>\n<li>기존변수를 조합해서 새 변수를 만드는 방식</li>\n<li>변수간 연관성을 고려할 수있다</li>\n<li>해석이 어렵다.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/FS_FE.png\"><br><strong>그림3 변수선택과 변수추출의 직관적 이해</strong></p>\n<h3 id=\"정리\"><a href=\"#정리\" class=\"headerlink\" title=\"정리\"></a><strong>정리</strong></h3><ul>\n<li><p><strong>선형변환</strong></p>\n<ul>\n<li>행렬은 선형변환이다.</li>\n<li>즉, 임의의 $\\mathbb{R}^2$ 벡터를 다른 $\\mathbb{R}^2$ 내부의 벡터로 변환 하는 과정은 특정 $M$라는 매트릭스를 곱하는 것과 같다.</li>\n</ul>\n</li>\n<li><p><strong>공분산행렬</strong></p>\n<ul>\n<li>공분산은 교차곱편차의 평균이며 각 변수들이 얼마나 함께 변화하는 지를 나타낸다.</li>\n<li>공분산을 표준화하면 상관계수이다. </li>\n<li>공분산행렬은 각 feature들의 변동이 얼마나 유사한지를 나타낸다.</li>\n<li>공분산행렬 자체는 단순히 정사각행렬의 대각성분을 각 변수의 분산으로 채우고 나머지를 공분산으로 채운 것이다.</li>\n<li>공분산행렬의 서로다른 eigenvector 끼리는 직교한다.따라서 해당 eigenvector를 기저벡터로 하게끔 좌표축이 바뀐 데이터는 서로 상관이 없게 된다.</li>\n</ul>\n</li>\n<li><p><strong>고윳값&#x2F;고유벡터</strong></p>\n<ul>\n<li>고윳값은 벡터의 크기이다</li>\n<li>고유벡터는 특정 벡터에 선형변환을 했을 때 크기만 바뀌고 뱡향은 바뀌지않는 벡터이다. 기본적으로 방향이다.</li>\n<li>공분산행렬의 고유벡터의 열벡터가 주성분이다.</li>\n<li>고유값은 기존벡터에서 고유벡터로 정사영했을 때의 분산이다.따라서 고유벡터로 사영할 경우 분산이 최대가 된다.</li>\n</ul>\n</li>\n<li><p><strong>주성분분석</strong></p>\n<ul>\n<li>주성분은 수치형 예측변수의 선형결합(linear combination)이다.</li>\n<li>주성분은 공분산행렬의 eigenvector이다.</li>\n<li>주성분은 서로간의 상관관계가 최소화 되어 중복성이 줄어들도록 한다 </li>\n<li>차원의 수만큼 주성분이 존재하며 모든 차원의 주성분의 설명량은 1이다.</li>\n<li>주성분의 일부만으로도 전체 분산의 대부분을 설명할 수 있다.</li>\n<li>첫번째 주성분이 가장 큰 분산을 가지고 있고 두번째 주성분은 첫번째 주성분과 직교한다는 전제 하에 두번째로 큰 분산을 가지고 있다.</li>\n<li>부하(loading)는 예측변수들을 주성분으로 변동할때 쓰이는 가중치이다. 계수이다.</li>\n<li>Screeplot은 성분들의 변동을 나타낸다. 정확히는 성분의 수에 따른 분산의 변화량을 나타낸다. </li>\n<li>다음의 두 가지 경우 PCA의 적용이 어렵다<ul>\n<li>데이터의 분포가 정규성을 띄지 않는 경우 적용 어려움<ul>\n<li>이 경우 커널 PCA 사용</li>\n</ul>\n</li>\n<li>분류 &#x2F; 예측 문제에 대해서 데이터의 라벨을 고려하지 않기 때문에 효과적 분리가 어려움<ul>\n<li>이 경우 PLS 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"구현하기-R-Python\"><a href=\"#구현하기-R-Python\" class=\"headerlink\" title=\"구현하기(R, Python)\"></a>구현하기(R, Python)</h2><blockquote>\n<p>사실 이해하는게 어렵지 다른 대부분의 분석들과 마찬가지로 구현 자체는 매우 쉽다.</p>\n</blockquote>\n<h3 id=\"Eigenvalue-Eigenvector-구하기\"><a href=\"#Eigenvalue-Eigenvector-구하기\" class=\"headerlink\" title=\"Eigenvalue, Eigenvector 구하기\"></a>Eigenvalue, Eigenvector 구하기</h3><ul>\n<li>R을 활용한 Eigenvalue, Eigenvector 계산 <figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># eigen()을 사용해 매우 쉽게 구할 수 있다</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> M <span class=\"operator\">&lt;-</span> as.matrix<span class=\"punctuation\">(</span>data.frame<span class=\"punctuation\">(</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"operator\">-</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"number\">0</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"operator\">-</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"operator\">-</span><span class=\"number\">1</span><span class=\"punctuation\">)</span><span class=\"punctuation\">)</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> M</span><br><span class=\"line\">     c.1...1..0. c.1..2..1. c..2..1...1.</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>           <span class=\"number\">1</span>          <span class=\"number\">1</span>           <span class=\"operator\">-</span><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>          <span class=\"operator\">-</span><span class=\"number\">1</span>          <span class=\"number\">2</span>            <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>           <span class=\"number\">0</span>          <span class=\"number\">1</span>           <span class=\"operator\">-</span><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> eigen<span class=\"punctuation\">(</span>M<span class=\"punctuation\">)</span></span><br><span class=\"line\">eigen<span class=\"punctuation\">(</span><span class=\"punctuation\">)</span> decomposition</span><br><span class=\"line\"><span class=\"operator\">$</span>values</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>  <span class=\"number\">2</span>  <span class=\"number\">1</span> <span class=\"operator\">-</span><span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">$</span>vectors</span><br><span class=\"line\">          <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>       <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span>          <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">3</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.3015113</span> <span class=\"operator\">-</span><span class=\"number\">0.8017837</span>  <span class=\"number\">7.071068e-01</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.9045340</span> <span class=\"operator\">-</span><span class=\"number\">0.5345225</span> <span class=\"operator\">-</span><span class=\"number\">1.922963e-16</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.3015113</span> <span class=\"operator\">-</span><span class=\"number\">0.2672612</span>  <span class=\"number\">7.071068e-01</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"공분산행렬-구하기\"><a href=\"#공분산행렬-구하기\" class=\"headerlink\" title=\"공분산행렬 구하기\"></a>공분산행렬 구하기</h3></li>\n<li>R을 활용한 Eigenvalue, Eigenvector 계산 <figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">```</span><br><span class=\"line\">- R을 활용한 Eigenvalue, Eigenvector 계산 </span><br><span class=\"line\">```python</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"PCA구현하기\"><a href=\"#PCA구현하기\" class=\"headerlink\" title=\"PCA구현하기\"></a>PCA구현하기</h3><ul>\n<li><p>R을 활용한 pca</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df <span class=\"operator\">&lt;-</span> data<span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"string\">&#x27;a&#x27;</span><span class=\"punctuation\">,</span><span class=\"string\">&#x27;b&#x27;</span><span class=\"punctuation\">)</span><span class=\"punctuation\">]</span></span><br><span class=\"line\">pca<span class=\"operator\">&lt;-</span>princomp<span class=\"punctuation\">(</span>df<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">pca<span class=\"operator\">$</span>loadings <span class=\"comment\"># 주성분 부하량 확인</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>python을 활용한 pca</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>) <span class=\"comment\"># n_components로 주성분의 수를 지정할 수 있다.</span></span><br><span class=\"line\">pca_values = pca.fit_transform(data_use) <span class=\"comment\"># 설명량</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;누적설명량 &#123;0&#125;&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"built_in\">sum</span>(pca.explained_variance_ratio_)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;각 차원의 설명량 &#123;0&#125;&#x27;</span>.<span class=\"built_in\">format</span>(pca.explained_variance_ratio_))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"PCA-시각화-관련-참고\"><a href=\"#PCA-시각화-관련-참고\" class=\"headerlink\" title=\"PCA 시각화 관련 참고\"></a>PCA 시각화 관련 참고</h3><ul>\n<li>seaborn을 활용한 시각화<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># seaborn을 활용한 시각화</span></span><br><span class=\"line\"></span><br><span class=\"line\">sns.scatterplot(pca_v[<span class=\"number\">0</span>],pca_v[<span class=\"number\">1</span>],data=pca_v,hue = <span class=\"string\">&#x27;target&#x27;</span>,</span><br><span class=\"line\">style = <span class=\"string\">&#x27;target&#x27;</span>,</span><br><span class=\"line\">s = <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>matplotlib을 활용한 시각화(1)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> plotly.express <span class=\"keyword\">as</span> px</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\">df = px.data.iris()</span><br><span class=\"line\">features = [<span class=\"string\">&quot;sepal_width&quot;</span>, <span class=\"string\">&quot;sepal_length&quot;</span>, <span class=\"string\">&quot;petal_width&quot;</span>, <span class=\"string\">&quot;petal_length&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">pca = PCA()</span><br><span class=\"line\">components = pca.fit_transform(df[features])</span><br><span class=\"line\">labels = &#123;</span><br><span class=\"line\">    <span class=\"built_in\">str</span>(i): <span class=\"string\">f&quot;PC <span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span> (<span class=\"subst\">&#123;var:<span class=\"number\">.1</span>f&#125;</span>%)&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(pca.explained_variance_ratio_ * <span class=\"number\">100</span>)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">fig = px.scatter_matrix(</span><br><span class=\"line\">    components,</span><br><span class=\"line\">    labels=labels,</span><br><span class=\"line\">    dimensions=<span class=\"built_in\">range</span>(<span class=\"number\">4</span>),</span><br><span class=\"line\">    color=df[<span class=\"string\">&quot;species&quot;</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">fig.update_traces(diagonal_visible=<span class=\"literal\">False</span>)</span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure></li>\n<li>matplotlib을 활용한 시각화(2)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig = plt.figure(figsize = (<span class=\"number\">8</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">ax = fig.add_subplot(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>) </span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>, fontsize = <span class=\"number\">15</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>, fontsize = <span class=\"number\">15</span>)</span><br><span class=\"line\">ax.set_title(<span class=\"string\">&#x27;2 component PCA&#x27;</span>, fontsize = <span class=\"number\">20</span>)</span><br><span class=\"line\">targets = [<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>]</span><br><span class=\"line\">colors = [<span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> target, color <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(targets,colors):</span><br><span class=\"line\">    indicesToKeep = finalDf[<span class=\"string\">&#x27;target&#x27;</span>] == target</span><br><span class=\"line\">    ax.scatter(finalDf.loc[indicesToKeep, <span class=\"string\">&#x27;principal component 1&#x27;</span>]</span><br><span class=\"line\">               , finalDf.loc[indicesToKeep, <span class=\"string\">&#x27;principal component 2&#x27;</span>]</span><br><span class=\"line\">               , c = color</span><br><span class=\"line\">               , s = <span class=\"number\">50</span>)</span><br><span class=\"line\">ax.legend(targets)</span><br><span class=\"line\">ax.grid()</span><br></pre></td></tr></table></figure>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2></li>\n<li>책<ul>\n<li>Practical Statistics for Data Science</li>\n</ul>\n</li>\n<li>블로그<ul>\n<li><a href=\"https://wiserloner.tistory.com/1297\">https://wiserloner.tistory.com/1297</a> # 고유값과 고유벡터</li>\n<li><a href=\"https://huidea.tistory.com/126\">https://huidea.tistory.com/126</a> #기술면접 참고</li>\n<li><a href=\"https://rfriend.tistory.com/380\">https://rfriend.tistory.com/380</a> #python 선형대수 함수</li>\n<li><a href=\"https://rfriend.tistory.com/181\">https://rfriend.tistory.com/181</a> #고유값과 고유벡터 구현</li>\n<li><a href=\"https://youtu.be/jNwf-JUGWgg\">https://youtu.be/jNwf-JUGWgg</a> # 공분산행렬의 이해</li>\n<li><a href=\"https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/\">https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/</a> # 행렬연산과 공분산행렬</li>\n<li>시각화<ul>\n<li><a href=\"https://plotly.com/python/pca-visualization/\">https://plotly.com/python/pca-visualization/</a></li>\n<li><a href=\"https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\">https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li><strong>비지도학습은 라벨이 달린 데이터를 이용해 데이터를 학습하는 과정 없이 데이터로부터 의미를 추출하는 것이다.</strong></li>\n<li><strong>비지도학습의 목적은 크게 차원축소와 클러스터링 두 가지로 나눌 수 있다.</strong></li>\n</ul>\n<hr>\n<ul>\n<li><strong>비지도학습</strong><ul>\n<li><strong>차원축소(PCA)</strong> : 데이터의 변수를 관리 가능한 수준으로 차원을 줄이는 것, 변수와 레코드의 수가 아주 큰 상황이라면 차원축소를을 EDA의 연장으로 볼 수있다. <ul>\n<li>PCA</li>\n<li>LDA</li>\n<li>SVD</li>\n<li>요인분석</li>\n<li>etc</li>\n</ul>\n</li>\n<li><strong>클러스터링</strong>: 라벨이 정해진 응답변수가 없는 상황에서 예측 규칙을 만드는 것(그룹화)<ul>\n<li>K평균 클러스터링</li>\n<li>계층적 클러스터링</li>\n<li>모델기반 클러스터링</li>\n<li>etc</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"주성분분석-PCA\"><a href=\"#주성분분석-PCA\" class=\"headerlink\" title=\"주성분분석(PCA)\"></a>주성분분석(PCA)</h2><blockquote>\n<p><strong>주성분분석은 데이터의 차원을 줄이기 위해, 공분산 행렬에서 고유 벡터&#x2F;고유값을 구하고 가장 분산이 큰 방향을 가진 고유벡터에 입력데이터를 선형변환하는 것이다.</strong></p>\n</blockquote>\n<p>위 말을 이해하기 위해서는 차원축소를 하는 이유와 선형대수 관련 개념을 조금 알아야 한다.</p>\n<h3 id=\"차원을-줄이는-이유\"><a href=\"#차원을-줄이는-이유\" class=\"headerlink\" title=\"차원을 줄이는 이유\"></a>차원을 줄이는 이유</h3><p>차원을 줄이는 이유는 크게 두 가지가 있는데 하나는 메모리 소모를 줄이기 위해서고 다른 하나는 차원의 저주 때문이다.</p>\n<p><strong>메모리 소모 문제</strong><br>R을 생각해보자. R을 써본 사람은 알겠지만 정말 개같이 느린 언어이다. 이는 R이 물리적 메모리를 저장하는 방식을 사용하기 때문이다.물론 파이썬도 그렇게 빠른 언어는 아니다. 넘파이가 빠른거지 파이썬이 빠른게 아니다. 그리고 넘파이는 C랑 포트란으로 짜여졌다.<br>이처럼 수치해석을 하기에는 너무나도 느린 언어들로 큰 데이터를 분석하다보면 시간은 시간대로 날려먹고 제대로된 결과를 뽑지 못할 가능성이 높다. 따라서 데이터양이(정확히는 데이터 n값과 predictor의 수의 곱이) 너무 많을 경우 PCA등의 차원축소기법을 사용해 데이터의 차원을 줄여 분석에 쓰이는 연산량을 줄이는 방법을 사용한다.</p>\n<p><strong>차원의 저주(curse of demention)</strong><br>데이터의 수보다 데이터의 차원이 더 큰 경우를 차원의 저주라고 하는데 이경우 모델링이 복잡해지고 예측력이 낮아진다. 이러면 모델링 자체가 쓸모없어지기에 차원이 너무 많은 경우 Feature Selection이나 Feature Extraction 등을 통해 사전 처리를 해줄 필요가 있다. </p>\n<h3 id=\"공분산행렬\"><a href=\"#공분산행렬\" class=\"headerlink\" title=\"공분산행렬\"></a><strong>공분산행렬</strong></h3><p>공분산행렬만 이해하면 PCA는 사실 그렇게 어렵지는 않다. PCA는 단순히 공분산행렬의 고유벡터를 기저벡터로 바꾼 것에 불과하다.일단 공분산이 뭔지 알아야 한다.<br>공분산은 교차곱편차의 평균이다. 이는 각 변수들의 편차들의 곱들을 모두 더해 n으로 나눈 값을 의미한다.</p>\n<p>데이터 매트릭스 X에 대해서 공분산행렬을 아래와 같이 나타낼 수 있다.(자유도때문에 n-1로 나눠줌)</p>\n<p>$$Cov(X) &#x3D; \\frac{1}{n-1}X^TX$$</p>\n<p>공분산은 <strong>기본적으로 변수들이 함께 변화하는 정도</strong>이다.</p>\n<p>공분산행렬은 <strong>대각성분을 설명변수의 분산으로 채우고 나머지를 공분산으로 채운 행렬이다.</strong></p>\n<p><img src=\"/covariance_matrix.png\"><br><strong>그림1. 한번에 이해하는 공분산 행렬</strong></p>\n<h3 id=\"선형변환\"><a href=\"#선형변환\" class=\"headerlink\" title=\"선형변환\"></a>선형변환</h3><ul>\n<li><strong>행렬은 선형변환을 나타낸다.</strong></li>\n<li>선형은 곱하고 더하는 것으로만 이루어져 있다.(문과식 이해의 한계)</li>\n<li>벡터 <code>[a,b]</code> 의 와 행렬 M을 곱해 벡터 <code>[c,d]</code>가 나왔을 경우 </li>\n<li><code>[c,d]</code>는 유닛 벡터 <code>[x,y]</code> 의 의 a배와 b 배의 합으로 이해할 수 있다. </li>\n<li>이처럼 선형변환은 기본적으로 행렬과 벡터의 곱의 형태로 나타낼 수잇다.</li>\n</ul>\n<p>다시 생각해 보면 다음과 같다.</p>\n<ul>\n<li><code>f</code> 라는 어떤 매핑을 사용하여 </li>\n<li>임의의 벡터 <code>[a, b]</code>에 대해서, </li>\n<li><code>[2a + b, a -2b ]</code>로 바꾼다는 것은 아래와 같이 나타낼 수 있다.</li>\n</ul>\n<p>$$\\begin{align}<br>f(\\begin{bmatrix}a \\ b \\end{bmatrix}) &#x3D; \\begin{bmatrix} 2a + b \\ a -2b \\  \\end{bmatrix}<br>\\end{align}$$</p>\n<p>여기서 <code>[2a + b, a -2b ]</code>는 <code>[c,d]</code> 이며 특정 유닛벡터 (여기서는 <code>[2+b/a, a/b-2]</code>) 에 각각 a배, b배 한 값이 된다.</p>\n<p><strong>여기서 <code>f</code>를 행렬을 곱하는 것으로 생각하면 놀랍게도 행렬과 벡터의 곱이 벡터의 선형변환과 동일하다는 것을 알 수 있다.</strong></p>\n<h3 id=\"Eigenvalue-Eigenvector\"><a href=\"#Eigenvalue-Eigenvector\" class=\"headerlink\" title=\"Eigenvalue,Eigenvector\"></a><strong>Eigenvalue,Eigenvector</strong></h3><ul>\n<li>일단 행렬이 선형변환이라는 것을 알아야 한다. <a href=\"https://angeloyeo.github.io/2019/07/15/Matrix_as_Linear_Transformation.html\">진짜 설명 오지는 레퍼런스</a>를 읽어보자.</li>\n<li>Eigenvector 는 특정 벡터에 대해 선형변환을 했을 때 크기만 바뀌고 방향은 바뀌지 않는 벡터(축)이다.(행렬의 방향성을 유지하는 선형변환의 주축)</li>\n<li>Eigenvector 는 행렬이 벡터에 작용하는 주축이다.</li>\n<li><strong>Eigenvector 는 어떤 행렬이 벡터에 작용하는 힘의 방향을 나타낸다.</strong><ul>\n<li>만약 그 행렬이 공분산 행렬일 경우 그 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지, 즉 어떤 방향으로 힘이 작용하는 지 나타낸다.-&gt; 공분산행렬의 Eigenvector가 원데이터의 분산이 최대가 되는 축이다.</li>\n</ul>\n</li>\n<li><strong>Eigenvalue는 Eigenvector의 방향으로 크기가 얼마나 변화하는 지를 나타내는 값이다.</strong></li>\n</ul>\n<!--\n$$ T \\cdot v = v' = \\lambda \\cdot v $$\n\n\n\n-->\n\n<p>$$A &#x3D; cov(v)$$</p>\n<p>$$Ax &#x3D; \\lambda x$$</p>\n<ul>\n<li>A는 행렬 v의 공분산행렬이다.</li>\n<li>여기서의 $\\lambda$가 Eigenvalue이다.</li>\n<li>x가 Eigenvector이다.</li>\n</ul>\n<p>x가 0이 아니여야 식이 성립하기 때문에 행렬식을 활용해 다음을 만족하는 벡터를 찾는다.</p>\n<p>$$(A-\\lambda I)\\vec{x}&#x3D;0$$</p>\n<p>$$det(A−\\lambda I)&#x3D;0$$</p>\n<p>$$\\begin{bmatrix} a &amp; b \\ c &amp; d \\end{bmatrix}\\begin{bmatrix} x \\ y \\end{bmatrix} &#x3D; \\begin{bmatrix} ax+by \\ cx+dy \\end{bmatrix} &#x3D; \\lambda \\begin{bmatrix} x \\ y \\end{bmatrix}<br>$$</p>\n<h3 id=\"주성분\"><a href=\"#주성분\" class=\"headerlink\" title=\"주성분\"></a><strong>주성분</strong></h3><p>결국 PCA에서 하고자 하는 것은 차원을 축소하면서 데이터 벡터를 <strong>어떤 벡터</strong>에 내적하는 것이 가능한 정보량(분산)을 유지하는 것인지 알아내는 것이다. 차원을 축소하면서 정보손실을 적게 하려면 원데이터의 분산이 최대가 되는 축을 찾아야 한다. 여기서 축이 되는 <strong>어떤 벡터</strong>가 주성분이며 공분산행렬의 eigenvector이다. </p>\n<p><strong>linear combination</strong><br>주성분은 변수들의 선형 결합으로 표현된다. 이는 어떤 주성분 PC1이 있다고 했을때 이 PC1는 마치 다중회귀식마냥 기존 변수들의 조합으로 표현된다는 것이다.</p>\n<p><img src=\"/PCA_lc.png\"><br><strong>그림2. 선형결합으로서의 주성분</strong></p>\n<h3 id=\"Feature-Selection-amp-Extraction\"><a href=\"#Feature-Selection-amp-Extraction\" class=\"headerlink\" title=\"Feature Selection &amp; Extraction\"></a><strong>Feature Selection &amp; Extraction</strong></h3><p>차원축소는 크게 변수선택과 변수추출로 나뉘며 PCA는 그 중 변수 추출에 속한다.</p>\n<ul>\n<li><strong>변수선택</strong><ul>\n<li>덜 중요한 피처를 제거하는 방식으로 차원을 축소하는 방식</li>\n<li>ex) <code>Lasso</code>,<code>Generic algorithm</code></li>\n</ul>\n</li>\n<li><strong>변수추출</strong><ul>\n<li>기존변수를 조합해서 새 변수를 만드는 방식</li>\n<li>변수간 연관성을 고려할 수있다</li>\n<li>해석이 어렵다.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/FS_FE.png\"><br><strong>그림3 변수선택과 변수추출의 직관적 이해</strong></p>\n<h3 id=\"정리\"><a href=\"#정리\" class=\"headerlink\" title=\"정리\"></a><strong>정리</strong></h3><ul>\n<li><p><strong>선형변환</strong></p>\n<ul>\n<li>행렬은 선형변환이다.</li>\n<li>즉, 임의의 $\\mathbb{R}^2$ 벡터를 다른 $\\mathbb{R}^2$ 내부의 벡터로 변환 하는 과정은 특정 $M$라는 매트릭스를 곱하는 것과 같다.</li>\n</ul>\n</li>\n<li><p><strong>공분산행렬</strong></p>\n<ul>\n<li>공분산은 교차곱편차의 평균이며 각 변수들이 얼마나 함께 변화하는 지를 나타낸다.</li>\n<li>공분산을 표준화하면 상관계수이다. </li>\n<li>공분산행렬은 각 feature들의 변동이 얼마나 유사한지를 나타낸다.</li>\n<li>공분산행렬 자체는 단순히 정사각행렬의 대각성분을 각 변수의 분산으로 채우고 나머지를 공분산으로 채운 것이다.</li>\n<li>공분산행렬의 서로다른 eigenvector 끼리는 직교한다.따라서 해당 eigenvector를 기저벡터로 하게끔 좌표축이 바뀐 데이터는 서로 상관이 없게 된다.</li>\n</ul>\n</li>\n<li><p><strong>고윳값&#x2F;고유벡터</strong></p>\n<ul>\n<li>고윳값은 벡터의 크기이다</li>\n<li>고유벡터는 특정 벡터에 선형변환을 했을 때 크기만 바뀌고 뱡향은 바뀌지않는 벡터이다. 기본적으로 방향이다.</li>\n<li>공분산행렬의 고유벡터의 열벡터가 주성분이다.</li>\n<li>고유값은 기존벡터에서 고유벡터로 정사영했을 때의 분산이다.따라서 고유벡터로 사영할 경우 분산이 최대가 된다.</li>\n</ul>\n</li>\n<li><p><strong>주성분분석</strong></p>\n<ul>\n<li>주성분은 수치형 예측변수의 선형결합(linear combination)이다.</li>\n<li>주성분은 공분산행렬의 eigenvector이다.</li>\n<li>주성분은 서로간의 상관관계가 최소화 되어 중복성이 줄어들도록 한다 </li>\n<li>차원의 수만큼 주성분이 존재하며 모든 차원의 주성분의 설명량은 1이다.</li>\n<li>주성분의 일부만으로도 전체 분산의 대부분을 설명할 수 있다.</li>\n<li>첫번째 주성분이 가장 큰 분산을 가지고 있고 두번째 주성분은 첫번째 주성분과 직교한다는 전제 하에 두번째로 큰 분산을 가지고 있다.</li>\n<li>부하(loading)는 예측변수들을 주성분으로 변동할때 쓰이는 가중치이다. 계수이다.</li>\n<li>Screeplot은 성분들의 변동을 나타낸다. 정확히는 성분의 수에 따른 분산의 변화량을 나타낸다. </li>\n<li>다음의 두 가지 경우 PCA의 적용이 어렵다<ul>\n<li>데이터의 분포가 정규성을 띄지 않는 경우 적용 어려움<ul>\n<li>이 경우 커널 PCA 사용</li>\n</ul>\n</li>\n<li>분류 &#x2F; 예측 문제에 대해서 데이터의 라벨을 고려하지 않기 때문에 효과적 분리가 어려움<ul>\n<li>이 경우 PLS 사용</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"구현하기-R-Python\"><a href=\"#구현하기-R-Python\" class=\"headerlink\" title=\"구현하기(R, Python)\"></a>구현하기(R, Python)</h2><blockquote>\n<p>사실 이해하는게 어렵지 다른 대부분의 분석들과 마찬가지로 구현 자체는 매우 쉽다.</p>\n</blockquote>\n<h3 id=\"Eigenvalue-Eigenvector-구하기\"><a href=\"#Eigenvalue-Eigenvector-구하기\" class=\"headerlink\" title=\"Eigenvalue, Eigenvector 구하기\"></a>Eigenvalue, Eigenvector 구하기</h3><ul>\n<li>R을 활용한 Eigenvalue, Eigenvector 계산 <figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># eigen()을 사용해 매우 쉽게 구할 수 있다</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> M <span class=\"operator\">&lt;-</span> as.matrix<span class=\"punctuation\">(</span>data.frame<span class=\"punctuation\">(</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"operator\">-</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"number\">0</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"operator\">-</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"operator\">-</span><span class=\"number\">1</span><span class=\"punctuation\">)</span><span class=\"punctuation\">)</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> M</span><br><span class=\"line\">     c.1...1..0. c.1..2..1. c..2..1...1.</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>           <span class=\"number\">1</span>          <span class=\"number\">1</span>           <span class=\"operator\">-</span><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>          <span class=\"operator\">-</span><span class=\"number\">1</span>          <span class=\"number\">2</span>            <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>           <span class=\"number\">0</span>          <span class=\"number\">1</span>           <span class=\"operator\">-</span><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> eigen<span class=\"punctuation\">(</span>M<span class=\"punctuation\">)</span></span><br><span class=\"line\">eigen<span class=\"punctuation\">(</span><span class=\"punctuation\">)</span> decomposition</span><br><span class=\"line\"><span class=\"operator\">$</span>values</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>  <span class=\"number\">2</span>  <span class=\"number\">1</span> <span class=\"operator\">-</span><span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">$</span>vectors</span><br><span class=\"line\">          <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>       <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span>          <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">3</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.3015113</span> <span class=\"operator\">-</span><span class=\"number\">0.8017837</span>  <span class=\"number\">7.071068e-01</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.9045340</span> <span class=\"operator\">-</span><span class=\"number\">0.5345225</span> <span class=\"operator\">-</span><span class=\"number\">1.922963e-16</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span> <span class=\"number\">0.3015113</span> <span class=\"operator\">-</span><span class=\"number\">0.2672612</span>  <span class=\"number\">7.071068e-01</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"공분산행렬-구하기\"><a href=\"#공분산행렬-구하기\" class=\"headerlink\" title=\"공분산행렬 구하기\"></a>공분산행렬 구하기</h3></li>\n<li>R을 활용한 Eigenvalue, Eigenvector 계산 <figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">```</span><br><span class=\"line\">- R을 활용한 Eigenvalue, Eigenvector 계산 </span><br><span class=\"line\">```python</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"PCA구현하기\"><a href=\"#PCA구현하기\" class=\"headerlink\" title=\"PCA구현하기\"></a>PCA구현하기</h3><ul>\n<li><p>R을 활용한 pca</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df <span class=\"operator\">&lt;-</span> data<span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"built_in\">c</span><span class=\"punctuation\">(</span><span class=\"string\">&#x27;a&#x27;</span><span class=\"punctuation\">,</span><span class=\"string\">&#x27;b&#x27;</span><span class=\"punctuation\">)</span><span class=\"punctuation\">]</span></span><br><span class=\"line\">pca<span class=\"operator\">&lt;-</span>princomp<span class=\"punctuation\">(</span>df<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">pca<span class=\"operator\">$</span>loadings <span class=\"comment\"># 주성분 부하량 확인</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>python을 활용한 pca</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>) <span class=\"comment\"># n_components로 주성분의 수를 지정할 수 있다.</span></span><br><span class=\"line\">pca_values = pca.fit_transform(data_use) <span class=\"comment\"># 설명량</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;누적설명량 &#123;0&#125;&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"built_in\">sum</span>(pca.explained_variance_ratio_)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;각 차원의 설명량 &#123;0&#125;&#x27;</span>.<span class=\"built_in\">format</span>(pca.explained_variance_ratio_))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"PCA-시각화-관련-참고\"><a href=\"#PCA-시각화-관련-참고\" class=\"headerlink\" title=\"PCA 시각화 관련 참고\"></a>PCA 시각화 관련 참고</h3><ul>\n<li>seaborn을 활용한 시각화<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># seaborn을 활용한 시각화</span></span><br><span class=\"line\"></span><br><span class=\"line\">sns.scatterplot(pca_v[<span class=\"number\">0</span>],pca_v[<span class=\"number\">1</span>],data=pca_v,hue = <span class=\"string\">&#x27;target&#x27;</span>,</span><br><span class=\"line\">style = <span class=\"string\">&#x27;target&#x27;</span>,</span><br><span class=\"line\">s = <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>matplotlib을 활용한 시각화(1)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> plotly.express <span class=\"keyword\">as</span> px</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\">df = px.data.iris()</span><br><span class=\"line\">features = [<span class=\"string\">&quot;sepal_width&quot;</span>, <span class=\"string\">&quot;sepal_length&quot;</span>, <span class=\"string\">&quot;petal_width&quot;</span>, <span class=\"string\">&quot;petal_length&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">pca = PCA()</span><br><span class=\"line\">components = pca.fit_transform(df[features])</span><br><span class=\"line\">labels = &#123;</span><br><span class=\"line\">    <span class=\"built_in\">str</span>(i): <span class=\"string\">f&quot;PC <span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span> (<span class=\"subst\">&#123;var:<span class=\"number\">.1</span>f&#125;</span>%)&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(pca.explained_variance_ratio_ * <span class=\"number\">100</span>)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">fig = px.scatter_matrix(</span><br><span class=\"line\">    components,</span><br><span class=\"line\">    labels=labels,</span><br><span class=\"line\">    dimensions=<span class=\"built_in\">range</span>(<span class=\"number\">4</span>),</span><br><span class=\"line\">    color=df[<span class=\"string\">&quot;species&quot;</span>]</span><br><span class=\"line\">)</span><br><span class=\"line\">fig.update_traces(diagonal_visible=<span class=\"literal\">False</span>)</span><br><span class=\"line\">fig.show()</span><br></pre></td></tr></table></figure></li>\n<li>matplotlib을 활용한 시각화(2)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig = plt.figure(figsize = (<span class=\"number\">8</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">ax = fig.add_subplot(<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>) </span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>, fontsize = <span class=\"number\">15</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>, fontsize = <span class=\"number\">15</span>)</span><br><span class=\"line\">ax.set_title(<span class=\"string\">&#x27;2 component PCA&#x27;</span>, fontsize = <span class=\"number\">20</span>)</span><br><span class=\"line\">targets = [<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>]</span><br><span class=\"line\">colors = [<span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> target, color <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(targets,colors):</span><br><span class=\"line\">    indicesToKeep = finalDf[<span class=\"string\">&#x27;target&#x27;</span>] == target</span><br><span class=\"line\">    ax.scatter(finalDf.loc[indicesToKeep, <span class=\"string\">&#x27;principal component 1&#x27;</span>]</span><br><span class=\"line\">               , finalDf.loc[indicesToKeep, <span class=\"string\">&#x27;principal component 2&#x27;</span>]</span><br><span class=\"line\">               , c = color</span><br><span class=\"line\">               , s = <span class=\"number\">50</span>)</span><br><span class=\"line\">ax.legend(targets)</span><br><span class=\"line\">ax.grid()</span><br></pre></td></tr></table></figure>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2></li>\n<li>책<ul>\n<li>Practical Statistics for Data Science</li>\n</ul>\n</li>\n<li>블로그<ul>\n<li><a href=\"https://wiserloner.tistory.com/1297\">https://wiserloner.tistory.com/1297</a> # 고유값과 고유벡터</li>\n<li><a href=\"https://huidea.tistory.com/126\">https://huidea.tistory.com/126</a> #기술면접 참고</li>\n<li><a href=\"https://rfriend.tistory.com/380\">https://rfriend.tistory.com/380</a> #python 선형대수 함수</li>\n<li><a href=\"https://rfriend.tistory.com/181\">https://rfriend.tistory.com/181</a> #고유값과 고유벡터 구현</li>\n<li><a href=\"https://youtu.be/jNwf-JUGWgg\">https://youtu.be/jNwf-JUGWgg</a> # 공분산행렬의 이해</li>\n<li><a href=\"https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/\">https://ratsgo.github.io/linear%20algebra/2017/03/14/operations/</a> # 행렬연산과 공분산행렬</li>\n<li>시각화<ul>\n<li><a href=\"https://plotly.com/python/pca-visualization/\">https://plotly.com/python/pca-visualization/</a></li>\n<li><a href=\"https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\">https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"/covariance_matrix.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Unsupervised Learning]주성분분석(PCA)의 이해","path":"2021/05/29/ML-US-PCA/","eyeCatchImage":"/covariance_matrix.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-05-29T07:59:23.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-05-29T07:59:23.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning > PCA","tags":["Unsupervised Learning","PCA"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Unsupervised Learning]KNN을 활용한 분류","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n---\n\n## 간단한 컨셉\n\n**KNN**\n\n- 새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.\n- 회귀문제와 분류문제 해결에 모두 사용되는 지도학습\n- 하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.\n- **K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).**\n- KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.\n- 거리측정방법\n    + 유클리디안 거리 : 데이터포인트 사이 직선 최단거리\n    + 마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.\n    + 맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.\n- 주의점\n    + 기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.\n    + 불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.\n\n- 장단점\n    +   장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 \n    +   단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림 \n\n\n## 구현\n\n- 유클라디안 거리를 활용한 KNN 구현\n\n```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef euclidean_distance(x1,x2):\n    return np.sqrt(np.sum((x1-x2)**2))\n\nclass KNN:\n\n    self __init__(self, k=3):\n        self.k = k \n\n    def fit(self, X, y): # triain sample and label\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        predicted_labels = [self._predict(x) for x in X]\n        return np.array\n\n\n    def _predict(self,x):\n        \"\"\"\n        1. 거리 계산하기\n\n        2. k nearest sample\n\n        3. majority vote, get most common class\n\n        \"\"\"\n\n        distances = [euclidean_distance(x,x_train) for x_train in X_train]\n\n        k_indices = np.argsort(distances)[:self.k]\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n\n        most_common = Counter(k_nearest_labels).most_common(1)\n\n        return most_common[0][0]\n\n```\n\n\n## 분류문제 풀이\n\n- iris 데이터를 바탕으로 분류문제 풀이\n\n```python\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\ncmap = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return accuracy\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1234\n    )\n\nk = 3\nclf = KNN(k=k)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(\"KNN classification 정확도\", accuracy(y_test, predictions))\n\n```\n\n\n```bash\n$KNN classification accuracy 1.0\n\n```\n\n- sklearn에서도 knn 분류기가 구현되어 있다.\n    + irsis data load까지는 동일하게 진행된다.\n\n```python\nfrom sklearn.neighbors import KNeiborsClassifier\n\nclf = KNeiborsClassifier(n_neighbors =3)\nclf.fit()\n\npred = clf.predict(X_test)\nprint(\"KNN classification 정확도\", clf.score(X_test,y_test))\n\n\n```\n\n\n## K값과 모델 복잡도의 관계\n\n- 위스콘신 유방암데이터로 구현한다.\n- k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.\n- k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.\n- k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.\n- 중간정도의 범위에서 k의 수를 선정할 필요가 있다.\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nimport matplotlib.pyplot as plt\n\ncancer = load_breast_cancer()\nX_train , X_test , y_train , y_test = train_test_split(cancer.data,\n                                                       cancer.target,\n                                                       stratify = cancer.target, # stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)\n                                                       random_state=42)\n\ntrain_acc = []\ntest_acc = []\n\n\nk_indices = range(1,11)\n\nfor k in k_indices:\n    clf = KNeiborsClassifier(n_neighbors=k)\n    clf.fit()\n    train_acc.append(clf.score(X_train,y_train))\n    test_acc.append(clf.score(X_test,y_test))\n\nplt.plot(neighbors_settings, training_accuracy, label=\"훈련 정확도\")\nplt.plot(neighbors_settings, test_accuracy, label=\"테스트 정확도\")\nplt.ylabel(\"정확도\")\nplt.xlabel(\"n_neighbors\")\nplt.legend(\n\n```\n\n\n![](https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024)\n\n## References\n\n- https://docs.python.org/3/library/collections.html\n- [파이싼 라이브러리를 활용한 머신러닝](https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/)\n- https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/","source":"_posts/ML-US-knn.md","raw":"---\ntitle: '[Unsupervised Learning]KNN을 활용한 분류'\ncategories:\n    - [Machine Learning]\ntags:\n  - KNN\n  - Unsupervised Learning\ndate:\nupdated:\n---\n\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n---\n\n## 간단한 컨셉\n\n**KNN**\n\n- 새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.\n- 회귀문제와 분류문제 해결에 모두 사용되는 지도학습\n- 하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.\n- **K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).**\n- KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.\n- 거리측정방법\n    + 유클리디안 거리 : 데이터포인트 사이 직선 최단거리\n    + 마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.\n    + 맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.\n- 주의점\n    + 기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.\n    + 불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.\n\n- 장단점\n    +   장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 \n    +   단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림 \n\n\n## 구현\n\n- 유클라디안 거리를 활용한 KNN 구현\n\n```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef euclidean_distance(x1,x2):\n    return np.sqrt(np.sum((x1-x2)**2))\n\nclass KNN:\n\n    self __init__(self, k=3):\n        self.k = k \n\n    def fit(self, X, y): # triain sample and label\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        predicted_labels = [self._predict(x) for x in X]\n        return np.array\n\n\n    def _predict(self,x):\n        \"\"\"\n        1. 거리 계산하기\n\n        2. k nearest sample\n\n        3. majority vote, get most common class\n\n        \"\"\"\n\n        distances = [euclidean_distance(x,x_train) for x_train in X_train]\n\n        k_indices = np.argsort(distances)[:self.k]\n        k_nearest_labels = [self.y_train[i] for i in k_indices]\n\n        most_common = Counter(k_nearest_labels).most_common(1)\n\n        return most_common[0][0]\n\n```\n\n\n## 분류문제 풀이\n\n- iris 데이터를 바탕으로 분류문제 풀이\n\n```python\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\ncmap = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return accuracy\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1234\n    )\n\nk = 3\nclf = KNN(k=k)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(\"KNN classification 정확도\", accuracy(y_test, predictions))\n\n```\n\n\n```bash\n$KNN classification accuracy 1.0\n\n```\n\n- sklearn에서도 knn 분류기가 구현되어 있다.\n    + irsis data load까지는 동일하게 진행된다.\n\n```python\nfrom sklearn.neighbors import KNeiborsClassifier\n\nclf = KNeiborsClassifier(n_neighbors =3)\nclf.fit()\n\npred = clf.predict(X_test)\nprint(\"KNN classification 정확도\", clf.score(X_test,y_test))\n\n\n```\n\n\n## K값과 모델 복잡도의 관계\n\n- 위스콘신 유방암데이터로 구현한다.\n- k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.\n- k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.\n- k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.\n- 중간정도의 범위에서 k의 수를 선정할 필요가 있다.\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nimport matplotlib.pyplot as plt\n\ncancer = load_breast_cancer()\nX_train , X_test , y_train , y_test = train_test_split(cancer.data,\n                                                       cancer.target,\n                                                       stratify = cancer.target, # stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)\n                                                       random_state=42)\n\ntrain_acc = []\ntest_acc = []\n\n\nk_indices = range(1,11)\n\nfor k in k_indices:\n    clf = KNeiborsClassifier(n_neighbors=k)\n    clf.fit()\n    train_acc.append(clf.score(X_train,y_train))\n    test_acc.append(clf.score(X_test,y_test))\n\nplt.plot(neighbors_settings, training_accuracy, label=\"훈련 정확도\")\nplt.plot(neighbors_settings, test_accuracy, label=\"테스트 정확도\")\nplt.ylabel(\"정확도\")\nplt.xlabel(\"n_neighbors\")\nplt.legend(\n\n```\n\n\n![](https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024)\n\n## References\n\n- https://docs.python.org/3/library/collections.html\n- [파이싼 라이브러리를 활용한 머신러닝](https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/)\n- https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/","slug":"ML-US-knn","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscb001ob36qds3ygnel","content":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<hr>\n<h2 id=\"간단한-컨셉\"><a href=\"#간단한-컨셉\" class=\"headerlink\" title=\"간단한 컨셉\"></a>간단한 컨셉</h2><p><strong>KNN</strong></p>\n<ul>\n<li><p>새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.</p>\n</li>\n<li><p>회귀문제와 분류문제 해결에 모두 사용되는 지도학습</p>\n</li>\n<li><p>하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.</p>\n</li>\n<li><p><strong>K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).</strong></p>\n</li>\n<li><p>KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.</p>\n</li>\n<li><p>거리측정방법</p>\n<ul>\n<li>유클리디안 거리 : 데이터포인트 사이 직선 최단거리</li>\n<li>마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.</li>\n<li>맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.</li>\n</ul>\n</li>\n<li><p>주의점</p>\n<ul>\n<li>기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.</li>\n<li>불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.</li>\n</ul>\n</li>\n<li><p>장단점</p>\n<ul>\n<li>장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 </li>\n<li>단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h2><ul>\n<li>유클라디안 거리를 활용한 KNN 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">euclidean_distance</span>(<span class=\"params\">x1,x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(np.<span class=\"built_in\">sum</span>((x1-x2)**<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">KNN</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    self __init__(self, k=<span class=\"number\">3</span>):</span><br><span class=\"line\">        self.k = k </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>): <span class=\"comment\"># triain sample and label</span></span><br><span class=\"line\">        self.X_train = X</span><br><span class=\"line\">        self.y_train = y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        predicted_labels = [self._predict(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_predict</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        1. 거리 계산하기</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        2. k nearest sample</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        3. majority vote, get most common class</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        distances = [euclidean_distance(x,x_train) <span class=\"keyword\">for</span> x_train <span class=\"keyword\">in</span> X_train]</span><br><span class=\"line\"></span><br><span class=\"line\">        k_indices = np.argsort(distances)[:self.k]</span><br><span class=\"line\">        k_nearest_labels = [self.y_train[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> k_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">        most_common = Counter(k_nearest_labels).most_common(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> most_common[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"분류문제-풀이\"><a href=\"#분류문제-풀이\" class=\"headerlink\" title=\"분류문제 풀이\"></a>분류문제 풀이</h2><ul>\n<li>iris 데이터를 바탕으로 분류문제 풀이</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">cmap = ListedColormap([<span class=\"string\">&quot;#FF0000&quot;</span>, <span class=\"string\">&quot;#00FF00&quot;</span>, <span class=\"string\">&quot;#0000FF&quot;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_true, y_pred</span>):</span><br><span class=\"line\">    accuracy = np.<span class=\"built_in\">sum</span>(y_true == y_pred) / <span class=\"built_in\">len</span>(y_true)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X, y = iris.data, iris.target</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class=\"line\">        X, y, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">1234</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">k = <span class=\"number\">3</span></span><br><span class=\"line\">clf = KNN(k=k)</span><br><span class=\"line\">clf.fit(X_train, y_train)</span><br><span class=\"line\">predictions = clf.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;KNN classification 정확도&quot;</span>, accuracy(y_test, predictions))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$KNN</span> classification accuracy 1.0</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>sklearn에서도 knn 분류기가 구현되어 있다.<ul>\n<li>irsis data load까지는 동일하게 진행된다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeiborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">clf = KNeiborsClassifier(n_neighbors =<span class=\"number\">3</span>)</span><br><span class=\"line\">clf.fit()</span><br><span class=\"line\"></span><br><span class=\"line\">pred = clf.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;KNN classification 정확도&quot;</span>, clf.score(X_test,y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"K값과-모델-복잡도의-관계\"><a href=\"#K값과-모델-복잡도의-관계\" class=\"headerlink\" title=\"K값과 모델 복잡도의 관계\"></a>K값과 모델 복잡도의 관계</h2><ul>\n<li>위스콘신 유방암데이터로 구현한다.</li>\n<li>k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.</li>\n<li>k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.</li>\n<li>k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.</li>\n<li>중간정도의 범위에서 k의 수를 선정할 필요가 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_breast_cancer</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">X_train , X_test , y_train , y_test = train_test_split(cancer.data,</span><br><span class=\"line\">                                                       cancer.target,</span><br><span class=\"line\">                                                       stratify = cancer.target, <span class=\"comment\"># stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)</span></span><br><span class=\"line\">                                                       random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_acc = []</span><br><span class=\"line\">test_acc = []</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">k_indices = <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">11</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_indices:</span><br><span class=\"line\">    clf = KNeiborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    clf.fit()</span><br><span class=\"line\">    train_acc.append(clf.score(X_train,y_train))</span><br><span class=\"line\">    test_acc.append(clf.score(X_test,y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(neighbors_settings, training_accuracy, label=<span class=\"string\">&quot;훈련 정확도&quot;</span>)</span><br><span class=\"line\">plt.plot(neighbors_settings, test_accuracy, label=<span class=\"string\">&quot;테스트 정확도&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;정확도&quot;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;n_neighbors&quot;</span>)</span><br><span class=\"line\">plt.legend(</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024\"></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.python.org/3/library/collections.html\">https://docs.python.org/3/library/collections.html</a></li>\n<li><a href=\"https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/\">파이싼 라이브러리를 활용한 머신러닝</a></li>\n<li><a href=\"https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/\">https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<hr>\n<h2 id=\"간단한-컨셉\"><a href=\"#간단한-컨셉\" class=\"headerlink\" title=\"간단한 컨셉\"></a>간단한 컨셉</h2><p><strong>KNN</strong></p>\n<ul>\n<li><p>새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.</p>\n</li>\n<li><p>회귀문제와 분류문제 해결에 모두 사용되는 지도학습</p>\n</li>\n<li><p>하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.</p>\n</li>\n<li><p><strong>K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).</strong></p>\n</li>\n<li><p>KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.</p>\n</li>\n<li><p>거리측정방법</p>\n<ul>\n<li>유클리디안 거리 : 데이터포인트 사이 직선 최단거리</li>\n<li>마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.</li>\n<li>맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.</li>\n</ul>\n</li>\n<li><p>주의점</p>\n<ul>\n<li>기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.</li>\n<li>불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.</li>\n</ul>\n</li>\n<li><p>장단점</p>\n<ul>\n<li>장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 </li>\n<li>단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h2><ul>\n<li>유클라디안 거리를 활용한 KNN 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">euclidean_distance</span>(<span class=\"params\">x1,x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(np.<span class=\"built_in\">sum</span>((x1-x2)**<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">KNN</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    self __init__(self, k=<span class=\"number\">3</span>):</span><br><span class=\"line\">        self.k = k </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X, y</span>): <span class=\"comment\"># triain sample and label</span></span><br><span class=\"line\">        self.X_train = X</span><br><span class=\"line\">        self.y_train = y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        predicted_labels = [self._predict(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.array</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_predict</span>(<span class=\"params\">self,x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        1. 거리 계산하기</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        2. k nearest sample</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        3. majority vote, get most common class</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        distances = [euclidean_distance(x,x_train) <span class=\"keyword\">for</span> x_train <span class=\"keyword\">in</span> X_train]</span><br><span class=\"line\"></span><br><span class=\"line\">        k_indices = np.argsort(distances)[:self.k]</span><br><span class=\"line\">        k_nearest_labels = [self.y_train[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> k_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">        most_common = Counter(k_nearest_labels).most_common(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> most_common[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"분류문제-풀이\"><a href=\"#분류문제-풀이\" class=\"headerlink\" title=\"분류문제 풀이\"></a>분류문제 풀이</h2><ul>\n<li>iris 데이터를 바탕으로 분류문제 풀이</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.colors <span class=\"keyword\">import</span> ListedColormap</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">cmap = ListedColormap([<span class=\"string\">&quot;#FF0000&quot;</span>, <span class=\"string\">&quot;#00FF00&quot;</span>, <span class=\"string\">&quot;#0000FF&quot;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_true, y_pred</span>):</span><br><span class=\"line\">    accuracy = np.<span class=\"built_in\">sum</span>(y_true == y_pred) / <span class=\"built_in\">len</span>(y_true)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X, y = iris.data, iris.target</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class=\"line\">        X, y, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">1234</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">k = <span class=\"number\">3</span></span><br><span class=\"line\">clf = KNN(k=k)</span><br><span class=\"line\">clf.fit(X_train, y_train)</span><br><span class=\"line\">predictions = clf.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;KNN classification 정확도&quot;</span>, accuracy(y_test, predictions))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$KNN</span> classification accuracy 1.0</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>sklearn에서도 knn 분류기가 구현되어 있다.<ul>\n<li>irsis data load까지는 동일하게 진행된다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeiborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">clf = KNeiborsClassifier(n_neighbors =<span class=\"number\">3</span>)</span><br><span class=\"line\">clf.fit()</span><br><span class=\"line\"></span><br><span class=\"line\">pred = clf.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;KNN classification 정확도&quot;</span>, clf.score(X_test,y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"K값과-모델-복잡도의-관계\"><a href=\"#K값과-모델-복잡도의-관계\" class=\"headerlink\" title=\"K값과 모델 복잡도의 관계\"></a>K값과 모델 복잡도의 관계</h2><ul>\n<li>위스콘신 유방암데이터로 구현한다.</li>\n<li>k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.</li>\n<li>k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.</li>\n<li>k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.</li>\n<li>중간정도의 범위에서 k의 수를 선정할 필요가 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_breast_cancer</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">X_train , X_test , y_train , y_test = train_test_split(cancer.data,</span><br><span class=\"line\">                                                       cancer.target,</span><br><span class=\"line\">                                                       stratify = cancer.target, <span class=\"comment\"># stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)</span></span><br><span class=\"line\">                                                       random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_acc = []</span><br><span class=\"line\">test_acc = []</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">k_indices = <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">11</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_indices:</span><br><span class=\"line\">    clf = KNeiborsClassifier(n_neighbors=k)</span><br><span class=\"line\">    clf.fit()</span><br><span class=\"line\">    train_acc.append(clf.score(X_train,y_train))</span><br><span class=\"line\">    test_acc.append(clf.score(X_test,y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(neighbors_settings, training_accuracy, label=<span class=\"string\">&quot;훈련 정확도&quot;</span>)</span><br><span class=\"line\">plt.plot(neighbors_settings, test_accuracy, label=<span class=\"string\">&quot;테스트 정확도&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;정확도&quot;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;n_neighbors&quot;</span>)</span><br><span class=\"line\">plt.legend(</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024\"></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.python.org/3/library/collections.html\">https://docs.python.org/3/library/collections.html</a></li>\n<li><a href=\"https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/\">파이싼 라이브러리를 활용한 머신러닝</a></li>\n<li><a href=\"https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/\">https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Unsupervised Learning]KNN을 활용한 분류","path":"2022/06/13/ML-US-knn/","eyeCatchImage":"https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["Unsupervised Learning","KNN"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[XAI]PDP Plot의 이해와 구현","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## PDP plot\n\n---\n**_Concept_**\n\n- **ICE(Indivisual Conditional Expectation)** :하나의 관측치에 대해 특정 feature의 값을 변화시킬 때 모델의 예측.\n- **marginal effect** :독립변수의 변화예 따른 종속변수의 변화\n- **Partial Dependence Plot** : 1개나 2개의 특성의 변화(상호작용)에 따른 모델 예측의 변화를 그린 것.\n\n---\n\n>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200130). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. \n\n- feature가 모델에 미치는 긍정적/부정적 영향 확인\n- 특정 feture에 대해 여유분(buffer)을 함께 표시 -> feature간 독립을 보장하지 못하는 환경에서 모델에 어느정도 있을 수 있는 지를 확인할 수 있게끔 함\n\n### 기본적인 컨셉에 대한 이해\n\n$$\\hat{f}_S(x_S)=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]=\\int\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)$$\n\n$$\\hat{f}_S(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})$$\n\n\n- $X_S$는 분석하고자 하는 feature이다.\n- $X_C$는 분석하고자 하는 feauture 외의 모델의 feauture들이다.\n- 여기서 $f(x_{S}, x_{C}^{(i)})$ 가 하나의 ICE 곡선을 나타낸다.\n- Partial Dependence는 단순히 $X_C$를 를 고정시킨 상태에서 $X_S$를 변화시키며 모델의 예측값을 계산 후 그 값을 평균한 것이다.\n- **target과 관련이 있는 특성에 대한 Global한 설명이 필요할 때 사용한다.**\n\n\n### ICE(Indivisual Conditional Expectation)\n\n- ICE 곡선은 하나의 관측치에 대해 관심 특성을 변화시킴에 따른 타겟값 변화 곡선. \n- PDP는 기본적으로 여러 ICE곡선의 평균이다.\n- `frac_to_plot` : 라인 수 조정 파라미터. 라인 수 혹은 비율\n\n- **ICE와 PDF에 대한 직관적 이해** : https://twitter.com/i/status/1066398522608635904\n\n\n- 부분 의존성 계산 및 PDP plot 그리기\n\n```python\n\nice = pdp.pdp_isolate(\n      model = clf,\n      dataset = df,\n      model_features=features\n      feature = \"feature_1\" # 분석하고자 하는 feature\n        )\n\n# PDP plot\n\nfig, axes = pdp.pdp_plot(\n            ice,\n            \"feature_1\",\n            plot_line = False,\n            frac_to_plot = 0.5,\n            plot_pts_dist = True\n                         )\n```\n\n\n\n### PDP interaction\n\n- 두 특성간 상호작용 확인\n- 등고선 그래프를 그렸을 때 특정 축에 평행할 경우 다른 축의 값에 상관없이 \n  + X축에 평행할 경우 모델의 예측 X축의 변수에 보다 의존적.\n  + Y축의 변수의 값에 상관없이 X축의 값에 따라 모델 예측이 결정됨 \n- 해석하기에는 Grid로 그래프를 그리는 것이 더 나을 수 있다. \n\n- skearn으로 구현한 등고선 그래프\n![ICE](https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png)\n\n- pdp plot 패키지로 구현한 상호작용\n  + 모델이 없이 두 feature의 상호작용에 따른 target의 값을 보여준다.\n\n```python\nfrom pdpbox import info_plots, get_dataset\n\ntest_titanic = get_dataset.titanic()\ntitanic_data = test_titanic['data']\ntitanic_target = test_titanic['target']\n\nfig, axes, summary_df = info_plots.target_plot_interact(\n    df=titanic_data, features=['Sex', ['Embarked_C', 'Embarked_Q', 'Embarked_S']],\n    feature_names=['Sex', 'Embarked'], target=titanic_target)\n\n```\n\n\n### PDP plot에서 범주형 변수 Decoding하기\n\n- 범주형 변수는 Ordinal Encoder나 target Encoder로 인코딩 한 후 사용된다.\n- 인코딩을 하게되면 학습 후 PDP 를 그릴 때 인코딩된 값이 나오게 되어 카테고리특성의 실제 값을 확인하기 어려운 문제가 있다.\n\n```python\n\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\n\ndf = sns.load_dataset('titanic')\ndf['age'] = df['age'].fillna(df['age'].median())\ndf = df.drop(columns='deck') # NaN 77%\ndf = df.dropna()\n\ntarget = 'survived'\nfeatures = df.columns.drop(['survived', 'alive'])\n\nX = df[features]\ny = df[target]\n\n# 파이프라인 생성 및 학습\npipe = make_pipeline(\n    OrdinalEncoder(), \n    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n)\npipe.fit(X, y);\n\n# \nencoder = pipe.named_steps['ordinalencoder']\nX_encoded = encoder.fit_transform(X)\nrf = pipe.named_steps['randomforestclassifier']\n\n```\n \n- 범주형 변수에 대한 ice plot\n\n```python\nimport matplotlib.pyplot as plt\nfrom pdpbox import pdp\nfeature = 'sex'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_encoded, model_features=features, feature=feature)\npdp.pdp_plot(pdp_dist, feature); # 인코딩된 sex 값을 확인할 수 있습니다\n```\n\n- 자동으로 PDP 카테고리 매핑\n\n```python\n# 이번에는 PDP 카테고리값 맵핑을 자동으로 해보겠습니다\n\nfeature = 'sex'\nfor item in encoder.mapping:\n    if item['col'] == feature:\n        feature_mapping = item['mapping'] # Series\n        \nfeature_mapping = feature_mapping[feature_mapping.index.dropna()]\ncategory_names = feature_mapping.index.tolist()\ncategory_codes = feature_mapping.values.tolist()\n\n\npdp.pdp_plot(pdp_dist, feature)\n\n# xticks labels 설정을 위한 리스트를 직접 넣지 않아도 됩니다 \nplt.xticks(category_codes, category_names);\n\n```\n\n- PDP 상호작용 \n\n```python\n# 2D PDP 를 Seaborn Heatmap으로 그리기 위해 데이터프레임으로 만듭니다\npdp = interaction.pdp.pivot_table(\n    values='preds', \n    columns=features[0], \n    index=features[1]\n)[::-1]\n\npdp = pdp.rename(columns=dict(zip(category_codes, category_names)))\nplt.figure(figsize=(6,5))\nsns.heatmap(pdp, annot=True, fmt='.2f', cmap='viridis')\nplt.title('PDP decoded categorical');\n```\n\n\n**References & annotation**\n\n- https://pdpbox.readthedocs.io/en/latest/index.html\n- https://christophm.github.io/interpretable-ml-book/pdp.html\n- https://scikit-learn.org/stable/modules/partial_dependence.html\n\n","source":"_posts/ML-XAI-PDP.md","raw":"---\ntitle: '[XAI]PDP Plot의 이해와 구현'\ncategories:\n  - Machine Learning\ndate:\nupdated:\ntags: \n  - XAI\nmathjax: true\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## PDP plot\n\n---\n**_Concept_**\n\n- **ICE(Indivisual Conditional Expectation)** :하나의 관측치에 대해 특정 feature의 값을 변화시킬 때 모델의 예측.\n- **marginal effect** :독립변수의 변화예 따른 종속변수의 변화\n- **Partial Dependence Plot** : 1개나 2개의 특성의 변화(상호작용)에 따른 모델 예측의 변화를 그린 것.\n\n---\n\n>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200130). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. \n\n- feature가 모델에 미치는 긍정적/부정적 영향 확인\n- 특정 feture에 대해 여유분(buffer)을 함께 표시 -> feature간 독립을 보장하지 못하는 환경에서 모델에 어느정도 있을 수 있는 지를 확인할 수 있게끔 함\n\n### 기본적인 컨셉에 대한 이해\n\n$$\\hat{f}_S(x_S)=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]=\\int\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)$$\n\n$$\\hat{f}_S(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})$$\n\n\n- $X_S$는 분석하고자 하는 feature이다.\n- $X_C$는 분석하고자 하는 feauture 외의 모델의 feauture들이다.\n- 여기서 $f(x_{S}, x_{C}^{(i)})$ 가 하나의 ICE 곡선을 나타낸다.\n- Partial Dependence는 단순히 $X_C$를 를 고정시킨 상태에서 $X_S$를 변화시키며 모델의 예측값을 계산 후 그 값을 평균한 것이다.\n- **target과 관련이 있는 특성에 대한 Global한 설명이 필요할 때 사용한다.**\n\n\n### ICE(Indivisual Conditional Expectation)\n\n- ICE 곡선은 하나의 관측치에 대해 관심 특성을 변화시킴에 따른 타겟값 변화 곡선. \n- PDP는 기본적으로 여러 ICE곡선의 평균이다.\n- `frac_to_plot` : 라인 수 조정 파라미터. 라인 수 혹은 비율\n\n- **ICE와 PDF에 대한 직관적 이해** : https://twitter.com/i/status/1066398522608635904\n\n\n- 부분 의존성 계산 및 PDP plot 그리기\n\n```python\n\nice = pdp.pdp_isolate(\n      model = clf,\n      dataset = df,\n      model_features=features\n      feature = \"feature_1\" # 분석하고자 하는 feature\n        )\n\n# PDP plot\n\nfig, axes = pdp.pdp_plot(\n            ice,\n            \"feature_1\",\n            plot_line = False,\n            frac_to_plot = 0.5,\n            plot_pts_dist = True\n                         )\n```\n\n\n\n### PDP interaction\n\n- 두 특성간 상호작용 확인\n- 등고선 그래프를 그렸을 때 특정 축에 평행할 경우 다른 축의 값에 상관없이 \n  + X축에 평행할 경우 모델의 예측 X축의 변수에 보다 의존적.\n  + Y축의 변수의 값에 상관없이 X축의 값에 따라 모델 예측이 결정됨 \n- 해석하기에는 Grid로 그래프를 그리는 것이 더 나을 수 있다. \n\n- skearn으로 구현한 등고선 그래프\n![ICE](https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png)\n\n- pdp plot 패키지로 구현한 상호작용\n  + 모델이 없이 두 feature의 상호작용에 따른 target의 값을 보여준다.\n\n```python\nfrom pdpbox import info_plots, get_dataset\n\ntest_titanic = get_dataset.titanic()\ntitanic_data = test_titanic['data']\ntitanic_target = test_titanic['target']\n\nfig, axes, summary_df = info_plots.target_plot_interact(\n    df=titanic_data, features=['Sex', ['Embarked_C', 'Embarked_Q', 'Embarked_S']],\n    feature_names=['Sex', 'Embarked'], target=titanic_target)\n\n```\n\n\n### PDP plot에서 범주형 변수 Decoding하기\n\n- 범주형 변수는 Ordinal Encoder나 target Encoder로 인코딩 한 후 사용된다.\n- 인코딩을 하게되면 학습 후 PDP 를 그릴 때 인코딩된 값이 나오게 되어 카테고리특성의 실제 값을 확인하기 어려운 문제가 있다.\n\n```python\n\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\n\ndf = sns.load_dataset('titanic')\ndf['age'] = df['age'].fillna(df['age'].median())\ndf = df.drop(columns='deck') # NaN 77%\ndf = df.dropna()\n\ntarget = 'survived'\nfeatures = df.columns.drop(['survived', 'alive'])\n\nX = df[features]\ny = df[target]\n\n# 파이프라인 생성 및 학습\npipe = make_pipeline(\n    OrdinalEncoder(), \n    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n)\npipe.fit(X, y);\n\n# \nencoder = pipe.named_steps['ordinalencoder']\nX_encoded = encoder.fit_transform(X)\nrf = pipe.named_steps['randomforestclassifier']\n\n```\n \n- 범주형 변수에 대한 ice plot\n\n```python\nimport matplotlib.pyplot as plt\nfrom pdpbox import pdp\nfeature = 'sex'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_encoded, model_features=features, feature=feature)\npdp.pdp_plot(pdp_dist, feature); # 인코딩된 sex 값을 확인할 수 있습니다\n```\n\n- 자동으로 PDP 카테고리 매핑\n\n```python\n# 이번에는 PDP 카테고리값 맵핑을 자동으로 해보겠습니다\n\nfeature = 'sex'\nfor item in encoder.mapping:\n    if item['col'] == feature:\n        feature_mapping = item['mapping'] # Series\n        \nfeature_mapping = feature_mapping[feature_mapping.index.dropna()]\ncategory_names = feature_mapping.index.tolist()\ncategory_codes = feature_mapping.values.tolist()\n\n\npdp.pdp_plot(pdp_dist, feature)\n\n# xticks labels 설정을 위한 리스트를 직접 넣지 않아도 됩니다 \nplt.xticks(category_codes, category_names);\n\n```\n\n- PDP 상호작용 \n\n```python\n# 2D PDP 를 Seaborn Heatmap으로 그리기 위해 데이터프레임으로 만듭니다\npdp = interaction.pdp.pivot_table(\n    values='preds', \n    columns=features[0], \n    index=features[1]\n)[::-1]\n\npdp = pdp.rename(columns=dict(zip(category_codes, category_names)))\nplt.figure(figsize=(6,5))\nsns.heatmap(pdp, annot=True, fmt='.2f', cmap='viridis')\nplt.title('PDP decoded categorical');\n```\n\n\n**References & annotation**\n\n- https://pdpbox.readthedocs.io/en/latest/index.html\n- https://christophm.github.io/interpretable-ml-book/pdp.html\n- https://scikit-learn.org/stable/modules/partial_dependence.html\n\n","slug":"ML-XAI-PDP","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscc001rb36qddgu0bw1","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"PDP-plot\"><a href=\"#PDP-plot\" class=\"headerlink\" title=\"PDP plot\"></a>PDP plot</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>ICE(Indivisual Conditional Expectation)</strong> :하나의 관측치에 대해 특정 feature의 값을 변화시킬 때 모델의 예측.</li>\n<li><strong>marginal effect</strong> :독립변수의 변화예 따른 종속변수의 변화</li>\n<li><strong>Partial Dependence Plot</strong> : 1개나 2개의 특성의 변화(상호작용)에 따른 모델 예측의 변화를 그린 것.</li>\n</ul>\n<hr>\n<blockquote>\n<p>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200130). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. </p>\n</blockquote>\n<ul>\n<li>feature가 모델에 미치는 긍정적&#x2F;부정적 영향 확인</li>\n<li>특정 feture에 대해 여유분(buffer)을 함께 표시 -&gt; feature간 독립을 보장하지 못하는 환경에서 모델에 어느정도 있을 수 있는 지를 확인할 수 있게끔 함</li>\n</ul>\n<h3 id=\"기본적인-컨셉에-대한-이해\"><a href=\"#기본적인-컨셉에-대한-이해\" class=\"headerlink\" title=\"기본적인 컨셉에 대한 이해\"></a>기본적인 컨셉에 대한 이해</h3><p>$$\\hat{f}<em>S(x_S)&#x3D;E</em>{X_C}\\left[\\hat{f}(x_S,X_C)\\right]&#x3D;\\int\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)$$</p>\n<p>$$\\hat{f}<em>S(x_S)&#x3D;\\frac{1}{n}\\sum</em>{i&#x3D;1}^n\\hat{f}(x_S,x^{(i)}_{C})$$</p>\n<ul>\n<li>$X_S$는 분석하고자 하는 feature이다.</li>\n<li>$X_C$는 분석하고자 하는 feauture 외의 모델의 feauture들이다.</li>\n<li>여기서 $f(x_{S}, x_{C}^{(i)})$ 가 하나의 ICE 곡선을 나타낸다.</li>\n<li>Partial Dependence는 단순히 $X_C$를 를 고정시킨 상태에서 $X_S$를 변화시키며 모델의 예측값을 계산 후 그 값을 평균한 것이다.</li>\n<li><strong>target과 관련이 있는 특성에 대한 Global한 설명이 필요할 때 사용한다.</strong></li>\n</ul>\n<h3 id=\"ICE-Indivisual-Conditional-Expectation\"><a href=\"#ICE-Indivisual-Conditional-Expectation\" class=\"headerlink\" title=\"ICE(Indivisual Conditional Expectation)\"></a>ICE(Indivisual Conditional Expectation)</h3><ul>\n<li><p>ICE 곡선은 하나의 관측치에 대해 관심 특성을 변화시킴에 따른 타겟값 변화 곡선. </p>\n</li>\n<li><p>PDP는 기본적으로 여러 ICE곡선의 평균이다.</p>\n</li>\n<li><p><code>frac_to_plot</code> : 라인 수 조정 파라미터. 라인 수 혹은 비율</p>\n</li>\n<li><p><strong>ICE와 PDF에 대한 직관적 이해</strong> : <a href=\"https://twitter.com/i/status/1066398522608635904\">https://twitter.com/i/status/1066398522608635904</a></p>\n</li>\n<li><p>부분 의존성 계산 및 PDP plot 그리기</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">ice = pdp.pdp_isolate(</span><br><span class=\"line\">      model = clf,</span><br><span class=\"line\">      dataset = df,</span><br><span class=\"line\">      model_features=features</span><br><span class=\"line\">      feature = <span class=\"string\">&quot;feature_1&quot;</span> <span class=\"comment\"># 분석하고자 하는 feature</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># PDP plot</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig, axes = pdp.pdp_plot(</span><br><span class=\"line\">            ice,</span><br><span class=\"line\">            <span class=\"string\">&quot;feature_1&quot;</span>,</span><br><span class=\"line\">            plot_line = <span class=\"literal\">False</span>,</span><br><span class=\"line\">            frac_to_plot = <span class=\"number\">0.5</span>,</span><br><span class=\"line\">            plot_pts_dist = <span class=\"literal\">True</span></span><br><span class=\"line\">                         )</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"PDP-interaction\"><a href=\"#PDP-interaction\" class=\"headerlink\" title=\"PDP interaction\"></a>PDP interaction</h3><ul>\n<li><p>두 특성간 상호작용 확인</p>\n</li>\n<li><p>등고선 그래프를 그렸을 때 특정 축에 평행할 경우 다른 축의 값에 상관없이 </p>\n<ul>\n<li>X축에 평행할 경우 모델의 예측 X축의 변수에 보다 의존적.</li>\n<li>Y축의 변수의 값에 상관없이 X축의 값에 따라 모델 예측이 결정됨</li>\n</ul>\n</li>\n<li><p>해석하기에는 Grid로 그래프를 그리는 것이 더 나을 수 있다. </p>\n</li>\n<li><p>skearn으로 구현한 등고선 그래프<br><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png\" alt=\"ICE\"></p>\n</li>\n<li><p>pdp plot 패키지로 구현한 상호작용</p>\n<ul>\n<li>모델이 없이 두 feature의 상호작용에 따른 target의 값을 보여준다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pdpbox <span class=\"keyword\">import</span> info_plots, get_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">test_titanic = get_dataset.titanic()</span><br><span class=\"line\">titanic_data = test_titanic[<span class=\"string\">&#x27;data&#x27;</span>]</span><br><span class=\"line\">titanic_target = test_titanic[<span class=\"string\">&#x27;target&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">fig, axes, summary_df = info_plots.target_plot_interact(</span><br><span class=\"line\">    df=titanic_data, features=[<span class=\"string\">&#x27;Sex&#x27;</span>, [<span class=\"string\">&#x27;Embarked_C&#x27;</span>, <span class=\"string\">&#x27;Embarked_Q&#x27;</span>, <span class=\"string\">&#x27;Embarked_S&#x27;</span>]],</span><br><span class=\"line\">    feature_names=[<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Embarked&#x27;</span>], target=titanic_target)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"PDP-plot에서-범주형-변수-Decoding하기\"><a href=\"#PDP-plot에서-범주형-변수-Decoding하기\" class=\"headerlink\" title=\"PDP plot에서 범주형 변수 Decoding하기\"></a>PDP plot에서 범주형 변수 Decoding하기</h3><ul>\n<li>범주형 변수는 Ordinal Encoder나 target Encoder로 인코딩 한 후 사용된다.</li>\n<li>인코딩을 하게되면 학습 후 PDP 를 그릴 때 인코딩된 값이 나오게 되어 카테고리특성의 실제 값을 확인하기 어려운 문제가 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span> make_pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">df = sns.load_dataset(<span class=\"string\">&#x27;titanic&#x27;</span>)</span><br><span class=\"line\">df[<span class=\"string\">&#x27;age&#x27;</span>] = df[<span class=\"string\">&#x27;age&#x27;</span>].fillna(df[<span class=\"string\">&#x27;age&#x27;</span>].median())</span><br><span class=\"line\">df = df.drop(columns=<span class=\"string\">&#x27;deck&#x27;</span>) <span class=\"comment\"># NaN 77%</span></span><br><span class=\"line\">df = df.dropna()</span><br><span class=\"line\"></span><br><span class=\"line\">target = <span class=\"string\">&#x27;survived&#x27;</span></span><br><span class=\"line\">features = df.columns.drop([<span class=\"string\">&#x27;survived&#x27;</span>, <span class=\"string\">&#x27;alive&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">X = df[features]</span><br><span class=\"line\">y = df[target]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 파이프라인 생성 및 학습</span></span><br><span class=\"line\">pipe = make_pipeline(</span><br><span class=\"line\">    OrdinalEncoder(), </span><br><span class=\"line\">    RandomForestClassifier(n_estimators=<span class=\"number\">100</span>, random_state=<span class=\"number\">42</span>, n_jobs=-<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">pipe.fit(X, y);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">encoder = pipe.named_steps[<span class=\"string\">&#x27;ordinalencoder&#x27;</span>]</span><br><span class=\"line\">X_encoded = encoder.fit_transform(X)</span><br><span class=\"line\">rf = pipe.named_steps[<span class=\"string\">&#x27;randomforestclassifier&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>범주형 변수에 대한 ice plot</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> pdpbox <span class=\"keyword\">import</span> pdp</span><br><span class=\"line\">feature = <span class=\"string\">&#x27;sex&#x27;</span></span><br><span class=\"line\">pdp_dist = pdp.pdp_isolate(model=rf, dataset=X_encoded, model_features=features, feature=feature)</span><br><span class=\"line\">pdp.pdp_plot(pdp_dist, feature); <span class=\"comment\"># 인코딩된 sex 값을 확인할 수 있습니다</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>자동으로 PDP 카테고리 매핑</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 이번에는 PDP 카테고리값 맵핑을 자동으로 해보겠습니다</span></span><br><span class=\"line\"></span><br><span class=\"line\">feature = <span class=\"string\">&#x27;sex&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> encoder.mapping:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> item[<span class=\"string\">&#x27;col&#x27;</span>] == feature:</span><br><span class=\"line\">        feature_mapping = item[<span class=\"string\">&#x27;mapping&#x27;</span>] <span class=\"comment\"># Series</span></span><br><span class=\"line\">        </span><br><span class=\"line\">feature_mapping = feature_mapping[feature_mapping.index.dropna()]</span><br><span class=\"line\">category_names = feature_mapping.index.tolist()</span><br><span class=\"line\">category_codes = feature_mapping.values.tolist()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">pdp.pdp_plot(pdp_dist, feature)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># xticks labels 설정을 위한 리스트를 직접 넣지 않아도 됩니다 </span></span><br><span class=\"line\">plt.xticks(category_codes, category_names);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PDP 상호작용</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 2D PDP 를 Seaborn Heatmap으로 그리기 위해 데이터프레임으로 만듭니다</span></span><br><span class=\"line\">pdp = interaction.pdp.pivot_table(</span><br><span class=\"line\">    values=<span class=\"string\">&#x27;preds&#x27;</span>, </span><br><span class=\"line\">    columns=features[<span class=\"number\">0</span>], </span><br><span class=\"line\">    index=features[<span class=\"number\">1</span>]</span><br><span class=\"line\">)[::-<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">pdp = pdp.rename(columns=<span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(category_codes, category_names)))</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">sns.heatmap(pdp, annot=<span class=\"literal\">True</span>, fmt=<span class=\"string\">&#x27;.2f&#x27;</span>, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;PDP decoded categorical&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>References &amp; annotation</strong></p>\n<ul>\n<li><a href=\"https://pdpbox.readthedocs.io/en/latest/index.html\">https://pdpbox.readthedocs.io/en/latest/index.html</a></li>\n<li><a href=\"https://christophm.github.io/interpretable-ml-book/pdp.html\">https://christophm.github.io/interpretable-ml-book/pdp.html</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/partial_dependence.html\">https://scikit-learn.org/stable/modules/partial_dependence.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"PDP-plot\"><a href=\"#PDP-plot\" class=\"headerlink\" title=\"PDP plot\"></a>PDP plot</h2><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>ICE(Indivisual Conditional Expectation)</strong> :하나의 관측치에 대해 특정 feature의 값을 변화시킬 때 모델의 예측.</li>\n<li><strong>marginal effect</strong> :독립변수의 변화예 따른 종속변수의 변화</li>\n<li><strong>Partial Dependence Plot</strong> : 1개나 2개의 특성의 변화(상호작용)에 따른 모델 예측의 변화를 그린 것.</li>\n</ul>\n<hr>\n<blockquote>\n<p>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200130). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. </p>\n</blockquote>\n<ul>\n<li>feature가 모델에 미치는 긍정적&#x2F;부정적 영향 확인</li>\n<li>특정 feture에 대해 여유분(buffer)을 함께 표시 -&gt; feature간 독립을 보장하지 못하는 환경에서 모델에 어느정도 있을 수 있는 지를 확인할 수 있게끔 함</li>\n</ul>\n<h3 id=\"기본적인-컨셉에-대한-이해\"><a href=\"#기본적인-컨셉에-대한-이해\" class=\"headerlink\" title=\"기본적인 컨셉에 대한 이해\"></a>기본적인 컨셉에 대한 이해</h3><p>$$\\hat{f}<em>S(x_S)&#x3D;E</em>{X_C}\\left[\\hat{f}(x_S,X_C)\\right]&#x3D;\\int\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)$$</p>\n<p>$$\\hat{f}<em>S(x_S)&#x3D;\\frac{1}{n}\\sum</em>{i&#x3D;1}^n\\hat{f}(x_S,x^{(i)}_{C})$$</p>\n<ul>\n<li>$X_S$는 분석하고자 하는 feature이다.</li>\n<li>$X_C$는 분석하고자 하는 feauture 외의 모델의 feauture들이다.</li>\n<li>여기서 $f(x_{S}, x_{C}^{(i)})$ 가 하나의 ICE 곡선을 나타낸다.</li>\n<li>Partial Dependence는 단순히 $X_C$를 를 고정시킨 상태에서 $X_S$를 변화시키며 모델의 예측값을 계산 후 그 값을 평균한 것이다.</li>\n<li><strong>target과 관련이 있는 특성에 대한 Global한 설명이 필요할 때 사용한다.</strong></li>\n</ul>\n<h3 id=\"ICE-Indivisual-Conditional-Expectation\"><a href=\"#ICE-Indivisual-Conditional-Expectation\" class=\"headerlink\" title=\"ICE(Indivisual Conditional Expectation)\"></a>ICE(Indivisual Conditional Expectation)</h3><ul>\n<li><p>ICE 곡선은 하나의 관측치에 대해 관심 특성을 변화시킴에 따른 타겟값 변화 곡선. </p>\n</li>\n<li><p>PDP는 기본적으로 여러 ICE곡선의 평균이다.</p>\n</li>\n<li><p><code>frac_to_plot</code> : 라인 수 조정 파라미터. 라인 수 혹은 비율</p>\n</li>\n<li><p><strong>ICE와 PDF에 대한 직관적 이해</strong> : <a href=\"https://twitter.com/i/status/1066398522608635904\">https://twitter.com/i/status/1066398522608635904</a></p>\n</li>\n<li><p>부분 의존성 계산 및 PDP plot 그리기</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">ice = pdp.pdp_isolate(</span><br><span class=\"line\">      model = clf,</span><br><span class=\"line\">      dataset = df,</span><br><span class=\"line\">      model_features=features</span><br><span class=\"line\">      feature = <span class=\"string\">&quot;feature_1&quot;</span> <span class=\"comment\"># 분석하고자 하는 feature</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># PDP plot</span></span><br><span class=\"line\"></span><br><span class=\"line\">fig, axes = pdp.pdp_plot(</span><br><span class=\"line\">            ice,</span><br><span class=\"line\">            <span class=\"string\">&quot;feature_1&quot;</span>,</span><br><span class=\"line\">            plot_line = <span class=\"literal\">False</span>,</span><br><span class=\"line\">            frac_to_plot = <span class=\"number\">0.5</span>,</span><br><span class=\"line\">            plot_pts_dist = <span class=\"literal\">True</span></span><br><span class=\"line\">                         )</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"PDP-interaction\"><a href=\"#PDP-interaction\" class=\"headerlink\" title=\"PDP interaction\"></a>PDP interaction</h3><ul>\n<li><p>두 특성간 상호작용 확인</p>\n</li>\n<li><p>등고선 그래프를 그렸을 때 특정 축에 평행할 경우 다른 축의 값에 상관없이 </p>\n<ul>\n<li>X축에 평행할 경우 모델의 예측 X축의 변수에 보다 의존적.</li>\n<li>Y축의 변수의 값에 상관없이 X축의 값에 따라 모델 예측이 결정됨</li>\n</ul>\n</li>\n<li><p>해석하기에는 Grid로 그래프를 그리는 것이 더 나을 수 있다. </p>\n</li>\n<li><p>skearn으로 구현한 등고선 그래프<br><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png\" alt=\"ICE\"></p>\n</li>\n<li><p>pdp plot 패키지로 구현한 상호작용</p>\n<ul>\n<li>모델이 없이 두 feature의 상호작용에 따른 target의 값을 보여준다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pdpbox <span class=\"keyword\">import</span> info_plots, get_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">test_titanic = get_dataset.titanic()</span><br><span class=\"line\">titanic_data = test_titanic[<span class=\"string\">&#x27;data&#x27;</span>]</span><br><span class=\"line\">titanic_target = test_titanic[<span class=\"string\">&#x27;target&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">fig, axes, summary_df = info_plots.target_plot_interact(</span><br><span class=\"line\">    df=titanic_data, features=[<span class=\"string\">&#x27;Sex&#x27;</span>, [<span class=\"string\">&#x27;Embarked_C&#x27;</span>, <span class=\"string\">&#x27;Embarked_Q&#x27;</span>, <span class=\"string\">&#x27;Embarked_S&#x27;</span>]],</span><br><span class=\"line\">    feature_names=[<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Embarked&#x27;</span>], target=titanic_target)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"PDP-plot에서-범주형-변수-Decoding하기\"><a href=\"#PDP-plot에서-범주형-변수-Decoding하기\" class=\"headerlink\" title=\"PDP plot에서 범주형 변수 Decoding하기\"></a>PDP plot에서 범주형 변수 Decoding하기</h3><ul>\n<li>범주형 변수는 Ordinal Encoder나 target Encoder로 인코딩 한 후 사용된다.</li>\n<li>인코딩을 하게되면 학습 후 PDP 를 그릴 때 인코딩된 값이 나오게 되어 카테고리특성의 실제 값을 확인하기 어려운 문제가 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.pipeline <span class=\"keyword\">import</span> make_pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">df = sns.load_dataset(<span class=\"string\">&#x27;titanic&#x27;</span>)</span><br><span class=\"line\">df[<span class=\"string\">&#x27;age&#x27;</span>] = df[<span class=\"string\">&#x27;age&#x27;</span>].fillna(df[<span class=\"string\">&#x27;age&#x27;</span>].median())</span><br><span class=\"line\">df = df.drop(columns=<span class=\"string\">&#x27;deck&#x27;</span>) <span class=\"comment\"># NaN 77%</span></span><br><span class=\"line\">df = df.dropna()</span><br><span class=\"line\"></span><br><span class=\"line\">target = <span class=\"string\">&#x27;survived&#x27;</span></span><br><span class=\"line\">features = df.columns.drop([<span class=\"string\">&#x27;survived&#x27;</span>, <span class=\"string\">&#x27;alive&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">X = df[features]</span><br><span class=\"line\">y = df[target]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 파이프라인 생성 및 학습</span></span><br><span class=\"line\">pipe = make_pipeline(</span><br><span class=\"line\">    OrdinalEncoder(), </span><br><span class=\"line\">    RandomForestClassifier(n_estimators=<span class=\"number\">100</span>, random_state=<span class=\"number\">42</span>, n_jobs=-<span class=\"number\">1</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">pipe.fit(X, y);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\">encoder = pipe.named_steps[<span class=\"string\">&#x27;ordinalencoder&#x27;</span>]</span><br><span class=\"line\">X_encoded = encoder.fit_transform(X)</span><br><span class=\"line\">rf = pipe.named_steps[<span class=\"string\">&#x27;randomforestclassifier&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>범주형 변수에 대한 ice plot</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> pdpbox <span class=\"keyword\">import</span> pdp</span><br><span class=\"line\">feature = <span class=\"string\">&#x27;sex&#x27;</span></span><br><span class=\"line\">pdp_dist = pdp.pdp_isolate(model=rf, dataset=X_encoded, model_features=features, feature=feature)</span><br><span class=\"line\">pdp.pdp_plot(pdp_dist, feature); <span class=\"comment\"># 인코딩된 sex 값을 확인할 수 있습니다</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>자동으로 PDP 카테고리 매핑</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 이번에는 PDP 카테고리값 맵핑을 자동으로 해보겠습니다</span></span><br><span class=\"line\"></span><br><span class=\"line\">feature = <span class=\"string\">&#x27;sex&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> encoder.mapping:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> item[<span class=\"string\">&#x27;col&#x27;</span>] == feature:</span><br><span class=\"line\">        feature_mapping = item[<span class=\"string\">&#x27;mapping&#x27;</span>] <span class=\"comment\"># Series</span></span><br><span class=\"line\">        </span><br><span class=\"line\">feature_mapping = feature_mapping[feature_mapping.index.dropna()]</span><br><span class=\"line\">category_names = feature_mapping.index.tolist()</span><br><span class=\"line\">category_codes = feature_mapping.values.tolist()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">pdp.pdp_plot(pdp_dist, feature)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># xticks labels 설정을 위한 리스트를 직접 넣지 않아도 됩니다 </span></span><br><span class=\"line\">plt.xticks(category_codes, category_names);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PDP 상호작용</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 2D PDP 를 Seaborn Heatmap으로 그리기 위해 데이터프레임으로 만듭니다</span></span><br><span class=\"line\">pdp = interaction.pdp.pivot_table(</span><br><span class=\"line\">    values=<span class=\"string\">&#x27;preds&#x27;</span>, </span><br><span class=\"line\">    columns=features[<span class=\"number\">0</span>], </span><br><span class=\"line\">    index=features[<span class=\"number\">1</span>]</span><br><span class=\"line\">)[::-<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">pdp = pdp.rename(columns=<span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(category_codes, category_names)))</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">sns.heatmap(pdp, annot=<span class=\"literal\">True</span>, fmt=<span class=\"string\">&#x27;.2f&#x27;</span>, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;PDP decoded categorical&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>References &amp; annotation</strong></p>\n<ul>\n<li><a href=\"https://pdpbox.readthedocs.io/en/latest/index.html\">https://pdpbox.readthedocs.io/en/latest/index.html</a></li>\n<li><a href=\"https://christophm.github.io/interpretable-ml-book/pdp.html\">https://christophm.github.io/interpretable-ml-book/pdp.html</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/partial_dependence.html\">https://scikit-learn.org/stable/modules/partial_dependence.html</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[XAI]PDP Plot의 이해와 구현","path":"2022/06/13/ML-XAI-PDP/","eyeCatchImage":"https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["XAI"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[XAI]Shap을 활용한 모델해석","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","source":"_posts/ML-XAI-shap.md","raw":"---\ntitle: '[XAI]Shap을 활용한 모델해석'\ncategories:\n    - Machine Learning\ndate:\nupdated:\ntags:\n  - XAI\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","slug":"ML-XAI-shap","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscd001vb36qe70q9vuv","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[XAI]Shap을 활용한 모델해석","path":"2022/06/13/ML-XAI-shap/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Machine Learning","tags":["XAI"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[NLP]NLU & QA task","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n# NLU\n\n> NLP의 하위 분야인 NLU를 소개하고 NLU 의 subtask 중 하나인 QA(Question Answering) 에 대해 정리\n\n: Natural Language Understanding\n기계가 자연어에 대한 Synthetic과 Semantic Understanding을 보인다면 그 기계는 NLU를 수행하고 있다고 볼 수 있다.\n\n## Question Answering(task)\n\n특정 자연어 텍스트를 기계가 올바르게 이해하고 답변하는지 평가하는 Reading Comprehension의 일종이다.\n\n질문에 대해 기계가 답변하는 QA의 형태를 가지고 있다는 점에서 NLU task라고 볼 수 있다.\n\nQA task의 경우 질문에 대한 답이 Input인 지문안에 분명히 존재하기 때문에 평가지표로 Accuracy와 F1 Score를 사용할 수 있다.\n\n#### 대표적인 데이터셋\n\n#### SQuAD\n\n: Stanford Question Answering Dataset\n\nQA와 같은 Reading Comprehension 문제의 해결을 위해 스탠포스에서 개발한 대표 Benchmark.\n\nInput은 지문인 Context와 지문 내에서 답을 찾을 수 있는 Question으로 구성된다.\n\nSQuAD 데이터 셋을 통해 QA task를 수행할 경우 기계는 출력값으로 지문(Context) 내에 포함된 질문의 답의 시작과 끝의 인덱스를 반환해야 한다,\n\n\n- ex) SQuAD의 예시\n\n![](https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png)\n\n### SOTA models for QA task\n\n#### T5 (Text to Text Transfer)\n\nT5는 전이학습을 기반으로한 구글에서 개발한 Transformer 기반 Language Model이다.\n\n현 시점에서 QA task에서 가장 높은 성능을 보이는 SOTA 모델의 하나이다.\n\n![](https://media.vlpt.us/images/yjinheon/post/2b661472-0508-4446-b808-95b7d2a5b4d2/Velog_1_12.png)\n\n#### 전이학습(Transfer Learning)\n\n- 전이학습은 이미 학습된 큰 데이터셋의 가중치를 가지고 와서 해결하고자 하는 다른 과제에 맞게 튜닝해서 사용하는 방법론이다.\n- T5 paper에서는 하나의 언어모델을 비지도로 pre-train한 뒤 세부 task에 따라 지도학습으로 fine tuning하는 방식을 사용하였다.\n\n#### XLNet: Generalized Autoregressive Pretraining for Language Understanding\n\nBERT의 Autoencoding method에 Autoregressive기법을 더해 성능을 개선시킨 언어모델.\n\n#### Autoregressive Pretrained method\n\n- AR(자기회귀) 방법론은 기존 BERT가 가지고 있는 masking을 보완하기 위해 사용되었다.\n- AR은 BERT가 가지고 있는 문제점을 보완할 수 있지만 알고리즘 특성상 단방향 만을 고려한다는 문제점이 있다.\n- BERT의 경우 AutoEncoding 과정에서 토큰이 독립적으로 예측되기 때문에 토큰 간 Dependency가 학습이 안되는 문제점이 있었다.\n- XLNet에서는 permutation을 이용해 모든 가능한 sequence를 고려해서 (Factorization Order) AR 방법론을 적용하여 AR과 AutoEncoding 방법론을 모두 보완해 성능을 개선시켰다.\n\n## References\n\n- https://rajpurkar.github.io/SQuAD-explorer/\n- [Text to Text Transfer](https://arxiv.org/pdf/1910.10683v3.pdf)\n- [XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding](https://arxiv.org/pdf/1906.08237v2.pdf)\n- [QA task의 평가지표](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Metrics-for-QA)\n- [T5 paper설명](https://yhdosu.github.io/2019/11/12/T5.html)","source":"_posts/NLP-NLU.md","raw":"---\ntitle: '[NLP]NLU & QA task'\ncategories:\n  - [NLP]\ntags:\n  - NLP\n  - NLU\n  - QA\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n# NLU\n\n> NLP의 하위 분야인 NLU를 소개하고 NLU 의 subtask 중 하나인 QA(Question Answering) 에 대해 정리\n\n: Natural Language Understanding\n기계가 자연어에 대한 Synthetic과 Semantic Understanding을 보인다면 그 기계는 NLU를 수행하고 있다고 볼 수 있다.\n\n## Question Answering(task)\n\n특정 자연어 텍스트를 기계가 올바르게 이해하고 답변하는지 평가하는 Reading Comprehension의 일종이다.\n\n질문에 대해 기계가 답변하는 QA의 형태를 가지고 있다는 점에서 NLU task라고 볼 수 있다.\n\nQA task의 경우 질문에 대한 답이 Input인 지문안에 분명히 존재하기 때문에 평가지표로 Accuracy와 F1 Score를 사용할 수 있다.\n\n#### 대표적인 데이터셋\n\n#### SQuAD\n\n: Stanford Question Answering Dataset\n\nQA와 같은 Reading Comprehension 문제의 해결을 위해 스탠포스에서 개발한 대표 Benchmark.\n\nInput은 지문인 Context와 지문 내에서 답을 찾을 수 있는 Question으로 구성된다.\n\nSQuAD 데이터 셋을 통해 QA task를 수행할 경우 기계는 출력값으로 지문(Context) 내에 포함된 질문의 답의 시작과 끝의 인덱스를 반환해야 한다,\n\n\n- ex) SQuAD의 예시\n\n![](https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png)\n\n### SOTA models for QA task\n\n#### T5 (Text to Text Transfer)\n\nT5는 전이학습을 기반으로한 구글에서 개발한 Transformer 기반 Language Model이다.\n\n현 시점에서 QA task에서 가장 높은 성능을 보이는 SOTA 모델의 하나이다.\n\n![](https://media.vlpt.us/images/yjinheon/post/2b661472-0508-4446-b808-95b7d2a5b4d2/Velog_1_12.png)\n\n#### 전이학습(Transfer Learning)\n\n- 전이학습은 이미 학습된 큰 데이터셋의 가중치를 가지고 와서 해결하고자 하는 다른 과제에 맞게 튜닝해서 사용하는 방법론이다.\n- T5 paper에서는 하나의 언어모델을 비지도로 pre-train한 뒤 세부 task에 따라 지도학습으로 fine tuning하는 방식을 사용하였다.\n\n#### XLNet: Generalized Autoregressive Pretraining for Language Understanding\n\nBERT의 Autoencoding method에 Autoregressive기법을 더해 성능을 개선시킨 언어모델.\n\n#### Autoregressive Pretrained method\n\n- AR(자기회귀) 방법론은 기존 BERT가 가지고 있는 masking을 보완하기 위해 사용되었다.\n- AR은 BERT가 가지고 있는 문제점을 보완할 수 있지만 알고리즘 특성상 단방향 만을 고려한다는 문제점이 있다.\n- BERT의 경우 AutoEncoding 과정에서 토큰이 독립적으로 예측되기 때문에 토큰 간 Dependency가 학습이 안되는 문제점이 있었다.\n- XLNet에서는 permutation을 이용해 모든 가능한 sequence를 고려해서 (Factorization Order) AR 방법론을 적용하여 AR과 AutoEncoding 방법론을 모두 보완해 성능을 개선시켰다.\n\n## References\n\n- https://rajpurkar.github.io/SQuAD-explorer/\n- [Text to Text Transfer](https://arxiv.org/pdf/1910.10683v3.pdf)\n- [XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding](https://arxiv.org/pdf/1906.08237v2.pdf)\n- [QA task의 평가지표](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Metrics-for-QA)\n- [T5 paper설명](https://yhdosu.github.io/2019/11/12/T5.html)","slug":"NLP-NLU","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscd001xb36qddedhew2","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h1 id=\"NLU\"><a href=\"#NLU\" class=\"headerlink\" title=\"NLU\"></a>NLU</h1><blockquote>\n<p>NLP의 하위 분야인 NLU를 소개하고 NLU 의 subtask 중 하나인 QA(Question Answering) 에 대해 정리</p>\n</blockquote>\n<p>: Natural Language Understanding<br>기계가 자연어에 대한 Synthetic과 Semantic Understanding을 보인다면 그 기계는 NLU를 수행하고 있다고 볼 수 있다.</p>\n<h2 id=\"Question-Answering-task\"><a href=\"#Question-Answering-task\" class=\"headerlink\" title=\"Question Answering(task)\"></a>Question Answering(task)</h2><p>특정 자연어 텍스트를 기계가 올바르게 이해하고 답변하는지 평가하는 Reading Comprehension의 일종이다.</p>\n<p>질문에 대해 기계가 답변하는 QA의 형태를 가지고 있다는 점에서 NLU task라고 볼 수 있다.</p>\n<p>QA task의 경우 질문에 대한 답이 Input인 지문안에 분명히 존재하기 때문에 평가지표로 Accuracy와 F1 Score를 사용할 수 있다.</p>\n<h4 id=\"대표적인-데이터셋\"><a href=\"#대표적인-데이터셋\" class=\"headerlink\" title=\"대표적인 데이터셋\"></a>대표적인 데이터셋</h4><h4 id=\"SQuAD\"><a href=\"#SQuAD\" class=\"headerlink\" title=\"SQuAD\"></a>SQuAD</h4><p>: Stanford Question Answering Dataset</p>\n<p>QA와 같은 Reading Comprehension 문제의 해결을 위해 스탠포스에서 개발한 대표 Benchmark.</p>\n<p>Input은 지문인 Context와 지문 내에서 답을 찾을 수 있는 Question으로 구성된다.</p>\n<p>SQuAD 데이터 셋을 통해 QA task를 수행할 경우 기계는 출력값으로 지문(Context) 내에 포함된 질문의 답의 시작과 끝의 인덱스를 반환해야 한다,</p>\n<ul>\n<li>ex) SQuAD의 예시</li>\n</ul>\n<p><img src=\"https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png\"></p>\n<h3 id=\"SOTA-models-for-QA-task\"><a href=\"#SOTA-models-for-QA-task\" class=\"headerlink\" title=\"SOTA models for QA task\"></a>SOTA models for QA task</h3><h4 id=\"T5-Text-to-Text-Transfer\"><a href=\"#T5-Text-to-Text-Transfer\" class=\"headerlink\" title=\"T5 (Text to Text Transfer)\"></a>T5 (Text to Text Transfer)</h4><p>T5는 전이학습을 기반으로한 구글에서 개발한 Transformer 기반 Language Model이다.</p>\n<p>현 시점에서 QA task에서 가장 높은 성능을 보이는 SOTA 모델의 하나이다.</p>\n<p><img src=\"https://media.vlpt.us/images/yjinheon/post/2b661472-0508-4446-b808-95b7d2a5b4d2/Velog_1_12.png\"></p>\n<h4 id=\"전이학습-Transfer-Learning\"><a href=\"#전이학습-Transfer-Learning\" class=\"headerlink\" title=\"전이학습(Transfer Learning)\"></a>전이학습(Transfer Learning)</h4><ul>\n<li>전이학습은 이미 학습된 큰 데이터셋의 가중치를 가지고 와서 해결하고자 하는 다른 과제에 맞게 튜닝해서 사용하는 방법론이다.</li>\n<li>T5 paper에서는 하나의 언어모델을 비지도로 pre-train한 뒤 세부 task에 따라 지도학습으로 fine tuning하는 방식을 사용하였다.</li>\n</ul>\n<h4 id=\"XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding\"><a href=\"#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding\" class=\"headerlink\" title=\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h4><p>BERT의 Autoencoding method에 Autoregressive기법을 더해 성능을 개선시킨 언어모델.</p>\n<h4 id=\"Autoregressive-Pretrained-method\"><a href=\"#Autoregressive-Pretrained-method\" class=\"headerlink\" title=\"Autoregressive Pretrained method\"></a>Autoregressive Pretrained method</h4><ul>\n<li>AR(자기회귀) 방법론은 기존 BERT가 가지고 있는 masking을 보완하기 위해 사용되었다.</li>\n<li>AR은 BERT가 가지고 있는 문제점을 보완할 수 있지만 알고리즘 특성상 단방향 만을 고려한다는 문제점이 있다.</li>\n<li>BERT의 경우 AutoEncoding 과정에서 토큰이 독립적으로 예측되기 때문에 토큰 간 Dependency가 학습이 안되는 문제점이 있었다.</li>\n<li>XLNet에서는 permutation을 이용해 모든 가능한 sequence를 고려해서 (Factorization Order) AR 방법론을 적용하여 AR과 AutoEncoding 방법론을 모두 보완해 성능을 개선시켰다.</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">https://rajpurkar.github.io/SQuAD-explorer/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1910.10683v3.pdf\">Text to Text Transfer</a></li>\n<li><a href=\"https://arxiv.org/pdf/1906.08237v2.pdf\">XLNet: Generalized Autoregressive Pretraining<br>for Language Understanding</a></li>\n<li><a href=\"https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Metrics-for-QA\">QA task의 평가지표</a></li>\n<li><a href=\"https://yhdosu.github.io/2019/11/12/T5.html\">T5 paper설명</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h1 id=\"NLU\"><a href=\"#NLU\" class=\"headerlink\" title=\"NLU\"></a>NLU</h1><blockquote>\n<p>NLP의 하위 분야인 NLU를 소개하고 NLU 의 subtask 중 하나인 QA(Question Answering) 에 대해 정리</p>\n</blockquote>\n<p>: Natural Language Understanding<br>기계가 자연어에 대한 Synthetic과 Semantic Understanding을 보인다면 그 기계는 NLU를 수행하고 있다고 볼 수 있다.</p>\n<h2 id=\"Question-Answering-task\"><a href=\"#Question-Answering-task\" class=\"headerlink\" title=\"Question Answering(task)\"></a>Question Answering(task)</h2><p>특정 자연어 텍스트를 기계가 올바르게 이해하고 답변하는지 평가하는 Reading Comprehension의 일종이다.</p>\n<p>질문에 대해 기계가 답변하는 QA의 형태를 가지고 있다는 점에서 NLU task라고 볼 수 있다.</p>\n<p>QA task의 경우 질문에 대한 답이 Input인 지문안에 분명히 존재하기 때문에 평가지표로 Accuracy와 F1 Score를 사용할 수 있다.</p>\n<h4 id=\"대표적인-데이터셋\"><a href=\"#대표적인-데이터셋\" class=\"headerlink\" title=\"대표적인 데이터셋\"></a>대표적인 데이터셋</h4><h4 id=\"SQuAD\"><a href=\"#SQuAD\" class=\"headerlink\" title=\"SQuAD\"></a>SQuAD</h4><p>: Stanford Question Answering Dataset</p>\n<p>QA와 같은 Reading Comprehension 문제의 해결을 위해 스탠포스에서 개발한 대표 Benchmark.</p>\n<p>Input은 지문인 Context와 지문 내에서 답을 찾을 수 있는 Question으로 구성된다.</p>\n<p>SQuAD 데이터 셋을 통해 QA task를 수행할 경우 기계는 출력값으로 지문(Context) 내에 포함된 질문의 답의 시작과 끝의 인덱스를 반환해야 한다,</p>\n<ul>\n<li>ex) SQuAD의 예시</li>\n</ul>\n<p><img src=\"https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png\"></p>\n<h3 id=\"SOTA-models-for-QA-task\"><a href=\"#SOTA-models-for-QA-task\" class=\"headerlink\" title=\"SOTA models for QA task\"></a>SOTA models for QA task</h3><h4 id=\"T5-Text-to-Text-Transfer\"><a href=\"#T5-Text-to-Text-Transfer\" class=\"headerlink\" title=\"T5 (Text to Text Transfer)\"></a>T5 (Text to Text Transfer)</h4><p>T5는 전이학습을 기반으로한 구글에서 개발한 Transformer 기반 Language Model이다.</p>\n<p>현 시점에서 QA task에서 가장 높은 성능을 보이는 SOTA 모델의 하나이다.</p>\n<p><img src=\"https://media.vlpt.us/images/yjinheon/post/2b661472-0508-4446-b808-95b7d2a5b4d2/Velog_1_12.png\"></p>\n<h4 id=\"전이학습-Transfer-Learning\"><a href=\"#전이학습-Transfer-Learning\" class=\"headerlink\" title=\"전이학습(Transfer Learning)\"></a>전이학습(Transfer Learning)</h4><ul>\n<li>전이학습은 이미 학습된 큰 데이터셋의 가중치를 가지고 와서 해결하고자 하는 다른 과제에 맞게 튜닝해서 사용하는 방법론이다.</li>\n<li>T5 paper에서는 하나의 언어모델을 비지도로 pre-train한 뒤 세부 task에 따라 지도학습으로 fine tuning하는 방식을 사용하였다.</li>\n</ul>\n<h4 id=\"XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding\"><a href=\"#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding\" class=\"headerlink\" title=\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\"></a>XLNet: Generalized Autoregressive Pretraining for Language Understanding</h4><p>BERT의 Autoencoding method에 Autoregressive기법을 더해 성능을 개선시킨 언어모델.</p>\n<h4 id=\"Autoregressive-Pretrained-method\"><a href=\"#Autoregressive-Pretrained-method\" class=\"headerlink\" title=\"Autoregressive Pretrained method\"></a>Autoregressive Pretrained method</h4><ul>\n<li>AR(자기회귀) 방법론은 기존 BERT가 가지고 있는 masking을 보완하기 위해 사용되었다.</li>\n<li>AR은 BERT가 가지고 있는 문제점을 보완할 수 있지만 알고리즘 특성상 단방향 만을 고려한다는 문제점이 있다.</li>\n<li>BERT의 경우 AutoEncoding 과정에서 토큰이 독립적으로 예측되기 때문에 토큰 간 Dependency가 학습이 안되는 문제점이 있었다.</li>\n<li>XLNet에서는 permutation을 이용해 모든 가능한 sequence를 고려해서 (Factorization Order) AR 방법론을 적용하여 AR과 AutoEncoding 방법론을 모두 보완해 성능을 개선시켰다.</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">https://rajpurkar.github.io/SQuAD-explorer/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1910.10683v3.pdf\">Text to Text Transfer</a></li>\n<li><a href=\"https://arxiv.org/pdf/1906.08237v2.pdf\">XLNet: Generalized Autoregressive Pretraining<br>for Language Understanding</a></li>\n<li><a href=\"https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Metrics-for-QA\">QA task의 평가지표</a></li>\n<li><a href=\"https://yhdosu.github.io/2019/11/12/T5.html\">T5 paper설명</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[NLP]NLU & QA task","path":"2022/06/13/NLP-NLU/","eyeCatchImage":"https://www.researchgate.net/publication/326569892/figure/fig1/AS:651759777234944@1532403048781/An-example-from-SQuAD-dataset.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"NLP","tags":["NLP","NLU","QA"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[NLP]Word Embedding과 Text Classification","date":"2022-03-02T10:28:01.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n# NLP subtask에 대한 정리\n\n: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리\n\n## 1.Word Embedding\n\n### 1.1 Word Embedding에서 고려하는 task\n\nWord Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.\n\nEmbedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.\n\n**단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.**\n\n![](https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png)\n\n#### 1.1.1 차원의 저주\n\n단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.\n\n문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.\n\n이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.\n\n#### 1.1.2 Distribution Hypothesis\n\nDistribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.\n\nWord embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. \n\n따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.\n\n\n#### 1.1.3 Predictive Method\n\nWord Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.\n\n### 1.2 대표적인 데이터셋\n\n#### Words in Context\n\nWord in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.\n\n과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.\n\nWord in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.\n\n데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.\n\n![](https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png)\n\n### Contextual Embedding(SOTA Technique)\n\nWord Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.\n\n과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.\n\n이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.\n\n#### biLM(bidirectional Language Model) as function\n\nContextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.\n\nELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.\n\n여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.\n\n## Text Classification\n\n### Text Classification의 주요 task\n\nText Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.\n\n### 대표적인 데이터셋\n#### IMDB Movie Review\n\nIMDB에 게시된 영화 리뷰와 Positive/Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.\n\n### BERT(SOTA Technique)\n\nBERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.\n\n#### Transformer\n\n- Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.\n- 기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.\n- Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.\n- 문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.\n  - Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.\n- BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.\n\nBERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)\n\n단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.\n\n#### fine tuning\n\nTransformer와 함께 BERT의 핵심 컨셉중 하나로 *기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것*을 뜻한다.\n\n\n## References\n\n- https://machinelearningmastery.com/what-are-word-embeddings/\n- https://pilehvar.github.io/wic/\n- [WiC: the Word-in-Context Dataset](https://arxiv.org/pdf/1808.09121v3.pdf)\n- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)\n- https://paperswithcode.com/method/elmo\n- [Positional Encoding의 이해](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)\n\n\n","source":"_posts/NLP-wordembedding.md","raw":"---\ntitle: '[NLP]Word Embedding과 Text Classification'\ncategories:\n  - [NLP]\ntags:\n  - Deep Learning\n  - NLP\n  - WordEmbedding\ndate: 2022-03-02 19:28:01\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n# NLP subtask에 대한 정리\n\n: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리\n\n## 1.Word Embedding\n\n### 1.1 Word Embedding에서 고려하는 task\n\nWord Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.\n\nEmbedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.\n\n**단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.**\n\n![](https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png)\n\n#### 1.1.1 차원의 저주\n\n단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.\n\n문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.\n\n이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.\n\n#### 1.1.2 Distribution Hypothesis\n\nDistribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.\n\nWord embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. \n\n따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.\n\n\n#### 1.1.3 Predictive Method\n\nWord Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.\n\n### 1.2 대표적인 데이터셋\n\n#### Words in Context\n\nWord in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.\n\n과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.\n\nWord in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.\n\n데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.\n\n![](https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png)\n\n### Contextual Embedding(SOTA Technique)\n\nWord Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.\n\n과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.\n\n이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.\n\n#### biLM(bidirectional Language Model) as function\n\nContextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.\n\nELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.\n\n여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.\n\n## Text Classification\n\n### Text Classification의 주요 task\n\nText Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.\n\n### 대표적인 데이터셋\n#### IMDB Movie Review\n\nIMDB에 게시된 영화 리뷰와 Positive/Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.\n\n### BERT(SOTA Technique)\n\nBERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.\n\n#### Transformer\n\n- Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.\n- 기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.\n- Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.\n- 문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.\n  - Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.\n- BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.\n\nBERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)\n\n단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.\n\n#### fine tuning\n\nTransformer와 함께 BERT의 핵심 컨셉중 하나로 *기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것*을 뜻한다.\n\n\n## References\n\n- https://machinelearningmastery.com/what-are-word-embeddings/\n- https://pilehvar.github.io/wic/\n- [WiC: the Word-in-Context Dataset](https://arxiv.org/pdf/1808.09121v3.pdf)\n- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)\n- https://paperswithcode.com/method/elmo\n- [Positional Encoding의 이해](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)\n\n\n","slug":"NLP-wordembedding","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsce001yb36qhnqi5opn","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h1 id=\"NLP-subtask에-대한-정리\"><a href=\"#NLP-subtask에-대한-정리\" class=\"headerlink\" title=\"NLP subtask에 대한 정리\"></a>NLP subtask에 대한 정리</h1><p>: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리</p>\n<h2 id=\"1-Word-Embedding\"><a href=\"#1-Word-Embedding\" class=\"headerlink\" title=\"1.Word Embedding\"></a>1.Word Embedding</h2><h3 id=\"1-1-Word-Embedding에서-고려하는-task\"><a href=\"#1-1-Word-Embedding에서-고려하는-task\" class=\"headerlink\" title=\"1.1 Word Embedding에서 고려하는 task\"></a>1.1 Word Embedding에서 고려하는 task</h3><p>Word Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.</p>\n<p>Embedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.</p>\n<p><strong>단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.</strong></p>\n<p><img src=\"https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png\"></p>\n<h4 id=\"1-1-1-차원의-저주\"><a href=\"#1-1-1-차원의-저주\" class=\"headerlink\" title=\"1.1.1 차원의 저주\"></a>1.1.1 차원의 저주</h4><p>단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.</p>\n<p>문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.</p>\n<p>이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.</p>\n<h4 id=\"1-1-2-Distribution-Hypothesis\"><a href=\"#1-1-2-Distribution-Hypothesis\" class=\"headerlink\" title=\"1.1.2 Distribution Hypothesis\"></a>1.1.2 Distribution Hypothesis</h4><p>Distribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.</p>\n<p>Word embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. </p>\n<p>따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.</p>\n<h4 id=\"1-1-3-Predictive-Method\"><a href=\"#1-1-3-Predictive-Method\" class=\"headerlink\" title=\"1.1.3 Predictive Method\"></a>1.1.3 Predictive Method</h4><p>Word Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.</p>\n<h3 id=\"1-2-대표적인-데이터셋\"><a href=\"#1-2-대표적인-데이터셋\" class=\"headerlink\" title=\"1.2 대표적인 데이터셋\"></a>1.2 대표적인 데이터셋</h3><h4 id=\"Words-in-Context\"><a href=\"#Words-in-Context\" class=\"headerlink\" title=\"Words in Context\"></a>Words in Context</h4><p>Word in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.</p>\n<p>과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.</p>\n<p>Word in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.</p>\n<p>데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.</p>\n<p><img src=\"https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png\"></p>\n<h3 id=\"Contextual-Embedding-SOTA-Technique\"><a href=\"#Contextual-Embedding-SOTA-Technique\" class=\"headerlink\" title=\"Contextual Embedding(SOTA Technique)\"></a>Contextual Embedding(SOTA Technique)</h3><p>Word Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.</p>\n<p>과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.</p>\n<p>이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.</p>\n<h4 id=\"biLM-bidirectional-Language-Model-as-function\"><a href=\"#biLM-bidirectional-Language-Model-as-function\" class=\"headerlink\" title=\"biLM(bidirectional Language Model) as function\"></a>biLM(bidirectional Language Model) as function</h4><p>Contextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.</p>\n<p>ELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.</p>\n<p>여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.</p>\n<h2 id=\"Text-Classification\"><a href=\"#Text-Classification\" class=\"headerlink\" title=\"Text Classification\"></a>Text Classification</h2><h3 id=\"Text-Classification의-주요-task\"><a href=\"#Text-Classification의-주요-task\" class=\"headerlink\" title=\"Text Classification의 주요 task\"></a>Text Classification의 주요 task</h3><p>Text Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.</p>\n<h3 id=\"대표적인-데이터셋\"><a href=\"#대표적인-데이터셋\" class=\"headerlink\" title=\"대표적인 데이터셋\"></a>대표적인 데이터셋</h3><h4 id=\"IMDB-Movie-Review\"><a href=\"#IMDB-Movie-Review\" class=\"headerlink\" title=\"IMDB Movie Review\"></a>IMDB Movie Review</h4><p>IMDB에 게시된 영화 리뷰와 Positive&#x2F;Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.</p>\n<h3 id=\"BERT-SOTA-Technique\"><a href=\"#BERT-SOTA-Technique\" class=\"headerlink\" title=\"BERT(SOTA Technique)\"></a>BERT(SOTA Technique)</h3><p>BERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.</p>\n<h4 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h4><ul>\n<li>Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.</li>\n<li>기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.</li>\n<li>Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.</li>\n<li>문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.<ul>\n<li>Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.</li>\n</ul>\n</li>\n<li>BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.</li>\n</ul>\n<p>BERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)</p>\n<p>단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.</p>\n<h4 id=\"fine-tuning\"><a href=\"#fine-tuning\" class=\"headerlink\" title=\"fine tuning\"></a>fine tuning</h4><p>Transformer와 함께 BERT의 핵심 컨셉중 하나로 <em>기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것</em>을 뜻한다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://machinelearningmastery.com/what-are-word-embeddings/\">https://machinelearningmastery.com/what-are-word-embeddings/</a></li>\n<li><a href=\"https://pilehvar.github.io/wic/\">https://pilehvar.github.io/wic/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1808.09121v3.pdf\">WiC: the Word-in-Context Dataset</a></li>\n<li><a href=\"https://arxiv.org/pdf/1802.05365.pdf\">Deep contextualized word representations</a></li>\n<li><a href=\"https://paperswithcode.com/method/elmo\">https://paperswithcode.com/method/elmo</a></li>\n<li><a href=\"https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\">Positional Encoding의 이해</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h1 id=\"NLP-subtask에-대한-정리\"><a href=\"#NLP-subtask에-대한-정리\" class=\"headerlink\" title=\"NLP subtask에 대한 정리\"></a>NLP subtask에 대한 정리</h1><p>: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리</p>\n<h2 id=\"1-Word-Embedding\"><a href=\"#1-Word-Embedding\" class=\"headerlink\" title=\"1.Word Embedding\"></a>1.Word Embedding</h2><h3 id=\"1-1-Word-Embedding에서-고려하는-task\"><a href=\"#1-1-Word-Embedding에서-고려하는-task\" class=\"headerlink\" title=\"1.1 Word Embedding에서 고려하는 task\"></a>1.1 Word Embedding에서 고려하는 task</h3><p>Word Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.</p>\n<p>Embedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.</p>\n<p><strong>단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.</strong></p>\n<p><img src=\"https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png\"></p>\n<h4 id=\"1-1-1-차원의-저주\"><a href=\"#1-1-1-차원의-저주\" class=\"headerlink\" title=\"1.1.1 차원의 저주\"></a>1.1.1 차원의 저주</h4><p>단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.</p>\n<p>문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.</p>\n<p>이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.</p>\n<h4 id=\"1-1-2-Distribution-Hypothesis\"><a href=\"#1-1-2-Distribution-Hypothesis\" class=\"headerlink\" title=\"1.1.2 Distribution Hypothesis\"></a>1.1.2 Distribution Hypothesis</h4><p>Distribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.</p>\n<p>Word embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. </p>\n<p>따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.</p>\n<h4 id=\"1-1-3-Predictive-Method\"><a href=\"#1-1-3-Predictive-Method\" class=\"headerlink\" title=\"1.1.3 Predictive Method\"></a>1.1.3 Predictive Method</h4><p>Word Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.</p>\n<h3 id=\"1-2-대표적인-데이터셋\"><a href=\"#1-2-대표적인-데이터셋\" class=\"headerlink\" title=\"1.2 대표적인 데이터셋\"></a>1.2 대표적인 데이터셋</h3><h4 id=\"Words-in-Context\"><a href=\"#Words-in-Context\" class=\"headerlink\" title=\"Words in Context\"></a>Words in Context</h4><p>Word in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.</p>\n<p>과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.</p>\n<p>Word in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.</p>\n<p>데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.</p>\n<p><img src=\"https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png\"></p>\n<h3 id=\"Contextual-Embedding-SOTA-Technique\"><a href=\"#Contextual-Embedding-SOTA-Technique\" class=\"headerlink\" title=\"Contextual Embedding(SOTA Technique)\"></a>Contextual Embedding(SOTA Technique)</h3><p>Word Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.</p>\n<p>과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.</p>\n<p>이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.</p>\n<h4 id=\"biLM-bidirectional-Language-Model-as-function\"><a href=\"#biLM-bidirectional-Language-Model-as-function\" class=\"headerlink\" title=\"biLM(bidirectional Language Model) as function\"></a>biLM(bidirectional Language Model) as function</h4><p>Contextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.</p>\n<p>ELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.</p>\n<p>여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.</p>\n<h2 id=\"Text-Classification\"><a href=\"#Text-Classification\" class=\"headerlink\" title=\"Text Classification\"></a>Text Classification</h2><h3 id=\"Text-Classification의-주요-task\"><a href=\"#Text-Classification의-주요-task\" class=\"headerlink\" title=\"Text Classification의 주요 task\"></a>Text Classification의 주요 task</h3><p>Text Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.</p>\n<h3 id=\"대표적인-데이터셋\"><a href=\"#대표적인-데이터셋\" class=\"headerlink\" title=\"대표적인 데이터셋\"></a>대표적인 데이터셋</h3><h4 id=\"IMDB-Movie-Review\"><a href=\"#IMDB-Movie-Review\" class=\"headerlink\" title=\"IMDB Movie Review\"></a>IMDB Movie Review</h4><p>IMDB에 게시된 영화 리뷰와 Positive&#x2F;Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.</p>\n<h3 id=\"BERT-SOTA-Technique\"><a href=\"#BERT-SOTA-Technique\" class=\"headerlink\" title=\"BERT(SOTA Technique)\"></a>BERT(SOTA Technique)</h3><p>BERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.</p>\n<h4 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h4><ul>\n<li>Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.</li>\n<li>기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.</li>\n<li>Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.</li>\n<li>문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.<ul>\n<li>Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.</li>\n</ul>\n</li>\n<li>BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.</li>\n</ul>\n<p>BERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)</p>\n<p>단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.</p>\n<h4 id=\"fine-tuning\"><a href=\"#fine-tuning\" class=\"headerlink\" title=\"fine tuning\"></a>fine tuning</h4><p>Transformer와 함께 BERT의 핵심 컨셉중 하나로 <em>기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것</em>을 뜻한다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://machinelearningmastery.com/what-are-word-embeddings/\">https://machinelearningmastery.com/what-are-word-embeddings/</a></li>\n<li><a href=\"https://pilehvar.github.io/wic/\">https://pilehvar.github.io/wic/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1808.09121v3.pdf\">WiC: the Word-in-Context Dataset</a></li>\n<li><a href=\"https://arxiv.org/pdf/1802.05365.pdf\">Deep contextualized word representations</a></li>\n<li><a href=\"https://paperswithcode.com/method/elmo\">https://paperswithcode.com/method/elmo</a></li>\n<li><a href=\"https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\">Positional Encoding의 이해</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[NLP]Word Embedding과 Text Classification","path":"2022/03/02/NLP-wordembedding/","eyeCatchImage":"https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-03-02T10:28:01.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-03-02T10:28:01.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"NLP","tags":["NLP","Deep Learning","WordEmbedding"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Data Transformation]Feature Scaling의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Feature Scaling\n\n<!--\n\n진짜 렉카\n\nstandar\n\n- https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n\n조건수의 의미\n- https://datascienceschool.net/03%20machine%20learning/04.03%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81.html\n\n\n조건수가 크면 약간의 오차만 있어도 해가 전혀 다른 값을 가진다. 따라서 조건수가 크면 회귀분석을 사용한 예측값도 오차가 커지게 된다.\n\n회귀분석에서 조건수가 커지는 경우는 크게 두 가지가 있다.\n\n1) 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.\n\n2) 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.\n\n\n다음의 경우\n\n- 데이터가 편포된 경우\n- 독립변수와 예측치가 비선형인 경우\n\n\n\nhttps://deepinsight.tistory.com/113\n\n-->\n\n### Feature Scaling에서 overflow와 underflow의 의미\n**no blog**\n\nA common pre-processing step is to normalize/rescale inputs so that they are not too high or low.\n\nHowever, even on normalized inputs, overflows and underflows can occur:\n\nUnderflow: Joint probability distribution often involves multiplying small individual probabilities. Many probabilistic algorithms involve multiplying probabilities of individual data points that leads to underflow. Example : Suppose you have 1000 data points, where the probability of each is < 1 lets say around 0.8, we have 0.8 ^ 1000 = 1.2302319e-97 which is close to 0. This is underflow.\nA common way to combat this is to work in the log probability space: http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point\n\nOverflow: Imagine you have a deep network, error gradients an keep accumulating and often become  vary large gradients. This results in an overflow where the values of the gradients become NAN. Weight regularization and gradient clipping are some common ways of dealing with this problem.\n\n### Scaling을 하는 이유\n---\n\nfeature scaling을 하는 가장 직관적인 이유는 **분석 단위(크기)를 맟줘주기 위해서이다.** 모델 학습시 각 feature의 크기를 맟줘 줌으로서 학습 시 특정 feature의 영향이 너무 커지는 것을 방지할 수 있다.\n\n**scaling은 공분산행렬의 조건수(condition number)를 감소시킨다.**\n\n조건수는 행렬에서 eigen value와 가장 작은 eigen value의 비율을 말한다.\n\n$$\\text{condition number} = \\dfrac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}$$\n\n조건수가 커질수록 약간의 오차에 대해서도 회귀방정식의 해의 오차가 민감하게 변하기 때문에 회귀식의 에러가 커진다.\n\n조건수의 eigen value는 분산을 바탕으로 구해지기 때문에 scaling을 통해 condition number를 줄일 수 있다.\n\nfeature가 scaling이 되지 않을 경우 조건수가 커져서 에러가 증폭된다.\n\n**scaling은 데이터의 크기와 범위를 제한해서 Gradient Explode나 Gradient Vanishing을 제한한다.**\n\n- feature마다 \n- 데이터의 범의를 줄여서 Neural network의 최적화 과정에서 수렴속도를 빠르게 한다.  \n- Neural network model에서 sigmoid 활성화 함수의 `saturation`(포화) 문제를 완화한다.(Gradient Vanishing) \n\n**정리하면 Scaling의 핵심은 데이터가 유사한 범위를 가지도록 데이터의 범위를 제한한다는 것이다.**\n\n1. 데이터의 범위를 제한해서 조건수를 낮춰 예측오차에 덜 민감해게끔 만든다.\n2. 데이터의 범위를 제한해서 기울기 폭발,소실 문제를 완화한다.\n\n### 어떤 경우에 Feature Scaling을 고려해야 하는가?\n---\n\n**거리 기반 모델의 경우 Scaling이 매우 중요하다**\n\n- KNN,K-means clausturing : 유클리디안 거리를 기반으로 데이터 유사성을 결정하기 때문에 Scaling의 영향을 크게 받는다.\n- SVM : margin(거리)를 최대화하는 것이 최적화 문제에 포함된 \n- PCA : 알고리즘의 목적 자체가 분산이 가장 큰 방향을 가지는 고유벡터를 찾는 것이기 때문에 Scaling의 영향을 크게 받는다. 따라서 반드시 사전에 모든 수치형 변수들의 Scaling을 해줘야 한다.\n\n\n**Gradient Descent 기반 모델의 경우**\n\n신경망을 기반으로 하는 모델의 경우 loss function을 최소화 하는 방식으로 최적화를 진행한다.\n이는 각 feature의 범위와 크기가 다를 경우 feature 마다 다른 step size를 적용해야 한다는 것을 뜻한다.\n따라서 scaling를 통해 범위를 맞춰줄 경우 gradient descent의 수렴이 보다 빠르게 이루어진다.\n\n\n**tree기반 알고리즘의 경우 Scaling에 따라 성능에 영향받지 않는다.**\n\n대표적으로 tree기반 알고리즘에 해당하는 CART,RandomForest 등은 학습의 대상이 거리와 관련이 없기 때문에 (학습의 대상이 일종의 분기점) Scaling을 해줄 필요가 없다. Scaling을 해주는 경우가 가끔 있지만 이는 Scailng을 요구하는 다른 알고리즘과의 비교를 위해서이다.\n\n\n**언제 Scaling을 하는가?**\n\n**반드시 학습데이터와 검증 데이터를 나눈 이후에 Scaling을 시행한다.**\n\n이는 data leakage로 인해 test data의 정보가 모델링에 포함 될 수 있기 때문이다.\n\n- train test split 이후의 Scaling 예시\n\n```python\nnormalizer = preprocessing.Normalizer().fit(X_train)\n\n\nX_train = normalizer.transform(X_train) \nX_test = normalizer.transform(X_test) \n```\n\n\n<!--\n\n언제 scaling을 하는가?\n\n렉카\n\nhttps://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\n\nNormalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n\nThis is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.\n\n[Precision thanks to Neil's comment] When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. Do not recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.\n\n\nGradient Descent의 의미\n\nGradient descent is an iterative optimisation algorithm that takes us to the minimum of a function.\nMachine learning algorithms like linear regression and logistic regression rely on gradient descent to minimise their loss functions or in other words, to reduce the error between the predicted values and the actual values.\nHaving features with varying degrees of magnitude and range will cause different step sizes for each feature. Therefore, to ensure that gradient descent converges more smoothly and quickly, we need to scale our features so that they share a similar scale.\nCheck out this video where Andrew Ng explains the gradient descent algorithm in more detail.\n\n\nData leakage와 싸우기 위한 5가지 팁!\n\n\n\n-일시적 제거 : 관찰이 일어난 시간보다 사실이나 관찰에 대해 배운 시간에 초점을 맞추어 관심 이벤트 직전의 모든 데이터를 제거하십시오.\n\n-소음 추가. 입력 데이터에 임의의 노이즈를 추가하여 누출 가능성이있는 변수의 영향을 부드럽게합니다.\n\n-누출 변수를 제거하십시오. 간단한 규칙 기반 모델을 평가하려면 OneR에 계좌 번호 및 ID 등과 같은 변수를 사용하여 이러한 변수가 누출인지 확인하고 누락 된 경우이를 제거하십시오. 변수가 누설 된 것으로 의심되면 제거하는 것을 고려하십시오.\n\n\n-파이프 라인을 사용하십시오. R의 caret 패키지 및 scikit-learn의 파이프 라인과 같은 교차 검증 폴드 내에서 일련의 데이터 준비 단계를 수행 할 수있는 파이프 라인 아키텍처를 많이 사용합니다.\n\n-데이터를 따로 보유하십시요. validation데이터 셋을 따로 보유한후 마지막에 최종적으로 모델을 평가하는데 사용하면 됩니다.\n\n\n-->\n\n\n\n### Scaler의 종류\n---\n\nScaling 대표적인 기법\n\n- **Normalization(정규화)**\n\n보통 값을 0,1 사이로 고정시킨다.\n\n- **Standardization(표준화)**\n\n#### MinMaxScaler\n\n- 대부분의 Scaler가 그런 것 처럼 이상치에 민감하다.\n- 데이터가 가우시안 분포가 아니고 사이즈가 작을 경우 유용하다.\n\n```python\n# 직접 구현\n\ndef minmax(x):\n    return (x-min(x))/(max(x)-min(x))\n\n\n# sklearn 에서 제공하는 MinMazScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train_new = scaler.fit_transform(X_train)\n\n```\n\n#### Robust Scaler\n\n- 중앙값과 IQR을 사용해 이상치의 영향을 줄인 Scaler\n\n\n```python\n# sklearn 구현\nfrom sklearn.preprocessing import RobustScaler\nrbs = RobustScaler()\nrbs.fit_transform(X_train)\n```\n\n#### StandardScaler\n\n- 기본적으로 정규분포를 가정한다.\n- 평균을 0, 분산은 1인 분포로 feature를 변환\n- 이상치의 영향이 크기 때문에 이상치가 많을 경우 사전에 제거해주거나 다른 Scaler를 고려해야함\n- 데이터의 최소 최대를 모르는 경우 사용\n\n\n```python\n# 직접구현\nimport numpy as np\n\ndef get_standard(x):\n    mean = np.mean(x)\n    rescale = x-mean/np.std(x)\n\n    return rescale\n\n# slearn\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_new = scaler.fit_transform(X_train)\n\n```\n\n #### Normalizer\n\n- column-wise가 아니라 row wise로 정규화를 적용\n- 최적화 과정에서 gradient exlosion이나 gradient vanishing을 막기 위해 사용\n\n**Reference & Annotaion**\n\n\n- [condition number의 의미](https://www.quora.com/What-are-condition-numbers-and-poor-conditioning-How-are-they-related-to-deep-learning)\n- [importance of feature scaling](https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1)\n- `saturation`은 sigmoid 활성화 함수의 특정구간에서 gradient가 0에 가까워지는 것이다.(Gradient Vanishing 문제)\n- https://www.youtube.com/watch?v=F6GSRDoB-Cg&t=74s\n","source":"_posts/Preprocessing-dt-Scaler.md","raw":"---\ntitle: '[Data Transformation]Feature Scaling의 이해'\ncategories:\n  - Preprocessing\ndate:\nupdated:\ntags:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## Feature Scaling\n\n<!--\n\n진짜 렉카\n\nstandar\n\n- https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n\n조건수의 의미\n- https://datascienceschool.net/03%20machine%20learning/04.03%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81.html\n\n\n조건수가 크면 약간의 오차만 있어도 해가 전혀 다른 값을 가진다. 따라서 조건수가 크면 회귀분석을 사용한 예측값도 오차가 커지게 된다.\n\n회귀분석에서 조건수가 커지는 경우는 크게 두 가지가 있다.\n\n1) 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.\n\n2) 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.\n\n\n다음의 경우\n\n- 데이터가 편포된 경우\n- 독립변수와 예측치가 비선형인 경우\n\n\n\nhttps://deepinsight.tistory.com/113\n\n-->\n\n### Feature Scaling에서 overflow와 underflow의 의미\n**no blog**\n\nA common pre-processing step is to normalize/rescale inputs so that they are not too high or low.\n\nHowever, even on normalized inputs, overflows and underflows can occur:\n\nUnderflow: Joint probability distribution often involves multiplying small individual probabilities. Many probabilistic algorithms involve multiplying probabilities of individual data points that leads to underflow. Example : Suppose you have 1000 data points, where the probability of each is < 1 lets say around 0.8, we have 0.8 ^ 1000 = 1.2302319e-97 which is close to 0. This is underflow.\nA common way to combat this is to work in the log probability space: http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point\n\nOverflow: Imagine you have a deep network, error gradients an keep accumulating and often become  vary large gradients. This results in an overflow where the values of the gradients become NAN. Weight regularization and gradient clipping are some common ways of dealing with this problem.\n\n### Scaling을 하는 이유\n---\n\nfeature scaling을 하는 가장 직관적인 이유는 **분석 단위(크기)를 맟줘주기 위해서이다.** 모델 학습시 각 feature의 크기를 맟줘 줌으로서 학습 시 특정 feature의 영향이 너무 커지는 것을 방지할 수 있다.\n\n**scaling은 공분산행렬의 조건수(condition number)를 감소시킨다.**\n\n조건수는 행렬에서 eigen value와 가장 작은 eigen value의 비율을 말한다.\n\n$$\\text{condition number} = \\dfrac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}$$\n\n조건수가 커질수록 약간의 오차에 대해서도 회귀방정식의 해의 오차가 민감하게 변하기 때문에 회귀식의 에러가 커진다.\n\n조건수의 eigen value는 분산을 바탕으로 구해지기 때문에 scaling을 통해 condition number를 줄일 수 있다.\n\nfeature가 scaling이 되지 않을 경우 조건수가 커져서 에러가 증폭된다.\n\n**scaling은 데이터의 크기와 범위를 제한해서 Gradient Explode나 Gradient Vanishing을 제한한다.**\n\n- feature마다 \n- 데이터의 범의를 줄여서 Neural network의 최적화 과정에서 수렴속도를 빠르게 한다.  \n- Neural network model에서 sigmoid 활성화 함수의 `saturation`(포화) 문제를 완화한다.(Gradient Vanishing) \n\n**정리하면 Scaling의 핵심은 데이터가 유사한 범위를 가지도록 데이터의 범위를 제한한다는 것이다.**\n\n1. 데이터의 범위를 제한해서 조건수를 낮춰 예측오차에 덜 민감해게끔 만든다.\n2. 데이터의 범위를 제한해서 기울기 폭발,소실 문제를 완화한다.\n\n### 어떤 경우에 Feature Scaling을 고려해야 하는가?\n---\n\n**거리 기반 모델의 경우 Scaling이 매우 중요하다**\n\n- KNN,K-means clausturing : 유클리디안 거리를 기반으로 데이터 유사성을 결정하기 때문에 Scaling의 영향을 크게 받는다.\n- SVM : margin(거리)를 최대화하는 것이 최적화 문제에 포함된 \n- PCA : 알고리즘의 목적 자체가 분산이 가장 큰 방향을 가지는 고유벡터를 찾는 것이기 때문에 Scaling의 영향을 크게 받는다. 따라서 반드시 사전에 모든 수치형 변수들의 Scaling을 해줘야 한다.\n\n\n**Gradient Descent 기반 모델의 경우**\n\n신경망을 기반으로 하는 모델의 경우 loss function을 최소화 하는 방식으로 최적화를 진행한다.\n이는 각 feature의 범위와 크기가 다를 경우 feature 마다 다른 step size를 적용해야 한다는 것을 뜻한다.\n따라서 scaling를 통해 범위를 맞춰줄 경우 gradient descent의 수렴이 보다 빠르게 이루어진다.\n\n\n**tree기반 알고리즘의 경우 Scaling에 따라 성능에 영향받지 않는다.**\n\n대표적으로 tree기반 알고리즘에 해당하는 CART,RandomForest 등은 학습의 대상이 거리와 관련이 없기 때문에 (학습의 대상이 일종의 분기점) Scaling을 해줄 필요가 없다. Scaling을 해주는 경우가 가끔 있지만 이는 Scailng을 요구하는 다른 알고리즘과의 비교를 위해서이다.\n\n\n**언제 Scaling을 하는가?**\n\n**반드시 학습데이터와 검증 데이터를 나눈 이후에 Scaling을 시행한다.**\n\n이는 data leakage로 인해 test data의 정보가 모델링에 포함 될 수 있기 때문이다.\n\n- train test split 이후의 Scaling 예시\n\n```python\nnormalizer = preprocessing.Normalizer().fit(X_train)\n\n\nX_train = normalizer.transform(X_train) \nX_test = normalizer.transform(X_test) \n```\n\n\n<!--\n\n언제 scaling을 하는가?\n\n렉카\n\nhttps://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\n\nNormalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n\nThis is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.\n\n[Precision thanks to Neil's comment] When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. Do not recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.\n\n\nGradient Descent의 의미\n\nGradient descent is an iterative optimisation algorithm that takes us to the minimum of a function.\nMachine learning algorithms like linear regression and logistic regression rely on gradient descent to minimise their loss functions or in other words, to reduce the error between the predicted values and the actual values.\nHaving features with varying degrees of magnitude and range will cause different step sizes for each feature. Therefore, to ensure that gradient descent converges more smoothly and quickly, we need to scale our features so that they share a similar scale.\nCheck out this video where Andrew Ng explains the gradient descent algorithm in more detail.\n\n\nData leakage와 싸우기 위한 5가지 팁!\n\n\n\n-일시적 제거 : 관찰이 일어난 시간보다 사실이나 관찰에 대해 배운 시간에 초점을 맞추어 관심 이벤트 직전의 모든 데이터를 제거하십시오.\n\n-소음 추가. 입력 데이터에 임의의 노이즈를 추가하여 누출 가능성이있는 변수의 영향을 부드럽게합니다.\n\n-누출 변수를 제거하십시오. 간단한 규칙 기반 모델을 평가하려면 OneR에 계좌 번호 및 ID 등과 같은 변수를 사용하여 이러한 변수가 누출인지 확인하고 누락 된 경우이를 제거하십시오. 변수가 누설 된 것으로 의심되면 제거하는 것을 고려하십시오.\n\n\n-파이프 라인을 사용하십시오. R의 caret 패키지 및 scikit-learn의 파이프 라인과 같은 교차 검증 폴드 내에서 일련의 데이터 준비 단계를 수행 할 수있는 파이프 라인 아키텍처를 많이 사용합니다.\n\n-데이터를 따로 보유하십시요. validation데이터 셋을 따로 보유한후 마지막에 최종적으로 모델을 평가하는데 사용하면 됩니다.\n\n\n-->\n\n\n\n### Scaler의 종류\n---\n\nScaling 대표적인 기법\n\n- **Normalization(정규화)**\n\n보통 값을 0,1 사이로 고정시킨다.\n\n- **Standardization(표준화)**\n\n#### MinMaxScaler\n\n- 대부분의 Scaler가 그런 것 처럼 이상치에 민감하다.\n- 데이터가 가우시안 분포가 아니고 사이즈가 작을 경우 유용하다.\n\n```python\n# 직접 구현\n\ndef minmax(x):\n    return (x-min(x))/(max(x)-min(x))\n\n\n# sklearn 에서 제공하는 MinMazScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train_new = scaler.fit_transform(X_train)\n\n```\n\n#### Robust Scaler\n\n- 중앙값과 IQR을 사용해 이상치의 영향을 줄인 Scaler\n\n\n```python\n# sklearn 구현\nfrom sklearn.preprocessing import RobustScaler\nrbs = RobustScaler()\nrbs.fit_transform(X_train)\n```\n\n#### StandardScaler\n\n- 기본적으로 정규분포를 가정한다.\n- 평균을 0, 분산은 1인 분포로 feature를 변환\n- 이상치의 영향이 크기 때문에 이상치가 많을 경우 사전에 제거해주거나 다른 Scaler를 고려해야함\n- 데이터의 최소 최대를 모르는 경우 사용\n\n\n```python\n# 직접구현\nimport numpy as np\n\ndef get_standard(x):\n    mean = np.mean(x)\n    rescale = x-mean/np.std(x)\n\n    return rescale\n\n# slearn\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_new = scaler.fit_transform(X_train)\n\n```\n\n #### Normalizer\n\n- column-wise가 아니라 row wise로 정규화를 적용\n- 최적화 과정에서 gradient exlosion이나 gradient vanishing을 막기 위해 사용\n\n**Reference & Annotaion**\n\n\n- [condition number의 의미](https://www.quora.com/What-are-condition-numbers-and-poor-conditioning-How-are-they-related-to-deep-learning)\n- [importance of feature scaling](https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1)\n- `saturation`은 sigmoid 활성화 함수의 특정구간에서 gradient가 0에 가까워지는 것이다.(Gradient Vanishing 문제)\n- https://www.youtube.com/watch?v=F6GSRDoB-Cg&t=74s\n","slug":"Preprocessing-dt-Scaler","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsce0022b36q8k6c2zvr","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h2><!--\n\n진짜 렉카\n\nstandar\n\n- https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n\n조건수의 의미\n- https://datascienceschool.net/03%20machine%20learning/04.03%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81.html\n\n\n조건수가 크면 약간의 오차만 있어도 해가 전혀 다른 값을 가진다. 따라서 조건수가 크면 회귀분석을 사용한 예측값도 오차가 커지게 된다.\n\n회귀분석에서 조건수가 커지는 경우는 크게 두 가지가 있다.\n\n1) 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.\n\n2) 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.\n\n\n다음의 경우\n\n- 데이터가 편포된 경우\n- 독립변수와 예측치가 비선형인 경우\n\n\n\nhttps://deepinsight.tistory.com/113\n\n-->\n\n<h3 id=\"Feature-Scaling에서-overflow와-underflow의-의미\"><a href=\"#Feature-Scaling에서-overflow와-underflow의-의미\" class=\"headerlink\" title=\"Feature Scaling에서 overflow와 underflow의 의미\"></a>Feature Scaling에서 overflow와 underflow의 의미</h3><p><strong>no blog</strong></p>\n<p>A common pre-processing step is to normalize&#x2F;rescale inputs so that they are not too high or low.</p>\n<p>However, even on normalized inputs, overflows and underflows can occur:</p>\n<p>Underflow: Joint probability distribution often involves multiplying small individual probabilities. Many probabilistic algorithms involve multiplying probabilities of individual data points that leads to underflow. Example : Suppose you have 1000 data points, where the probability of each is &lt; 1 lets say around 0.8, we have 0.8 ^ 1000 &#x3D; 1.2302319e-97 which is close to 0. This is underflow.<br>A common way to combat this is to work in the log probability space: <a href=\"http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point\">http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point</a></p>\n<p>Overflow: Imagine you have a deep network, error gradients an keep accumulating and often become  vary large gradients. This results in an overflow where the values of the gradients become NAN. Weight regularization and gradient clipping are some common ways of dealing with this problem.</p>\n<h3 id=\"Scaling을-하는-이유\"><a href=\"#Scaling을-하는-이유\" class=\"headerlink\" title=\"Scaling을 하는 이유\"></a>Scaling을 하는 이유</h3><hr>\n<p>feature scaling을 하는 가장 직관적인 이유는 <strong>분석 단위(크기)를 맟줘주기 위해서이다.</strong> 모델 학습시 각 feature의 크기를 맟줘 줌으로서 학습 시 특정 feature의 영향이 너무 커지는 것을 방지할 수 있다.</p>\n<p><strong>scaling은 공분산행렬의 조건수(condition number)를 감소시킨다.</strong></p>\n<p>조건수는 행렬에서 eigen value와 가장 작은 eigen value의 비율을 말한다.</p>\n<p>$$\\text{condition number} &#x3D; \\dfrac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}$$</p>\n<p>조건수가 커질수록 약간의 오차에 대해서도 회귀방정식의 해의 오차가 민감하게 변하기 때문에 회귀식의 에러가 커진다.</p>\n<p>조건수의 eigen value는 분산을 바탕으로 구해지기 때문에 scaling을 통해 condition number를 줄일 수 있다.</p>\n<p>feature가 scaling이 되지 않을 경우 조건수가 커져서 에러가 증폭된다.</p>\n<p><strong>scaling은 데이터의 크기와 범위를 제한해서 Gradient Explode나 Gradient Vanishing을 제한한다.</strong></p>\n<ul>\n<li>feature마다 </li>\n<li>데이터의 범의를 줄여서 Neural network의 최적화 과정에서 수렴속도를 빠르게 한다.  </li>\n<li>Neural network model에서 sigmoid 활성화 함수의 <code>saturation</code>(포화) 문제를 완화한다.(Gradient Vanishing)</li>\n</ul>\n<p><strong>정리하면 Scaling의 핵심은 데이터가 유사한 범위를 가지도록 데이터의 범위를 제한한다는 것이다.</strong></p>\n<ol>\n<li>데이터의 범위를 제한해서 조건수를 낮춰 예측오차에 덜 민감해게끔 만든다.</li>\n<li>데이터의 범위를 제한해서 기울기 폭발,소실 문제를 완화한다.</li>\n</ol>\n<h3 id=\"어떤-경우에-Feature-Scaling을-고려해야-하는가\"><a href=\"#어떤-경우에-Feature-Scaling을-고려해야-하는가\" class=\"headerlink\" title=\"어떤 경우에 Feature Scaling을 고려해야 하는가?\"></a>어떤 경우에 Feature Scaling을 고려해야 하는가?</h3><hr>\n<p><strong>거리 기반 모델의 경우 Scaling이 매우 중요하다</strong></p>\n<ul>\n<li>KNN,K-means clausturing : 유클리디안 거리를 기반으로 데이터 유사성을 결정하기 때문에 Scaling의 영향을 크게 받는다.</li>\n<li>SVM : margin(거리)를 최대화하는 것이 최적화 문제에 포함된 </li>\n<li>PCA : 알고리즘의 목적 자체가 분산이 가장 큰 방향을 가지는 고유벡터를 찾는 것이기 때문에 Scaling의 영향을 크게 받는다. 따라서 반드시 사전에 모든 수치형 변수들의 Scaling을 해줘야 한다.</li>\n</ul>\n<p><strong>Gradient Descent 기반 모델의 경우</strong></p>\n<p>신경망을 기반으로 하는 모델의 경우 loss function을 최소화 하는 방식으로 최적화를 진행한다.<br>이는 각 feature의 범위와 크기가 다를 경우 feature 마다 다른 step size를 적용해야 한다는 것을 뜻한다.<br>따라서 scaling를 통해 범위를 맞춰줄 경우 gradient descent의 수렴이 보다 빠르게 이루어진다.</p>\n<p><strong>tree기반 알고리즘의 경우 Scaling에 따라 성능에 영향받지 않는다.</strong></p>\n<p>대표적으로 tree기반 알고리즘에 해당하는 CART,RandomForest 등은 학습의 대상이 거리와 관련이 없기 때문에 (학습의 대상이 일종의 분기점) Scaling을 해줄 필요가 없다. Scaling을 해주는 경우가 가끔 있지만 이는 Scailng을 요구하는 다른 알고리즘과의 비교를 위해서이다.</p>\n<p><strong>언제 Scaling을 하는가?</strong></p>\n<p><strong>반드시 학습데이터와 검증 데이터를 나눈 이후에 Scaling을 시행한다.</strong></p>\n<p>이는 data leakage로 인해 test data의 정보가 모델링에 포함 될 수 있기 때문이다.</p>\n<ul>\n<li>train test split 이후의 Scaling 예시</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">normalizer = preprocessing.Normalizer().fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = normalizer.transform(X_train) </span><br><span class=\"line\">X_test = normalizer.transform(X_test) </span><br></pre></td></tr></table></figure>\n\n\n<!--\n\n언제 scaling을 하는가?\n\n렉카\n\nhttps://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\n\nNormalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n\nThis is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.\n\n[Precision thanks to Neil's comment] When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. Do not recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.\n\n\nGradient Descent의 의미\n\nGradient descent is an iterative optimisation algorithm that takes us to the minimum of a function.\nMachine learning algorithms like linear regression and logistic regression rely on gradient descent to minimise their loss functions or in other words, to reduce the error between the predicted values and the actual values.\nHaving features with varying degrees of magnitude and range will cause different step sizes for each feature. Therefore, to ensure that gradient descent converges more smoothly and quickly, we need to scale our features so that they share a similar scale.\nCheck out this video where Andrew Ng explains the gradient descent algorithm in more detail.\n\n\nData leakage와 싸우기 위한 5가지 팁!\n\n\n\n-일시적 제거 : 관찰이 일어난 시간보다 사실이나 관찰에 대해 배운 시간에 초점을 맞추어 관심 이벤트 직전의 모든 데이터를 제거하십시오.\n\n-소음 추가. 입력 데이터에 임의의 노이즈를 추가하여 누출 가능성이있는 변수의 영향을 부드럽게합니다.\n\n-누출 변수를 제거하십시오. 간단한 규칙 기반 모델을 평가하려면 OneR에 계좌 번호 및 ID 등과 같은 변수를 사용하여 이러한 변수가 누출인지 확인하고 누락 된 경우이를 제거하십시오. 변수가 누설 된 것으로 의심되면 제거하는 것을 고려하십시오.\n\n\n-파이프 라인을 사용하십시오. R의 caret 패키지 및 scikit-learn의 파이프 라인과 같은 교차 검증 폴드 내에서 일련의 데이터 준비 단계를 수행 할 수있는 파이프 라인 아키텍처를 많이 사용합니다.\n\n-데이터를 따로 보유하십시요. validation데이터 셋을 따로 보유한후 마지막에 최종적으로 모델을 평가하는데 사용하면 됩니다.\n\n\n-->\n\n\n\n<h3 id=\"Scaler의-종류\"><a href=\"#Scaler의-종류\" class=\"headerlink\" title=\"Scaler의 종류\"></a>Scaler의 종류</h3><hr>\n<p>Scaling 대표적인 기법</p>\n<ul>\n<li><strong>Normalization(정규화)</strong></li>\n</ul>\n<p>보통 값을 0,1 사이로 고정시킨다.</p>\n<ul>\n<li><strong>Standardization(표준화)</strong></li>\n</ul>\n<h4 id=\"MinMaxScaler\"><a href=\"#MinMaxScaler\" class=\"headerlink\" title=\"MinMaxScaler\"></a>MinMaxScaler</h4><ul>\n<li>대부분의 Scaler가 그런 것 처럼 이상치에 민감하다.</li>\n<li>데이터가 가우시안 분포가 아니고 사이즈가 작을 경우 유용하다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 직접 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">minmax</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x-<span class=\"built_in\">min</span>(x))/(<span class=\"built_in\">max</span>(x)-<span class=\"built_in\">min</span>(x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sklearn 에서 제공하는 MinMazScaler</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = MinMaxScaler()</span><br><span class=\"line\"></span><br><span class=\"line\">X_train_new = scaler.fit_transform(X_train)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Robust-Scaler\"><a href=\"#Robust-Scaler\" class=\"headerlink\" title=\"Robust Scaler\"></a>Robust Scaler</h4><ul>\n<li>중앙값과 IQR을 사용해 이상치의 영향을 줄인 Scaler</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sklearn 구현</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> RobustScaler</span><br><span class=\"line\">rbs = RobustScaler()</span><br><span class=\"line\">rbs.fit_transform(X_train)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"StandardScaler\"><a href=\"#StandardScaler\" class=\"headerlink\" title=\"StandardScaler\"></a>StandardScaler</h4><ul>\n<li>기본적으로 정규분포를 가정한다.</li>\n<li>평균을 0, 분산은 1인 분포로 feature를 변환</li>\n<li>이상치의 영향이 크기 때문에 이상치가 많을 경우 사전에 제거해주거나 다른 Scaler를 고려해야함</li>\n<li>데이터의 최소 최대를 모르는 경우 사용</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 직접구현</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_standard</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    mean = np.mean(x)</span><br><span class=\"line\">    rescale = x-mean/np.std(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> rescale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># slearn</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\"></span><br><span class=\"line\">X_train_new = scaler.fit_transform(X_train)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Normalizer\"><a href=\"#Normalizer\" class=\"headerlink\" title=\"Normalizer\"></a>Normalizer</h4><ul>\n<li>column-wise가 아니라 row wise로 정규화를 적용</li>\n<li>최적화 과정에서 gradient exlosion이나 gradient vanishing을 막기 위해 사용</li>\n</ul>\n<p><strong>Reference &amp; Annotaion</strong></p>\n<ul>\n<li><a href=\"https://www.quora.com/What-are-condition-numbers-and-poor-conditioning-How-are-they-related-to-deep-learning\">condition number의 의미</a></li>\n<li><a href=\"https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1\">importance of feature scaling</a></li>\n<li><code>saturation</code>은 sigmoid 활성화 함수의 특정구간에서 gradient가 0에 가까워지는 것이다.(Gradient Vanishing 문제)</li>\n<li><a href=\"https://www.youtube.com/watch?v=F6GSRDoB-Cg&amp;t=74s\">https://www.youtube.com/watch?v=F6GSRDoB-Cg&amp;t=74s</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"Feature-Scaling\"><a href=\"#Feature-Scaling\" class=\"headerlink\" title=\"Feature Scaling\"></a>Feature Scaling</h2><!--\n\n진짜 렉카\n\nstandar\n\n- https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n\n조건수의 의미\n- https://datascienceschool.net/03%20machine%20learning/04.03%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81.html\n\n\n조건수가 크면 약간의 오차만 있어도 해가 전혀 다른 값을 가진다. 따라서 조건수가 크면 회귀분석을 사용한 예측값도 오차가 커지게 된다.\n\n회귀분석에서 조건수가 커지는 경우는 크게 두 가지가 있다.\n\n1) 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.\n\n2) 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.\n\n\n다음의 경우\n\n- 데이터가 편포된 경우\n- 독립변수와 예측치가 비선형인 경우\n\n\n\nhttps://deepinsight.tistory.com/113\n\n-->\n\n<h3 id=\"Feature-Scaling에서-overflow와-underflow의-의미\"><a href=\"#Feature-Scaling에서-overflow와-underflow의-의미\" class=\"headerlink\" title=\"Feature Scaling에서 overflow와 underflow의 의미\"></a>Feature Scaling에서 overflow와 underflow의 의미</h3><p><strong>no blog</strong></p>\n<p>A common pre-processing step is to normalize&#x2F;rescale inputs so that they are not too high or low.</p>\n<p>However, even on normalized inputs, overflows and underflows can occur:</p>\n<p>Underflow: Joint probability distribution often involves multiplying small individual probabilities. Many probabilistic algorithms involve multiplying probabilities of individual data points that leads to underflow. Example : Suppose you have 1000 data points, where the probability of each is &lt; 1 lets say around 0.8, we have 0.8 ^ 1000 &#x3D; 1.2302319e-97 which is close to 0. This is underflow.<br>A common way to combat this is to work in the log probability space: <a href=\"http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point\">http://blog.smola.org/post/987977550/log-probabilities-semirings-and-floating-point</a></p>\n<p>Overflow: Imagine you have a deep network, error gradients an keep accumulating and often become  vary large gradients. This results in an overflow where the values of the gradients become NAN. Weight regularization and gradient clipping are some common ways of dealing with this problem.</p>\n<h3 id=\"Scaling을-하는-이유\"><a href=\"#Scaling을-하는-이유\" class=\"headerlink\" title=\"Scaling을 하는 이유\"></a>Scaling을 하는 이유</h3><hr>\n<p>feature scaling을 하는 가장 직관적인 이유는 <strong>분석 단위(크기)를 맟줘주기 위해서이다.</strong> 모델 학습시 각 feature의 크기를 맟줘 줌으로서 학습 시 특정 feature의 영향이 너무 커지는 것을 방지할 수 있다.</p>\n<p><strong>scaling은 공분산행렬의 조건수(condition number)를 감소시킨다.</strong></p>\n<p>조건수는 행렬에서 eigen value와 가장 작은 eigen value의 비율을 말한다.</p>\n<p>$$\\text{condition number} &#x3D; \\dfrac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}}$$</p>\n<p>조건수가 커질수록 약간의 오차에 대해서도 회귀방정식의 해의 오차가 민감하게 변하기 때문에 회귀식의 에러가 커진다.</p>\n<p>조건수의 eigen value는 분산을 바탕으로 구해지기 때문에 scaling을 통해 condition number를 줄일 수 있다.</p>\n<p>feature가 scaling이 되지 않을 경우 조건수가 커져서 에러가 증폭된다.</p>\n<p><strong>scaling은 데이터의 크기와 범위를 제한해서 Gradient Explode나 Gradient Vanishing을 제한한다.</strong></p>\n<ul>\n<li>feature마다 </li>\n<li>데이터의 범의를 줄여서 Neural network의 최적화 과정에서 수렴속도를 빠르게 한다.  </li>\n<li>Neural network model에서 sigmoid 활성화 함수의 <code>saturation</code>(포화) 문제를 완화한다.(Gradient Vanishing)</li>\n</ul>\n<p><strong>정리하면 Scaling의 핵심은 데이터가 유사한 범위를 가지도록 데이터의 범위를 제한한다는 것이다.</strong></p>\n<ol>\n<li>데이터의 범위를 제한해서 조건수를 낮춰 예측오차에 덜 민감해게끔 만든다.</li>\n<li>데이터의 범위를 제한해서 기울기 폭발,소실 문제를 완화한다.</li>\n</ol>\n<h3 id=\"어떤-경우에-Feature-Scaling을-고려해야-하는가\"><a href=\"#어떤-경우에-Feature-Scaling을-고려해야-하는가\" class=\"headerlink\" title=\"어떤 경우에 Feature Scaling을 고려해야 하는가?\"></a>어떤 경우에 Feature Scaling을 고려해야 하는가?</h3><hr>\n<p><strong>거리 기반 모델의 경우 Scaling이 매우 중요하다</strong></p>\n<ul>\n<li>KNN,K-means clausturing : 유클리디안 거리를 기반으로 데이터 유사성을 결정하기 때문에 Scaling의 영향을 크게 받는다.</li>\n<li>SVM : margin(거리)를 최대화하는 것이 최적화 문제에 포함된 </li>\n<li>PCA : 알고리즘의 목적 자체가 분산이 가장 큰 방향을 가지는 고유벡터를 찾는 것이기 때문에 Scaling의 영향을 크게 받는다. 따라서 반드시 사전에 모든 수치형 변수들의 Scaling을 해줘야 한다.</li>\n</ul>\n<p><strong>Gradient Descent 기반 모델의 경우</strong></p>\n<p>신경망을 기반으로 하는 모델의 경우 loss function을 최소화 하는 방식으로 최적화를 진행한다.<br>이는 각 feature의 범위와 크기가 다를 경우 feature 마다 다른 step size를 적용해야 한다는 것을 뜻한다.<br>따라서 scaling를 통해 범위를 맞춰줄 경우 gradient descent의 수렴이 보다 빠르게 이루어진다.</p>\n<p><strong>tree기반 알고리즘의 경우 Scaling에 따라 성능에 영향받지 않는다.</strong></p>\n<p>대표적으로 tree기반 알고리즘에 해당하는 CART,RandomForest 등은 학습의 대상이 거리와 관련이 없기 때문에 (학습의 대상이 일종의 분기점) Scaling을 해줄 필요가 없다. Scaling을 해주는 경우가 가끔 있지만 이는 Scailng을 요구하는 다른 알고리즘과의 비교를 위해서이다.</p>\n<p><strong>언제 Scaling을 하는가?</strong></p>\n<p><strong>반드시 학습데이터와 검증 데이터를 나눈 이후에 Scaling을 시행한다.</strong></p>\n<p>이는 data leakage로 인해 test data의 정보가 모델링에 포함 될 수 있기 때문이다.</p>\n<ul>\n<li>train test split 이후의 Scaling 예시</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">normalizer = preprocessing.Normalizer().fit(X_train)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train = normalizer.transform(X_train) </span><br><span class=\"line\">X_test = normalizer.transform(X_test) </span><br></pre></td></tr></table></figure>\n\n\n<!--\n\n언제 scaling을 하는가?\n\n렉카\n\nhttps://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n\nhttps://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n\n\nNormalization across instances should be done after splitting the data between training and test set, using only the data from the training set.\n\nThis is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.\n\n[Precision thanks to Neil's comment] When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. Do not recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.\n\n\nGradient Descent의 의미\n\nGradient descent is an iterative optimisation algorithm that takes us to the minimum of a function.\nMachine learning algorithms like linear regression and logistic regression rely on gradient descent to minimise their loss functions or in other words, to reduce the error between the predicted values and the actual values.\nHaving features with varying degrees of magnitude and range will cause different step sizes for each feature. Therefore, to ensure that gradient descent converges more smoothly and quickly, we need to scale our features so that they share a similar scale.\nCheck out this video where Andrew Ng explains the gradient descent algorithm in more detail.\n\n\nData leakage와 싸우기 위한 5가지 팁!\n\n\n\n-일시적 제거 : 관찰이 일어난 시간보다 사실이나 관찰에 대해 배운 시간에 초점을 맞추어 관심 이벤트 직전의 모든 데이터를 제거하십시오.\n\n-소음 추가. 입력 데이터에 임의의 노이즈를 추가하여 누출 가능성이있는 변수의 영향을 부드럽게합니다.\n\n-누출 변수를 제거하십시오. 간단한 규칙 기반 모델을 평가하려면 OneR에 계좌 번호 및 ID 등과 같은 변수를 사용하여 이러한 변수가 누출인지 확인하고 누락 된 경우이를 제거하십시오. 변수가 누설 된 것으로 의심되면 제거하는 것을 고려하십시오.\n\n\n-파이프 라인을 사용하십시오. R의 caret 패키지 및 scikit-learn의 파이프 라인과 같은 교차 검증 폴드 내에서 일련의 데이터 준비 단계를 수행 할 수있는 파이프 라인 아키텍처를 많이 사용합니다.\n\n-데이터를 따로 보유하십시요. validation데이터 셋을 따로 보유한후 마지막에 최종적으로 모델을 평가하는데 사용하면 됩니다.\n\n\n-->\n\n\n\n<h3 id=\"Scaler의-종류\"><a href=\"#Scaler의-종류\" class=\"headerlink\" title=\"Scaler의 종류\"></a>Scaler의 종류</h3><hr>\n<p>Scaling 대표적인 기법</p>\n<ul>\n<li><strong>Normalization(정규화)</strong></li>\n</ul>\n<p>보통 값을 0,1 사이로 고정시킨다.</p>\n<ul>\n<li><strong>Standardization(표준화)</strong></li>\n</ul>\n<h4 id=\"MinMaxScaler\"><a href=\"#MinMaxScaler\" class=\"headerlink\" title=\"MinMaxScaler\"></a>MinMaxScaler</h4><ul>\n<li>대부분의 Scaler가 그런 것 처럼 이상치에 민감하다.</li>\n<li>데이터가 가우시안 분포가 아니고 사이즈가 작을 경우 유용하다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 직접 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">minmax</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x-<span class=\"built_in\">min</span>(x))/(<span class=\"built_in\">max</span>(x)-<span class=\"built_in\">min</span>(x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># sklearn 에서 제공하는 MinMazScaler</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> MinMaxScaler</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = MinMaxScaler()</span><br><span class=\"line\"></span><br><span class=\"line\">X_train_new = scaler.fit_transform(X_train)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Robust-Scaler\"><a href=\"#Robust-Scaler\" class=\"headerlink\" title=\"Robust Scaler\"></a>Robust Scaler</h4><ul>\n<li>중앙값과 IQR을 사용해 이상치의 영향을 줄인 Scaler</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sklearn 구현</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> RobustScaler</span><br><span class=\"line\">rbs = RobustScaler()</span><br><span class=\"line\">rbs.fit_transform(X_train)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"StandardScaler\"><a href=\"#StandardScaler\" class=\"headerlink\" title=\"StandardScaler\"></a>StandardScaler</h4><ul>\n<li>기본적으로 정규분포를 가정한다.</li>\n<li>평균을 0, 분산은 1인 분포로 feature를 변환</li>\n<li>이상치의 영향이 크기 때문에 이상치가 많을 경우 사전에 제거해주거나 다른 Scaler를 고려해야함</li>\n<li>데이터의 최소 최대를 모르는 경우 사용</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 직접구현</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_standard</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    mean = np.mean(x)</span><br><span class=\"line\">    rescale = x-mean/np.std(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> rescale</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># slearn</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\"></span><br><span class=\"line\">X_train_new = scaler.fit_transform(X_train)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Normalizer\"><a href=\"#Normalizer\" class=\"headerlink\" title=\"Normalizer\"></a>Normalizer</h4><ul>\n<li>column-wise가 아니라 row wise로 정규화를 적용</li>\n<li>최적화 과정에서 gradient exlosion이나 gradient vanishing을 막기 위해 사용</li>\n</ul>\n<p><strong>Reference &amp; Annotaion</strong></p>\n<ul>\n<li><a href=\"https://www.quora.com/What-are-condition-numbers-and-poor-conditioning-How-are-they-related-to-deep-learning\">condition number의 의미</a></li>\n<li><a href=\"https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1\">importance of feature scaling</a></li>\n<li><code>saturation</code>은 sigmoid 활성화 함수의 특정구간에서 gradient가 0에 가까워지는 것이다.(Gradient Vanishing 문제)</li>\n<li><a href=\"https://www.youtube.com/watch?v=F6GSRDoB-Cg&amp;t=74s\">https://www.youtube.com/watch?v=F6GSRDoB-Cg&amp;t=74s</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Data Transformation]Feature Scaling의 이해","path":"2022/06/13/Preprocessing-dt-Scaler/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]numpy 연산과 활용법","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## numpy\n\n**numpy 기능들**\n\n- 벡터 배열상에서 데이터 가공, 정제, 부분집합 필터링, 변형 및 기타연산\n- 정렬, unique 탐색, 집합연산같은 일반적인 배열처리 알고리즘\n\n\nnumpy의 중요한 특징은 **파이썬 반복문을 사용하지 않고** 대용량 배열에 대한 복잡한 연산이 가능하다는 것이다.\n\n기본적으로 C랑 포트란 기반으로 짜여졌기 때문에 같은 연산이라면 pandas랑 비교도 안되게 빠르게 수행할 수 있다.\n\n---\n\n### numpy basics\n\n- 배열 생성\n```python\nIn [5]: import numpy as np\n   ...: arr = np.arange(0,10) # 0에서 9까지 의 \n   ...:\n   ...: arr.reshape(2,5) # 배열 형태 변환\nOut[5]:\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\nIn [6]: import numpy as np\n   ...: arr = np.arange(0,10)\n   ...:\n   ...: arr=arr.reshape(2,5)\n\n```\n- 배열 형태 확인(ndim,shape,len)\n\n```python\n\nIn [7]: arr.ndim # 차원 수\nOut[7]: 2\n\nIn [8]: arr.shape # 모양\nOut[8]: (2, 5)\n\nIn [9]: len(arr)\nOut[9]: 10\n\n```\n\n- 배열 데이터 타입 확인(dtype)\n\n```python\nIn [15]: arr_2 = np.random.rand(5)\n\nIn [16]: arr_2\nOut[16]: array([0.55155657, 0.32745746, 0.92681611, 0.04614794, 0.17832697])\n\nIn [17]: arr_2.dtype\nOut[17]: dtype('float64')\n\nIn [18]: arr_3 = np.array([5,6,7],dtype = np.float64)\n\nIn [19]: arr_3.dtype\nOut[19]: dtype('float64')\n\n\n```\n\n- 배열 데이터 타입 변환(astype)\n\n```python\n\nIn [24]: arr_2=arr_2.astype(np.int64)\n\nIn [25]: arr_2.dtype\nOut[25]: dtype('int64')\n\nIn [26]: arr_3=arr_3.astype(np.int64)\n\nIn [27]: arr_3.dtype\nOut[27]: dtype('int64')\n\n```\n\n\n\n\n**배열 생성함수들**\n\n- `array` : 입력데이터를 다차원 배열로 변환.\n- `arange` : `range`와 동일하지만 다차원 배열을 반환.\n- `np.empty(a)` ,`np.empty_like(M)` : 0 이나 1로 값을 초기화하지 않은 배열을 반환.\n- `np.ones(a)` : a 크기의 1으로 채워진 배열을 반환 \n- `np.ones_like(M)` M 배열의 사이즈와 같은 1으로 채워진 배열을 반환\n- `np.zeros(a)` : a 크기의 0으로 채워진 배열을 반환 \n- `np.zeros_like(M)` M 배열의 사이즈와 같은 0으로 채워진 배열을 반환\n- **`np.full` :인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.(자주쓴다.)**\n```python\n# np full 용법\n\n```\n\n\n- `np.full_like` : 인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.\n- `eye` , `identity`: N * N 크기의 단위행렬생성\n- `np.random.rand(n)` : n 크기 난수 배열 생성\n\n\n\n### numpy indexing\n\n- 기본 파이썬 list indexing과 유사하지만 차원이 복잡해지면 어려워진다.\n\n#### 기초 슬라이싱과 인덱싱\n\n- 기본적으로 list의 그것과 다를 건 없다.\n- `i:j:k` 형태로 인덱싱한다.\n    + i는 starting index \n    + j는 stopping index(j-1 까지 슬라이싱된다.)\n    + k는 step\n\n```python\n\nIn [1]: import numpy as np\n\nIn [2]: x = np.array(range(10))\n\nIn [3]: x[1:7:2]\nOut[3]: array([1, 3, 5])\n\nIn [4]: x[-2:10]\nOut[4]: array([8, 9])\n\nIn [5]: x[-3:3:-1]\nOut[5]: array([7, 6, 5, 4])\n\nIn [6]: x[5:]\nOut[6]: array([5, 6, 7, 8, 9])\n\n\n```\n\n- 인덱스 리스트를 통해 쉽게 배열의 값에 접근할 수 있다.\n\n```python\n\nIn [50]: arr_2d = np.arange(1,10).reshape(3,3)\n\nIn [51]: arr_2d\nOut[51]:\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\nIn [52]: arr_2d[0][2]\nOut[52]: 3\n\n# 보통 두 번째 방법을 많이 사용한다.\n\nIn [53]: arr_2d[0,2]\nOut[53]: 3\n\n```\n\n- 다차원 슬라이싱하기\n\n```python\n\nIn [54]: arr_2d[:2,1:]\nOut[54]:\narray([[2, 3],\n       [5, 6]])\n\n```\n\n\n\n- 좀 복잡한 형태의 다차원 슬라이싱\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg)\n\n\n```python\n\n# 다차원 배열 인덱싱 예시\n [[0  1  2  3  4  5] \n  [6 7 8 9 10 11]\n  [12 13 14 15 16 17]\n  [18 19 20 21 22 23]\n  [24 25 26 27 28 29]\n  [30 31 32 33 34 35]]\n\na[0, 3:5]  =  [3 4]\n\na[4:, 4:] = [[28 29],\n             [34 35]]\n\na[:, 2] =  [2 8 14 20 26 32]\n\na[2:;2, ::2] = [[12 14 16],\n                [24 26 28]]\n\n```\n\n- `:` 연산자를 통해  `:` 가 위치하는 축의 모든 값에 접근할 수 있다.\n- 배열 자체에 `[:]` 를 사용할 경우 배열의 모든 값이 할당된다.\n    + **기본적으로 데이터가 복사되지 않는다.**\n    + 데이터를 복사해야 할 경우 `copy` 함수를 따로 사용한다.\n\n```python\nIn [44]: arr\nOut[44]:\narray([[0.23061655, 0.86734388, 0.27967631],\n       [0.63734555, 0.47048728, 0.04833744],\n       [0.99362969, 0.87636748, 0.59988875]])\n\nIn [45]: arr[:,1]\nOut[45]: array([0.86734388, 0.47048728, 0.87636748])\n\nIn [46]: arr[:,1]\nOut[46]: array([0.86734388, 0.47048728, 0.87636748])\n\nIn [47]: arr[:]\nOut[47]:\narray([[0.23061655, 0.86734388, 0.27967631],\n       [0.63734555, 0.47048728, 0.04833744],\n       [0.99362969, 0.87636748, 0.59988875]])\n```\n\n- **배열의 일부는 원본배열의 View 이기 때문에 파이썬 `list` 와 달리 배열의 일부에 대한 변경은 그대로 원본배열에 반영된다.**\n\n```python\nIn [28]: arr_new = np.arange(10)\n\nIn [29]: arr_new[4:7] = 10\n\nIn [30]: arr_new\nOut[30]: array([ 0,  1,  2,  3, 10, 10, 10,  7,  8,  9])\n```\n\n\n#### boolen indexing\n\n- 실질적으로 가장 자주쓰는 인덱싱이다.\n\n```python\n\nIn [62]: temp=np.random.rand(3,3)\n\nIn [63]: temp[temp>0.5]\nOut[63]:\narray([0.77402793, 0.59064775, 0.67170741, 0.51967736, 0.75161734,\n       0.98559447])\n\n\n```\n\n\n#### fancy indexing\n\n- 인덱싱과 슬라이싱의 차이는 입력된 범위의 값을 가져오느냐 연속된 값들을 가져오느냐의 차이밖에 없다.\n\n```python\nIn [56]: array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    ...:\n    ...: \n    ...: array[[0, 2], :3]\nOut[56]:\narray([[1, 2, 3],\n       [7, 8, 9]])\n```\n\n\n\n### numpy 배열 변형하기(array transformation)\n\n#### Transpose\n\n```python\n i = np.transpose(b) Permute array dimensions\n i.T Permute array dimensions\n ```\n #### Changing Array Shape\n \n```python\nb.ravel() #Flatten the array\ng.reshape(3,-2) #Reshape, but don’t change data\nb.flatten() # rabel() 과 같지만 배열의 copy를 생\n\n\n``` \n#### Adding/Removing Elements\n \n```python\n h.resize((2,6)) Return a new array with shape (2,6)\n np.append(h,g) Append items to an array\n np.insert(a, 1, 5) Insert items in an array\n np.delete(a,[1]) Delete items from an array\n``` \n\n#### Combine array\n\n```python\n\nIn [11]: arr_1 = np.arange(1,5)\n\nIn [12]: arr_2 = np.arange(6,10)\n\nIn [13]: np.concatenate((arr_1,arr_2),axis=0)\nOut[13]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [14]: np.vstack((arr_1,arr_2))\nOut[14]:\narray([[1, 2, 3, 4],\n       [6, 7, 8, 9]])\n\nIn [15]: np.r_[arr_1,arr_2]\nOut[15]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [16]: np.hstack((arr_1,arr_2))\nOut[16]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [17]: np.c_[arr_1,arr_2]\nOut[17]:\narray([[1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n```\n\n### Split array\n\n```python\n\nIn [19]: np.hsplit(arr_1,2)\nOut[19]: [array([1, 2]), array([3, 4])]\n\n\nIn [24]: arr_3 = np.arange(1,10).reshape(3,3)\n\n# 행 단위 split\n\nIn [25]: np.vsplit(arr_3,3)\nOut[25]: [array([[1, 2, 3]]), array([[4, 5, 6]]), array([[7, 8, 9]])]\n\n```\n\n### numpy method\n\n#### np.where\n\n- `np.where(조건,if true 값,else 값)` 방식으로 사용한다.\n- 기본적으로 조건에 기반해 새로운 배열을 생성한다.\n- **logical statement를 vectorize한다.**\n\n\n```python\nOut[63]: df\n   A   B   C     D\n0  9  14  10  0.24\n1  8   2  17  0.56\n2  3  18  16  0.12\n3  3   4  16  0.88\n4  9   8  16  0.61\n5  7   3  17  0.44\n\n\n\nIn [64]: df[\"E\"] = np.where((df[\"B\"] > 10) & (df[\"C\"] > 10), 1, 0)\n    ...: df\nOut[64]:\n   A   B   C     D  E\n0  9  14  10  0.24  0\n1  8   2  17  0.56  0\n2  3  18  16  0.12  1\n3  3   4  16  0.88  0\n4  9   8  16  0.61  0\n5  7   3  17  0.44  0\n\n\n```\n\n#### np.select\n\n- `np.where`의 multiple condition 버전이다.\n- 2개 이상의 조건을 한번에 처리해야 할경우 pandas를 사용하는 것 보다 `np.select`를 활용해 한번에 처리하는 것이 낫다.\n\n![](https://i.imgur.com/Z3XHteT.png)\n\n```python\n# np select 예시\nIn [65]: conditions = [\n    ...:   (df[\"B\"] >= 10) & (df[\"A\"] == 0),\n    ...:   (df[\"B\"] >= 10) & (df[\"A\"] == 8)\n    ...: ]\n    ...: values = [1, 2]\n    ...: df[\"F\"] = np.select(conditions, values, default=0)\n    ...: df\n    ...:\nOut[65]:\n   A   B   C     D  E  F\n0  9  14  10  0.24  0  0\n1  8   2  17  0.56  0  0\n2  3  18  16  0.12  1  0\n3  3   4  16  0.88  0  0\n4  9   8  16  0.61  0  0\n5  7   3  17  0.44  0  0\n```\n\n#### np.log\n\n- 자연로그를 리턴한다.\n  - np.log(np.e) 는 1을 리턴한다.\n- 데이터 정규화시 사용.\n\n```python\nnp.log([1, np.e, np.e**2, 0])\narray([  0.,   1.,   2., -Inf])\n\n```\n\n#### np.sort(배열 정렬하기)\n\n- `np.sort(M)` 로 배열을 정렬한다.\n    + M.sort() 는 배열 자체를 정렬한 결과를 리턴하지만 `np.sort(M)`은 배열의 복사본을 정렬해 리턴한다,\n\n```python\nIn [39]: arr_1d = np.random.randint(0, 10, 10)\n    ...:\n    ...: arr_2d = np.random.randint(0, 10, (3, 4))\n    ...:\n    ...: print(arr_1d)\n    ...: print(arr_2d)\n[2 0 1 8 5 2 8 0 3 9]\n[[8 4 3 1]\n [0 4 6 7]\n [2 1 0 5]]\n\nIn [40]: print(np.sort(arr_1d))\n[0 0 1 2 2 3 5 8 8 9]\n\n\n```\n\n-  `np.sort(M)[::-1]` : 역순 정렬  \n\n```python\n\nIn [41]: print(np.sort(arr_1d)[::-1])\n[9 8 8 5 3 2 2 1 0 0]\n\n```\n\n\n\n- 행과 열 기준으로 정렬이 가능하다.\n    + `np.sort(x, axis=1)` : 열 기준\n    + `np.sort(x, axis=0)` : 행 기준\n\n```python\nIn [42]: print(np.sort(arr_2d, axis=0))\n    ...: print(np.sort(arr_2d, axis=1))\n[[0 1 0 1]\n [2 4 3 5]\n [8 4 6 7]]\n[[1 3 4 8]\n [0 4 6 7]\n [0 1 2 5]]\n```\n\n\n#### np.pad\n\n- 배열을 일정한 고정길이로 만들기 위해 특정한 값으로 채우는 함수.\n\n```python\nZ = np.ones((5,5))\nZ.pad(pad_width=1, \n      mode='constant', # 특정한 값을 지정해서 패딩할 경우\n      constant_values=0) # 값 지정\n\nZ      \n[[0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0.]]\n\n```\n\n- 2차원 배열 패딩\n\n```python\nIn [16]: m = np.arange(1,9).reshape(2,4)\n\nIn [17]: m\nOut[17]:\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n## 행과 열 모두 2개씩 0으로 패딩 \nIn [20]: np.pad(m,((2,2),(2,2)),'constant',constant_values =0)\nOut[20]:\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 2, 3, 4, 0, 0],\n       [0, 0, 5, 6, 7, 8, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]])\n\n\nIn [21]: np.pad(m,((1,2),(2,3)),'constant',constant_values =0)\nOut[21]:\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 2, 3, 4, 0, 0, 0],\n       [0, 0, 5, 6, 7, 8, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n\n```\n\n\n\n\n### numpy broadcasting\n\n\n- numpy의 연산은 기본적으로 같은 크기의 배열간의 연산을 전제한다.\n- 하지만 특정 조건을 만족했을 때 numpy는 자동적으로 크기가 다른 배열간의 연산을 수행하기도 하는데 이를 `broadcasting`이라 한다.\n- `broadcasting` 연산이 성립되기 위한 다음의 3가지 규칙이 존재한다.\n    + **규칙 1: 두 배열의 차원 수가 다를 경우, 크기가 작은 배열의 모양은 맨 앞(왼쪽)에 패딩됨**\n    + **규칙 2: 두 배열의 모양이 임의의 차원에서 일치하지 않으면 해당 차원에서 모양이 1인 배열은 다른 모양과 일치하도록 확장됨**\n    + **규칙 3: 어떤 차원에서든 크기가 일치하지 않고 둘 다 1과 같지 않으면 오류가 발생.**\n\n- **단순히 한쪽의 크기를 맞춰서 연산이 가능하게끔 만드는 것이라고 생각하면 이해하기 쉽다.**\n\n![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)\n\n- `a` 의 차원이 더 작기 때문에 규식 1,2 에 따라 연산시 `a` 가 padding 되고 확장됨.\n\n```python\n\nIn [5]: M = np.ones((2, 3))\n   ...: a = np.arange(3)\n\nIn [6]: print(M.shape)\n(2, 3)\n\nIn [7]: print(a.shape)\n(3,)\n\nIn [8]: M + a\nOut[8]:\narray([[1., 2., 3.],\n       [1., 2., 3.]])\n\nIn [9]: a.shape\nOut[9]: (3,)\n\n```\n\n**References**\n\n- [Numpy 공식문서](https://numpy.org/doc/stable/index.html)\n- [numpy array transfrormation](https://www.datacamp.com/community/blog/python-numpy-cheat-sheet)\n- [numpy indexing](https://www.geeksforgeeks.org/numpy-indexing/)\n- [numpy broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)\n- [numpy padding](https://sparrow.dev/numpy-pad/)\n- [numpy select](https://towardsdatascience.com/3-numpy-functions-to-facilitate-data-analysis-with-pandas-b1ad342a569)\n","source":"_posts/Preprocessing-numpy-basics.md","raw":"---\ntitle: '[Python]numpy 연산과 활용법'\ncategories:\n  - - Preprocessing\ntags:\n  - numpy\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## numpy\n\n**numpy 기능들**\n\n- 벡터 배열상에서 데이터 가공, 정제, 부분집합 필터링, 변형 및 기타연산\n- 정렬, unique 탐색, 집합연산같은 일반적인 배열처리 알고리즘\n\n\nnumpy의 중요한 특징은 **파이썬 반복문을 사용하지 않고** 대용량 배열에 대한 복잡한 연산이 가능하다는 것이다.\n\n기본적으로 C랑 포트란 기반으로 짜여졌기 때문에 같은 연산이라면 pandas랑 비교도 안되게 빠르게 수행할 수 있다.\n\n---\n\n### numpy basics\n\n- 배열 생성\n```python\nIn [5]: import numpy as np\n   ...: arr = np.arange(0,10) # 0에서 9까지 의 \n   ...:\n   ...: arr.reshape(2,5) # 배열 형태 변환\nOut[5]:\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\nIn [6]: import numpy as np\n   ...: arr = np.arange(0,10)\n   ...:\n   ...: arr=arr.reshape(2,5)\n\n```\n- 배열 형태 확인(ndim,shape,len)\n\n```python\n\nIn [7]: arr.ndim # 차원 수\nOut[7]: 2\n\nIn [8]: arr.shape # 모양\nOut[8]: (2, 5)\n\nIn [9]: len(arr)\nOut[9]: 10\n\n```\n\n- 배열 데이터 타입 확인(dtype)\n\n```python\nIn [15]: arr_2 = np.random.rand(5)\n\nIn [16]: arr_2\nOut[16]: array([0.55155657, 0.32745746, 0.92681611, 0.04614794, 0.17832697])\n\nIn [17]: arr_2.dtype\nOut[17]: dtype('float64')\n\nIn [18]: arr_3 = np.array([5,6,7],dtype = np.float64)\n\nIn [19]: arr_3.dtype\nOut[19]: dtype('float64')\n\n\n```\n\n- 배열 데이터 타입 변환(astype)\n\n```python\n\nIn [24]: arr_2=arr_2.astype(np.int64)\n\nIn [25]: arr_2.dtype\nOut[25]: dtype('int64')\n\nIn [26]: arr_3=arr_3.astype(np.int64)\n\nIn [27]: arr_3.dtype\nOut[27]: dtype('int64')\n\n```\n\n\n\n\n**배열 생성함수들**\n\n- `array` : 입력데이터를 다차원 배열로 변환.\n- `arange` : `range`와 동일하지만 다차원 배열을 반환.\n- `np.empty(a)` ,`np.empty_like(M)` : 0 이나 1로 값을 초기화하지 않은 배열을 반환.\n- `np.ones(a)` : a 크기의 1으로 채워진 배열을 반환 \n- `np.ones_like(M)` M 배열의 사이즈와 같은 1으로 채워진 배열을 반환\n- `np.zeros(a)` : a 크기의 0으로 채워진 배열을 반환 \n- `np.zeros_like(M)` M 배열의 사이즈와 같은 0으로 채워진 배열을 반환\n- **`np.full` :인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.(자주쓴다.)**\n```python\n# np full 용법\n\n```\n\n\n- `np.full_like` : 인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.\n- `eye` , `identity`: N * N 크기의 단위행렬생성\n- `np.random.rand(n)` : n 크기 난수 배열 생성\n\n\n\n### numpy indexing\n\n- 기본 파이썬 list indexing과 유사하지만 차원이 복잡해지면 어려워진다.\n\n#### 기초 슬라이싱과 인덱싱\n\n- 기본적으로 list의 그것과 다를 건 없다.\n- `i:j:k` 형태로 인덱싱한다.\n    + i는 starting index \n    + j는 stopping index(j-1 까지 슬라이싱된다.)\n    + k는 step\n\n```python\n\nIn [1]: import numpy as np\n\nIn [2]: x = np.array(range(10))\n\nIn [3]: x[1:7:2]\nOut[3]: array([1, 3, 5])\n\nIn [4]: x[-2:10]\nOut[4]: array([8, 9])\n\nIn [5]: x[-3:3:-1]\nOut[5]: array([7, 6, 5, 4])\n\nIn [6]: x[5:]\nOut[6]: array([5, 6, 7, 8, 9])\n\n\n```\n\n- 인덱스 리스트를 통해 쉽게 배열의 값에 접근할 수 있다.\n\n```python\n\nIn [50]: arr_2d = np.arange(1,10).reshape(3,3)\n\nIn [51]: arr_2d\nOut[51]:\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\nIn [52]: arr_2d[0][2]\nOut[52]: 3\n\n# 보통 두 번째 방법을 많이 사용한다.\n\nIn [53]: arr_2d[0,2]\nOut[53]: 3\n\n```\n\n- 다차원 슬라이싱하기\n\n```python\n\nIn [54]: arr_2d[:2,1:]\nOut[54]:\narray([[2, 3],\n       [5, 6]])\n\n```\n\n\n\n- 좀 복잡한 형태의 다차원 슬라이싱\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg)\n\n\n```python\n\n# 다차원 배열 인덱싱 예시\n [[0  1  2  3  4  5] \n  [6 7 8 9 10 11]\n  [12 13 14 15 16 17]\n  [18 19 20 21 22 23]\n  [24 25 26 27 28 29]\n  [30 31 32 33 34 35]]\n\na[0, 3:5]  =  [3 4]\n\na[4:, 4:] = [[28 29],\n             [34 35]]\n\na[:, 2] =  [2 8 14 20 26 32]\n\na[2:;2, ::2] = [[12 14 16],\n                [24 26 28]]\n\n```\n\n- `:` 연산자를 통해  `:` 가 위치하는 축의 모든 값에 접근할 수 있다.\n- 배열 자체에 `[:]` 를 사용할 경우 배열의 모든 값이 할당된다.\n    + **기본적으로 데이터가 복사되지 않는다.**\n    + 데이터를 복사해야 할 경우 `copy` 함수를 따로 사용한다.\n\n```python\nIn [44]: arr\nOut[44]:\narray([[0.23061655, 0.86734388, 0.27967631],\n       [0.63734555, 0.47048728, 0.04833744],\n       [0.99362969, 0.87636748, 0.59988875]])\n\nIn [45]: arr[:,1]\nOut[45]: array([0.86734388, 0.47048728, 0.87636748])\n\nIn [46]: arr[:,1]\nOut[46]: array([0.86734388, 0.47048728, 0.87636748])\n\nIn [47]: arr[:]\nOut[47]:\narray([[0.23061655, 0.86734388, 0.27967631],\n       [0.63734555, 0.47048728, 0.04833744],\n       [0.99362969, 0.87636748, 0.59988875]])\n```\n\n- **배열의 일부는 원본배열의 View 이기 때문에 파이썬 `list` 와 달리 배열의 일부에 대한 변경은 그대로 원본배열에 반영된다.**\n\n```python\nIn [28]: arr_new = np.arange(10)\n\nIn [29]: arr_new[4:7] = 10\n\nIn [30]: arr_new\nOut[30]: array([ 0,  1,  2,  3, 10, 10, 10,  7,  8,  9])\n```\n\n\n#### boolen indexing\n\n- 실질적으로 가장 자주쓰는 인덱싱이다.\n\n```python\n\nIn [62]: temp=np.random.rand(3,3)\n\nIn [63]: temp[temp>0.5]\nOut[63]:\narray([0.77402793, 0.59064775, 0.67170741, 0.51967736, 0.75161734,\n       0.98559447])\n\n\n```\n\n\n#### fancy indexing\n\n- 인덱싱과 슬라이싱의 차이는 입력된 범위의 값을 가져오느냐 연속된 값들을 가져오느냐의 차이밖에 없다.\n\n```python\nIn [56]: array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    ...:\n    ...: \n    ...: array[[0, 2], :3]\nOut[56]:\narray([[1, 2, 3],\n       [7, 8, 9]])\n```\n\n\n\n### numpy 배열 변형하기(array transformation)\n\n#### Transpose\n\n```python\n i = np.transpose(b) Permute array dimensions\n i.T Permute array dimensions\n ```\n #### Changing Array Shape\n \n```python\nb.ravel() #Flatten the array\ng.reshape(3,-2) #Reshape, but don’t change data\nb.flatten() # rabel() 과 같지만 배열의 copy를 생\n\n\n``` \n#### Adding/Removing Elements\n \n```python\n h.resize((2,6)) Return a new array with shape (2,6)\n np.append(h,g) Append items to an array\n np.insert(a, 1, 5) Insert items in an array\n np.delete(a,[1]) Delete items from an array\n``` \n\n#### Combine array\n\n```python\n\nIn [11]: arr_1 = np.arange(1,5)\n\nIn [12]: arr_2 = np.arange(6,10)\n\nIn [13]: np.concatenate((arr_1,arr_2),axis=0)\nOut[13]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [14]: np.vstack((arr_1,arr_2))\nOut[14]:\narray([[1, 2, 3, 4],\n       [6, 7, 8, 9]])\n\nIn [15]: np.r_[arr_1,arr_2]\nOut[15]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [16]: np.hstack((arr_1,arr_2))\nOut[16]: array([1, 2, 3, 4, 6, 7, 8, 9])\n\nIn [17]: np.c_[arr_1,arr_2]\nOut[17]:\narray([[1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n```\n\n### Split array\n\n```python\n\nIn [19]: np.hsplit(arr_1,2)\nOut[19]: [array([1, 2]), array([3, 4])]\n\n\nIn [24]: arr_3 = np.arange(1,10).reshape(3,3)\n\n# 행 단위 split\n\nIn [25]: np.vsplit(arr_3,3)\nOut[25]: [array([[1, 2, 3]]), array([[4, 5, 6]]), array([[7, 8, 9]])]\n\n```\n\n### numpy method\n\n#### np.where\n\n- `np.where(조건,if true 값,else 값)` 방식으로 사용한다.\n- 기본적으로 조건에 기반해 새로운 배열을 생성한다.\n- **logical statement를 vectorize한다.**\n\n\n```python\nOut[63]: df\n   A   B   C     D\n0  9  14  10  0.24\n1  8   2  17  0.56\n2  3  18  16  0.12\n3  3   4  16  0.88\n4  9   8  16  0.61\n5  7   3  17  0.44\n\n\n\nIn [64]: df[\"E\"] = np.where((df[\"B\"] > 10) & (df[\"C\"] > 10), 1, 0)\n    ...: df\nOut[64]:\n   A   B   C     D  E\n0  9  14  10  0.24  0\n1  8   2  17  0.56  0\n2  3  18  16  0.12  1\n3  3   4  16  0.88  0\n4  9   8  16  0.61  0\n5  7   3  17  0.44  0\n\n\n```\n\n#### np.select\n\n- `np.where`의 multiple condition 버전이다.\n- 2개 이상의 조건을 한번에 처리해야 할경우 pandas를 사용하는 것 보다 `np.select`를 활용해 한번에 처리하는 것이 낫다.\n\n![](https://i.imgur.com/Z3XHteT.png)\n\n```python\n# np select 예시\nIn [65]: conditions = [\n    ...:   (df[\"B\"] >= 10) & (df[\"A\"] == 0),\n    ...:   (df[\"B\"] >= 10) & (df[\"A\"] == 8)\n    ...: ]\n    ...: values = [1, 2]\n    ...: df[\"F\"] = np.select(conditions, values, default=0)\n    ...: df\n    ...:\nOut[65]:\n   A   B   C     D  E  F\n0  9  14  10  0.24  0  0\n1  8   2  17  0.56  0  0\n2  3  18  16  0.12  1  0\n3  3   4  16  0.88  0  0\n4  9   8  16  0.61  0  0\n5  7   3  17  0.44  0  0\n```\n\n#### np.log\n\n- 자연로그를 리턴한다.\n  - np.log(np.e) 는 1을 리턴한다.\n- 데이터 정규화시 사용.\n\n```python\nnp.log([1, np.e, np.e**2, 0])\narray([  0.,   1.,   2., -Inf])\n\n```\n\n#### np.sort(배열 정렬하기)\n\n- `np.sort(M)` 로 배열을 정렬한다.\n    + M.sort() 는 배열 자체를 정렬한 결과를 리턴하지만 `np.sort(M)`은 배열의 복사본을 정렬해 리턴한다,\n\n```python\nIn [39]: arr_1d = np.random.randint(0, 10, 10)\n    ...:\n    ...: arr_2d = np.random.randint(0, 10, (3, 4))\n    ...:\n    ...: print(arr_1d)\n    ...: print(arr_2d)\n[2 0 1 8 5 2 8 0 3 9]\n[[8 4 3 1]\n [0 4 6 7]\n [2 1 0 5]]\n\nIn [40]: print(np.sort(arr_1d))\n[0 0 1 2 2 3 5 8 8 9]\n\n\n```\n\n-  `np.sort(M)[::-1]` : 역순 정렬  \n\n```python\n\nIn [41]: print(np.sort(arr_1d)[::-1])\n[9 8 8 5 3 2 2 1 0 0]\n\n```\n\n\n\n- 행과 열 기준으로 정렬이 가능하다.\n    + `np.sort(x, axis=1)` : 열 기준\n    + `np.sort(x, axis=0)` : 행 기준\n\n```python\nIn [42]: print(np.sort(arr_2d, axis=0))\n    ...: print(np.sort(arr_2d, axis=1))\n[[0 1 0 1]\n [2 4 3 5]\n [8 4 6 7]]\n[[1 3 4 8]\n [0 4 6 7]\n [0 1 2 5]]\n```\n\n\n#### np.pad\n\n- 배열을 일정한 고정길이로 만들기 위해 특정한 값으로 채우는 함수.\n\n```python\nZ = np.ones((5,5))\nZ.pad(pad_width=1, \n      mode='constant', # 특정한 값을 지정해서 패딩할 경우\n      constant_values=0) # 값 지정\n\nZ      \n[[0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0.]]\n\n```\n\n- 2차원 배열 패딩\n\n```python\nIn [16]: m = np.arange(1,9).reshape(2,4)\n\nIn [17]: m\nOut[17]:\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n## 행과 열 모두 2개씩 0으로 패딩 \nIn [20]: np.pad(m,((2,2),(2,2)),'constant',constant_values =0)\nOut[20]:\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 2, 3, 4, 0, 0],\n       [0, 0, 5, 6, 7, 8, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]])\n\n\nIn [21]: np.pad(m,((1,2),(2,3)),'constant',constant_values =0)\nOut[21]:\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 2, 3, 4, 0, 0, 0],\n       [0, 0, 5, 6, 7, 8, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n\n```\n\n\n\n\n### numpy broadcasting\n\n\n- numpy의 연산은 기본적으로 같은 크기의 배열간의 연산을 전제한다.\n- 하지만 특정 조건을 만족했을 때 numpy는 자동적으로 크기가 다른 배열간의 연산을 수행하기도 하는데 이를 `broadcasting`이라 한다.\n- `broadcasting` 연산이 성립되기 위한 다음의 3가지 규칙이 존재한다.\n    + **규칙 1: 두 배열의 차원 수가 다를 경우, 크기가 작은 배열의 모양은 맨 앞(왼쪽)에 패딩됨**\n    + **규칙 2: 두 배열의 모양이 임의의 차원에서 일치하지 않으면 해당 차원에서 모양이 1인 배열은 다른 모양과 일치하도록 확장됨**\n    + **규칙 3: 어떤 차원에서든 크기가 일치하지 않고 둘 다 1과 같지 않으면 오류가 발생.**\n\n- **단순히 한쪽의 크기를 맞춰서 연산이 가능하게끔 만드는 것이라고 생각하면 이해하기 쉽다.**\n\n![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)\n\n- `a` 의 차원이 더 작기 때문에 규식 1,2 에 따라 연산시 `a` 가 padding 되고 확장됨.\n\n```python\n\nIn [5]: M = np.ones((2, 3))\n   ...: a = np.arange(3)\n\nIn [6]: print(M.shape)\n(2, 3)\n\nIn [7]: print(a.shape)\n(3,)\n\nIn [8]: M + a\nOut[8]:\narray([[1., 2., 3.],\n       [1., 2., 3.]])\n\nIn [9]: a.shape\nOut[9]: (3,)\n\n```\n\n**References**\n\n- [Numpy 공식문서](https://numpy.org/doc/stable/index.html)\n- [numpy array transfrormation](https://www.datacamp.com/community/blog/python-numpy-cheat-sheet)\n- [numpy indexing](https://www.geeksforgeeks.org/numpy-indexing/)\n- [numpy broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)\n- [numpy padding](https://sparrow.dev/numpy-pad/)\n- [numpy select](https://towardsdatascience.com/3-numpy-functions-to-facilitate-data-analysis-with-pandas-b1ad342a569)\n","slug":"Preprocessing-numpy-basics","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscf0024b36q732s13z2","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h2><p><strong>numpy 기능들</strong></p>\n<ul>\n<li>벡터 배열상에서 데이터 가공, 정제, 부분집합 필터링, 변형 및 기타연산</li>\n<li>정렬, unique 탐색, 집합연산같은 일반적인 배열처리 알고리즘</li>\n</ul>\n<p>numpy의 중요한 특징은 <strong>파이썬 반복문을 사용하지 않고</strong> 대용량 배열에 대한 복잡한 연산이 가능하다는 것이다.</p>\n<p>기본적으로 C랑 포트란 기반으로 짜여졌기 때문에 같은 연산이라면 pandas랑 비교도 안되게 빠르게 수행할 수 있다.</p>\n<hr>\n<h3 id=\"numpy-basics\"><a href=\"#numpy-basics\" class=\"headerlink\" title=\"numpy basics\"></a>numpy basics</h3><ul>\n<li>배열 생성<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">5</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">   ...: arr = np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>) <span class=\"comment\"># 0에서 9까지 의 </span></span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: arr.reshape(<span class=\"number\">2</span>,<span class=\"number\">5</span>) <span class=\"comment\"># 배열 형태 변환</span></span><br><span class=\"line\">Out[<span class=\"number\">5</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">   ...: arr = np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: arr=arr.reshape(<span class=\"number\">2</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>배열 형태 확인(ndim,shape,len)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: arr.ndim <span class=\"comment\"># 차원 수</span></span><br><span class=\"line\">Out[<span class=\"number\">7</span>]: <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: arr.shape <span class=\"comment\"># 모양</span></span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: (<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">10</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>배열 데이터 타입 확인(dtype)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">15</span>]: arr_2 = np.random.rand(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">16</span>]: arr_2</span><br><span class=\"line\">Out[<span class=\"number\">16</span>]: array([<span class=\"number\">0.55155657</span>, <span class=\"number\">0.32745746</span>, <span class=\"number\">0.92681611</span>, <span class=\"number\">0.04614794</span>, <span class=\"number\">0.17832697</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: arr_2.dtype</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]: dtype(<span class=\"string\">&#x27;float64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">18</span>]: arr_3 = np.array([<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>],dtype = np.float64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">19</span>]: arr_3.dtype</span><br><span class=\"line\">Out[<span class=\"number\">19</span>]: dtype(<span class=\"string\">&#x27;float64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>배열 데이터 타입 변환(astype)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">24</span>]: arr_2=arr_2.astype(np.int64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">25</span>]: arr_2.dtype</span><br><span class=\"line\">Out[<span class=\"number\">25</span>]: dtype(<span class=\"string\">&#x27;int64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">26</span>]: arr_3=arr_3.astype(np.int64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">27</span>]: arr_3.dtype</span><br><span class=\"line\">Out[<span class=\"number\">27</span>]: dtype(<span class=\"string\">&#x27;int64&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<p><strong>배열 생성함수들</strong></p>\n<ul>\n<li><p><code>array</code> : 입력데이터를 다차원 배열로 변환.</p>\n</li>\n<li><p><code>arange</code> : <code>range</code>와 동일하지만 다차원 배열을 반환.</p>\n</li>\n<li><p><code>np.empty(a)</code> ,<code>np.empty_like(M)</code> : 0 이나 1로 값을 초기화하지 않은 배열을 반환.</p>\n</li>\n<li><p><code>np.ones(a)</code> : a 크기의 1으로 채워진 배열을 반환 </p>\n</li>\n<li><p><code>np.ones_like(M)</code> M 배열의 사이즈와 같은 1으로 채워진 배열을 반환</p>\n</li>\n<li><p><code>np.zeros(a)</code> : a 크기의 0으로 채워진 배열을 반환 </p>\n</li>\n<li><p><code>np.zeros_like(M)</code> M 배열의 사이즈와 같은 0으로 채워진 배열을 반환</p>\n</li>\n<li><p><strong><code>np.full</code> :인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.(자주쓴다.)</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># np full 용법</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n</li>\n<li><p><code>np.full_like</code> : 인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.</p>\n</li>\n<li><p><code>eye</code> , <code>identity</code>: N * N 크기의 단위행렬생성</p>\n</li>\n<li><p><code>np.random.rand(n)</code> : n 크기 난수 배열 생성</p>\n</li>\n</ul>\n<h3 id=\"numpy-indexing\"><a href=\"#numpy-indexing\" class=\"headerlink\" title=\"numpy indexing\"></a>numpy indexing</h3><ul>\n<li>기본 파이썬 list indexing과 유사하지만 차원이 복잡해지면 어려워진다.</li>\n</ul>\n<h4 id=\"기초-슬라이싱과-인덱싱\"><a href=\"#기초-슬라이싱과-인덱싱\" class=\"headerlink\" title=\"기초 슬라이싱과 인덱싱\"></a>기초 슬라이싱과 인덱싱</h4><ul>\n<li>기본적으로 list의 그것과 다를 건 없다.</li>\n<li><code>i:j:k</code> 형태로 인덱싱한다.<ul>\n<li>i는 starting index </li>\n<li>j는 stopping index(j-1 까지 슬라이싱된다.)</li>\n<li>k는 step</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: x = np.array(<span class=\"built_in\">range</span>(<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: x[<span class=\"number\">1</span>:<span class=\"number\">7</span>:<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">4</span>]: x[-<span class=\"number\">2</span>:<span class=\"number\">10</span>]</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: array([<span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: x[-<span class=\"number\">3</span>:<span class=\"number\">3</span>:-<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">5</span>]: array([<span class=\"number\">7</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: x[<span class=\"number\">5</span>:]</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: array([<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>인덱스 리스트를 통해 쉽게 배열의 값에 접근할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">50</span>]: arr_2d = np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">51</span>]: arr_2d</span><br><span class=\"line\">Out[<span class=\"number\">51</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">52</span>]: arr_2d[<span class=\"number\">0</span>][<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">52</span>]: <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 보통 두 번째 방법을 많이 사용한다.</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">53</span>]: arr_2d[<span class=\"number\">0</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">53</span>]: <span class=\"number\">3</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>다차원 슬라이싱하기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">54</span>]: arr_2d[:<span class=\"number\">2</span>,<span class=\"number\">1</span>:]</span><br><span class=\"line\">Out[<span class=\"number\">54</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li>좀 복잡한 형태의 다차원 슬라이싱</li>\n</ul>\n<p><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 다차원 배열 인덱싱 예시</span></span><br><span class=\"line\"> [[<span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>] </span><br><span class=\"line\">  [<span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span> <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">  [<span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span> <span class=\"number\">17</span>]</span><br><span class=\"line\">  [<span class=\"number\">18</span> <span class=\"number\">19</span> <span class=\"number\">20</span> <span class=\"number\">21</span> <span class=\"number\">22</span> <span class=\"number\">23</span>]</span><br><span class=\"line\">  [<span class=\"number\">24</span> <span class=\"number\">25</span> <span class=\"number\">26</span> <span class=\"number\">27</span> <span class=\"number\">28</span> <span class=\"number\">29</span>]</span><br><span class=\"line\">  [<span class=\"number\">30</span> <span class=\"number\">31</span> <span class=\"number\">32</span> <span class=\"number\">33</span> <span class=\"number\">34</span> <span class=\"number\">35</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">0</span>, <span class=\"number\">3</span>:<span class=\"number\">5</span>]  =  [<span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">4</span>:, <span class=\"number\">4</span>:] = [[<span class=\"number\">28</span> <span class=\"number\">29</span>],</span><br><span class=\"line\">             [<span class=\"number\">34</span> <span class=\"number\">35</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">a[:, <span class=\"number\">2</span>] =  [<span class=\"number\">2</span> <span class=\"number\">8</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">26</span> <span class=\"number\">32</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">2</span>:;<span class=\"number\">2</span>, ::<span class=\"number\">2</span>] = [[<span class=\"number\">12</span> <span class=\"number\">14</span> <span class=\"number\">16</span>],</span><br><span class=\"line\">                [<span class=\"number\">24</span> <span class=\"number\">26</span> <span class=\"number\">28</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>:</code> 연산자를 통해  <code>:</code> 가 위치하는 축의 모든 값에 접근할 수 있다.</li>\n<li>배열 자체에 <code>[:]</code> 를 사용할 경우 배열의 모든 값이 할당된다.<ul>\n<li><strong>기본적으로 데이터가 복사되지 않는다.</strong></li>\n<li>데이터를 복사해야 할 경우 <code>copy</code> 함수를 따로 사용한다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">44</span>]: arr</span><br><span class=\"line\">Out[<span class=\"number\">44</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0.23061655</span>, <span class=\"number\">0.86734388</span>, <span class=\"number\">0.27967631</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.63734555</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.04833744</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.99362969</span>, <span class=\"number\">0.87636748</span>, <span class=\"number\">0.59988875</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">45</span>]: arr[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">45</span>]: array([<span class=\"number\">0.86734388</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.87636748</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">46</span>]: arr[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">46</span>]: array([<span class=\"number\">0.86734388</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.87636748</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">47</span>]: arr[:]</span><br><span class=\"line\">Out[<span class=\"number\">47</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0.23061655</span>, <span class=\"number\">0.86734388</span>, <span class=\"number\">0.27967631</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.63734555</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.04833744</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.99362969</span>, <span class=\"number\">0.87636748</span>, <span class=\"number\">0.59988875</span>]])</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>배열의 일부는 원본배열의 View 이기 때문에 파이썬 <code>list</code> 와 달리 배열의 일부에 대한 변경은 그대로 원본배열에 반영된다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">28</span>]: arr_new = np.arange(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">29</span>]: arr_new[<span class=\"number\">4</span>:<span class=\"number\">7</span>] = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">30</span>]: arr_new</span><br><span class=\"line\">Out[<span class=\"number\">30</span>]: array([ <span class=\"number\">0</span>,  <span class=\"number\">1</span>,  <span class=\"number\">2</span>,  <span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>,  <span class=\"number\">7</span>,  <span class=\"number\">8</span>,  <span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"boolen-indexing\"><a href=\"#boolen-indexing\" class=\"headerlink\" title=\"boolen indexing\"></a>boolen indexing</h4><ul>\n<li>실질적으로 가장 자주쓰는 인덱싱이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">62</span>]: temp=np.random.rand(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">63</span>]: temp[temp&gt;<span class=\"number\">0.5</span>]</span><br><span class=\"line\">Out[<span class=\"number\">63</span>]:</span><br><span class=\"line\">array([<span class=\"number\">0.77402793</span>, <span class=\"number\">0.59064775</span>, <span class=\"number\">0.67170741</span>, <span class=\"number\">0.51967736</span>, <span class=\"number\">0.75161734</span>,</span><br><span class=\"line\">       <span class=\"number\">0.98559447</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"fancy-indexing\"><a href=\"#fancy-indexing\" class=\"headerlink\" title=\"fancy indexing\"></a>fancy indexing</h4><ul>\n<li>인덱싱과 슬라이싱의 차이는 입력된 범위의 값을 가져오느냐 연속된 값들을 가져오느냐의 차이밖에 없다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">56</span>]: array = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: </span><br><span class=\"line\">    ...: array[[<span class=\"number\">0</span>, <span class=\"number\">2</span>], :<span class=\"number\">3</span>]</span><br><span class=\"line\">Out[<span class=\"number\">56</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"numpy-배열-변형하기-array-transformation\"><a href=\"#numpy-배열-변형하기-array-transformation\" class=\"headerlink\" title=\"numpy 배열 변형하기(array transformation)\"></a>numpy 배열 변형하기(array transformation)</h3><h4 id=\"Transpose\"><a href=\"#Transpose\" class=\"headerlink\" title=\"Transpose\"></a>Transpose</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i = np.transpose(b) Permute array dimensions</span><br><span class=\"line\">i.T Permute array dimensions</span><br></pre></td></tr></table></figure>\n<h4 id=\"Changing-Array-Shape\"><a href=\"#Changing-Array-Shape\" class=\"headerlink\" title=\"Changing Array Shape\"></a>Changing Array Shape</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.ravel() <span class=\"comment\">#Flatten the array</span></span><br><span class=\"line\">g.reshape(<span class=\"number\">3</span>,-<span class=\"number\">2</span>) <span class=\"comment\">#Reshape, but don’t change data</span></span><br><span class=\"line\">b.flatten() <span class=\"comment\"># rabel() 과 같지만 배열의 copy를 생</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Adding-x2F-Removing-Elements\"><a href=\"#Adding-x2F-Removing-Elements\" class=\"headerlink\" title=\"Adding&#x2F;Removing Elements\"></a>Adding&#x2F;Removing Elements</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">h.resize((<span class=\"number\">2</span>,<span class=\"number\">6</span>)) Return a new array <span class=\"keyword\">with</span> shape (<span class=\"number\">2</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">np.append(h,g) Append items to an array</span><br><span class=\"line\">np.insert(a, <span class=\"number\">1</span>, <span class=\"number\">5</span>) Insert items <span class=\"keyword\">in</span> an array</span><br><span class=\"line\">np.delete(a,[<span class=\"number\">1</span>]) Delete items <span class=\"keyword\">from</span> an array</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Combine-array\"><a href=\"#Combine-array\" class=\"headerlink\" title=\"Combine array\"></a>Combine array</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: arr_1 = np.arange(<span class=\"number\">1</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: arr_2 = np.arange(<span class=\"number\">6</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">13</span>]: np.concatenate((arr_1,arr_2),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: np.vstack((arr_1,arr_2))</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">15</span>]: np.r_[arr_1,arr_2]</span><br><span class=\"line\">Out[<span class=\"number\">15</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">16</span>]: np.hstack((arr_1,arr_2))</span><br><span class=\"line\">Out[<span class=\"number\">16</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: np.c_[arr_1,arr_2]</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">2</span>, <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">3</span>, <span class=\"number\">8</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Split-array\"><a href=\"#Split-array\" class=\"headerlink\" title=\"Split array\"></a>Split array</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">19</span>]: np.hsplit(arr_1,<span class=\"number\">2</span>)</span><br><span class=\"line\">Out[<span class=\"number\">19</span>]: [array([<span class=\"number\">1</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">3</span>, <span class=\"number\">4</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">24</span>]: arr_3 = np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행 단위 split</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">25</span>]: np.vsplit(arr_3,<span class=\"number\">3</span>)</span><br><span class=\"line\">Out[<span class=\"number\">25</span>]: [array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]]), array([[<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"numpy-method\"><a href=\"#numpy-method\" class=\"headerlink\" title=\"numpy method\"></a>numpy method</h3><h4 id=\"np-where\"><a href=\"#np-where\" class=\"headerlink\" title=\"np.where\"></a>np.where</h4><ul>\n<li><code>np.where(조건,if true 값,else 값)</code> 방식으로 사용한다.</li>\n<li>기본적으로 조건에 기반해 새로운 배열을 생성한다.</li>\n<li><strong>logical statement를 vectorize한다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Out[<span class=\"number\">63</span>]: df</span><br><span class=\"line\">   A   B   C     D</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">64</span>]: df[<span class=\"string\">&quot;E&quot;</span>] = np.where((df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] &gt; <span class=\"number\">10</span>), <span class=\"number\">1</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    ...: df</span><br><span class=\"line\">Out[<span class=\"number\">64</span>]:</span><br><span class=\"line\">   A   B   C     D  E</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span>  <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span>  <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-select\"><a href=\"#np-select\" class=\"headerlink\" title=\"np.select\"></a>np.select</h4><ul>\n<li><code>np.where</code>의 multiple condition 버전이다.</li>\n<li>2개 이상의 조건을 한번에 처리해야 할경우 pandas를 사용하는 것 보다 <code>np.select</code>를 활용해 한번에 처리하는 것이 낫다.</li>\n</ul>\n<p><img src=\"https://i.imgur.com/Z3XHteT.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># np select 예시</span></span><br><span class=\"line\">In [<span class=\"number\">65</span>]: conditions = [</span><br><span class=\"line\">    ...:   (df[<span class=\"string\">&quot;B&quot;</span>] &gt;= <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;A&quot;</span>] == <span class=\"number\">0</span>),</span><br><span class=\"line\">    ...:   (df[<span class=\"string\">&quot;B&quot;</span>] &gt;= <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;A&quot;</span>] == <span class=\"number\">8</span>)</span><br><span class=\"line\">    ...: ]</span><br><span class=\"line\">    ...: values = [<span class=\"number\">1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">    ...: df[<span class=\"string\">&quot;F&quot;</span>] = np.select(conditions, values, default=<span class=\"number\">0</span>)</span><br><span class=\"line\">    ...: df</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">Out[<span class=\"number\">65</span>]:</span><br><span class=\"line\">   A   B   C     D  E  F</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span>  <span class=\"number\">1</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-log\"><a href=\"#np-log\" class=\"headerlink\" title=\"np.log\"></a>np.log</h4><ul>\n<li>자연로그를 리턴한다.<ul>\n<li>np.log(np.e) 는 1을 리턴한다.</li>\n</ul>\n</li>\n<li>데이터 정규화시 사용.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">np.log([<span class=\"number\">1</span>, np.e, np.e**<span class=\"number\">2</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">array([  <span class=\"number\">0.</span>,   <span class=\"number\">1.</span>,   <span class=\"number\">2.</span>, -Inf])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-sort-배열-정렬하기\"><a href=\"#np-sort-배열-정렬하기\" class=\"headerlink\" title=\"np.sort(배열 정렬하기)\"></a>np.sort(배열 정렬하기)</h4><ul>\n<li><code>np.sort(M)</code> 로 배열을 정렬한다.<ul>\n<li>M.sort() 는 배열 자체를 정렬한 결과를 리턴하지만 <code>np.sort(M)</code>은 배열의 복사본을 정렬해 리턴한다,</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">39</span>]: arr_1d = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: arr_2d = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(arr_1d)</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(arr_2d)</span><br><span class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">8</span> <span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">8</span> <span class=\"number\">0</span> <span class=\"number\">3</span> <span class=\"number\">9</span>]</span><br><span class=\"line\">[[<span class=\"number\">8</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">40</span>]: <span class=\"built_in\">print</span>(np.sort(arr_1d))</span><br><span class=\"line\">[<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">8</span> <span class=\"number\">8</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>np.sort(M)[::-1]</code> : 역순 정렬</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">41</span>]: <span class=\"built_in\">print</span>(np.sort(arr_1d)[::-<span class=\"number\">1</span>])</span><br><span class=\"line\">[<span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">8</span> <span class=\"number\">5</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li>행과 열 기준으로 정렬이 가능하다.<ul>\n<li><code>np.sort(x, axis=1)</code> : 열 기준</li>\n<li><code>np.sort(x, axis=0)</code> : 행 기준</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">42</span>]: <span class=\"built_in\">print</span>(np.sort(arr_2d, axis=<span class=\"number\">0</span>))</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(np.sort(arr_2d, axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">[[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">8</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]]</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">8</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">5</span>]]</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"np-pad\"><a href=\"#np-pad\" class=\"headerlink\" title=\"np.pad\"></a>np.pad</h4><ul>\n<li>배열을 일정한 고정길이로 만들기 위해 특정한 값으로 채우는 함수.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = np.ones((<span class=\"number\">5</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">Z.pad(pad_width=<span class=\"number\">1</span>, </span><br><span class=\"line\">      mode=<span class=\"string\">&#x27;constant&#x27;</span>, <span class=\"comment\"># 특정한 값을 지정해서 패딩할 경우</span></span><br><span class=\"line\">      constant_values=<span class=\"number\">0</span>) <span class=\"comment\"># 값 지정</span></span><br><span class=\"line\"></span><br><span class=\"line\">Z      </span><br><span class=\"line\">[[<span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>2차원 배열 패딩</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">16</span>]: m = np.arange(<span class=\"number\">1</span>,<span class=\"number\">9</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: m</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 행과 열 모두 2개씩 0으로 패딩 </span></span><br><span class=\"line\">In [<span class=\"number\">20</span>]: np.pad(m,((<span class=\"number\">2</span>,<span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">2</span>)),<span class=\"string\">&#x27;constant&#x27;</span>,constant_values =<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">20</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">21</span>]: np.pad(m,((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>)),<span class=\"string\">&#x27;constant&#x27;</span>,constant_values =<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">21</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<h3 id=\"numpy-broadcasting\"><a href=\"#numpy-broadcasting\" class=\"headerlink\" title=\"numpy broadcasting\"></a>numpy broadcasting</h3><ul>\n<li><p>numpy의 연산은 기본적으로 같은 크기의 배열간의 연산을 전제한다.</p>\n</li>\n<li><p>하지만 특정 조건을 만족했을 때 numpy는 자동적으로 크기가 다른 배열간의 연산을 수행하기도 하는데 이를 <code>broadcasting</code>이라 한다.</p>\n</li>\n<li><p><code>broadcasting</code> 연산이 성립되기 위한 다음의 3가지 규칙이 존재한다.</p>\n<ul>\n<li><strong>규칙 1: 두 배열의 차원 수가 다를 경우, 크기가 작은 배열의 모양은 맨 앞(왼쪽)에 패딩됨</strong></li>\n<li><strong>규칙 2: 두 배열의 모양이 임의의 차원에서 일치하지 않으면 해당 차원에서 모양이 1인 배열은 다른 모양과 일치하도록 확장됨</strong></li>\n<li><strong>규칙 3: 어떤 차원에서든 크기가 일치하지 않고 둘 다 1과 같지 않으면 오류가 발생.</strong></li>\n</ul>\n</li>\n<li><p><strong>단순히 한쪽의 크기를 맞춰서 연산이 가능하게끔 만드는 것이라고 생각하면 이해하기 쉽다.</strong></p>\n</li>\n</ul>\n<p><img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png\"></p>\n<ul>\n<li><code>a</code> 의 차원이 더 작기 때문에 규식 1,2 에 따라 연산시 <code>a</code> 가 padding 되고 확장됨.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: M = np.ones((<span class=\"number\">2</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">   ...: a = np.arange(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"built_in\">print</span>(M.shape)</span><br><span class=\"line\">(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: <span class=\"built_in\">print</span>(a.shape)</span><br><span class=\"line\">(<span class=\"number\">3</span>,)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: M + a</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1.</span>, <span class=\"number\">2.</span>, <span class=\"number\">3.</span>],</span><br><span class=\"line\">       [<span class=\"number\">1.</span>, <span class=\"number\">2.</span>, <span class=\"number\">3.</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: a.shape</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: (<span class=\"number\">3</span>,)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>References</strong></p>\n<ul>\n<li><a href=\"https://numpy.org/doc/stable/index.html\">Numpy 공식문서</a></li>\n<li><a href=\"https://www.datacamp.com/community/blog/python-numpy-cheat-sheet\">numpy array transfrormation</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/numpy-indexing/\">numpy indexing</a></li>\n<li><a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html\">numpy broadcasting</a></li>\n<li><a href=\"https://sparrow.dev/numpy-pad/\">numpy padding</a></li>\n<li><a href=\"https://towardsdatascience.com/3-numpy-functions-to-facilitate-data-analysis-with-pandas-b1ad342a569\">numpy select</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h2><p><strong>numpy 기능들</strong></p>\n<ul>\n<li>벡터 배열상에서 데이터 가공, 정제, 부분집합 필터링, 변형 및 기타연산</li>\n<li>정렬, unique 탐색, 집합연산같은 일반적인 배열처리 알고리즘</li>\n</ul>\n<p>numpy의 중요한 특징은 <strong>파이썬 반복문을 사용하지 않고</strong> 대용량 배열에 대한 복잡한 연산이 가능하다는 것이다.</p>\n<p>기본적으로 C랑 포트란 기반으로 짜여졌기 때문에 같은 연산이라면 pandas랑 비교도 안되게 빠르게 수행할 수 있다.</p>\n<hr>\n<h3 id=\"numpy-basics\"><a href=\"#numpy-basics\" class=\"headerlink\" title=\"numpy basics\"></a>numpy basics</h3><ul>\n<li>배열 생성<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">5</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">   ...: arr = np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>) <span class=\"comment\"># 0에서 9까지 의 </span></span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: arr.reshape(<span class=\"number\">2</span>,<span class=\"number\">5</span>) <span class=\"comment\"># 배열 형태 변환</span></span><br><span class=\"line\">Out[<span class=\"number\">5</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">   ...: arr = np.arange(<span class=\"number\">0</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: arr=arr.reshape(<span class=\"number\">2</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n<li>배열 형태 확인(ndim,shape,len)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: arr.ndim <span class=\"comment\"># 차원 수</span></span><br><span class=\"line\">Out[<span class=\"number\">7</span>]: <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: arr.shape <span class=\"comment\"># 모양</span></span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: (<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: <span class=\"built_in\">len</span>(arr)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">10</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>배열 데이터 타입 확인(dtype)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">15</span>]: arr_2 = np.random.rand(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">16</span>]: arr_2</span><br><span class=\"line\">Out[<span class=\"number\">16</span>]: array([<span class=\"number\">0.55155657</span>, <span class=\"number\">0.32745746</span>, <span class=\"number\">0.92681611</span>, <span class=\"number\">0.04614794</span>, <span class=\"number\">0.17832697</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: arr_2.dtype</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]: dtype(<span class=\"string\">&#x27;float64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">18</span>]: arr_3 = np.array([<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>],dtype = np.float64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">19</span>]: arr_3.dtype</span><br><span class=\"line\">Out[<span class=\"number\">19</span>]: dtype(<span class=\"string\">&#x27;float64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>배열 데이터 타입 변환(astype)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">24</span>]: arr_2=arr_2.astype(np.int64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">25</span>]: arr_2.dtype</span><br><span class=\"line\">Out[<span class=\"number\">25</span>]: dtype(<span class=\"string\">&#x27;int64&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">26</span>]: arr_3=arr_3.astype(np.int64)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">27</span>]: arr_3.dtype</span><br><span class=\"line\">Out[<span class=\"number\">27</span>]: dtype(<span class=\"string\">&#x27;int64&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<p><strong>배열 생성함수들</strong></p>\n<ul>\n<li><p><code>array</code> : 입력데이터를 다차원 배열로 변환.</p>\n</li>\n<li><p><code>arange</code> : <code>range</code>와 동일하지만 다차원 배열을 반환.</p>\n</li>\n<li><p><code>np.empty(a)</code> ,<code>np.empty_like(M)</code> : 0 이나 1로 값을 초기화하지 않은 배열을 반환.</p>\n</li>\n<li><p><code>np.ones(a)</code> : a 크기의 1으로 채워진 배열을 반환 </p>\n</li>\n<li><p><code>np.ones_like(M)</code> M 배열의 사이즈와 같은 1으로 채워진 배열을 반환</p>\n</li>\n<li><p><code>np.zeros(a)</code> : a 크기의 0으로 채워진 배열을 반환 </p>\n</li>\n<li><p><code>np.zeros_like(M)</code> M 배열의 사이즈와 같은 0으로 채워진 배열을 반환</p>\n</li>\n<li><p><strong><code>np.full</code> :인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.(자주쓴다.)</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># np full 용법</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n</li>\n<li><p><code>np.full_like</code> : 인자로 값과 형태를 받고. 인자로 받은 형태에 값을 채운다.</p>\n</li>\n<li><p><code>eye</code> , <code>identity</code>: N * N 크기의 단위행렬생성</p>\n</li>\n<li><p><code>np.random.rand(n)</code> : n 크기 난수 배열 생성</p>\n</li>\n</ul>\n<h3 id=\"numpy-indexing\"><a href=\"#numpy-indexing\" class=\"headerlink\" title=\"numpy indexing\"></a>numpy indexing</h3><ul>\n<li>기본 파이썬 list indexing과 유사하지만 차원이 복잡해지면 어려워진다.</li>\n</ul>\n<h4 id=\"기초-슬라이싱과-인덱싱\"><a href=\"#기초-슬라이싱과-인덱싱\" class=\"headerlink\" title=\"기초 슬라이싱과 인덱싱\"></a>기초 슬라이싱과 인덱싱</h4><ul>\n<li>기본적으로 list의 그것과 다를 건 없다.</li>\n<li><code>i:j:k</code> 형태로 인덱싱한다.<ul>\n<li>i는 starting index </li>\n<li>j는 stopping index(j-1 까지 슬라이싱된다.)</li>\n<li>k는 step</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: <span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: x = np.array(<span class=\"built_in\">range</span>(<span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: x[<span class=\"number\">1</span>:<span class=\"number\">7</span>:<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">4</span>]: x[-<span class=\"number\">2</span>:<span class=\"number\">10</span>]</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: array([<span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: x[-<span class=\"number\">3</span>:<span class=\"number\">3</span>:-<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">5</span>]: array([<span class=\"number\">7</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: x[<span class=\"number\">5</span>:]</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: array([<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>인덱스 리스트를 통해 쉽게 배열의 값에 접근할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">50</span>]: arr_2d = np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">51</span>]: arr_2d</span><br><span class=\"line\">Out[<span class=\"number\">51</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">52</span>]: arr_2d[<span class=\"number\">0</span>][<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">52</span>]: <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 보통 두 번째 방법을 많이 사용한다.</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">53</span>]: arr_2d[<span class=\"number\">0</span>,<span class=\"number\">2</span>]</span><br><span class=\"line\">Out[<span class=\"number\">53</span>]: <span class=\"number\">3</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>다차원 슬라이싱하기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">54</span>]: arr_2d[:<span class=\"number\">2</span>,<span class=\"number\">1</span>:]</span><br><span class=\"line\">Out[<span class=\"number\">54</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li>좀 복잡한 형태의 다차원 슬라이싱</li>\n</ul>\n<p><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 다차원 배열 인덱싱 예시</span></span><br><span class=\"line\"> [[<span class=\"number\">0</span>  <span class=\"number\">1</span>  <span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">4</span>  <span class=\"number\">5</span>] </span><br><span class=\"line\">  [<span class=\"number\">6</span> <span class=\"number\">7</span> <span class=\"number\">8</span> <span class=\"number\">9</span> <span class=\"number\">10</span> <span class=\"number\">11</span>]</span><br><span class=\"line\">  [<span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span> <span class=\"number\">17</span>]</span><br><span class=\"line\">  [<span class=\"number\">18</span> <span class=\"number\">19</span> <span class=\"number\">20</span> <span class=\"number\">21</span> <span class=\"number\">22</span> <span class=\"number\">23</span>]</span><br><span class=\"line\">  [<span class=\"number\">24</span> <span class=\"number\">25</span> <span class=\"number\">26</span> <span class=\"number\">27</span> <span class=\"number\">28</span> <span class=\"number\">29</span>]</span><br><span class=\"line\">  [<span class=\"number\">30</span> <span class=\"number\">31</span> <span class=\"number\">32</span> <span class=\"number\">33</span> <span class=\"number\">34</span> <span class=\"number\">35</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">0</span>, <span class=\"number\">3</span>:<span class=\"number\">5</span>]  =  [<span class=\"number\">3</span> <span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">4</span>:, <span class=\"number\">4</span>:] = [[<span class=\"number\">28</span> <span class=\"number\">29</span>],</span><br><span class=\"line\">             [<span class=\"number\">34</span> <span class=\"number\">35</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">a[:, <span class=\"number\">2</span>] =  [<span class=\"number\">2</span> <span class=\"number\">8</span> <span class=\"number\">14</span> <span class=\"number\">20</span> <span class=\"number\">26</span> <span class=\"number\">32</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">a[<span class=\"number\">2</span>:;<span class=\"number\">2</span>, ::<span class=\"number\">2</span>] = [[<span class=\"number\">12</span> <span class=\"number\">14</span> <span class=\"number\">16</span>],</span><br><span class=\"line\">                [<span class=\"number\">24</span> <span class=\"number\">26</span> <span class=\"number\">28</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>:</code> 연산자를 통해  <code>:</code> 가 위치하는 축의 모든 값에 접근할 수 있다.</li>\n<li>배열 자체에 <code>[:]</code> 를 사용할 경우 배열의 모든 값이 할당된다.<ul>\n<li><strong>기본적으로 데이터가 복사되지 않는다.</strong></li>\n<li>데이터를 복사해야 할 경우 <code>copy</code> 함수를 따로 사용한다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">44</span>]: arr</span><br><span class=\"line\">Out[<span class=\"number\">44</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0.23061655</span>, <span class=\"number\">0.86734388</span>, <span class=\"number\">0.27967631</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.63734555</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.04833744</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.99362969</span>, <span class=\"number\">0.87636748</span>, <span class=\"number\">0.59988875</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">45</span>]: arr[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">45</span>]: array([<span class=\"number\">0.86734388</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.87636748</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">46</span>]: arr[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">Out[<span class=\"number\">46</span>]: array([<span class=\"number\">0.86734388</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.87636748</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">47</span>]: arr[:]</span><br><span class=\"line\">Out[<span class=\"number\">47</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0.23061655</span>, <span class=\"number\">0.86734388</span>, <span class=\"number\">0.27967631</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.63734555</span>, <span class=\"number\">0.47048728</span>, <span class=\"number\">0.04833744</span>],</span><br><span class=\"line\">       [<span class=\"number\">0.99362969</span>, <span class=\"number\">0.87636748</span>, <span class=\"number\">0.59988875</span>]])</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>배열의 일부는 원본배열의 View 이기 때문에 파이썬 <code>list</code> 와 달리 배열의 일부에 대한 변경은 그대로 원본배열에 반영된다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">28</span>]: arr_new = np.arange(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">29</span>]: arr_new[<span class=\"number\">4</span>:<span class=\"number\">7</span>] = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">30</span>]: arr_new</span><br><span class=\"line\">Out[<span class=\"number\">30</span>]: array([ <span class=\"number\">0</span>,  <span class=\"number\">1</span>,  <span class=\"number\">2</span>,  <span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>,  <span class=\"number\">7</span>,  <span class=\"number\">8</span>,  <span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"boolen-indexing\"><a href=\"#boolen-indexing\" class=\"headerlink\" title=\"boolen indexing\"></a>boolen indexing</h4><ul>\n<li>실질적으로 가장 자주쓰는 인덱싱이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">62</span>]: temp=np.random.rand(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">63</span>]: temp[temp&gt;<span class=\"number\">0.5</span>]</span><br><span class=\"line\">Out[<span class=\"number\">63</span>]:</span><br><span class=\"line\">array([<span class=\"number\">0.77402793</span>, <span class=\"number\">0.59064775</span>, <span class=\"number\">0.67170741</span>, <span class=\"number\">0.51967736</span>, <span class=\"number\">0.75161734</span>,</span><br><span class=\"line\">       <span class=\"number\">0.98559447</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"fancy-indexing\"><a href=\"#fancy-indexing\" class=\"headerlink\" title=\"fancy indexing\"></a>fancy indexing</h4><ul>\n<li>인덱싱과 슬라이싱의 차이는 입력된 범위의 값을 가져오느냐 연속된 값들을 가져오느냐의 차이밖에 없다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">56</span>]: array = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: </span><br><span class=\"line\">    ...: array[[<span class=\"number\">0</span>, <span class=\"number\">2</span>], :<span class=\"number\">3</span>]</span><br><span class=\"line\">Out[<span class=\"number\">56</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>],</span><br><span class=\"line\">       [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"numpy-배열-변형하기-array-transformation\"><a href=\"#numpy-배열-변형하기-array-transformation\" class=\"headerlink\" title=\"numpy 배열 변형하기(array transformation)\"></a>numpy 배열 변형하기(array transformation)</h3><h4 id=\"Transpose\"><a href=\"#Transpose\" class=\"headerlink\" title=\"Transpose\"></a>Transpose</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i = np.transpose(b) Permute array dimensions</span><br><span class=\"line\">i.T Permute array dimensions</span><br></pre></td></tr></table></figure>\n<h4 id=\"Changing-Array-Shape\"><a href=\"#Changing-Array-Shape\" class=\"headerlink\" title=\"Changing Array Shape\"></a>Changing Array Shape</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.ravel() <span class=\"comment\">#Flatten the array</span></span><br><span class=\"line\">g.reshape(<span class=\"number\">3</span>,-<span class=\"number\">2</span>) <span class=\"comment\">#Reshape, but don’t change data</span></span><br><span class=\"line\">b.flatten() <span class=\"comment\"># rabel() 과 같지만 배열의 copy를 생</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Adding-x2F-Removing-Elements\"><a href=\"#Adding-x2F-Removing-Elements\" class=\"headerlink\" title=\"Adding&#x2F;Removing Elements\"></a>Adding&#x2F;Removing Elements</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">h.resize((<span class=\"number\">2</span>,<span class=\"number\">6</span>)) Return a new array <span class=\"keyword\">with</span> shape (<span class=\"number\">2</span>,<span class=\"number\">6</span>)</span><br><span class=\"line\">np.append(h,g) Append items to an array</span><br><span class=\"line\">np.insert(a, <span class=\"number\">1</span>, <span class=\"number\">5</span>) Insert items <span class=\"keyword\">in</span> an array</span><br><span class=\"line\">np.delete(a,[<span class=\"number\">1</span>]) Delete items <span class=\"keyword\">from</span> an array</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Combine-array\"><a href=\"#Combine-array\" class=\"headerlink\" title=\"Combine array\"></a>Combine array</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: arr_1 = np.arange(<span class=\"number\">1</span>,<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: arr_2 = np.arange(<span class=\"number\">6</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">13</span>]: np.concatenate((arr_1,arr_2),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: np.vstack((arr_1,arr_2))</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">15</span>]: np.r_[arr_1,arr_2]</span><br><span class=\"line\">Out[<span class=\"number\">15</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">16</span>]: np.hstack((arr_1,arr_2))</span><br><span class=\"line\">Out[<span class=\"number\">16</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: np.c_[arr_1,arr_2]</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">6</span>],</span><br><span class=\"line\">       [<span class=\"number\">2</span>, <span class=\"number\">7</span>],</span><br><span class=\"line\">       [<span class=\"number\">3</span>, <span class=\"number\">8</span>],</span><br><span class=\"line\">       [<span class=\"number\">4</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Split-array\"><a href=\"#Split-array\" class=\"headerlink\" title=\"Split array\"></a>Split array</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">19</span>]: np.hsplit(arr_1,<span class=\"number\">2</span>)</span><br><span class=\"line\">Out[<span class=\"number\">19</span>]: [array([<span class=\"number\">1</span>, <span class=\"number\">2</span>]), array([<span class=\"number\">3</span>, <span class=\"number\">4</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">24</span>]: arr_3 = np.arange(<span class=\"number\">1</span>,<span class=\"number\">10</span>).reshape(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행 단위 split</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">25</span>]: np.vsplit(arr_3,<span class=\"number\">3</span>)</span><br><span class=\"line\">Out[<span class=\"number\">25</span>]: [array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]]), array([[<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]]), array([[<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"numpy-method\"><a href=\"#numpy-method\" class=\"headerlink\" title=\"numpy method\"></a>numpy method</h3><h4 id=\"np-where\"><a href=\"#np-where\" class=\"headerlink\" title=\"np.where\"></a>np.where</h4><ul>\n<li><code>np.where(조건,if true 값,else 값)</code> 방식으로 사용한다.</li>\n<li>기본적으로 조건에 기반해 새로운 배열을 생성한다.</li>\n<li><strong>logical statement를 vectorize한다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Out[<span class=\"number\">63</span>]: df</span><br><span class=\"line\">   A   B   C     D</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">64</span>]: df[<span class=\"string\">&quot;E&quot;</span>] = np.where((df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] &gt; <span class=\"number\">10</span>), <span class=\"number\">1</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">    ...: df</span><br><span class=\"line\">Out[<span class=\"number\">64</span>]:</span><br><span class=\"line\">   A   B   C     D  E</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span>  <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span>  <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-select\"><a href=\"#np-select\" class=\"headerlink\" title=\"np.select\"></a>np.select</h4><ul>\n<li><code>np.where</code>의 multiple condition 버전이다.</li>\n<li>2개 이상의 조건을 한번에 처리해야 할경우 pandas를 사용하는 것 보다 <code>np.select</code>를 활용해 한번에 처리하는 것이 낫다.</li>\n</ul>\n<p><img src=\"https://i.imgur.com/Z3XHteT.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># np select 예시</span></span><br><span class=\"line\">In [<span class=\"number\">65</span>]: conditions = [</span><br><span class=\"line\">    ...:   (df[<span class=\"string\">&quot;B&quot;</span>] &gt;= <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;A&quot;</span>] == <span class=\"number\">0</span>),</span><br><span class=\"line\">    ...:   (df[<span class=\"string\">&quot;B&quot;</span>] &gt;= <span class=\"number\">10</span>) &amp; (df[<span class=\"string\">&quot;A&quot;</span>] == <span class=\"number\">8</span>)</span><br><span class=\"line\">    ...: ]</span><br><span class=\"line\">    ...: values = [<span class=\"number\">1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">    ...: df[<span class=\"string\">&quot;F&quot;</span>] = np.select(conditions, values, default=<span class=\"number\">0</span>)</span><br><span class=\"line\">    ...: df</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">Out[<span class=\"number\">65</span>]:</span><br><span class=\"line\">   A   B   C     D  E  F</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">9</span>  <span class=\"number\">14</span>  <span class=\"number\">10</span>  <span class=\"number\">0.24</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">8</span>   <span class=\"number\">2</span>  <span class=\"number\">17</span>  <span class=\"number\">0.56</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">3</span>  <span class=\"number\">18</span>  <span class=\"number\">16</span>  <span class=\"number\">0.12</span>  <span class=\"number\">1</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">3</span>   <span class=\"number\">4</span>  <span class=\"number\">16</span>  <span class=\"number\">0.88</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">4</span>  <span class=\"number\">9</span>   <span class=\"number\">8</span>  <span class=\"number\">16</span>  <span class=\"number\">0.61</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">5</span>  <span class=\"number\">7</span>   <span class=\"number\">3</span>  <span class=\"number\">17</span>  <span class=\"number\">0.44</span>  <span class=\"number\">0</span>  <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-log\"><a href=\"#np-log\" class=\"headerlink\" title=\"np.log\"></a>np.log</h4><ul>\n<li>자연로그를 리턴한다.<ul>\n<li>np.log(np.e) 는 1을 리턴한다.</li>\n</ul>\n</li>\n<li>데이터 정규화시 사용.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">np.log([<span class=\"number\">1</span>, np.e, np.e**<span class=\"number\">2</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">array([  <span class=\"number\">0.</span>,   <span class=\"number\">1.</span>,   <span class=\"number\">2.</span>, -Inf])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"np-sort-배열-정렬하기\"><a href=\"#np-sort-배열-정렬하기\" class=\"headerlink\" title=\"np.sort(배열 정렬하기)\"></a>np.sort(배열 정렬하기)</h4><ul>\n<li><code>np.sort(M)</code> 로 배열을 정렬한다.<ul>\n<li>M.sort() 는 배열 자체를 정렬한 결과를 리턴하지만 <code>np.sort(M)</code>은 배열의 복사본을 정렬해 리턴한다,</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">39</span>]: arr_1d = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: arr_2d = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">10</span>, (<span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(arr_1d)</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(arr_2d)</span><br><span class=\"line\">[<span class=\"number\">2</span> <span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">8</span> <span class=\"number\">5</span> <span class=\"number\">2</span> <span class=\"number\">8</span> <span class=\"number\">0</span> <span class=\"number\">3</span> <span class=\"number\">9</span>]</span><br><span class=\"line\">[[<span class=\"number\">8</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">5</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">40</span>]: <span class=\"built_in\">print</span>(np.sort(arr_1d))</span><br><span class=\"line\">[<span class=\"number\">0</span> <span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">3</span> <span class=\"number\">5</span> <span class=\"number\">8</span> <span class=\"number\">8</span> <span class=\"number\">9</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>np.sort(M)[::-1]</code> : 역순 정렬</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">41</span>]: <span class=\"built_in\">print</span>(np.sort(arr_1d)[::-<span class=\"number\">1</span>])</span><br><span class=\"line\">[<span class=\"number\">9</span> <span class=\"number\">8</span> <span class=\"number\">8</span> <span class=\"number\">5</span> <span class=\"number\">3</span> <span class=\"number\">2</span> <span class=\"number\">2</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li>행과 열 기준으로 정렬이 가능하다.<ul>\n<li><code>np.sort(x, axis=1)</code> : 열 기준</li>\n<li><code>np.sort(x, axis=0)</code> : 행 기준</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">42</span>]: <span class=\"built_in\">print</span>(np.sort(arr_2d, axis=<span class=\"number\">0</span>))</span><br><span class=\"line\">    ...: <span class=\"built_in\">print</span>(np.sort(arr_2d, axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">[[<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">0</span> <span class=\"number\">1</span>]</span><br><span class=\"line\"> [<span class=\"number\">2</span> <span class=\"number\">4</span> <span class=\"number\">3</span> <span class=\"number\">5</span>]</span><br><span class=\"line\"> [<span class=\"number\">8</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]]</span><br><span class=\"line\">[[<span class=\"number\">1</span> <span class=\"number\">3</span> <span class=\"number\">4</span> <span class=\"number\">8</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">4</span> <span class=\"number\">6</span> <span class=\"number\">7</span>]</span><br><span class=\"line\"> [<span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">5</span>]]</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"np-pad\"><a href=\"#np-pad\" class=\"headerlink\" title=\"np.pad\"></a>np.pad</h4><ul>\n<li>배열을 일정한 고정길이로 만들기 위해 특정한 값으로 채우는 함수.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Z = np.ones((<span class=\"number\">5</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">Z.pad(pad_width=<span class=\"number\">1</span>, </span><br><span class=\"line\">      mode=<span class=\"string\">&#x27;constant&#x27;</span>, <span class=\"comment\"># 특정한 값을 지정해서 패딩할 경우</span></span><br><span class=\"line\">      constant_values=<span class=\"number\">0</span>) <span class=\"comment\"># 값 지정</span></span><br><span class=\"line\"></span><br><span class=\"line\">Z      </span><br><span class=\"line\">[[<span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">0.</span>]</span><br><span class=\"line\"> [<span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span> <span class=\"number\">0.</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>2차원 배열 패딩</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">16</span>]: m = np.arange(<span class=\"number\">1</span>,<span class=\"number\">9</span>).reshape(<span class=\"number\">2</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">17</span>]: m</span><br><span class=\"line\">Out[<span class=\"number\">17</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],</span><br><span class=\"line\">       [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 행과 열 모두 2개씩 0으로 패딩 </span></span><br><span class=\"line\">In [<span class=\"number\">20</span>]: np.pad(m,((<span class=\"number\">2</span>,<span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">2</span>)),<span class=\"string\">&#x27;constant&#x27;</span>,constant_values =<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">20</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">21</span>]: np.pad(m,((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>)),<span class=\"string\">&#x27;constant&#x27;</span>,constant_values =<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">21</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">       [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<h3 id=\"numpy-broadcasting\"><a href=\"#numpy-broadcasting\" class=\"headerlink\" title=\"numpy broadcasting\"></a>numpy broadcasting</h3><ul>\n<li><p>numpy의 연산은 기본적으로 같은 크기의 배열간의 연산을 전제한다.</p>\n</li>\n<li><p>하지만 특정 조건을 만족했을 때 numpy는 자동적으로 크기가 다른 배열간의 연산을 수행하기도 하는데 이를 <code>broadcasting</code>이라 한다.</p>\n</li>\n<li><p><code>broadcasting</code> 연산이 성립되기 위한 다음의 3가지 규칙이 존재한다.</p>\n<ul>\n<li><strong>규칙 1: 두 배열의 차원 수가 다를 경우, 크기가 작은 배열의 모양은 맨 앞(왼쪽)에 패딩됨</strong></li>\n<li><strong>규칙 2: 두 배열의 모양이 임의의 차원에서 일치하지 않으면 해당 차원에서 모양이 1인 배열은 다른 모양과 일치하도록 확장됨</strong></li>\n<li><strong>규칙 3: 어떤 차원에서든 크기가 일치하지 않고 둘 다 1과 같지 않으면 오류가 발생.</strong></li>\n</ul>\n</li>\n<li><p><strong>단순히 한쪽의 크기를 맞춰서 연산이 가능하게끔 만드는 것이라고 생각하면 이해하기 쉽다.</strong></p>\n</li>\n</ul>\n<p><img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png\"></p>\n<ul>\n<li><code>a</code> 의 차원이 더 작기 때문에 규식 1,2 에 따라 연산시 <code>a</code> 가 padding 되고 확장됨.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: M = np.ones((<span class=\"number\">2</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">   ...: a = np.arange(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"built_in\">print</span>(M.shape)</span><br><span class=\"line\">(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: <span class=\"built_in\">print</span>(a.shape)</span><br><span class=\"line\">(<span class=\"number\">3</span>,)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: M + a</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]:</span><br><span class=\"line\">array([[<span class=\"number\">1.</span>, <span class=\"number\">2.</span>, <span class=\"number\">3.</span>],</span><br><span class=\"line\">       [<span class=\"number\">1.</span>, <span class=\"number\">2.</span>, <span class=\"number\">3.</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: a.shape</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: (<span class=\"number\">3</span>,)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>References</strong></p>\n<ul>\n<li><a href=\"https://numpy.org/doc/stable/index.html\">Numpy 공식문서</a></li>\n<li><a href=\"https://www.datacamp.com/community/blog/python-numpy-cheat-sheet\">numpy array transfrormation</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/numpy-indexing/\">numpy indexing</a></li>\n<li><a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html\">numpy broadcasting</a></li>\n<li><a href=\"https://sparrow.dev/numpy-pad/\">numpy padding</a></li>\n<li><a href=\"https://towardsdatascience.com/3-numpy-functions-to-facilitate-data-analysis-with-pandas-b1ad342a569\">numpy select</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]numpy 연산과 활용법","path":"2022/06/13/Preprocessing-numpy-basics/","eyeCatchImage":"https://media.geeksforgeeks.org/wp-content/uploads/Numpy1.jpg","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["numpy"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[pandas]기본자료형을 DataFrame으로 변환하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n<center>Kaggle Customer Score Dataset</center>\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\nPython으로 데이터 분석을 하다보면 pandas를 찾게 되는 경우가 많다.\nPython 기본 자료형을 pandas에서 제공하는 자료형으로 변환하는 데 익숙해지면 전처리작업을 보다 수월할게 할 수 있다.\n\n **여기서는 작업을 하면서 자주쓰게 되는 자료형 변환 방법들을 정리하였다**\n\n---\n\n아래 데이터를 사용해서 연습해보자\n\n|   Age |   CustomerID | Genre   |\n|------:|-------------:|:--------|\n|    19 |            1 | Male    |\n|    21 |            2 | Male    |\n|    20 |            3 | Female  |\n|    23 |            4 | Female  |\n|    31 |            5 | Female  |\n\n\n## Collection to DataFrame\n\n\n\n### Dictionary to DataFrame\n\n기본적으로 하나의 row가 하나의 dictionary형태로 list에 들어간다.\n\n```python\n\ndata={'Age': {0: 19, 1: 21, 2: 20, 3: 23, 4: 31},\n 'CustomerID': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n 'Genre': {0: 'Male', 1: 'Male', 2: 'Female', 3: 'Female', 4: 'Female'}}\ndf = pd.DataFrame(data)\n\n```\n\n### Tuple to DataFrame\n\nTuple은 분석단계에서보다는 DataBase와 연결해서 CRUD할 일이 있을 경우 자주 사용된다.\n\n```python\ndata=[(19, 1, 'Male'),\n (21, 2, 'Male'),\n (20, 3, 'Female'),\n (23, 4, 'Female'),\n (31, 5, 'Female')]\n\n\ndf = pd.DataFrame(data, columns=['Age', 'ID', 'Gender'])\n\n```\n\n### List to DataFrame\n\nlist의 경우 `list(zip(lst, lst2, lst3))` 로 tuple 형태로 데이터를 변환해준 뒤 DF를 만든다.\n\n```\n\ndf = pd.DataFrame(list(zip(lst, lst2, lst3)),\n               columns=['Age', 'ID', 'Gender'])\ndf\n\n```\n\n\n## DataFrame to Collection\n반대로 DataFrame에서 Python 기본 자료형을 받아야와 할 경우도 있다.\n\n이 경우는 pandas library에서 제공하는 함수들을 통해 쉽게 해결할 수 있다.\n\n### Dataframe to Ditonary\n\n```python\n\ndf.to_dict()\n\n>>{'Age': {0: 19, 1: 21, 2: 20, 3: 23, 4: 31},\n 'CustomerID': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n 'Genre': {0: 'Male', 1: 'Male', 2: 'Female', 3: 'Female', 4: 'Female'}}\n\n```\n\n### Dataframe to Tuple\n\n`itertuple()`을 사용할 시 name을 default로 넣으면 컬럼명도 같이 반환된다.\n\n```python\nlist(df.itertuples(index=False,name=None)) # df to tuple\n\n>>[(19, 1, 'Male'),\n (21, 2, 'Male'),\n (20, 3, 'Female'),\n (23, 4, 'Female'),\n (31, 5, 'Female')]\n\n```\n\n### DataFrame to List\n\n컬럼을 list로 변환\n\n```python\ndf.columns.values.tolist()\n```\n\n값을 list로 변환\n\n```python\ndf.values.tolist()\n\n>>[[19, 1, 'Male'],\n [21, 2, 'Male'],\n [20, 3, 'Female'],\n [23, 4, 'Female'],\n [31, 5, 'Female']]\n```\n\n## References\n\n- https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\n- https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples","source":"_posts/Preprocessing-pandas-collection-to-df.md","raw":"---\ntitle: '[pandas]기본자료형을 DataFrame으로 변환하기'\ncategories:\n    - [Preprocessing]\ntags:\n  - [pandas]\ndate:\nupdated:\n---\n\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n<center>Kaggle Customer Score Dataset</center>\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\nPython으로 데이터 분석을 하다보면 pandas를 찾게 되는 경우가 많다.\nPython 기본 자료형을 pandas에서 제공하는 자료형으로 변환하는 데 익숙해지면 전처리작업을 보다 수월할게 할 수 있다.\n\n **여기서는 작업을 하면서 자주쓰게 되는 자료형 변환 방법들을 정리하였다**\n\n---\n\n아래 데이터를 사용해서 연습해보자\n\n|   Age |   CustomerID | Genre   |\n|------:|-------------:|:--------|\n|    19 |            1 | Male    |\n|    21 |            2 | Male    |\n|    20 |            3 | Female  |\n|    23 |            4 | Female  |\n|    31 |            5 | Female  |\n\n\n## Collection to DataFrame\n\n\n\n### Dictionary to DataFrame\n\n기본적으로 하나의 row가 하나의 dictionary형태로 list에 들어간다.\n\n```python\n\ndata={'Age': {0: 19, 1: 21, 2: 20, 3: 23, 4: 31},\n 'CustomerID': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n 'Genre': {0: 'Male', 1: 'Male', 2: 'Female', 3: 'Female', 4: 'Female'}}\ndf = pd.DataFrame(data)\n\n```\n\n### Tuple to DataFrame\n\nTuple은 분석단계에서보다는 DataBase와 연결해서 CRUD할 일이 있을 경우 자주 사용된다.\n\n```python\ndata=[(19, 1, 'Male'),\n (21, 2, 'Male'),\n (20, 3, 'Female'),\n (23, 4, 'Female'),\n (31, 5, 'Female')]\n\n\ndf = pd.DataFrame(data, columns=['Age', 'ID', 'Gender'])\n\n```\n\n### List to DataFrame\n\nlist의 경우 `list(zip(lst, lst2, lst3))` 로 tuple 형태로 데이터를 변환해준 뒤 DF를 만든다.\n\n```\n\ndf = pd.DataFrame(list(zip(lst, lst2, lst3)),\n               columns=['Age', 'ID', 'Gender'])\ndf\n\n```\n\n\n## DataFrame to Collection\n반대로 DataFrame에서 Python 기본 자료형을 받아야와 할 경우도 있다.\n\n이 경우는 pandas library에서 제공하는 함수들을 통해 쉽게 해결할 수 있다.\n\n### Dataframe to Ditonary\n\n```python\n\ndf.to_dict()\n\n>>{'Age': {0: 19, 1: 21, 2: 20, 3: 23, 4: 31},\n 'CustomerID': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n 'Genre': {0: 'Male', 1: 'Male', 2: 'Female', 3: 'Female', 4: 'Female'}}\n\n```\n\n### Dataframe to Tuple\n\n`itertuple()`을 사용할 시 name을 default로 넣으면 컬럼명도 같이 반환된다.\n\n```python\nlist(df.itertuples(index=False,name=None)) # df to tuple\n\n>>[(19, 1, 'Male'),\n (21, 2, 'Male'),\n (20, 3, 'Female'),\n (23, 4, 'Female'),\n (31, 5, 'Female')]\n\n```\n\n### DataFrame to List\n\n컬럼을 list로 변환\n\n```python\ndf.columns.values.tolist()\n```\n\n값을 list로 변환\n\n```python\ndf.values.tolist()\n\n>>[[19, 1, 'Male'],\n [21, 2, 'Male'],\n [20, 3, 'Female'],\n [23, 4, 'Female'],\n [31, 5, 'Female']]\n```\n\n## References\n\n- https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\n- https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples","slug":"Preprocessing-pandas-collection-to-df","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscf0028b36qfuvl1vuj","content":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n<center>Kaggle Customer Score Dataset</center>\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>Python으로 데이터 분석을 하다보면 pandas를 찾게 되는 경우가 많다.<br>Python 기본 자료형을 pandas에서 제공하는 자료형으로 변환하는 데 익숙해지면 전처리작업을 보다 수월할게 할 수 있다.</p>\n<p> <strong>여기서는 작업을 하면서 자주쓰게 되는 자료형 변환 방법들을 정리하였다</strong></p>\n<hr>\n<p>아래 데이터를 사용해서 연습해보자</p>\n<table>\n<thead>\n<tr>\n<th align=\"right\">Age</th>\n<th align=\"right\">CustomerID</th>\n<th align=\"left\">Genre</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"right\">19</td>\n<td align=\"right\">1</td>\n<td align=\"left\">Male</td>\n</tr>\n<tr>\n<td align=\"right\">21</td>\n<td align=\"right\">2</td>\n<td align=\"left\">Male</td>\n</tr>\n<tr>\n<td align=\"right\">20</td>\n<td align=\"right\">3</td>\n<td align=\"left\">Female</td>\n</tr>\n<tr>\n<td align=\"right\">23</td>\n<td align=\"right\">4</td>\n<td align=\"left\">Female</td>\n</tr>\n<tr>\n<td align=\"right\">31</td>\n<td align=\"right\">5</td>\n<td align=\"left\">Female</td>\n</tr>\n</tbody></table>\n<h2 id=\"Collection-to-DataFrame\"><a href=\"#Collection-to-DataFrame\" class=\"headerlink\" title=\"Collection to DataFrame\"></a>Collection to DataFrame</h2><h3 id=\"Dictionary-to-DataFrame\"><a href=\"#Dictionary-to-DataFrame\" class=\"headerlink\" title=\"Dictionary to DataFrame\"></a>Dictionary to DataFrame</h3><p>기본적으로 하나의 row가 하나의 dictionary형태로 list에 들어간다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">data=&#123;<span class=\"string\">&#x27;Age&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">19</span>, <span class=\"number\">1</span>: <span class=\"number\">21</span>, <span class=\"number\">2</span>: <span class=\"number\">20</span>, <span class=\"number\">3</span>: <span class=\"number\">23</span>, <span class=\"number\">4</span>: <span class=\"number\">31</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;CustomerID&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">1</span>, <span class=\"number\">1</span>: <span class=\"number\">2</span>, <span class=\"number\">2</span>: <span class=\"number\">3</span>, <span class=\"number\">3</span>: <span class=\"number\">4</span>, <span class=\"number\">4</span>: <span class=\"number\">5</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;Genre&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">1</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">2</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">3</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">4</span>: <span class=\"string\">&#x27;Female&#x27;</span>&#125;&#125;</span><br><span class=\"line\">df = pd.DataFrame(data)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tuple-to-DataFrame\"><a href=\"#Tuple-to-DataFrame\" class=\"headerlink\" title=\"Tuple to DataFrame\"></a>Tuple to DataFrame</h3><p>Tuple은 분석단계에서보다는 DataBase와 연결해서 CRUD할 일이 있을 경우 자주 사용된다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data=[(<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(data, columns=[<span class=\"string\">&#x27;Age&#x27;</span>, <span class=\"string\">&#x27;ID&#x27;</span>, <span class=\"string\">&#x27;Gender&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"List-to-DataFrame\"><a href=\"#List-to-DataFrame\" class=\"headerlink\" title=\"List to DataFrame\"></a>List to DataFrame</h3><p>list의 경우 <code>list(zip(lst, lst2, lst3))</code> 로 tuple 형태로 데이터를 변환해준 뒤 DF를 만든다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(list(zip(lst, lst2, lst3)),</span><br><span class=\"line\">               columns=[&#x27;Age&#x27;, &#x27;ID&#x27;, &#x27;Gender&#x27;])</span><br><span class=\"line\">df</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"DataFrame-to-Collection\"><a href=\"#DataFrame-to-Collection\" class=\"headerlink\" title=\"DataFrame to Collection\"></a>DataFrame to Collection</h2><p>반대로 DataFrame에서 Python 기본 자료형을 받아야와 할 경우도 있다.</p>\n<p>이 경우는 pandas library에서 제공하는 함수들을 통해 쉽게 해결할 수 있다.</p>\n<h3 id=\"Dataframe-to-Ditonary\"><a href=\"#Dataframe-to-Ditonary\" class=\"headerlink\" title=\"Dataframe to Ditonary\"></a>Dataframe to Ditonary</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df.to_dict()</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&#123;<span class=\"string\">&#x27;Age&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">19</span>, <span class=\"number\">1</span>: <span class=\"number\">21</span>, <span class=\"number\">2</span>: <span class=\"number\">20</span>, <span class=\"number\">3</span>: <span class=\"number\">23</span>, <span class=\"number\">4</span>: <span class=\"number\">31</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;CustomerID&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">1</span>, <span class=\"number\">1</span>: <span class=\"number\">2</span>, <span class=\"number\">2</span>: <span class=\"number\">3</span>, <span class=\"number\">3</span>: <span class=\"number\">4</span>, <span class=\"number\">4</span>: <span class=\"number\">5</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;Genre&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">1</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">2</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">3</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">4</span>: <span class=\"string\">&#x27;Female&#x27;</span>&#125;&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Dataframe-to-Tuple\"><a href=\"#Dataframe-to-Tuple\" class=\"headerlink\" title=\"Dataframe to Tuple\"></a>Dataframe to Tuple</h3><p><code>itertuple()</code>을 사용할 시 name을 default로 넣으면 컬럼명도 같이 반환된다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>(df.itertuples(index=<span class=\"literal\">False</span>,name=<span class=\"literal\">None</span>)) <span class=\"comment\"># df to tuple</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;[(<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>)]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DataFrame-to-List\"><a href=\"#DataFrame-to-List\" class=\"headerlink\" title=\"DataFrame to List\"></a>DataFrame to List</h3><p>컬럼을 list로 변환</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.columns.values.tolist()</span><br></pre></td></tr></table></figure>\n\n<p>값을 list로 변환</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.values.tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;[[<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>]]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\">https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/</a></li>\n<li><a href=\"https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples\">https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n<center>Kaggle Customer Score Dataset</center>\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>Python으로 데이터 분석을 하다보면 pandas를 찾게 되는 경우가 많다.<br>Python 기본 자료형을 pandas에서 제공하는 자료형으로 변환하는 데 익숙해지면 전처리작업을 보다 수월할게 할 수 있다.</p>\n<p> <strong>여기서는 작업을 하면서 자주쓰게 되는 자료형 변환 방법들을 정리하였다</strong></p>\n<hr>\n<p>아래 데이터를 사용해서 연습해보자</p>\n<table>\n<thead>\n<tr>\n<th align=\"right\">Age</th>\n<th align=\"right\">CustomerID</th>\n<th align=\"left\">Genre</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"right\">19</td>\n<td align=\"right\">1</td>\n<td align=\"left\">Male</td>\n</tr>\n<tr>\n<td align=\"right\">21</td>\n<td align=\"right\">2</td>\n<td align=\"left\">Male</td>\n</tr>\n<tr>\n<td align=\"right\">20</td>\n<td align=\"right\">3</td>\n<td align=\"left\">Female</td>\n</tr>\n<tr>\n<td align=\"right\">23</td>\n<td align=\"right\">4</td>\n<td align=\"left\">Female</td>\n</tr>\n<tr>\n<td align=\"right\">31</td>\n<td align=\"right\">5</td>\n<td align=\"left\">Female</td>\n</tr>\n</tbody></table>\n<h2 id=\"Collection-to-DataFrame\"><a href=\"#Collection-to-DataFrame\" class=\"headerlink\" title=\"Collection to DataFrame\"></a>Collection to DataFrame</h2><h3 id=\"Dictionary-to-DataFrame\"><a href=\"#Dictionary-to-DataFrame\" class=\"headerlink\" title=\"Dictionary to DataFrame\"></a>Dictionary to DataFrame</h3><p>기본적으로 하나의 row가 하나의 dictionary형태로 list에 들어간다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">data=&#123;<span class=\"string\">&#x27;Age&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">19</span>, <span class=\"number\">1</span>: <span class=\"number\">21</span>, <span class=\"number\">2</span>: <span class=\"number\">20</span>, <span class=\"number\">3</span>: <span class=\"number\">23</span>, <span class=\"number\">4</span>: <span class=\"number\">31</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;CustomerID&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">1</span>, <span class=\"number\">1</span>: <span class=\"number\">2</span>, <span class=\"number\">2</span>: <span class=\"number\">3</span>, <span class=\"number\">3</span>: <span class=\"number\">4</span>, <span class=\"number\">4</span>: <span class=\"number\">5</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;Genre&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">1</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">2</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">3</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">4</span>: <span class=\"string\">&#x27;Female&#x27;</span>&#125;&#125;</span><br><span class=\"line\">df = pd.DataFrame(data)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Tuple-to-DataFrame\"><a href=\"#Tuple-to-DataFrame\" class=\"headerlink\" title=\"Tuple to DataFrame\"></a>Tuple to DataFrame</h3><p>Tuple은 분석단계에서보다는 DataBase와 연결해서 CRUD할 일이 있을 경우 자주 사용된다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data=[(<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(data, columns=[<span class=\"string\">&#x27;Age&#x27;</span>, <span class=\"string\">&#x27;ID&#x27;</span>, <span class=\"string\">&#x27;Gender&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"List-to-DataFrame\"><a href=\"#List-to-DataFrame\" class=\"headerlink\" title=\"List to DataFrame\"></a>List to DataFrame</h3><p>list의 경우 <code>list(zip(lst, lst2, lst3))</code> 로 tuple 형태로 데이터를 변환해준 뒤 DF를 만든다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(list(zip(lst, lst2, lst3)),</span><br><span class=\"line\">               columns=[&#x27;Age&#x27;, &#x27;ID&#x27;, &#x27;Gender&#x27;])</span><br><span class=\"line\">df</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"DataFrame-to-Collection\"><a href=\"#DataFrame-to-Collection\" class=\"headerlink\" title=\"DataFrame to Collection\"></a>DataFrame to Collection</h2><p>반대로 DataFrame에서 Python 기본 자료형을 받아야와 할 경우도 있다.</p>\n<p>이 경우는 pandas library에서 제공하는 함수들을 통해 쉽게 해결할 수 있다.</p>\n<h3 id=\"Dataframe-to-Ditonary\"><a href=\"#Dataframe-to-Ditonary\" class=\"headerlink\" title=\"Dataframe to Ditonary\"></a>Dataframe to Ditonary</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df.to_dict()</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&#123;<span class=\"string\">&#x27;Age&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">19</span>, <span class=\"number\">1</span>: <span class=\"number\">21</span>, <span class=\"number\">2</span>: <span class=\"number\">20</span>, <span class=\"number\">3</span>: <span class=\"number\">23</span>, <span class=\"number\">4</span>: <span class=\"number\">31</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;CustomerID&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"number\">1</span>, <span class=\"number\">1</span>: <span class=\"number\">2</span>, <span class=\"number\">2</span>: <span class=\"number\">3</span>, <span class=\"number\">3</span>: <span class=\"number\">4</span>, <span class=\"number\">4</span>: <span class=\"number\">5</span>&#125;,</span><br><span class=\"line\"> <span class=\"string\">&#x27;Genre&#x27;</span>: &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">1</span>: <span class=\"string\">&#x27;Male&#x27;</span>, <span class=\"number\">2</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">3</span>: <span class=\"string\">&#x27;Female&#x27;</span>, <span class=\"number\">4</span>: <span class=\"string\">&#x27;Female&#x27;</span>&#125;&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Dataframe-to-Tuple\"><a href=\"#Dataframe-to-Tuple\" class=\"headerlink\" title=\"Dataframe to Tuple\"></a>Dataframe to Tuple</h3><p><code>itertuple()</code>을 사용할 시 name을 default로 넣으면 컬럼명도 같이 반환된다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>(df.itertuples(index=<span class=\"literal\">False</span>,name=<span class=\"literal\">None</span>)) <span class=\"comment\"># df to tuple</span></span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;[(<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>),</span><br><span class=\"line\"> (<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>)]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DataFrame-to-List\"><a href=\"#DataFrame-to-List\" class=\"headerlink\" title=\"DataFrame to List\"></a>DataFrame to List</h3><p>컬럼을 list로 변환</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.columns.values.tolist()</span><br></pre></td></tr></table></figure>\n\n<p>값을 list로 변환</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.values.tolist()</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;[[<span class=\"number\">19</span>, <span class=\"number\">1</span>, <span class=\"string\">&#x27;Male&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">21</span>, <span class=\"number\">2</span>, <span class=\"string\">&#x27;Male&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">20</span>, <span class=\"number\">3</span>, <span class=\"string\">&#x27;Female&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">23</span>, <span class=\"number\">4</span>, <span class=\"string\">&#x27;Female&#x27;</span>],</span><br><span class=\"line\"> [<span class=\"number\">31</span>, <span class=\"number\">5</span>, <span class=\"string\">&#x27;Female&#x27;</span>]]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\">https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/</a></li>\n<li><a href=\"https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples\">https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[pandas]기본자료형을 DataFrame으로 변환하기","path":"2022/06/13/Preprocessing-pandas-collection-to-df/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["pandas"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[pandas]pandas의 특정 열 제외한 모든 컬럼 출력하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n## tricks\nhttps://towardsdatascience.com/30-examples-to-master-pandas-f8a2da751fa4\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**DataFrame의 특정 열 제외하기**\n\n---\n\n## 하나의 컬럼을 DF에서 제거할 결우\n\n기본적으로 두 가지 방법이 있다.\n\n**drop 활용**\n```python\ndf.drop(column_name, axis=1)\n```\n\n**.loc 활용**\n\n```python\ndf.loc[:, df.columns != col]\n```\ndrop이 익숙하다보니 좀 더 많이 사용하게 된다.\n\n## 여러 컬럼을 DF에서 제거할 경우\n\n**indexing 사용**\n```python\ndf[df.columns[~df.columns.isin(['지울','컬럼'])]]\n```\n\n\n**difference 사용**\n```python\ndf[df.columns.difference(['지울', '칼럼'])]\n\n```\n\ndifference가 있는걸 모르고 계속 isin을 써왔다.\ndifference에도 익숙해져야 겠다.\n\n\n## References\n\n- https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use","source":"_posts/Preprocessing-pandas-remove_col.md","raw":"---\ntitle: '[pandas]pandas의 특정 열 제외한 모든 컬럼 출력하기'\ncategories:\n  - Preprocessing\ntags:\n  - pandas\n  - Python\ndate:\nupdated:\n---\n\n<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n## tricks\nhttps://towardsdatascience.com/30-examples-to-master-pandas-f8a2da751fa4\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**DataFrame의 특정 열 제외하기**\n\n---\n\n## 하나의 컬럼을 DF에서 제거할 결우\n\n기본적으로 두 가지 방법이 있다.\n\n**drop 활용**\n```python\ndf.drop(column_name, axis=1)\n```\n\n**.loc 활용**\n\n```python\ndf.loc[:, df.columns != col]\n```\ndrop이 익숙하다보니 좀 더 많이 사용하게 된다.\n\n## 여러 컬럼을 DF에서 제거할 경우\n\n**indexing 사용**\n```python\ndf[df.columns[~df.columns.isin(['지울','컬럼'])]]\n```\n\n\n**difference 사용**\n```python\ndf[df.columns.difference(['지울', '칼럼'])]\n\n```\n\ndifference가 있는걸 모르고 계속 isin을 써왔다.\ndifference에도 익숙해져야 겠다.\n\n\n## References\n\n- https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use","slug":"Preprocessing-pandas-remove_col","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscg002ab36qc1qq3ztt","content":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n## tricks\nhttps://towardsdatascience.com/30-examples-to-master-pandas-f8a2da751fa4\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>DataFrame의 특정 열 제외하기</strong></p>\n<hr>\n<h2 id=\"하나의-컬럼을-DF에서-제거할-결우\"><a href=\"#하나의-컬럼을-DF에서-제거할-결우\" class=\"headerlink\" title=\"하나의 컬럼을 DF에서 제거할 결우\"></a>하나의 컬럼을 DF에서 제거할 결우</h2><p>기본적으로 두 가지 방법이 있다.</p>\n<p><strong>drop 활용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop(column_name, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>.loc 활용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[:, df.columns != col]</span><br></pre></td></tr></table></figure>\n<p>drop이 익숙하다보니 좀 더 많이 사용하게 된다.</p>\n<h2 id=\"여러-컬럼을-DF에서-제거할-경우\"><a href=\"#여러-컬럼을-DF에서-제거할-경우\" class=\"headerlink\" title=\"여러 컬럼을 DF에서 제거할 경우\"></a>여러 컬럼을 DF에서 제거할 경우</h2><p><strong>indexing 사용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.columns[~df.columns.isin([<span class=\"string\">&#x27;지울&#x27;</span>,<span class=\"string\">&#x27;컬럼&#x27;</span>])]]</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>difference 사용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.columns.difference([<span class=\"string\">&#x27;지울&#x27;</span>, <span class=\"string\">&#x27;칼럼&#x27;</span>])]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>difference가 있는걸 모르고 계속 isin을 써왔다.<br>difference에도 익숙해져야 겠다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use\">https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n- ML\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n## tricks\nhttps://towardsdatascience.com/30-examples-to-master-pandas-f8a2da751fa4\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>DataFrame의 특정 열 제외하기</strong></p>\n<hr>\n<h2 id=\"하나의-컬럼을-DF에서-제거할-결우\"><a href=\"#하나의-컬럼을-DF에서-제거할-결우\" class=\"headerlink\" title=\"하나의 컬럼을 DF에서 제거할 결우\"></a>하나의 컬럼을 DF에서 제거할 결우</h2><p>기본적으로 두 가지 방법이 있다.</p>\n<p><strong>drop 활용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop(column_name, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>.loc 활용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[:, df.columns != col]</span><br></pre></td></tr></table></figure>\n<p>drop이 익숙하다보니 좀 더 많이 사용하게 된다.</p>\n<h2 id=\"여러-컬럼을-DF에서-제거할-경우\"><a href=\"#여러-컬럼을-DF에서-제거할-경우\" class=\"headerlink\" title=\"여러 컬럼을 DF에서 제거할 경우\"></a>여러 컬럼을 DF에서 제거할 경우</h2><p><strong>indexing 사용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.columns[~df.columns.isin([<span class=\"string\">&#x27;지울&#x27;</span>,<span class=\"string\">&#x27;컬럼&#x27;</span>])]]</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>difference 사용</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df.columns.difference([<span class=\"string\">&#x27;지울&#x27;</span>, <span class=\"string\">&#x27;칼럼&#x27;</span>])]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>difference가 있는걸 모르고 계속 isin을 써왔다.<br>difference에도 익숙해져야 겠다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use\">https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[pandas]pandas의 특정 열 제외한 모든 컬럼 출력하기","path":"2022/06/13/Preprocessing-pandas-remove_col/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["Python","pandas"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[pandas]Pandas Groupby용법 간단히 정리","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n## Intro\npandas에서 제공하는 groupby는 기본적으로 데이터 범주별 요약통계량을 계산하는 일을 한다. sql의 groupby나 R dplyr의 groupby와 유사하다고 생각하면 된다.  \n여기서는 전처리과정에서 자주쓰게 되는 groupby 용법을 살펴본다.\n\n---\n\n## 기본적인 용법들\n\n\n```python\n# 데이터 불러오기\nimport pandas as pd\ndrinks = pd.read_csv('http://bit.ly/drinksbycountry')\n```\n\n\n```python\ndrinks.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 기초적인 용법 : 대륙별 beer_servings 평균\ndrinks.groupby('continent').beer_servings.mean()\n```\n\n\n\n\n    continent\n    Africa            61.471698\n    Asia              37.045455\n    Europe           193.777778\n    North America    145.434783\n    Oceania           89.687500\n    South America    175.083333\n    Name: beer_servings, dtype: float64\n\n\n\n.agg()와 같은 집계함수를 사용해 한 변수의 여러 요약통계량을 구하는 것이 가능하다.\n\n\n```python\ndrinks[drinks.continent=='Asia'].beer_servings.agg(['count','mean','max','min'])\n```\n\n\n\n\n    count     44.000000\n    mean      37.045455\n    max      247.000000\n    min        0.000000\n    Name: beer_servings, dtype: float64\n\n\n\n\n```python\ndrinks.groupby('continent').beer_servings.agg(['count','mean','max','min'])\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>53</td>\n      <td>61.471698</td>\n      <td>376</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>44</td>\n      <td>37.045455</td>\n      <td>247</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>45</td>\n      <td>193.777778</td>\n      <td>361</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>23</td>\n      <td>145.434783</td>\n      <td>285</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>16</td>\n      <td>89.687500</td>\n      <td>306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>12</td>\n      <td>175.083333</td>\n      <td>333</td>\n      <td>93</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 분석할 칼럼을 지정해주지 않으면 모든 numeric의 평균을 그룹별로 반환한다.\ndrinks.groupby('continent').mean()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>61.471698</td>\n      <td>16.339623</td>\n      <td>16.264151</td>\n      <td>3.007547</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>37.045455</td>\n      <td>60.840909</td>\n      <td>9.068182</td>\n      <td>2.170455</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>193.777778</td>\n      <td>132.555556</td>\n      <td>142.222222</td>\n      <td>8.617778</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>145.434783</td>\n      <td>165.739130</td>\n      <td>24.521739</td>\n      <td>5.995652</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>89.687500</td>\n      <td>58.437500</td>\n      <td>35.625000</td>\n      <td>3.381250</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>175.083333</td>\n      <td>114.750000</td>\n      <td>62.416667</td>\n      <td>6.308333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# m\n%matplotlib inline\ndrinks.groupby('continent').mean().plot(kind='bar')\n```\n\n\n![](https://i.imgur.com/KvoD6CS.png)    \n\n\n## 응용하기\n- Groupby에서 특정 그룹에 접근하기\n- Groupby에서 특정 그룹에 접근 후 필터링 하기 (filter 사용)\n- pd.cut 을 사용한 파생변수 만들기\n\n\n```python\n# 아시아 그룹만 \ndrinks.groupby('continent').get_group('Asia').head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Bahrain</td>\n      <td>42</td>\n      <td>63</td>\n      <td>7</td>\n      <td>2.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Bangladesh</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Bhutan</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Brunei</td>\n      <td>31</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>Asia</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 여러 그룹의 통계량을 조건걸어서 구할 경우\ndrinks.groupby(['wine_servings', 'continent']).get_group((0, 'Asia')).total_litres_of_pure_alcohol.sum()\n```\n\n\n\n\n    6.2\n\n\n\n\n```python\n# pd.cut을 활용한 연속형 변수의 구간화 변수생성\ndrinks['Range'] = drinks.groupby('country').beer_servings.apply(pd.cut, bins=2)\n```\n\n\n```python\ndrinks.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n      <th>Range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n      <td>(-0.001, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n      <td>(88.911, 89.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n      <td>(24.975, 25.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n      <td>(244.755, 245.0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n      <td>(216.783, 217.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nTF를 반환하는 lamba 함수를 작성할 경우 any()나 all()을 써서 값을 반환해줄 필요가 있다.\n\n```python\n## filter를 사용한 조건식. 위의 결과와 같은 값을 리턴한다.\ndrinks.groupby(['wine_servings','continent']).filter(lambda x : ((x.wine_servings == 0) & (x.continent=='Asia') ).any()).total_litres_of_pure_alcohol.sum()\n\n```\n\n\n\n\n    6.2\n\n\n## References\n* https://www.youtube.com/watch?v=qy0fDqoMJx8\n* https://pandas.pydata.org/docs/reference/groupby.html\n\n\n## 다음에 정리할 것\n* any()와 all() 관련 함수\n* filter\n* assign\n* pd.cut과 np.digitize를 활용한 연속형 변수의 구간화\n* pandas query as dplyr filter\n","source":"_posts/Preprocessing-pandas_groupby.md","raw":"---\ntitle: \"[pandas]Pandas Groupby용법 간단히 정리\"\ndate: \nupdated:\ncategories: \n      - Preprocessing\ntags:\n  - [pandas]\n---\n\n## Intro\npandas에서 제공하는 groupby는 기본적으로 데이터 범주별 요약통계량을 계산하는 일을 한다. sql의 groupby나 R dplyr의 groupby와 유사하다고 생각하면 된다.  \n여기서는 전처리과정에서 자주쓰게 되는 groupby 용법을 살펴본다.\n\n---\n\n## 기본적인 용법들\n\n\n```python\n# 데이터 불러오기\nimport pandas as pd\ndrinks = pd.read_csv('http://bit.ly/drinksbycountry')\n```\n\n\n```python\ndrinks.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 기초적인 용법 : 대륙별 beer_servings 평균\ndrinks.groupby('continent').beer_servings.mean()\n```\n\n\n\n\n    continent\n    Africa            61.471698\n    Asia              37.045455\n    Europe           193.777778\n    North America    145.434783\n    Oceania           89.687500\n    South America    175.083333\n    Name: beer_servings, dtype: float64\n\n\n\n.agg()와 같은 집계함수를 사용해 한 변수의 여러 요약통계량을 구하는 것이 가능하다.\n\n\n```python\ndrinks[drinks.continent=='Asia'].beer_servings.agg(['count','mean','max','min'])\n```\n\n\n\n\n    count     44.000000\n    mean      37.045455\n    max      247.000000\n    min        0.000000\n    Name: beer_servings, dtype: float64\n\n\n\n\n```python\ndrinks.groupby('continent').beer_servings.agg(['count','mean','max','min'])\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>53</td>\n      <td>61.471698</td>\n      <td>376</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>44</td>\n      <td>37.045455</td>\n      <td>247</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>45</td>\n      <td>193.777778</td>\n      <td>361</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>23</td>\n      <td>145.434783</td>\n      <td>285</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>16</td>\n      <td>89.687500</td>\n      <td>306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>12</td>\n      <td>175.083333</td>\n      <td>333</td>\n      <td>93</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 분석할 칼럼을 지정해주지 않으면 모든 numeric의 평균을 그룹별로 반환한다.\ndrinks.groupby('continent').mean()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>61.471698</td>\n      <td>16.339623</td>\n      <td>16.264151</td>\n      <td>3.007547</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>37.045455</td>\n      <td>60.840909</td>\n      <td>9.068182</td>\n      <td>2.170455</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>193.777778</td>\n      <td>132.555556</td>\n      <td>142.222222</td>\n      <td>8.617778</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>145.434783</td>\n      <td>165.739130</td>\n      <td>24.521739</td>\n      <td>5.995652</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>89.687500</td>\n      <td>58.437500</td>\n      <td>35.625000</td>\n      <td>3.381250</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>175.083333</td>\n      <td>114.750000</td>\n      <td>62.416667</td>\n      <td>6.308333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# m\n%matplotlib inline\ndrinks.groupby('continent').mean().plot(kind='bar')\n```\n\n\n![](https://i.imgur.com/KvoD6CS.png)    \n\n\n## 응용하기\n- Groupby에서 특정 그룹에 접근하기\n- Groupby에서 특정 그룹에 접근 후 필터링 하기 (filter 사용)\n- pd.cut 을 사용한 파생변수 만들기\n\n\n```python\n# 아시아 그룹만 \ndrinks.groupby('continent').get_group('Asia').head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Bahrain</td>\n      <td>42</td>\n      <td>63</td>\n      <td>7</td>\n      <td>2.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Bangladesh</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Bhutan</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Brunei</td>\n      <td>31</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>Asia</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# 여러 그룹의 통계량을 조건걸어서 구할 경우\ndrinks.groupby(['wine_servings', 'continent']).get_group((0, 'Asia')).total_litres_of_pure_alcohol.sum()\n```\n\n\n\n\n    6.2\n\n\n\n\n```python\n# pd.cut을 활용한 연속형 변수의 구간화 변수생성\ndrinks['Range'] = drinks.groupby('country').beer_servings.apply(pd.cut, bins=2)\n```\n\n\n```python\ndrinks.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n      <th>Range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n      <td>(-0.001, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n      <td>(88.911, 89.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n      <td>(24.975, 25.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n      <td>(244.755, 245.0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n      <td>(216.783, 217.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nTF를 반환하는 lamba 함수를 작성할 경우 any()나 all()을 써서 값을 반환해줄 필요가 있다.\n\n```python\n## filter를 사용한 조건식. 위의 결과와 같은 값을 리턴한다.\ndrinks.groupby(['wine_servings','continent']).filter(lambda x : ((x.wine_servings == 0) & (x.continent=='Asia') ).any()).total_litres_of_pure_alcohol.sum()\n\n```\n\n\n\n\n    6.2\n\n\n## References\n* https://www.youtube.com/watch?v=qy0fDqoMJx8\n* https://pandas.pydata.org/docs/reference/groupby.html\n\n\n## 다음에 정리할 것\n* any()와 all() 관련 함수\n* filter\n* assign\n* pd.cut과 np.digitize를 활용한 연속형 변수의 구간화\n* pandas query as dplyr filter\n","slug":"Preprocessing-pandas_groupby","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscg002db36q53p84ifr","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>pandas에서 제공하는 groupby는 기본적으로 데이터 범주별 요약통계량을 계산하는 일을 한다. sql의 groupby나 R dplyr의 groupby와 유사하다고 생각하면 된다.<br>여기서는 전처리과정에서 자주쓰게 되는 groupby 용법을 살펴본다.</p>\n<hr>\n<h2 id=\"기본적인-용법들\"><a href=\"#기본적인-용법들\" class=\"headerlink\" title=\"기본적인 용법들\"></a>기본적인 용법들</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 데이터 불러오기</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">drinks = pd.read_csv(<span class=\"string\">&#x27;http://bit.ly/drinksbycountry&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기초적인 용법 : 대륙별 beer_servings 평균</span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).beer_servings.mean()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>continent\nAfrica            61.471698\nAsia              37.045455\nEurope           193.777778\nNorth America    145.434783\nOceania           89.687500\nSouth America    175.083333\nName: beer_servings, dtype: float64\n</code></pre>\n<p>.agg()와 같은 집계함수를 사용해 한 변수의 여러 요약통계량을 구하는 것이 가능하다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks[drinks.continent==<span class=\"string\">&#x27;Asia&#x27;</span>].beer_servings.agg([<span class=\"string\">&#x27;count&#x27;</span>,<span class=\"string\">&#x27;mean&#x27;</span>,<span class=\"string\">&#x27;max&#x27;</span>,<span class=\"string\">&#x27;min&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>count     44.000000\nmean      37.045455\nmax      247.000000\nmin        0.000000\nName: beer_servings, dtype: float64\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).beer_servings.agg([<span class=\"string\">&#x27;count&#x27;</span>,<span class=\"string\">&#x27;mean&#x27;</span>,<span class=\"string\">&#x27;max&#x27;</span>,<span class=\"string\">&#x27;min&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>53</td>\n      <td>61.471698</td>\n      <td>376</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>44</td>\n      <td>37.045455</td>\n      <td>247</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>45</td>\n      <td>193.777778</td>\n      <td>361</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>23</td>\n      <td>145.434783</td>\n      <td>285</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>16</td>\n      <td>89.687500</td>\n      <td>306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>12</td>\n      <td>175.083333</td>\n      <td>333</td>\n      <td>93</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 분석할 칼럼을 지정해주지 않으면 모든 numeric의 평균을 그룹별로 반환한다.</span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).mean()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>61.471698</td>\n      <td>16.339623</td>\n      <td>16.264151</td>\n      <td>3.007547</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>37.045455</td>\n      <td>60.840909</td>\n      <td>9.068182</td>\n      <td>2.170455</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>193.777778</td>\n      <td>132.555556</td>\n      <td>142.222222</td>\n      <td>8.617778</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>145.434783</td>\n      <td>165.739130</td>\n      <td>24.521739</td>\n      <td>5.995652</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>89.687500</td>\n      <td>58.437500</td>\n      <td>35.625000</td>\n      <td>3.381250</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>175.083333</td>\n      <td>114.750000</td>\n      <td>62.416667</td>\n      <td>6.308333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># m</span></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).mean().plot(kind=<span class=\"string\">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://i.imgur.com/KvoD6CS.png\">    </p>\n<h2 id=\"응용하기\"><a href=\"#응용하기\" class=\"headerlink\" title=\"응용하기\"></a>응용하기</h2><ul>\n<li>Groupby에서 특정 그룹에 접근하기</li>\n<li>Groupby에서 특정 그룹에 접근 후 필터링 하기 (filter 사용)</li>\n<li>pd.cut 을 사용한 파생변수 만들기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 아시아 그룹만 </span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).get_group(<span class=\"string\">&#x27;Asia&#x27;</span>).head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Bahrain</td>\n      <td>42</td>\n      <td>63</td>\n      <td>7</td>\n      <td>2.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Bangladesh</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Bhutan</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Brunei</td>\n      <td>31</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>Asia</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 여러 그룹의 통계량을 조건걸어서 구할 경우</span></span><br><span class=\"line\">drinks.groupby([<span class=\"string\">&#x27;wine_servings&#x27;</span>, <span class=\"string\">&#x27;continent&#x27;</span>]).get_group((<span class=\"number\">0</span>, <span class=\"string\">&#x27;Asia&#x27;</span>)).total_litres_of_pure_alcohol.<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>6.2\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># pd.cut을 활용한 연속형 변수의 구간화 변수생성</span></span><br><span class=\"line\">drinks[<span class=\"string\">&#x27;Range&#x27;</span>] = drinks.groupby(<span class=\"string\">&#x27;country&#x27;</span>).beer_servings.apply(pd.cut, bins=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n      <th>Range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n      <td>(-0.001, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n      <td>(88.911, 89.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n      <td>(24.975, 25.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n      <td>(244.755, 245.0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n      <td>(216.783, 217.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n<p>TF를 반환하는 lamba 함수를 작성할 경우 any()나 all()을 써서 값을 반환해줄 필요가 있다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">## filter를 사용한 조건식. 위의 결과와 같은 값을 리턴한다.</span></span><br><span class=\"line\">drinks.groupby([<span class=\"string\">&#x27;wine_servings&#x27;</span>,<span class=\"string\">&#x27;continent&#x27;</span>]).<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x : ((x.wine_servings == <span class=\"number\">0</span>) &amp; (x.continent==<span class=\"string\">&#x27;Asia&#x27;</span>) ).<span class=\"built_in\">any</span>()).total_litres_of_pure_alcohol.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>6.2\n</code></pre>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.youtube.com/watch?v=qy0fDqoMJx8\">https://www.youtube.com/watch?v=qy0fDqoMJx8</a></li>\n<li><a href=\"https://pandas.pydata.org/docs/reference/groupby.html\">https://pandas.pydata.org/docs/reference/groupby.html</a></li>\n</ul>\n<h2 id=\"다음에-정리할-것\"><a href=\"#다음에-정리할-것\" class=\"headerlink\" title=\"다음에 정리할 것\"></a>다음에 정리할 것</h2><ul>\n<li>any()와 all() 관련 함수</li>\n<li>filter</li>\n<li>assign</li>\n<li>pd.cut과 np.digitize를 활용한 연속형 변수의 구간화</li>\n<li>pandas query as dplyr filter</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>pandas에서 제공하는 groupby는 기본적으로 데이터 범주별 요약통계량을 계산하는 일을 한다. sql의 groupby나 R dplyr의 groupby와 유사하다고 생각하면 된다.<br>여기서는 전처리과정에서 자주쓰게 되는 groupby 용법을 살펴본다.</p>\n<hr>\n<h2 id=\"기본적인-용법들\"><a href=\"#기본적인-용법들\" class=\"headerlink\" title=\"기본적인 용법들\"></a>기본적인 용법들</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 데이터 불러오기</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">drinks = pd.read_csv(<span class=\"string\">&#x27;http://bit.ly/drinksbycountry&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기초적인 용법 : 대륙별 beer_servings 평균</span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).beer_servings.mean()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>continent\nAfrica            61.471698\nAsia              37.045455\nEurope           193.777778\nNorth America    145.434783\nOceania           89.687500\nSouth America    175.083333\nName: beer_servings, dtype: float64\n</code></pre>\n<p>.agg()와 같은 집계함수를 사용해 한 변수의 여러 요약통계량을 구하는 것이 가능하다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks[drinks.continent==<span class=\"string\">&#x27;Asia&#x27;</span>].beer_servings.agg([<span class=\"string\">&#x27;count&#x27;</span>,<span class=\"string\">&#x27;mean&#x27;</span>,<span class=\"string\">&#x27;max&#x27;</span>,<span class=\"string\">&#x27;min&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>count     44.000000\nmean      37.045455\nmax      247.000000\nmin        0.000000\nName: beer_servings, dtype: float64\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).beer_servings.agg([<span class=\"string\">&#x27;count&#x27;</span>,<span class=\"string\">&#x27;mean&#x27;</span>,<span class=\"string\">&#x27;max&#x27;</span>,<span class=\"string\">&#x27;min&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>53</td>\n      <td>61.471698</td>\n      <td>376</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>44</td>\n      <td>37.045455</td>\n      <td>247</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>45</td>\n      <td>193.777778</td>\n      <td>361</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>23</td>\n      <td>145.434783</td>\n      <td>285</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>16</td>\n      <td>89.687500</td>\n      <td>306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>12</td>\n      <td>175.083333</td>\n      <td>333</td>\n      <td>93</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 분석할 칼럼을 지정해주지 않으면 모든 numeric의 평균을 그룹별로 반환한다.</span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).mean()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n    </tr>\n    <tr>\n      <th>continent</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Africa</th>\n      <td>61.471698</td>\n      <td>16.339623</td>\n      <td>16.264151</td>\n      <td>3.007547</td>\n    </tr>\n    <tr>\n      <th>Asia</th>\n      <td>37.045455</td>\n      <td>60.840909</td>\n      <td>9.068182</td>\n      <td>2.170455</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>193.777778</td>\n      <td>132.555556</td>\n      <td>142.222222</td>\n      <td>8.617778</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>145.434783</td>\n      <td>165.739130</td>\n      <td>24.521739</td>\n      <td>5.995652</td>\n    </tr>\n    <tr>\n      <th>Oceania</th>\n      <td>89.687500</td>\n      <td>58.437500</td>\n      <td>35.625000</td>\n      <td>3.381250</td>\n    </tr>\n    <tr>\n      <th>South America</th>\n      <td>175.083333</td>\n      <td>114.750000</td>\n      <td>62.416667</td>\n      <td>6.308333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># m</span></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).mean().plot(kind=<span class=\"string\">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://i.imgur.com/KvoD6CS.png\">    </p>\n<h2 id=\"응용하기\"><a href=\"#응용하기\" class=\"headerlink\" title=\"응용하기\"></a>응용하기</h2><ul>\n<li>Groupby에서 특정 그룹에 접근하기</li>\n<li>Groupby에서 특정 그룹에 접근 후 필터링 하기 (filter 사용)</li>\n<li>pd.cut 을 사용한 파생변수 만들기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 아시아 그룹만 </span></span><br><span class=\"line\">drinks.groupby(<span class=\"string\">&#x27;continent&#x27;</span>).get_group(<span class=\"string\">&#x27;Asia&#x27;</span>).head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Bahrain</td>\n      <td>42</td>\n      <td>63</td>\n      <td>7</td>\n      <td>2.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Bangladesh</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Bhutan</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n      <td>Asia</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Brunei</td>\n      <td>31</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>Asia</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 여러 그룹의 통계량을 조건걸어서 구할 경우</span></span><br><span class=\"line\">drinks.groupby([<span class=\"string\">&#x27;wine_servings&#x27;</span>, <span class=\"string\">&#x27;continent&#x27;</span>]).get_group((<span class=\"number\">0</span>, <span class=\"string\">&#x27;Asia&#x27;</span>)).total_litres_of_pure_alcohol.<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>6.2\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># pd.cut을 활용한 연속형 변수의 구간화 변수생성</span></span><br><span class=\"line\">drinks[<span class=\"string\">&#x27;Range&#x27;</span>] = drinks.groupby(<span class=\"string\">&#x27;country&#x27;</span>).beer_servings.apply(pd.cut, bins=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>beer_servings</th>\n      <th>spirit_servings</th>\n      <th>wine_servings</th>\n      <th>total_litres_of_pure_alcohol</th>\n      <th>continent</th>\n      <th>Range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>Asia</td>\n      <td>(-0.001, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>89</td>\n      <td>132</td>\n      <td>54</td>\n      <td>4.9</td>\n      <td>Europe</td>\n      <td>(88.911, 89.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>25</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.7</td>\n      <td>Africa</td>\n      <td>(24.975, 25.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>245</td>\n      <td>138</td>\n      <td>312</td>\n      <td>12.4</td>\n      <td>Europe</td>\n      <td>(244.755, 245.0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>217</td>\n      <td>57</td>\n      <td>45</td>\n      <td>5.9</td>\n      <td>Africa</td>\n      <td>(216.783, 217.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n<p>TF를 반환하는 lamba 함수를 작성할 경우 any()나 all()을 써서 값을 반환해줄 필요가 있다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">## filter를 사용한 조건식. 위의 결과와 같은 값을 리턴한다.</span></span><br><span class=\"line\">drinks.groupby([<span class=\"string\">&#x27;wine_servings&#x27;</span>,<span class=\"string\">&#x27;continent&#x27;</span>]).<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x : ((x.wine_servings == <span class=\"number\">0</span>) &amp; (x.continent==<span class=\"string\">&#x27;Asia&#x27;</span>) ).<span class=\"built_in\">any</span>()).total_litres_of_pure_alcohol.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>6.2\n</code></pre>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.youtube.com/watch?v=qy0fDqoMJx8\">https://www.youtube.com/watch?v=qy0fDqoMJx8</a></li>\n<li><a href=\"https://pandas.pydata.org/docs/reference/groupby.html\">https://pandas.pydata.org/docs/reference/groupby.html</a></li>\n</ul>\n<h2 id=\"다음에-정리할-것\"><a href=\"#다음에-정리할-것\" class=\"headerlink\" title=\"다음에 정리할 것\"></a>다음에 정리할 것</h2><ul>\n<li>any()와 all() 관련 함수</li>\n<li>filter</li>\n<li>assign</li>\n<li>pd.cut과 np.digitize를 활용한 연속형 변수의 구간화</li>\n<li>pandas query as dplyr filter</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://i.imgur.com/KvoD6CS.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[pandas]Pandas Groupby용법 간단히 정리","path":"2022/06/13/Preprocessing-pandas_groupby/","eyeCatchImage":"https://i.imgur.com/KvoD6CS.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["pandas"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[pandas]Pandas를 활용한 데이터분석 시작하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"## Intro\n- **알아야 하는 것**\n  - pandas data structure\n  - reading data\n  - dealing with missing data\n  - slicing & indexing\n    - loc & iloc\n  - map\n    - map\n    - apply.map\n    - apply\n  - groupby\n  - pandas eda\n    - info()\n    - head()\n    - value_counts()\n    - describe\n    - dtypes()\n---\n\n## pandas 자료구조\n\n### Sereies\n> **Series는 일련의 객체를 담을 수 있고 인덱스를 가지고 있는 1차원 배열의 자료구조이다.**  \n\n- 기본적으로 고정길이의 Ordered Dictionary라고 생각하면 편하다.(사전형을 대체하여 쓸수 있다)\n- index를 지정하지 않을 경우 기본색인인 정수에서 n-1까지의 숫자가 표시된다.\n- Series의 배열과 색인 객체는 value와 index속성을 통해 얻을 수 있다.\n\n```python\n# Series 생성\ns = pd.Series([1,2,3,4])\n\nIn [4]: s\nOut[4]: \n0    1\n1    2\n2    3\n3    4\ndtype: int64\n# Series 객체반환\n\nIn [6]: s.values\nOut[6]: array([1, 2, 3, 4]) # 1차원 배열형태로 반환됨\n\nIn [7]: s.index\nOut[7]: RangeIndex(start=0, stop=4, step=1) # range(4) 반환\n\n# Series 생성(index 지정)\n\ns2 = pd.Series([1,2,3,4],index = ['a','b','c','d'])\n\n```\n**사전형을 대체하여 Series 사용하기**\n\n```python\n# 조건 반환\n'b' in s2\n\n'e' in s2\n\n# dictionary로 부터 Series 생성하기\nsdic = {\"A\":10,\"B\":20,\"C\":40}\ns3 = pd.Series(sdic)\n\nIn [9]: s3\nOut[9]: \nA    10\nB    20\nC    40\ndtype: int64\n```\n**Series에서 누락된 데이터 찾기**\n\n```python\n\npd.isnull(s) # null인 값 찾기\n\npd.notnull(s) # null 아닌 값 찾기\n\ns.isnull() # 인스턴스 메서드로 null값 찾기\n\n```\n**Series의 name속성**\n- Series 객체와 index 모두 name 속성을 가질 수 있다\n  - DF인덱싱, 슬라이싱에 쓸 수 있다.\n- 리스트 객체를 대입하여 Series의 index를 변경할 수 있다.\n\n```python\n# name 속성 부여하기\ns.name  = 'population'\nobj.index.name = 'state'\n\n# index 대입하기\ndf.index = ['H','J','K','L']\n\n```\n### DataFrame\n\n> **R의 데이터 프레임의 pandas버전이다.  \n> 로우와 칼럼에 대한 인덱스를 가지고 있다.**\n- 단순히 인덱스와 모양이 같은 Series 객체들을 담고있는 Dictionary라고 생각하면 된다.\n\n**DF 생성하기**\n- dictionary를 이용해 쉽게 DF를 만들 수 있다.\n- 인스탄스 메서드 head()를 이용해 상위 5개 값을 출력할 수 있다.\n- dictionary에 없는 값을 넘길 경우 Nan으로 저장된다.\n```python\n# 생성\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n 'year': [2000, 2001, 2002, 2001, 2002],\n 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n\ndf = DataFrame(data)\n\ndf.head() # 상위 5개 출력\n\nOut[13]: \n    state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n\ndf2 = pd.DataFrame(data,\n                     columns=['year', 'state', 'pop', 'debt'],\n                     index=['one', 'two', 'three', 'four', 'five'])\n\n\n```\n**loc속성과 iloc속성 활용 Series, 행열 접근하기**\n\n1. 행번호로 접근하기 (iloc)(index location)\n- **: 는 '전체' 를 의미한다. (중요!)**\n- 인덱싱 범위에 따라 반환되는 객체의 타입이 달라질 수 있다(DF,Series)\n```python\n# 행 접근\ndf2.iloc[0] # 첫번째\ndf2.iloc[2] # 세번째\ndf2.iloc[-1] # 마지막 행\n\n# 열 접근\ndf2.iloc[:,0] # 첫번째 열\ndf2.iloc[:,2] # 세번째 열\ndf2.iloc[:,-1] # 마지막 열\n\n```\n```python\n# indexing with iloc\ndf2.iloc[0:4] # 첫 4개행\ndf2.iloc[:,0:2] # 첫 2개 열\ndf2.iloc[[0,2]:,[0,2]]  # 1,3 행, 1,3 열\n\n```\n\n2. label이나 조건으로 접근하기 (loc)(location)\n- 범위지정시 loc는 포함이고 iloc나 기타 python은 포함되지 않음\n- iloc의 경우 기본적으로 인덱스 기반 슬라이싱이고 loc는 이름기반 슬라이싱이어서 범위 지정시 주의 필요\n```python\n\n# 행접근\ndf2.loc[:,'year']\n# 열접근\ndf2.loc[:,'year']\n# 특정 값 접근\ndf2.loc['one','year']\n\n# 인덱스가 숫자일 경우\n\ndf2.loc[2]\n\n```\n3. loc를 활용한 조건문 \n- 조건문을 사용해 배열이나 Series, DF를 반환할 수 있다\n- values를 사용해 배열을 추출할 수 있다\n- **loc가 반환하는 결과는 기본적으로 copy가 아니라 view이기 때문에 값을 대입하거나 수정 할 수 있다**\n\n```python\nimport pandas as pd\nfrom random import randint\ndf = pd.DataFrame({'A': [randint(1, 9) for x in range(10)],\n                   'B': [randint(1, 9)*10 for x in range(10)],\n                   'C': [randint(1, 9)*100 for x in range(10)]})\n\n# 조건문으로 Boolen Series 반환하기\n\ndf[\"B\"] > 50\n\n(df[\"B\"] > 50) & (df[\"C\"] == 900)\n\n# loc에 바로 조건문을 넣을 경우\n\ndf.loc[(df[\"B\"] > 50) & (df[\"C\"] == 900), \"A\"] # 행적용 조건문\n\n```\n## 데이터 전처리 방법들(index)\n### Cleaning\n> **noise가 있을 경우 제거, inconsistency 수정**\n\n### Integration\n> **데이터 나누기, 합치기(필요에 따라)**\n\n### Transformation\n> **데이터형태변환 , 보통 datatype을 맞춰주거나 정규화를 하는 것을 말한다.**\n### Redution\n> **차원축소, 요인분석등을 사용해 분석 변수들을 줄이는 것**\n\n## References\n- https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe\n- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html","source":"_posts/Preprocessing-pandas_overview.md","raw":"---\ntitle: \"[pandas]Pandas를 활용한 데이터분석 시작하기\"\ndate: \nupdated:\ncategories: \n        - [Preprocessing]\ntags:\n  - [pandas]\n---\n## Intro\n- **알아야 하는 것**\n  - pandas data structure\n  - reading data\n  - dealing with missing data\n  - slicing & indexing\n    - loc & iloc\n  - map\n    - map\n    - apply.map\n    - apply\n  - groupby\n  - pandas eda\n    - info()\n    - head()\n    - value_counts()\n    - describe\n    - dtypes()\n---\n\n## pandas 자료구조\n\n### Sereies\n> **Series는 일련의 객체를 담을 수 있고 인덱스를 가지고 있는 1차원 배열의 자료구조이다.**  \n\n- 기본적으로 고정길이의 Ordered Dictionary라고 생각하면 편하다.(사전형을 대체하여 쓸수 있다)\n- index를 지정하지 않을 경우 기본색인인 정수에서 n-1까지의 숫자가 표시된다.\n- Series의 배열과 색인 객체는 value와 index속성을 통해 얻을 수 있다.\n\n```python\n# Series 생성\ns = pd.Series([1,2,3,4])\n\nIn [4]: s\nOut[4]: \n0    1\n1    2\n2    3\n3    4\ndtype: int64\n# Series 객체반환\n\nIn [6]: s.values\nOut[6]: array([1, 2, 3, 4]) # 1차원 배열형태로 반환됨\n\nIn [7]: s.index\nOut[7]: RangeIndex(start=0, stop=4, step=1) # range(4) 반환\n\n# Series 생성(index 지정)\n\ns2 = pd.Series([1,2,3,4],index = ['a','b','c','d'])\n\n```\n**사전형을 대체하여 Series 사용하기**\n\n```python\n# 조건 반환\n'b' in s2\n\n'e' in s2\n\n# dictionary로 부터 Series 생성하기\nsdic = {\"A\":10,\"B\":20,\"C\":40}\ns3 = pd.Series(sdic)\n\nIn [9]: s3\nOut[9]: \nA    10\nB    20\nC    40\ndtype: int64\n```\n**Series에서 누락된 데이터 찾기**\n\n```python\n\npd.isnull(s) # null인 값 찾기\n\npd.notnull(s) # null 아닌 값 찾기\n\ns.isnull() # 인스턴스 메서드로 null값 찾기\n\n```\n**Series의 name속성**\n- Series 객체와 index 모두 name 속성을 가질 수 있다\n  - DF인덱싱, 슬라이싱에 쓸 수 있다.\n- 리스트 객체를 대입하여 Series의 index를 변경할 수 있다.\n\n```python\n# name 속성 부여하기\ns.name  = 'population'\nobj.index.name = 'state'\n\n# index 대입하기\ndf.index = ['H','J','K','L']\n\n```\n### DataFrame\n\n> **R의 데이터 프레임의 pandas버전이다.  \n> 로우와 칼럼에 대한 인덱스를 가지고 있다.**\n- 단순히 인덱스와 모양이 같은 Series 객체들을 담고있는 Dictionary라고 생각하면 된다.\n\n**DF 생성하기**\n- dictionary를 이용해 쉽게 DF를 만들 수 있다.\n- 인스탄스 메서드 head()를 이용해 상위 5개 값을 출력할 수 있다.\n- dictionary에 없는 값을 넘길 경우 Nan으로 저장된다.\n```python\n# 생성\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n 'year': [2000, 2001, 2002, 2001, 2002],\n 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n\ndf = DataFrame(data)\n\ndf.head() # 상위 5개 출력\n\nOut[13]: \n    state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n\ndf2 = pd.DataFrame(data,\n                     columns=['year', 'state', 'pop', 'debt'],\n                     index=['one', 'two', 'three', 'four', 'five'])\n\n\n```\n**loc속성과 iloc속성 활용 Series, 행열 접근하기**\n\n1. 행번호로 접근하기 (iloc)(index location)\n- **: 는 '전체' 를 의미한다. (중요!)**\n- 인덱싱 범위에 따라 반환되는 객체의 타입이 달라질 수 있다(DF,Series)\n```python\n# 행 접근\ndf2.iloc[0] # 첫번째\ndf2.iloc[2] # 세번째\ndf2.iloc[-1] # 마지막 행\n\n# 열 접근\ndf2.iloc[:,0] # 첫번째 열\ndf2.iloc[:,2] # 세번째 열\ndf2.iloc[:,-1] # 마지막 열\n\n```\n```python\n# indexing with iloc\ndf2.iloc[0:4] # 첫 4개행\ndf2.iloc[:,0:2] # 첫 2개 열\ndf2.iloc[[0,2]:,[0,2]]  # 1,3 행, 1,3 열\n\n```\n\n2. label이나 조건으로 접근하기 (loc)(location)\n- 범위지정시 loc는 포함이고 iloc나 기타 python은 포함되지 않음\n- iloc의 경우 기본적으로 인덱스 기반 슬라이싱이고 loc는 이름기반 슬라이싱이어서 범위 지정시 주의 필요\n```python\n\n# 행접근\ndf2.loc[:,'year']\n# 열접근\ndf2.loc[:,'year']\n# 특정 값 접근\ndf2.loc['one','year']\n\n# 인덱스가 숫자일 경우\n\ndf2.loc[2]\n\n```\n3. loc를 활용한 조건문 \n- 조건문을 사용해 배열이나 Series, DF를 반환할 수 있다\n- values를 사용해 배열을 추출할 수 있다\n- **loc가 반환하는 결과는 기본적으로 copy가 아니라 view이기 때문에 값을 대입하거나 수정 할 수 있다**\n\n```python\nimport pandas as pd\nfrom random import randint\ndf = pd.DataFrame({'A': [randint(1, 9) for x in range(10)],\n                   'B': [randint(1, 9)*10 for x in range(10)],\n                   'C': [randint(1, 9)*100 for x in range(10)]})\n\n# 조건문으로 Boolen Series 반환하기\n\ndf[\"B\"] > 50\n\n(df[\"B\"] > 50) & (df[\"C\"] == 900)\n\n# loc에 바로 조건문을 넣을 경우\n\ndf.loc[(df[\"B\"] > 50) & (df[\"C\"] == 900), \"A\"] # 행적용 조건문\n\n```\n## 데이터 전처리 방법들(index)\n### Cleaning\n> **noise가 있을 경우 제거, inconsistency 수정**\n\n### Integration\n> **데이터 나누기, 합치기(필요에 따라)**\n\n### Transformation\n> **데이터형태변환 , 보통 datatype을 맞춰주거나 정규화를 하는 것을 말한다.**\n### Redution\n> **차원축소, 요인분석등을 사용해 분석 변수들을 줄이는 것**\n\n## References\n- https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe\n- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html","slug":"Preprocessing-pandas_overview","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsch002fb36qfmk7ho2x","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><ul>\n<li><strong>알아야 하는 것</strong><ul>\n<li>pandas data structure</li>\n<li>reading data</li>\n<li>dealing with missing data</li>\n<li>slicing &amp; indexing<ul>\n<li>loc &amp; iloc</li>\n</ul>\n</li>\n<li>map<ul>\n<li>map</li>\n<li>apply.map</li>\n<li>apply</li>\n</ul>\n</li>\n<li>groupby</li>\n<li>pandas eda<ul>\n<li>info()</li>\n<li>head()</li>\n<li>value_counts()</li>\n<li>describe</li>\n<li>dtypes()</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"pandas-자료구조\"><a href=\"#pandas-자료구조\" class=\"headerlink\" title=\"pandas 자료구조\"></a>pandas 자료구조</h2><h3 id=\"Sereies\"><a href=\"#Sereies\" class=\"headerlink\" title=\"Sereies\"></a>Sereies</h3><blockquote>\n<p><strong>Series는 일련의 객체를 담을 수 있고 인덱스를 가지고 있는 1차원 배열의 자료구조이다.</strong>  </p>\n</blockquote>\n<ul>\n<li>기본적으로 고정길이의 Ordered Dictionary라고 생각하면 편하다.(사전형을 대체하여 쓸수 있다)</li>\n<li>index를 지정하지 않을 경우 기본색인인 정수에서 n-1까지의 숫자가 표시된다.</li>\n<li>Series의 배열과 색인 객체는 value와 index속성을 통해 얻을 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Series 생성</span></span><br><span class=\"line\">s = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">4</span>]: s</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: </span><br><span class=\"line\"><span class=\"number\">0</span>    <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">1</span>    <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">3</span>    <span class=\"number\">4</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"><span class=\"comment\"># Series 객체반환</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: s.values</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]) <span class=\"comment\"># 1차원 배열형태로 반환됨</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: s.index</span><br><span class=\"line\">Out[<span class=\"number\">7</span>]: RangeIndex(start=<span class=\"number\">0</span>, stop=<span class=\"number\">4</span>, step=<span class=\"number\">1</span>) <span class=\"comment\"># range(4) 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Series 생성(index 지정)</span></span><br><span class=\"line\"></span><br><span class=\"line\">s2 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>],index = [<span class=\"string\">&#x27;a&#x27;</span>,<span class=\"string\">&#x27;b&#x27;</span>,<span class=\"string\">&#x27;c&#x27;</span>,<span class=\"string\">&#x27;d&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>사전형을 대체하여 Series 사용하기</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 조건 반환</span></span><br><span class=\"line\"><span class=\"string\">&#x27;b&#x27;</span> <span class=\"keyword\">in</span> s2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;e&#x27;</span> <span class=\"keyword\">in</span> s2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># dictionary로 부터 Series 생성하기</span></span><br><span class=\"line\">sdic = &#123;<span class=\"string\">&quot;A&quot;</span>:<span class=\"number\">10</span>,<span class=\"string\">&quot;B&quot;</span>:<span class=\"number\">20</span>,<span class=\"string\">&quot;C&quot;</span>:<span class=\"number\">40</span>&#125;</span><br><span class=\"line\">s3 = pd.Series(sdic)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: s3</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: </span><br><span class=\"line\">A    <span class=\"number\">10</span></span><br><span class=\"line\">B    <span class=\"number\">20</span></span><br><span class=\"line\">C    <span class=\"number\">40</span></span><br><span class=\"line\">dtype: int64</span><br></pre></td></tr></table></figure>\n<p><strong>Series에서 누락된 데이터 찾기</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">pd.isnull(s) <span class=\"comment\"># null인 값 찾기</span></span><br><span class=\"line\"></span><br><span class=\"line\">pd.notnull(s) <span class=\"comment\"># null 아닌 값 찾기</span></span><br><span class=\"line\"></span><br><span class=\"line\">s.isnull() <span class=\"comment\"># 인스턴스 메서드로 null값 찾기</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>Series의 name속성</strong></p>\n<ul>\n<li>Series 객체와 index 모두 name 속성을 가질 수 있다<ul>\n<li>DF인덱싱, 슬라이싱에 쓸 수 있다.</li>\n</ul>\n</li>\n<li>리스트 객체를 대입하여 Series의 index를 변경할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># name 속성 부여하기</span></span><br><span class=\"line\">s.name  = <span class=\"string\">&#x27;population&#x27;</span></span><br><span class=\"line\">obj.index.name = <span class=\"string\">&#x27;state&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># index 대입하기</span></span><br><span class=\"line\">df.index = [<span class=\"string\">&#x27;H&#x27;</span>,<span class=\"string\">&#x27;J&#x27;</span>,<span class=\"string\">&#x27;K&#x27;</span>,<span class=\"string\">&#x27;L&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DataFrame\"><a href=\"#DataFrame\" class=\"headerlink\" title=\"DataFrame\"></a>DataFrame</h3><blockquote>\n<p><strong>R의 데이터 프레임의 pandas버전이다.<br>로우와 칼럼에 대한 인덱스를 가지고 있다.</strong></p>\n</blockquote>\n<ul>\n<li>단순히 인덱스와 모양이 같은 Series 객체들을 담고있는 Dictionary라고 생각하면 된다.</li>\n</ul>\n<p><strong>DF 생성하기</strong></p>\n<ul>\n<li>dictionary를 이용해 쉽게 DF를 만들 수 있다.</li>\n<li>인스탄스 메서드 head()를 이용해 상위 5개 값을 출력할 수 있다.</li>\n<li>dictionary에 없는 값을 넘길 경우 Nan으로 저장된다.<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 생성</span></span><br><span class=\"line\">data = &#123;<span class=\"string\">&#x27;state&#x27;</span>: [<span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Nevada&#x27;</span>, <span class=\"string\">&#x27;Nevada&#x27;</span>],</span><br><span class=\"line\"> <span class=\"string\">&#x27;year&#x27;</span>: [<span class=\"number\">2000</span>, <span class=\"number\">2001</span>, <span class=\"number\">2002</span>, <span class=\"number\">2001</span>, <span class=\"number\">2002</span>],</span><br><span class=\"line\"> <span class=\"string\">&#x27;pop&#x27;</span>: [<span class=\"number\">1.5</span>, <span class=\"number\">1.7</span>, <span class=\"number\">3.6</span>, <span class=\"number\">2.4</span>, <span class=\"number\">2.9</span>]&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">df = DataFrame(data)</span><br><span class=\"line\"></span><br><span class=\"line\">df.head() <span class=\"comment\"># 상위 5개 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: </span><br><span class=\"line\">    state  year  pop</span><br><span class=\"line\"><span class=\"number\">0</span>    Ohio  <span class=\"number\">2000</span>  <span class=\"number\">1.5</span></span><br><span class=\"line\"><span class=\"number\">1</span>    Ohio  <span class=\"number\">2001</span>  <span class=\"number\">1.7</span></span><br><span class=\"line\"><span class=\"number\">2</span>    Ohio  <span class=\"number\">2002</span>  <span class=\"number\">3.6</span></span><br><span class=\"line\"><span class=\"number\">3</span>  Nevada  <span class=\"number\">2001</span>  <span class=\"number\">2.4</span></span><br><span class=\"line\"><span class=\"number\">4</span>  Nevada  <span class=\"number\">2002</span>  <span class=\"number\">2.9</span></span><br><span class=\"line\"></span><br><span class=\"line\">df2 = pd.DataFrame(data,</span><br><span class=\"line\">                     columns=[<span class=\"string\">&#x27;year&#x27;</span>, <span class=\"string\">&#x27;state&#x27;</span>, <span class=\"string\">&#x27;pop&#x27;</span>, <span class=\"string\">&#x27;debt&#x27;</span>],</span><br><span class=\"line\">                     index=[<span class=\"string\">&#x27;one&#x27;</span>, <span class=\"string\">&#x27;two&#x27;</span>, <span class=\"string\">&#x27;three&#x27;</span>, <span class=\"string\">&#x27;four&#x27;</span>, <span class=\"string\">&#x27;five&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><strong>loc속성과 iloc속성 활용 Series, 행열 접근하기</strong></p>\n<ol>\n<li>행번호로 접근하기 (iloc)(index location)</li>\n</ol>\n<ul>\n<li><strong>: 는 ‘전체’ 를 의미한다. (중요!)</strong></li>\n<li>인덱싱 범위에 따라 반환되는 객체의 타입이 달라질 수 있다(DF,Series)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 행 접근</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">0</span>] <span class=\"comment\"># 첫번째</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">2</span>] <span class=\"comment\"># 세번째</span></span><br><span class=\"line\">df2.iloc[-<span class=\"number\">1</span>] <span class=\"comment\"># 마지막 행</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 열 접근</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">0</span>] <span class=\"comment\"># 첫번째 열</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">2</span>] <span class=\"comment\"># 세번째 열</span></span><br><span class=\"line\">df2.iloc[:,-<span class=\"number\">1</span>] <span class=\"comment\"># 마지막 열</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># indexing with iloc</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">0</span>:<span class=\"number\">4</span>] <span class=\"comment\"># 첫 4개행</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">2</span>] <span class=\"comment\"># 첫 2개 열</span></span><br><span class=\"line\">df2.iloc[[<span class=\"number\">0</span>,<span class=\"number\">2</span>]:,[<span class=\"number\">0</span>,<span class=\"number\">2</span>]]  <span class=\"comment\"># 1,3 행, 1,3 열</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"2\">\n<li>label이나 조건으로 접근하기 (loc)(location)</li>\n</ol>\n<ul>\n<li>범위지정시 loc는 포함이고 iloc나 기타 python은 포함되지 않음</li>\n<li>iloc의 경우 기본적으로 인덱스 기반 슬라이싱이고 loc는 이름기반 슬라이싱이어서 범위 지정시 주의 필요<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행접근</span></span><br><span class=\"line\">df2.loc[:,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 열접근</span></span><br><span class=\"line\">df2.loc[:,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 특정 값 접근</span></span><br><span class=\"line\">df2.loc[<span class=\"string\">&#x27;one&#x27;</span>,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 인덱스가 숫자일 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\">df2.loc[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"3\">\n<li>loc를 활용한 조건문</li>\n</ol>\n<ul>\n<li>조건문을 사용해 배열이나 Series, DF를 반환할 수 있다</li>\n<li>values를 사용해 배열을 추출할 수 있다</li>\n<li><strong>loc가 반환하는 결과는 기본적으로 copy가 아니라 view이기 때문에 값을 대입하거나 수정 할 수 있다</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> random <span class=\"keyword\">import</span> randint</span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;A&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;B&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>)*<span class=\"number\">10</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;C&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>)*<span class=\"number\">100</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 조건문으로 Boolen Series 반환하기</span></span><br><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span></span><br><span class=\"line\"></span><br><span class=\"line\">(df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] == <span class=\"number\">900</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loc에 바로 조건문을 넣을 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\">df.loc[(df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] == <span class=\"number\">900</span>), <span class=\"string\">&quot;A&quot;</span>] <span class=\"comment\"># 행적용 조건문</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"데이터-전처리-방법들-index\"><a href=\"#데이터-전처리-방법들-index\" class=\"headerlink\" title=\"데이터 전처리 방법들(index)\"></a>데이터 전처리 방법들(index)</h2><h3 id=\"Cleaning\"><a href=\"#Cleaning\" class=\"headerlink\" title=\"Cleaning\"></a>Cleaning</h3><blockquote>\n<p><strong>noise가 있을 경우 제거, inconsistency 수정</strong></p>\n</blockquote>\n<h3 id=\"Integration\"><a href=\"#Integration\" class=\"headerlink\" title=\"Integration\"></a>Integration</h3><blockquote>\n<p><strong>데이터 나누기, 합치기(필요에 따라)</strong></p>\n</blockquote>\n<h3 id=\"Transformation\"><a href=\"#Transformation\" class=\"headerlink\" title=\"Transformation\"></a>Transformation</h3><blockquote>\n<p><strong>데이터형태변환 , 보통 datatype을 맞춰주거나 정규화를 하는 것을 말한다.</strong></p>\n</blockquote>\n<h3 id=\"Redution\"><a href=\"#Redution\" class=\"headerlink\" title=\"Redution\"></a>Redution</h3><blockquote>\n<p><strong>차원축소, 요인분석등을 사용해 분석 변수들을 줄이는 것</strong></p>\n</blockquote>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe\">https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe</a></li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><ul>\n<li><strong>알아야 하는 것</strong><ul>\n<li>pandas data structure</li>\n<li>reading data</li>\n<li>dealing with missing data</li>\n<li>slicing &amp; indexing<ul>\n<li>loc &amp; iloc</li>\n</ul>\n</li>\n<li>map<ul>\n<li>map</li>\n<li>apply.map</li>\n<li>apply</li>\n</ul>\n</li>\n<li>groupby</li>\n<li>pandas eda<ul>\n<li>info()</li>\n<li>head()</li>\n<li>value_counts()</li>\n<li>describe</li>\n<li>dtypes()</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"pandas-자료구조\"><a href=\"#pandas-자료구조\" class=\"headerlink\" title=\"pandas 자료구조\"></a>pandas 자료구조</h2><h3 id=\"Sereies\"><a href=\"#Sereies\" class=\"headerlink\" title=\"Sereies\"></a>Sereies</h3><blockquote>\n<p><strong>Series는 일련의 객체를 담을 수 있고 인덱스를 가지고 있는 1차원 배열의 자료구조이다.</strong>  </p>\n</blockquote>\n<ul>\n<li>기본적으로 고정길이의 Ordered Dictionary라고 생각하면 편하다.(사전형을 대체하여 쓸수 있다)</li>\n<li>index를 지정하지 않을 경우 기본색인인 정수에서 n-1까지의 숫자가 표시된다.</li>\n<li>Series의 배열과 색인 객체는 value와 index속성을 통해 얻을 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Series 생성</span></span><br><span class=\"line\">s = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">4</span>]: s</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: </span><br><span class=\"line\"><span class=\"number\">0</span>    <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">1</span>    <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">3</span>    <span class=\"number\">4</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"><span class=\"comment\"># Series 객체반환</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: s.values</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]) <span class=\"comment\"># 1차원 배열형태로 반환됨</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: s.index</span><br><span class=\"line\">Out[<span class=\"number\">7</span>]: RangeIndex(start=<span class=\"number\">0</span>, stop=<span class=\"number\">4</span>, step=<span class=\"number\">1</span>) <span class=\"comment\"># range(4) 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Series 생성(index 지정)</span></span><br><span class=\"line\"></span><br><span class=\"line\">s2 = pd.Series([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>],index = [<span class=\"string\">&#x27;a&#x27;</span>,<span class=\"string\">&#x27;b&#x27;</span>,<span class=\"string\">&#x27;c&#x27;</span>,<span class=\"string\">&#x27;d&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>사전형을 대체하여 Series 사용하기</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 조건 반환</span></span><br><span class=\"line\"><span class=\"string\">&#x27;b&#x27;</span> <span class=\"keyword\">in</span> s2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;e&#x27;</span> <span class=\"keyword\">in</span> s2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># dictionary로 부터 Series 생성하기</span></span><br><span class=\"line\">sdic = &#123;<span class=\"string\">&quot;A&quot;</span>:<span class=\"number\">10</span>,<span class=\"string\">&quot;B&quot;</span>:<span class=\"number\">20</span>,<span class=\"string\">&quot;C&quot;</span>:<span class=\"number\">40</span>&#125;</span><br><span class=\"line\">s3 = pd.Series(sdic)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: s3</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: </span><br><span class=\"line\">A    <span class=\"number\">10</span></span><br><span class=\"line\">B    <span class=\"number\">20</span></span><br><span class=\"line\">C    <span class=\"number\">40</span></span><br><span class=\"line\">dtype: int64</span><br></pre></td></tr></table></figure>\n<p><strong>Series에서 누락된 데이터 찾기</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">pd.isnull(s) <span class=\"comment\"># null인 값 찾기</span></span><br><span class=\"line\"></span><br><span class=\"line\">pd.notnull(s) <span class=\"comment\"># null 아닌 값 찾기</span></span><br><span class=\"line\"></span><br><span class=\"line\">s.isnull() <span class=\"comment\"># 인스턴스 메서드로 null값 찾기</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>Series의 name속성</strong></p>\n<ul>\n<li>Series 객체와 index 모두 name 속성을 가질 수 있다<ul>\n<li>DF인덱싱, 슬라이싱에 쓸 수 있다.</li>\n</ul>\n</li>\n<li>리스트 객체를 대입하여 Series의 index를 변경할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># name 속성 부여하기</span></span><br><span class=\"line\">s.name  = <span class=\"string\">&#x27;population&#x27;</span></span><br><span class=\"line\">obj.index.name = <span class=\"string\">&#x27;state&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># index 대입하기</span></span><br><span class=\"line\">df.index = [<span class=\"string\">&#x27;H&#x27;</span>,<span class=\"string\">&#x27;J&#x27;</span>,<span class=\"string\">&#x27;K&#x27;</span>,<span class=\"string\">&#x27;L&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DataFrame\"><a href=\"#DataFrame\" class=\"headerlink\" title=\"DataFrame\"></a>DataFrame</h3><blockquote>\n<p><strong>R의 데이터 프레임의 pandas버전이다.<br>로우와 칼럼에 대한 인덱스를 가지고 있다.</strong></p>\n</blockquote>\n<ul>\n<li>단순히 인덱스와 모양이 같은 Series 객체들을 담고있는 Dictionary라고 생각하면 된다.</li>\n</ul>\n<p><strong>DF 생성하기</strong></p>\n<ul>\n<li>dictionary를 이용해 쉽게 DF를 만들 수 있다.</li>\n<li>인스탄스 메서드 head()를 이용해 상위 5개 값을 출력할 수 있다.</li>\n<li>dictionary에 없는 값을 넘길 경우 Nan으로 저장된다.<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 생성</span></span><br><span class=\"line\">data = &#123;<span class=\"string\">&#x27;state&#x27;</span>: [<span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Ohio&#x27;</span>, <span class=\"string\">&#x27;Nevada&#x27;</span>, <span class=\"string\">&#x27;Nevada&#x27;</span>],</span><br><span class=\"line\"> <span class=\"string\">&#x27;year&#x27;</span>: [<span class=\"number\">2000</span>, <span class=\"number\">2001</span>, <span class=\"number\">2002</span>, <span class=\"number\">2001</span>, <span class=\"number\">2002</span>],</span><br><span class=\"line\"> <span class=\"string\">&#x27;pop&#x27;</span>: [<span class=\"number\">1.5</span>, <span class=\"number\">1.7</span>, <span class=\"number\">3.6</span>, <span class=\"number\">2.4</span>, <span class=\"number\">2.9</span>]&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">df = DataFrame(data)</span><br><span class=\"line\"></span><br><span class=\"line\">df.head() <span class=\"comment\"># 상위 5개 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: </span><br><span class=\"line\">    state  year  pop</span><br><span class=\"line\"><span class=\"number\">0</span>    Ohio  <span class=\"number\">2000</span>  <span class=\"number\">1.5</span></span><br><span class=\"line\"><span class=\"number\">1</span>    Ohio  <span class=\"number\">2001</span>  <span class=\"number\">1.7</span></span><br><span class=\"line\"><span class=\"number\">2</span>    Ohio  <span class=\"number\">2002</span>  <span class=\"number\">3.6</span></span><br><span class=\"line\"><span class=\"number\">3</span>  Nevada  <span class=\"number\">2001</span>  <span class=\"number\">2.4</span></span><br><span class=\"line\"><span class=\"number\">4</span>  Nevada  <span class=\"number\">2002</span>  <span class=\"number\">2.9</span></span><br><span class=\"line\"></span><br><span class=\"line\">df2 = pd.DataFrame(data,</span><br><span class=\"line\">                     columns=[<span class=\"string\">&#x27;year&#x27;</span>, <span class=\"string\">&#x27;state&#x27;</span>, <span class=\"string\">&#x27;pop&#x27;</span>, <span class=\"string\">&#x27;debt&#x27;</span>],</span><br><span class=\"line\">                     index=[<span class=\"string\">&#x27;one&#x27;</span>, <span class=\"string\">&#x27;two&#x27;</span>, <span class=\"string\">&#x27;three&#x27;</span>, <span class=\"string\">&#x27;four&#x27;</span>, <span class=\"string\">&#x27;five&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<p><strong>loc속성과 iloc속성 활용 Series, 행열 접근하기</strong></p>\n<ol>\n<li>행번호로 접근하기 (iloc)(index location)</li>\n</ol>\n<ul>\n<li><strong>: 는 ‘전체’ 를 의미한다. (중요!)</strong></li>\n<li>인덱싱 범위에 따라 반환되는 객체의 타입이 달라질 수 있다(DF,Series)<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 행 접근</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">0</span>] <span class=\"comment\"># 첫번째</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">2</span>] <span class=\"comment\"># 세번째</span></span><br><span class=\"line\">df2.iloc[-<span class=\"number\">1</span>] <span class=\"comment\"># 마지막 행</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 열 접근</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">0</span>] <span class=\"comment\"># 첫번째 열</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">2</span>] <span class=\"comment\"># 세번째 열</span></span><br><span class=\"line\">df2.iloc[:,-<span class=\"number\">1</span>] <span class=\"comment\"># 마지막 열</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># indexing with iloc</span></span><br><span class=\"line\">df2.iloc[<span class=\"number\">0</span>:<span class=\"number\">4</span>] <span class=\"comment\"># 첫 4개행</span></span><br><span class=\"line\">df2.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">2</span>] <span class=\"comment\"># 첫 2개 열</span></span><br><span class=\"line\">df2.iloc[[<span class=\"number\">0</span>,<span class=\"number\">2</span>]:,[<span class=\"number\">0</span>,<span class=\"number\">2</span>]]  <span class=\"comment\"># 1,3 행, 1,3 열</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"2\">\n<li>label이나 조건으로 접근하기 (loc)(location)</li>\n</ol>\n<ul>\n<li>범위지정시 loc는 포함이고 iloc나 기타 python은 포함되지 않음</li>\n<li>iloc의 경우 기본적으로 인덱스 기반 슬라이싱이고 loc는 이름기반 슬라이싱이어서 범위 지정시 주의 필요<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행접근</span></span><br><span class=\"line\">df2.loc[:,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 열접근</span></span><br><span class=\"line\">df2.loc[:,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 특정 값 접근</span></span><br><span class=\"line\">df2.loc[<span class=\"string\">&#x27;one&#x27;</span>,<span class=\"string\">&#x27;year&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 인덱스가 숫자일 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\">df2.loc[<span class=\"number\">2</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<ol start=\"3\">\n<li>loc를 활용한 조건문</li>\n</ol>\n<ul>\n<li>조건문을 사용해 배열이나 Series, DF를 반환할 수 있다</li>\n<li>values를 사용해 배열을 추출할 수 있다</li>\n<li><strong>loc가 반환하는 결과는 기본적으로 copy가 아니라 view이기 때문에 값을 대입하거나 수정 할 수 있다</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> random <span class=\"keyword\">import</span> randint</span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;A&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;B&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>)*<span class=\"number\">10</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;C&#x27;</span>: [randint(<span class=\"number\">1</span>, <span class=\"number\">9</span>)*<span class=\"number\">100</span> <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 조건문으로 Boolen Series 반환하기</span></span><br><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span></span><br><span class=\"line\"></span><br><span class=\"line\">(df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] == <span class=\"number\">900</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loc에 바로 조건문을 넣을 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\">df.loc[(df[<span class=\"string\">&quot;B&quot;</span>] &gt; <span class=\"number\">50</span>) &amp; (df[<span class=\"string\">&quot;C&quot;</span>] == <span class=\"number\">900</span>), <span class=\"string\">&quot;A&quot;</span>] <span class=\"comment\"># 행적용 조건문</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"데이터-전처리-방법들-index\"><a href=\"#데이터-전처리-방법들-index\" class=\"headerlink\" title=\"데이터 전처리 방법들(index)\"></a>데이터 전처리 방법들(index)</h2><h3 id=\"Cleaning\"><a href=\"#Cleaning\" class=\"headerlink\" title=\"Cleaning\"></a>Cleaning</h3><blockquote>\n<p><strong>noise가 있을 경우 제거, inconsistency 수정</strong></p>\n</blockquote>\n<h3 id=\"Integration\"><a href=\"#Integration\" class=\"headerlink\" title=\"Integration\"></a>Integration</h3><blockquote>\n<p><strong>데이터 나누기, 합치기(필요에 따라)</strong></p>\n</blockquote>\n<h3 id=\"Transformation\"><a href=\"#Transformation\" class=\"headerlink\" title=\"Transformation\"></a>Transformation</h3><blockquote>\n<p><strong>데이터형태변환 , 보통 datatype을 맞춰주거나 정규화를 하는 것을 말한다.</strong></p>\n</blockquote>\n<h3 id=\"Redution\"><a href=\"#Redution\" class=\"headerlink\" title=\"Redution\"></a>Redution</h3><blockquote>\n<p><strong>차원축소, 요인분석등을 사용해 분석 변수들을 줄이는 것</strong></p>\n</blockquote>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe\">https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe</a></li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.repeat.html</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[pandas]Pandas를 활용한 데이터분석 시작하기","path":"2022/06/13/Preprocessing-pandas_overview/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["pandas"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[pandas]pandas 함수와 기초용법들","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n## **pandas tricks**\n> pandas관련 자주 사용할만한 코드 정리\n\n### pandas version확인\n```python\npd.__version__ # pandas version확인\n```\n```python\npd.show_versions() #의존성 패키지 확인\n```\n\n### DF 생성하기\n> 여러 방법이 있지만 보통 dictionary를 사용한다.\n```python\ndf = pd.DataFrame({'col one':[100, 200], 'col two':[300, 400]})\ndf\n```\n```python\n# 난수생성을 통핸 DF생성\npd.DataFrame(np.random.rand(4, 8))\n```\n\n### 열이름 변경하기\n\n```python\n# dictionary 형태로 변경하기\ndf = df.rename({'col one' : 'col_one','col two': 'col_two'}, axis = 'columns' ) # 적용할 axis지정 rename\n\ndf.add_prefix('X_') #컬럼에 접두어 X 추가\ndf.add_suffix('_Y') #컬럼에 접미어 Y 추가\n```\n```python\n# list를 매핑해 변경하기\ndf.columns = ['col_one', 'col_two']\n```\n\n\n### 행순서 뒤집기\n```python\ndrinks.loc[::-1].head()\n```\n\n### reverse column order\n```python\n\n\ndrinks.loc[:, ::-1].head() # [start:end:(step)]에 대한 이해 필요\n# start, end가 비어있고 step이 -1이기에 순서가 역순으로 바뀜\n\n```\n\n### datatype 기준으로 컬럼 선택하기\n```python\ndrinks.dtypes # 모든 열의 dtype 확인\n```\n```python\ndrinks.select_dtypes(include='number').head() # dtype이 numeric인 데이터 추출\n\ndrinks.select_dtypes(include=['number', 'object', 'category', 'datetime']).head()\n```\n### 문자열 numeric으로 변환하기\n```python\ndf = pd.DataFrame({'col_one':['1.1', '2.2', '3.3'],\n                   'col_two':['4.4', '5.5', '6.6'],\n                   'col_three':['7.7', '8.8', '-']})\ndf\n\n# astype()을 활용한 변환\ndf.astype({'col_one':'float', 'col_two':'float'}).dtypes\n\n# to_numeric을 활용한 변환\npd.to_numeric(df.col_three, errors='coerce')\n\n# df 전체에 적용(numeric 변환 후 fillna)\ndf = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n\n# , 이 포함된 숫자형태의 문자열의 경우 replace사용\ndef toInt(string):\n    string = int(string.replace(',',''))\n    returen string\n\n```\n### DF 사이즈 줄이기\n\n```python\n# 메모리 사용정도 확인\ndrinks.info(memory_usage='deep')\n\n# 컬럼지정을 활용한 데이터 줄이기\ndtypes = {'continent':'category'}\nsmaller_drinks = pd.read_csv('http://bit.ly/drinksbycountry', usecols=cols, dtype=dtypes)\nsmaller_drinks.info(memory_usage='deep')\n```\n### Build a DataFrame from mulfiple files (row-wise)\n```python\nfrom glob import glob\n\n# 정규식,와일드카드 관련문서 참고\n# stocks로 시작하는 data폴더 내 모든 csv 파일 \nstock_files = sorted(glob('data/stocks*.csv'))\nstock_files\n\n['data/stocks1.csv', 'data/stocks2.csv', 'data/stocks3.csv'] # 리스트 형태로 반환\n\n\n```\n\n```python\n# 파일합치기\npd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) # ignore index는 각 파일의 index를 무시하고 초기화하는 옵션이다.\n\n\n```\n### Build a DataFrame from mulfiple files (column-wise)\n```python\n\n# 축옵션만 넣어주면 된다\npd.concat((pd.read_csv(file) for file in drink_files), axis='columns').head()\n\n\n```\n\n### 클립보드에서 df불러오기\n```python\ndf = pd.read_clipboard()\ndf\n```\n\n### DF subsetting 하기\n```python\n\n# frac으로 원db의 75% 할당\nmovies_1 = movies.sample(frac=0.75, random_state=1234)\n\n# 나머지\nmovies_2 = movies.drop(movies_1.index)\n\n\n```\n\n### isin을 활용한 DF필터링\n```python\n# inin을 사용해 특정열에 대해 값에 대한조건을 넣어줄 수 있다.\n\n# 포함하고 뽑기\nmovies[movies.genre.isin(['Action','Drama','Western'])].head()\n\n# 제외하고 뽑기\nmovies[~movies.genre.isin(['Action', 'Drama', 'Western'])].head()\n```\n\n### value_counts()를 관측값 구하기\n```python\n# 우선 카테고리(장르)별 관측값를 구한다\ncounts = movies.genre.value_counts()\ncounts\n\n```\n```python\n# count에서 상위3개를 구한다.\ncounts.nlargest(3)\n\n```\n### 결측값 처리하기\n```python\n\n# 결측값 조건걸기\nufo = dropna(thresh = len(ufo)*0.9, axis = 'columns') # 90% 이상 값이 있는 컬럼만 유지\n\n```\n```python\n# 열별로 결측값의 수 세기\nufo.isna().sum()\n```\n```python\n# NA가 하나라도 있는 열 삭제\nufo.dropna(axis='columns').head()\n```\n### .split를 활용한 문자열 나누기\n```python\ndf = pd.DataFrame({'name':['John Arthur Doe', 'Jane Ann Smith'],\n                   'location':['Los Angeles, CA', 'Washington, DC']})\ndf\n```\n```python\ndf[['first', 'middle', 'last']] = df.name.str.split(' ', expand=True)\ndf\n```\n![](/image/output.png)\n### 리스트를 DF로 변환하기\n```python\ndf = pd.DataFrame({'col_one':['a', 'b', 'c'], 'col_two':[[10, 40], [20, 50], [30, 60]]})\ndf\n```\n![](/image/output2.png)\n```python\ndf_new = df.col_two.apply(pd.Series) # apply를 활용한 df 생성\ndf_new\n```\n### Aggregate by multiple funtions\n```python\n# aggregate를 활용한 요약통계량 산출하기\norders.groupby('order_id').item_price.agg(['sum', 'count']).head()\n```\n\n### Combine the output of an aggregation by multiple funtions\n\n```python\n# transform()은 입력된 개체와 동일하게 인덱스된 객체를 반환하며 다중연산에 쓰인다.\ntotal_price = orders.groupby('order_id').item_price.transform('sum')\n\n# transform() 관련레퍼런스\n# https://kongdols-room.tistory.com/169 \n```\n### .loc를 활용한 행열 슬라이싱\n```python\n\ntitanic.describe().loc['min':'max']\n\ntitanic.describe().loc['min':'max', 'Pclass':'Parch']\n\n```\n### 계층적 index를 가지는 Series DF로 변환하기\n- 부모자식 노드처럼 계층이 있는 인덱스를 가지는 DF를 만들 수있다\n- 잘 쓰진 않는 것 같다\n```python\n# 계층적\n# https://nittaku.tistory.com/122\n\ntitanic.groupby(['Sex', 'Pclass']).Survived.mean()\n\n# changing multiple Series into a DF\ntitanic.groupby(['Sex', 'Pclass']).Survived.mean().unstack()\n\n```\n### 피벗테이블 만들기\n```python\n\ntitanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean')\n# pivot_table에서 aggfunc 파라미터를 'count' 으로 바꿀 경우 단순 crosstable을 반환한다\n\n# margins = True option으로 행열합을 DF에 추가한다\ntitanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean',\n                    margins=True)\n```\n### bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기\n\n```python\n# use bin with the labels\npd.cut(titanic.Age, bins=[0, 18, 25, 99], labels=['child', 'young adult', 'adult']).head(10)\n```\n### DF 표시형식 바꾸기\n```python\n# set_option을 통해 표시형식 바꾸기\npd.set_option('display.float_format', '{:.2f}'.format)\n```\n### DF 꾸미기 (Style a DataFrame)\n```python\nformat_dict = {'Date':'{:%m/%d/%y}', 'Close':'${:.2f}', 'Volume':'{:,}'}\n\ndf.style.format(format_dict) # 스타일 바꾸기\n\n```\n### ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기\n\n```python\ni\nmport pandas_profiling\npandas_profiliing.PrifileReport(titanic)\n\n```\n\n### glob을 사용해 여러 csv파일을 하나의 df로 합치기\n-\n```python\n\nimport pandas as pd\nimport glob\n\npath = r'C:\\DRO\\DCL_rawdata_files' # use your path\nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n\n```\n### DF에 컬럼 추가하기\n```python\n\ndic = {'Name': ['Jai', 'Princi', 'Gaurav', 'Anuj'],\n        'Height': [5.1, 6.2, 5.1, 5.2],\n        'Qualification': ['Msc', 'MA', 'Msc', 'Msc']}\n  \n# Convert the dictionary into DataFrame\ndf = pd.DataFrame(data)\n  \n# Declare a list that is to be converted into a column\naddress = ['Delhi', 'Bangalore', 'Chennai', 'Patna']\n  \n# Using 'Address' as the column name\n# and equating it to the list\ndf['Address'] = address\n\n```\n### apply 등을 활용한 파생변수 생성하기\n-DF전체에 적용하거나 DF일부에 적용할 수 있다.\n```python\n\n\n```\n### lambda를 활용한 함수 적용\n\n\n### \n\n\n###\nhttp://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\n\n\nhttps://wikidocs.net/46758\n\nhttps://data-make.tistory.com/123\n\n## 3. References\n- https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\n- https://www.youtube.com/watch?v=RlIiVeig3hc\n- https://kongdols-room.tistory.com/169 \n- https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/","source":"_posts/Preprocessing-pandas_tricks.md","raw":"---\ntitle: \"[pandas]pandas 함수와 기초용법들\"\ndate: \nupdated:\ncategories: \n        - [Preprocessing]\ntags:\n  - [pandas]\n---\n\n## **pandas tricks**\n> pandas관련 자주 사용할만한 코드 정리\n\n### pandas version확인\n```python\npd.__version__ # pandas version확인\n```\n```python\npd.show_versions() #의존성 패키지 확인\n```\n\n### DF 생성하기\n> 여러 방법이 있지만 보통 dictionary를 사용한다.\n```python\ndf = pd.DataFrame({'col one':[100, 200], 'col two':[300, 400]})\ndf\n```\n```python\n# 난수생성을 통핸 DF생성\npd.DataFrame(np.random.rand(4, 8))\n```\n\n### 열이름 변경하기\n\n```python\n# dictionary 형태로 변경하기\ndf = df.rename({'col one' : 'col_one','col two': 'col_two'}, axis = 'columns' ) # 적용할 axis지정 rename\n\ndf.add_prefix('X_') #컬럼에 접두어 X 추가\ndf.add_suffix('_Y') #컬럼에 접미어 Y 추가\n```\n```python\n# list를 매핑해 변경하기\ndf.columns = ['col_one', 'col_two']\n```\n\n\n### 행순서 뒤집기\n```python\ndrinks.loc[::-1].head()\n```\n\n### reverse column order\n```python\n\n\ndrinks.loc[:, ::-1].head() # [start:end:(step)]에 대한 이해 필요\n# start, end가 비어있고 step이 -1이기에 순서가 역순으로 바뀜\n\n```\n\n### datatype 기준으로 컬럼 선택하기\n```python\ndrinks.dtypes # 모든 열의 dtype 확인\n```\n```python\ndrinks.select_dtypes(include='number').head() # dtype이 numeric인 데이터 추출\n\ndrinks.select_dtypes(include=['number', 'object', 'category', 'datetime']).head()\n```\n### 문자열 numeric으로 변환하기\n```python\ndf = pd.DataFrame({'col_one':['1.1', '2.2', '3.3'],\n                   'col_two':['4.4', '5.5', '6.6'],\n                   'col_three':['7.7', '8.8', '-']})\ndf\n\n# astype()을 활용한 변환\ndf.astype({'col_one':'float', 'col_two':'float'}).dtypes\n\n# to_numeric을 활용한 변환\npd.to_numeric(df.col_three, errors='coerce')\n\n# df 전체에 적용(numeric 변환 후 fillna)\ndf = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n\n# , 이 포함된 숫자형태의 문자열의 경우 replace사용\ndef toInt(string):\n    string = int(string.replace(',',''))\n    returen string\n\n```\n### DF 사이즈 줄이기\n\n```python\n# 메모리 사용정도 확인\ndrinks.info(memory_usage='deep')\n\n# 컬럼지정을 활용한 데이터 줄이기\ndtypes = {'continent':'category'}\nsmaller_drinks = pd.read_csv('http://bit.ly/drinksbycountry', usecols=cols, dtype=dtypes)\nsmaller_drinks.info(memory_usage='deep')\n```\n### Build a DataFrame from mulfiple files (row-wise)\n```python\nfrom glob import glob\n\n# 정규식,와일드카드 관련문서 참고\n# stocks로 시작하는 data폴더 내 모든 csv 파일 \nstock_files = sorted(glob('data/stocks*.csv'))\nstock_files\n\n['data/stocks1.csv', 'data/stocks2.csv', 'data/stocks3.csv'] # 리스트 형태로 반환\n\n\n```\n\n```python\n# 파일합치기\npd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) # ignore index는 각 파일의 index를 무시하고 초기화하는 옵션이다.\n\n\n```\n### Build a DataFrame from mulfiple files (column-wise)\n```python\n\n# 축옵션만 넣어주면 된다\npd.concat((pd.read_csv(file) for file in drink_files), axis='columns').head()\n\n\n```\n\n### 클립보드에서 df불러오기\n```python\ndf = pd.read_clipboard()\ndf\n```\n\n### DF subsetting 하기\n```python\n\n# frac으로 원db의 75% 할당\nmovies_1 = movies.sample(frac=0.75, random_state=1234)\n\n# 나머지\nmovies_2 = movies.drop(movies_1.index)\n\n\n```\n\n### isin을 활용한 DF필터링\n```python\n# inin을 사용해 특정열에 대해 값에 대한조건을 넣어줄 수 있다.\n\n# 포함하고 뽑기\nmovies[movies.genre.isin(['Action','Drama','Western'])].head()\n\n# 제외하고 뽑기\nmovies[~movies.genre.isin(['Action', 'Drama', 'Western'])].head()\n```\n\n### value_counts()를 관측값 구하기\n```python\n# 우선 카테고리(장르)별 관측값를 구한다\ncounts = movies.genre.value_counts()\ncounts\n\n```\n```python\n# count에서 상위3개를 구한다.\ncounts.nlargest(3)\n\n```\n### 결측값 처리하기\n```python\n\n# 결측값 조건걸기\nufo = dropna(thresh = len(ufo)*0.9, axis = 'columns') # 90% 이상 값이 있는 컬럼만 유지\n\n```\n```python\n# 열별로 결측값의 수 세기\nufo.isna().sum()\n```\n```python\n# NA가 하나라도 있는 열 삭제\nufo.dropna(axis='columns').head()\n```\n### .split를 활용한 문자열 나누기\n```python\ndf = pd.DataFrame({'name':['John Arthur Doe', 'Jane Ann Smith'],\n                   'location':['Los Angeles, CA', 'Washington, DC']})\ndf\n```\n```python\ndf[['first', 'middle', 'last']] = df.name.str.split(' ', expand=True)\ndf\n```\n![](/image/output.png)\n### 리스트를 DF로 변환하기\n```python\ndf = pd.DataFrame({'col_one':['a', 'b', 'c'], 'col_two':[[10, 40], [20, 50], [30, 60]]})\ndf\n```\n![](/image/output2.png)\n```python\ndf_new = df.col_two.apply(pd.Series) # apply를 활용한 df 생성\ndf_new\n```\n### Aggregate by multiple funtions\n```python\n# aggregate를 활용한 요약통계량 산출하기\norders.groupby('order_id').item_price.agg(['sum', 'count']).head()\n```\n\n### Combine the output of an aggregation by multiple funtions\n\n```python\n# transform()은 입력된 개체와 동일하게 인덱스된 객체를 반환하며 다중연산에 쓰인다.\ntotal_price = orders.groupby('order_id').item_price.transform('sum')\n\n# transform() 관련레퍼런스\n# https://kongdols-room.tistory.com/169 \n```\n### .loc를 활용한 행열 슬라이싱\n```python\n\ntitanic.describe().loc['min':'max']\n\ntitanic.describe().loc['min':'max', 'Pclass':'Parch']\n\n```\n### 계층적 index를 가지는 Series DF로 변환하기\n- 부모자식 노드처럼 계층이 있는 인덱스를 가지는 DF를 만들 수있다\n- 잘 쓰진 않는 것 같다\n```python\n# 계층적\n# https://nittaku.tistory.com/122\n\ntitanic.groupby(['Sex', 'Pclass']).Survived.mean()\n\n# changing multiple Series into a DF\ntitanic.groupby(['Sex', 'Pclass']).Survived.mean().unstack()\n\n```\n### 피벗테이블 만들기\n```python\n\ntitanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean')\n# pivot_table에서 aggfunc 파라미터를 'count' 으로 바꿀 경우 단순 crosstable을 반환한다\n\n# margins = True option으로 행열합을 DF에 추가한다\ntitanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean',\n                    margins=True)\n```\n### bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기\n\n```python\n# use bin with the labels\npd.cut(titanic.Age, bins=[0, 18, 25, 99], labels=['child', 'young adult', 'adult']).head(10)\n```\n### DF 표시형식 바꾸기\n```python\n# set_option을 통해 표시형식 바꾸기\npd.set_option('display.float_format', '{:.2f}'.format)\n```\n### DF 꾸미기 (Style a DataFrame)\n```python\nformat_dict = {'Date':'{:%m/%d/%y}', 'Close':'${:.2f}', 'Volume':'{:,}'}\n\ndf.style.format(format_dict) # 스타일 바꾸기\n\n```\n### ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기\n\n```python\ni\nmport pandas_profiling\npandas_profiliing.PrifileReport(titanic)\n\n```\n\n### glob을 사용해 여러 csv파일을 하나의 df로 합치기\n-\n```python\n\nimport pandas as pd\nimport glob\n\npath = r'C:\\DRO\\DCL_rawdata_files' # use your path\nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n\n```\n### DF에 컬럼 추가하기\n```python\n\ndic = {'Name': ['Jai', 'Princi', 'Gaurav', 'Anuj'],\n        'Height': [5.1, 6.2, 5.1, 5.2],\n        'Qualification': ['Msc', 'MA', 'Msc', 'Msc']}\n  \n# Convert the dictionary into DataFrame\ndf = pd.DataFrame(data)\n  \n# Declare a list that is to be converted into a column\naddress = ['Delhi', 'Bangalore', 'Chennai', 'Patna']\n  \n# Using 'Address' as the column name\n# and equating it to the list\ndf['Address'] = address\n\n```\n### apply 등을 활용한 파생변수 생성하기\n-DF전체에 적용하거나 DF일부에 적용할 수 있다.\n```python\n\n\n```\n### lambda를 활용한 함수 적용\n\n\n### \n\n\n###\nhttp://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\n\n\nhttps://wikidocs.net/46758\n\nhttps://data-make.tistory.com/123\n\n## 3. References\n- https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\n- https://www.youtube.com/watch?v=RlIiVeig3hc\n- https://kongdols-room.tistory.com/169 \n- https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/","slug":"Preprocessing-pandas_tricks","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsch002jb36qbie8avsw","content":"<h2 id=\"pandas-tricks\"><a href=\"#pandas-tricks\" class=\"headerlink\" title=\"pandas tricks\"></a><strong>pandas tricks</strong></h2><blockquote>\n<p>pandas관련 자주 사용할만한 코드 정리</p>\n</blockquote>\n<h3 id=\"pandas-version확인\"><a href=\"#pandas-version확인\" class=\"headerlink\" title=\"pandas version확인\"></a>pandas version확인</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pd.__version__ <span class=\"comment\"># pandas version확인</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pd.show_versions() <span class=\"comment\">#의존성 패키지 확인</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DF-생성하기\"><a href=\"#DF-생성하기\" class=\"headerlink\" title=\"DF 생성하기\"></a>DF 생성하기</h3><blockquote>\n<p>여러 방법이 있지만 보통 dictionary를 사용한다.</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col one&#x27;</span>:[<span class=\"number\">100</span>, <span class=\"number\">200</span>], <span class=\"string\">&#x27;col two&#x27;</span>:[<span class=\"number\">300</span>, <span class=\"number\">400</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 난수생성을 통핸 DF생성</span></span><br><span class=\"line\">pd.DataFrame(np.random.rand(<span class=\"number\">4</span>, <span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"열이름-변경하기\"><a href=\"#열이름-변경하기\" class=\"headerlink\" title=\"열이름 변경하기\"></a>열이름 변경하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dictionary 형태로 변경하기</span></span><br><span class=\"line\">df = df.rename(&#123;<span class=\"string\">&#x27;col one&#x27;</span> : <span class=\"string\">&#x27;col_one&#x27;</span>,<span class=\"string\">&#x27;col two&#x27;</span>: <span class=\"string\">&#x27;col_two&#x27;</span>&#125;, axis = <span class=\"string\">&#x27;columns&#x27;</span> ) <span class=\"comment\"># 적용할 axis지정 rename</span></span><br><span class=\"line\"></span><br><span class=\"line\">df.add_prefix(<span class=\"string\">&#x27;X_&#x27;</span>) <span class=\"comment\">#컬럼에 접두어 X 추가</span></span><br><span class=\"line\">df.add_suffix(<span class=\"string\">&#x27;_Y&#x27;</span>) <span class=\"comment\">#컬럼에 접미어 Y 추가</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># list를 매핑해 변경하기</span></span><br><span class=\"line\">df.columns = [<span class=\"string\">&#x27;col_one&#x27;</span>, <span class=\"string\">&#x27;col_two&#x27;</span>]</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"행순서-뒤집기\"><a href=\"#행순서-뒤집기\" class=\"headerlink\" title=\"행순서 뒤집기\"></a>행순서 뒤집기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.loc[::-<span class=\"number\">1</span>].head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"reverse-column-order\"><a href=\"#reverse-column-order\" class=\"headerlink\" title=\"reverse column order\"></a>reverse column order</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">drinks.loc[:, ::-<span class=\"number\">1</span>].head() <span class=\"comment\"># [start:end:(step)]에 대한 이해 필요</span></span><br><span class=\"line\"><span class=\"comment\"># start, end가 비어있고 step이 -1이기에 순서가 역순으로 바뀜</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"datatype-기준으로-컬럼-선택하기\"><a href=\"#datatype-기준으로-컬럼-선택하기\" class=\"headerlink\" title=\"datatype 기준으로 컬럼 선택하기\"></a>datatype 기준으로 컬럼 선택하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.dtypes <span class=\"comment\"># 모든 열의 dtype 확인</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.select_dtypes(include=<span class=\"string\">&#x27;number&#x27;</span>).head() <span class=\"comment\"># dtype이 numeric인 데이터 추출</span></span><br><span class=\"line\"></span><br><span class=\"line\">drinks.select_dtypes(include=[<span class=\"string\">&#x27;number&#x27;</span>, <span class=\"string\">&#x27;object&#x27;</span>, <span class=\"string\">&#x27;category&#x27;</span>, <span class=\"string\">&#x27;datetime&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>\n<h3 id=\"문자열-numeric으로-변환하기\"><a href=\"#문자열-numeric으로-변환하기\" class=\"headerlink\" title=\"문자열 numeric으로 변환하기\"></a>문자열 numeric으로 변환하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:[<span class=\"string\">&#x27;1.1&#x27;</span>, <span class=\"string\">&#x27;2.2&#x27;</span>, <span class=\"string\">&#x27;3.3&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;col_two&#x27;</span>:[<span class=\"string\">&#x27;4.4&#x27;</span>, <span class=\"string\">&#x27;5.5&#x27;</span>, <span class=\"string\">&#x27;6.6&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;col_three&#x27;</span>:[<span class=\"string\">&#x27;7.7&#x27;</span>, <span class=\"string\">&#x27;8.8&#x27;</span>, <span class=\"string\">&#x27;-&#x27;</span>]&#125;)</span><br><span class=\"line\">df</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># astype()을 활용한 변환</span></span><br><span class=\"line\">df.astype(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:<span class=\"string\">&#x27;float&#x27;</span>, <span class=\"string\">&#x27;col_two&#x27;</span>:<span class=\"string\">&#x27;float&#x27;</span>&#125;).dtypes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># to_numeric을 활용한 변환</span></span><br><span class=\"line\">pd.to_numeric(df.col_three, errors=<span class=\"string\">&#x27;coerce&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># df 전체에 적용(numeric 변환 후 fillna)</span></span><br><span class=\"line\">df = df.apply(pd.to_numeric, errors=<span class=\"string\">&#x27;coerce&#x27;</span>).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># , 이 포함된 숫자형태의 문자열의 경우 replace사용</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">toInt</span>(<span class=\"params\">string</span>):</span><br><span class=\"line\">    string = <span class=\"built_in\">int</span>(string.replace(<span class=\"string\">&#x27;,&#x27;</span>,<span class=\"string\">&#x27;&#x27;</span>))</span><br><span class=\"line\">    returen string</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-사이즈-줄이기\"><a href=\"#DF-사이즈-줄이기\" class=\"headerlink\" title=\"DF 사이즈 줄이기\"></a>DF 사이즈 줄이기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 메모리 사용정도 확인</span></span><br><span class=\"line\">drinks.info(memory_usage=<span class=\"string\">&#x27;deep&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 컬럼지정을 활용한 데이터 줄이기</span></span><br><span class=\"line\">dtypes = &#123;<span class=\"string\">&#x27;continent&#x27;</span>:<span class=\"string\">&#x27;category&#x27;</span>&#125;</span><br><span class=\"line\">smaller_drinks = pd.read_csv(<span class=\"string\">&#x27;http://bit.ly/drinksbycountry&#x27;</span>, usecols=cols, dtype=dtypes)</span><br><span class=\"line\">smaller_drinks.info(memory_usage=<span class=\"string\">&#x27;deep&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Build-a-DataFrame-from-mulfiple-files-row-wise\"><a href=\"#Build-a-DataFrame-from-mulfiple-files-row-wise\" class=\"headerlink\" title=\"Build a DataFrame from mulfiple files (row-wise)\"></a>Build a DataFrame from mulfiple files (row-wise)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> glob <span class=\"keyword\">import</span> glob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 정규식,와일드카드 관련문서 참고</span></span><br><span class=\"line\"><span class=\"comment\"># stocks로 시작하는 data폴더 내 모든 csv 파일 </span></span><br><span class=\"line\">stock_files = <span class=\"built_in\">sorted</span>(glob(<span class=\"string\">&#x27;data/stocks*.csv&#x27;</span>))</span><br><span class=\"line\">stock_files</span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"string\">&#x27;data/stocks1.csv&#x27;</span>, <span class=\"string\">&#x27;data/stocks2.csv&#x27;</span>, <span class=\"string\">&#x27;data/stocks3.csv&#x27;</span>] <span class=\"comment\"># 리스트 형태로 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 파일합치기</span></span><br><span class=\"line\">pd.concat((pd.read_csv(file) <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> stock_files), ignore_index=<span class=\"literal\">True</span>) <span class=\"comment\"># ignore index는 각 파일의 index를 무시하고 초기화하는 옵션이다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"Build-a-DataFrame-from-mulfiple-files-column-wise\"><a href=\"#Build-a-DataFrame-from-mulfiple-files-column-wise\" class=\"headerlink\" title=\"Build a DataFrame from mulfiple files (column-wise)\"></a>Build a DataFrame from mulfiple files (column-wise)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 축옵션만 넣어주면 된다</span></span><br><span class=\"line\">pd.concat((pd.read_csv(file) <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> drink_files), axis=<span class=\"string\">&#x27;columns&#x27;</span>).head()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"클립보드에서-df불러오기\"><a href=\"#클립보드에서-df불러오기\" class=\"headerlink\" title=\"클립보드에서 df불러오기\"></a>클립보드에서 df불러오기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_clipboard()</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DF-subsetting-하기\"><a href=\"#DF-subsetting-하기\" class=\"headerlink\" title=\"DF subsetting 하기\"></a>DF subsetting 하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frac으로 원db의 75% 할당</span></span><br><span class=\"line\">movies_1 = movies.sample(frac=<span class=\"number\">0.75</span>, random_state=<span class=\"number\">1234</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 나머지</span></span><br><span class=\"line\">movies_2 = movies.drop(movies_1.index)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"isin을-활용한-DF필터링\"><a href=\"#isin을-활용한-DF필터링\" class=\"headerlink\" title=\"isin을 활용한 DF필터링\"></a>isin을 활용한 DF필터링</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># inin을 사용해 특정열에 대해 값에 대한조건을 넣어줄 수 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 포함하고 뽑기</span></span><br><span class=\"line\">movies[movies.genre.isin([<span class=\"string\">&#x27;Action&#x27;</span>,<span class=\"string\">&#x27;Drama&#x27;</span>,<span class=\"string\">&#x27;Western&#x27;</span>])].head()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 제외하고 뽑기</span></span><br><span class=\"line\">movies[~movies.genre.isin([<span class=\"string\">&#x27;Action&#x27;</span>, <span class=\"string\">&#x27;Drama&#x27;</span>, <span class=\"string\">&#x27;Western&#x27;</span>])].head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"value-counts-를-관측값-구하기\"><a href=\"#value-counts-를-관측값-구하기\" class=\"headerlink\" title=\"value_counts()를 관측값 구하기\"></a>value_counts()를 관측값 구하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 우선 카테고리(장르)별 관측값를 구한다</span></span><br><span class=\"line\">counts = movies.genre.value_counts()</span><br><span class=\"line\">counts</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># count에서 상위3개를 구한다.</span></span><br><span class=\"line\">counts.nlargest(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"결측값-처리하기\"><a href=\"#결측값-처리하기\" class=\"headerlink\" title=\"결측값 처리하기\"></a>결측값 처리하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 결측값 조건걸기</span></span><br><span class=\"line\">ufo = dropna(thresh = <span class=\"built_in\">len</span>(ufo)*<span class=\"number\">0.9</span>, axis = <span class=\"string\">&#x27;columns&#x27;</span>) <span class=\"comment\"># 90% 이상 값이 있는 컬럼만 유지</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 열별로 결측값의 수 세기</span></span><br><span class=\"line\">ufo.isna().<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># NA가 하나라도 있는 열 삭제</span></span><br><span class=\"line\">ufo.dropna(axis=<span class=\"string\">&#x27;columns&#x27;</span>).head()</span><br></pre></td></tr></table></figure>\n<h3 id=\"split를-활용한-문자열-나누기\"><a href=\"#split를-활용한-문자열-나누기\" class=\"headerlink\" title=\".split를 활용한 문자열 나누기\"></a>.split를 활용한 문자열 나누기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;name&#x27;</span>:[<span class=\"string\">&#x27;John Arthur Doe&#x27;</span>, <span class=\"string\">&#x27;Jane Ann Smith&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;location&#x27;</span>:[<span class=\"string\">&#x27;Los Angeles, CA&#x27;</span>, <span class=\"string\">&#x27;Washington, DC&#x27;</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[[<span class=\"string\">&#x27;first&#x27;</span>, <span class=\"string\">&#x27;middle&#x27;</span>, <span class=\"string\">&#x27;last&#x27;</span>]] = df.name.<span class=\"built_in\">str</span>.split(<span class=\"string\">&#x27; &#x27;</span>, expand=<span class=\"literal\">True</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<p><img src=\"/image/output.png\"></p>\n<h3 id=\"리스트를-DF로-변환하기\"><a href=\"#리스트를-DF로-변환하기\" class=\"headerlink\" title=\"리스트를 DF로 변환하기\"></a>리스트를 DF로 변환하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:[<span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>, <span class=\"string\">&#x27;c&#x27;</span>], <span class=\"string\">&#x27;col_two&#x27;</span>:[[<span class=\"number\">10</span>, <span class=\"number\">40</span>], [<span class=\"number\">20</span>, <span class=\"number\">50</span>], [<span class=\"number\">30</span>, <span class=\"number\">60</span>]]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<p><img src=\"/image/output2.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_new = df.col_two.apply(pd.Series) <span class=\"comment\"># apply를 활용한 df 생성</span></span><br><span class=\"line\">df_new</span><br></pre></td></tr></table></figure>\n<h3 id=\"Aggregate-by-multiple-funtions\"><a href=\"#Aggregate-by-multiple-funtions\" class=\"headerlink\" title=\"Aggregate by multiple funtions\"></a>Aggregate by multiple funtions</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># aggregate를 활용한 요약통계량 산출하기</span></span><br><span class=\"line\">orders.groupby(<span class=\"string\">&#x27;order_id&#x27;</span>).item_price.agg([<span class=\"string\">&#x27;sum&#x27;</span>, <span class=\"string\">&#x27;count&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Combine-the-output-of-an-aggregation-by-multiple-funtions\"><a href=\"#Combine-the-output-of-an-aggregation-by-multiple-funtions\" class=\"headerlink\" title=\"Combine the output of an aggregation by multiple funtions\"></a>Combine the output of an aggregation by multiple funtions</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform()은 입력된 개체와 동일하게 인덱스된 객체를 반환하며 다중연산에 쓰인다.</span></span><br><span class=\"line\">total_price = orders.groupby(<span class=\"string\">&#x27;order_id&#x27;</span>).item_price.transform(<span class=\"string\">&#x27;sum&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># transform() 관련레퍼런스</span></span><br><span class=\"line\"><span class=\"comment\"># https://kongdols-room.tistory.com/169 </span></span><br></pre></td></tr></table></figure>\n<h3 id=\"loc를-활용한-행열-슬라이싱\"><a href=\"#loc를-활용한-행열-슬라이싱\" class=\"headerlink\" title=\".loc를 활용한 행열 슬라이싱\"></a>.loc를 활용한 행열 슬라이싱</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">titanic.describe().loc[<span class=\"string\">&#x27;min&#x27;</span>:<span class=\"string\">&#x27;max&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">titanic.describe().loc[<span class=\"string\">&#x27;min&#x27;</span>:<span class=\"string\">&#x27;max&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>:<span class=\"string\">&#x27;Parch&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"계층적-index를-가지는-Series-DF로-변환하기\"><a href=\"#계층적-index를-가지는-Series-DF로-변환하기\" class=\"headerlink\" title=\"계층적 index를 가지는 Series DF로 변환하기\"></a>계층적 index를 가지는 Series DF로 변환하기</h3><ul>\n<li>부모자식 노드처럼 계층이 있는 인덱스를 가지는 DF를 만들 수있다</li>\n<li>잘 쓰진 않는 것 같다<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 계층적</span></span><br><span class=\"line\"><span class=\"comment\"># https://nittaku.tistory.com/122</span></span><br><span class=\"line\"></span><br><span class=\"line\">titanic.groupby([<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>]).Survived.mean()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># changing multiple Series into a DF</span></span><br><span class=\"line\">titanic.groupby([<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>]).Survived.mean().unstack()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"피벗테이블-만들기\"><a href=\"#피벗테이블-만들기\" class=\"headerlink\" title=\"피벗테이블 만들기\"></a>피벗테이블 만들기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">titanic.pivot_table(index=<span class=\"string\">&#x27;Sex&#x27;</span>, columns=<span class=\"string\">&#x27;Pclass&#x27;</span>, values=<span class=\"string\">&#x27;Survived&#x27;</span>, aggfunc=<span class=\"string\">&#x27;mean&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># pivot_table에서 aggfunc 파라미터를 &#x27;count&#x27; 으로 바꿀 경우 단순 crosstable을 반환한다</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># margins = True option으로 행열합을 DF에 추가한다</span></span><br><span class=\"line\">titanic.pivot_table(index=<span class=\"string\">&#x27;Sex&#x27;</span>, columns=<span class=\"string\">&#x27;Pclass&#x27;</span>, values=<span class=\"string\">&#x27;Survived&#x27;</span>, aggfunc=<span class=\"string\">&#x27;mean&#x27;</span>,</span><br><span class=\"line\">                    margins=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기\"><a href=\"#bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기\" class=\"headerlink\" title=\"bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기\"></a>bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기</h3></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># use bin with the labels</span></span><br><span class=\"line\">pd.cut(titanic.Age, bins=[<span class=\"number\">0</span>, <span class=\"number\">18</span>, <span class=\"number\">25</span>, <span class=\"number\">99</span>], labels=[<span class=\"string\">&#x27;child&#x27;</span>, <span class=\"string\">&#x27;young adult&#x27;</span>, <span class=\"string\">&#x27;adult&#x27;</span>]).head(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-표시형식-바꾸기\"><a href=\"#DF-표시형식-바꾸기\" class=\"headerlink\" title=\"DF 표시형식 바꾸기\"></a>DF 표시형식 바꾸기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># set_option을 통해 표시형식 바꾸기</span></span><br><span class=\"line\">pd.set_option(<span class=\"string\">&#x27;display.float_format&#x27;</span>, <span class=\"string\">&#x27;&#123;:.2f&#125;&#x27;</span>.<span class=\"built_in\">format</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-꾸미기-Style-a-DataFrame\"><a href=\"#DF-꾸미기-Style-a-DataFrame\" class=\"headerlink\" title=\"DF 꾸미기 (Style a DataFrame)\"></a>DF 꾸미기 (Style a DataFrame)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">format_dict = &#123;<span class=\"string\">&#x27;Date&#x27;</span>:<span class=\"string\">&#x27;&#123;:%m/%d/%y&#125;&#x27;</span>, <span class=\"string\">&#x27;Close&#x27;</span>:<span class=\"string\">&#x27;$&#123;:.2f&#125;&#x27;</span>, <span class=\"string\">&#x27;Volume&#x27;</span>:<span class=\"string\">&#x27;&#123;:,&#125;&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">df.style.<span class=\"built_in\">format</span>(format_dict) <span class=\"comment\"># 스타일 바꾸기</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"ProfileReport를-통해-DF-구조-통계량-한번에-확인하기\"><a href=\"#ProfileReport를-통해-DF-구조-통계량-한번에-확인하기\" class=\"headerlink\" title=\"ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기\"></a>ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i</span><br><span class=\"line\">mport pandas_profiling</span><br><span class=\"line\">pandas_profiliing.PrifileReport(titanic)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"glob을-사용해-여러-csv파일을-하나의-df로-합치기\"><a href=\"#glob을-사용해-여러-csv파일을-하나의-df로-합치기\" class=\"headerlink\" title=\"glob을 사용해 여러 csv파일을 하나의 df로 합치기\"></a>glob을 사용해 여러 csv파일을 하나의 df로 합치기</h3><ul>\n<li><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> glob</span><br><span class=\"line\"></span><br><span class=\"line\">path = <span class=\"string\">r&#x27;C:\\DRO\\DCL_rawdata_files&#x27;</span> <span class=\"comment\"># use your path</span></span><br><span class=\"line\">all_files = glob.glob(path + <span class=\"string\">&quot;/*.csv&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">li = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> filename <span class=\"keyword\">in</span> all_files:</span><br><span class=\"line\">    df = pd.read_csv(filename, index_col=<span class=\"literal\">None</span>, header=<span class=\"number\">0</span>)</span><br><span class=\"line\">    li.append(df)</span><br><span class=\"line\"></span><br><span class=\"line\">frame = pd.concat(li, axis=<span class=\"number\">0</span>, ignore_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DF에-컬럼-추가하기\"><a href=\"#DF에-컬럼-추가하기\" class=\"headerlink\" title=\"DF에 컬럼 추가하기\"></a>DF에 컬럼 추가하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">dic = &#123;<span class=\"string\">&#x27;Name&#x27;</span>: [<span class=\"string\">&#x27;Jai&#x27;</span>, <span class=\"string\">&#x27;Princi&#x27;</span>, <span class=\"string\">&#x27;Gaurav&#x27;</span>, <span class=\"string\">&#x27;Anuj&#x27;</span>],</span><br><span class=\"line\">        <span class=\"string\">&#x27;Height&#x27;</span>: [<span class=\"number\">5.1</span>, <span class=\"number\">6.2</span>, <span class=\"number\">5.1</span>, <span class=\"number\">5.2</span>],</span><br><span class=\"line\">        <span class=\"string\">&#x27;Qualification&#x27;</span>: [<span class=\"string\">&#x27;Msc&#x27;</span>, <span class=\"string\">&#x27;MA&#x27;</span>, <span class=\"string\">&#x27;Msc&#x27;</span>, <span class=\"string\">&#x27;Msc&#x27;</span>]&#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Convert the dictionary into DataFrame</span></span><br><span class=\"line\">df = pd.DataFrame(data)</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Declare a list that is to be converted into a column</span></span><br><span class=\"line\">address = [<span class=\"string\">&#x27;Delhi&#x27;</span>, <span class=\"string\">&#x27;Bangalore&#x27;</span>, <span class=\"string\">&#x27;Chennai&#x27;</span>, <span class=\"string\">&#x27;Patna&#x27;</span>]</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Using &#x27;Address&#x27; as the column name</span></span><br><span class=\"line\"><span class=\"comment\"># and equating it to the list</span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;Address&#x27;</span>] = address</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"apply-등을-활용한-파생변수-생성하기\"><a href=\"#apply-등을-활용한-파생변수-생성하기\" class=\"headerlink\" title=\"apply 등을 활용한 파생변수 생성하기\"></a>apply 등을 활용한 파생변수 생성하기</h3></li>\n</ul>\n<p>-DF전체에 적용하거나 DF일부에 적용할 수 있다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"lambda를-활용한-함수-적용\"><a href=\"#lambda를-활용한-함수-적용\" class=\"headerlink\" title=\"lambda를 활용한 함수 적용\"></a>lambda를 활용한 함수 적용</h3><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\">http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/</a></p>\n<p><a href=\"https://wikidocs.net/46758\">https://wikidocs.net/46758</a></p>\n<p><a href=\"https://data-make.tistory.com/123\">https://data-make.tistory.com/123</a></p>\n<h2 id=\"3-References\"><a href=\"#3-References\" class=\"headerlink\" title=\"3. References\"></a>3. References</h2><ul>\n<li><a href=\"https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\">https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=RlIiVeig3hc\">https://www.youtube.com/watch?v=RlIiVeig3hc</a></li>\n<li><a href=\"https://kongdols-room.tistory.com/169\">https://kongdols-room.tistory.com/169</a> </li>\n<li><a href=\"https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/\">https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"pandas-tricks\"><a href=\"#pandas-tricks\" class=\"headerlink\" title=\"pandas tricks\"></a><strong>pandas tricks</strong></h2><blockquote>\n<p>pandas관련 자주 사용할만한 코드 정리</p>\n</blockquote>\n<h3 id=\"pandas-version확인\"><a href=\"#pandas-version확인\" class=\"headerlink\" title=\"pandas version확인\"></a>pandas version확인</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pd.__version__ <span class=\"comment\"># pandas version확인</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pd.show_versions() <span class=\"comment\">#의존성 패키지 확인</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DF-생성하기\"><a href=\"#DF-생성하기\" class=\"headerlink\" title=\"DF 생성하기\"></a>DF 생성하기</h3><blockquote>\n<p>여러 방법이 있지만 보통 dictionary를 사용한다.</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col one&#x27;</span>:[<span class=\"number\">100</span>, <span class=\"number\">200</span>], <span class=\"string\">&#x27;col two&#x27;</span>:[<span class=\"number\">300</span>, <span class=\"number\">400</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 난수생성을 통핸 DF생성</span></span><br><span class=\"line\">pd.DataFrame(np.random.rand(<span class=\"number\">4</span>, <span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"열이름-변경하기\"><a href=\"#열이름-변경하기\" class=\"headerlink\" title=\"열이름 변경하기\"></a>열이름 변경하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># dictionary 형태로 변경하기</span></span><br><span class=\"line\">df = df.rename(&#123;<span class=\"string\">&#x27;col one&#x27;</span> : <span class=\"string\">&#x27;col_one&#x27;</span>,<span class=\"string\">&#x27;col two&#x27;</span>: <span class=\"string\">&#x27;col_two&#x27;</span>&#125;, axis = <span class=\"string\">&#x27;columns&#x27;</span> ) <span class=\"comment\"># 적용할 axis지정 rename</span></span><br><span class=\"line\"></span><br><span class=\"line\">df.add_prefix(<span class=\"string\">&#x27;X_&#x27;</span>) <span class=\"comment\">#컬럼에 접두어 X 추가</span></span><br><span class=\"line\">df.add_suffix(<span class=\"string\">&#x27;_Y&#x27;</span>) <span class=\"comment\">#컬럼에 접미어 Y 추가</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># list를 매핑해 변경하기</span></span><br><span class=\"line\">df.columns = [<span class=\"string\">&#x27;col_one&#x27;</span>, <span class=\"string\">&#x27;col_two&#x27;</span>]</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"행순서-뒤집기\"><a href=\"#행순서-뒤집기\" class=\"headerlink\" title=\"행순서 뒤집기\"></a>행순서 뒤집기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.loc[::-<span class=\"number\">1</span>].head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"reverse-column-order\"><a href=\"#reverse-column-order\" class=\"headerlink\" title=\"reverse column order\"></a>reverse column order</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">drinks.loc[:, ::-<span class=\"number\">1</span>].head() <span class=\"comment\"># [start:end:(step)]에 대한 이해 필요</span></span><br><span class=\"line\"><span class=\"comment\"># start, end가 비어있고 step이 -1이기에 순서가 역순으로 바뀜</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"datatype-기준으로-컬럼-선택하기\"><a href=\"#datatype-기준으로-컬럼-선택하기\" class=\"headerlink\" title=\"datatype 기준으로 컬럼 선택하기\"></a>datatype 기준으로 컬럼 선택하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.dtypes <span class=\"comment\"># 모든 열의 dtype 확인</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drinks.select_dtypes(include=<span class=\"string\">&#x27;number&#x27;</span>).head() <span class=\"comment\"># dtype이 numeric인 데이터 추출</span></span><br><span class=\"line\"></span><br><span class=\"line\">drinks.select_dtypes(include=[<span class=\"string\">&#x27;number&#x27;</span>, <span class=\"string\">&#x27;object&#x27;</span>, <span class=\"string\">&#x27;category&#x27;</span>, <span class=\"string\">&#x27;datetime&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>\n<h3 id=\"문자열-numeric으로-변환하기\"><a href=\"#문자열-numeric으로-변환하기\" class=\"headerlink\" title=\"문자열 numeric으로 변환하기\"></a>문자열 numeric으로 변환하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:[<span class=\"string\">&#x27;1.1&#x27;</span>, <span class=\"string\">&#x27;2.2&#x27;</span>, <span class=\"string\">&#x27;3.3&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;col_two&#x27;</span>:[<span class=\"string\">&#x27;4.4&#x27;</span>, <span class=\"string\">&#x27;5.5&#x27;</span>, <span class=\"string\">&#x27;6.6&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;col_three&#x27;</span>:[<span class=\"string\">&#x27;7.7&#x27;</span>, <span class=\"string\">&#x27;8.8&#x27;</span>, <span class=\"string\">&#x27;-&#x27;</span>]&#125;)</span><br><span class=\"line\">df</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># astype()을 활용한 변환</span></span><br><span class=\"line\">df.astype(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:<span class=\"string\">&#x27;float&#x27;</span>, <span class=\"string\">&#x27;col_two&#x27;</span>:<span class=\"string\">&#x27;float&#x27;</span>&#125;).dtypes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># to_numeric을 활용한 변환</span></span><br><span class=\"line\">pd.to_numeric(df.col_three, errors=<span class=\"string\">&#x27;coerce&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># df 전체에 적용(numeric 변환 후 fillna)</span></span><br><span class=\"line\">df = df.apply(pd.to_numeric, errors=<span class=\"string\">&#x27;coerce&#x27;</span>).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># , 이 포함된 숫자형태의 문자열의 경우 replace사용</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">toInt</span>(<span class=\"params\">string</span>):</span><br><span class=\"line\">    string = <span class=\"built_in\">int</span>(string.replace(<span class=\"string\">&#x27;,&#x27;</span>,<span class=\"string\">&#x27;&#x27;</span>))</span><br><span class=\"line\">    returen string</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-사이즈-줄이기\"><a href=\"#DF-사이즈-줄이기\" class=\"headerlink\" title=\"DF 사이즈 줄이기\"></a>DF 사이즈 줄이기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 메모리 사용정도 확인</span></span><br><span class=\"line\">drinks.info(memory_usage=<span class=\"string\">&#x27;deep&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 컬럼지정을 활용한 데이터 줄이기</span></span><br><span class=\"line\">dtypes = &#123;<span class=\"string\">&#x27;continent&#x27;</span>:<span class=\"string\">&#x27;category&#x27;</span>&#125;</span><br><span class=\"line\">smaller_drinks = pd.read_csv(<span class=\"string\">&#x27;http://bit.ly/drinksbycountry&#x27;</span>, usecols=cols, dtype=dtypes)</span><br><span class=\"line\">smaller_drinks.info(memory_usage=<span class=\"string\">&#x27;deep&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Build-a-DataFrame-from-mulfiple-files-row-wise\"><a href=\"#Build-a-DataFrame-from-mulfiple-files-row-wise\" class=\"headerlink\" title=\"Build a DataFrame from mulfiple files (row-wise)\"></a>Build a DataFrame from mulfiple files (row-wise)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> glob <span class=\"keyword\">import</span> glob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 정규식,와일드카드 관련문서 참고</span></span><br><span class=\"line\"><span class=\"comment\"># stocks로 시작하는 data폴더 내 모든 csv 파일 </span></span><br><span class=\"line\">stock_files = <span class=\"built_in\">sorted</span>(glob(<span class=\"string\">&#x27;data/stocks*.csv&#x27;</span>))</span><br><span class=\"line\">stock_files</span><br><span class=\"line\"></span><br><span class=\"line\">[<span class=\"string\">&#x27;data/stocks1.csv&#x27;</span>, <span class=\"string\">&#x27;data/stocks2.csv&#x27;</span>, <span class=\"string\">&#x27;data/stocks3.csv&#x27;</span>] <span class=\"comment\"># 리스트 형태로 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 파일합치기</span></span><br><span class=\"line\">pd.concat((pd.read_csv(file) <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> stock_files), ignore_index=<span class=\"literal\">True</span>) <span class=\"comment\"># ignore index는 각 파일의 index를 무시하고 초기화하는 옵션이다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"Build-a-DataFrame-from-mulfiple-files-column-wise\"><a href=\"#Build-a-DataFrame-from-mulfiple-files-column-wise\" class=\"headerlink\" title=\"Build a DataFrame from mulfiple files (column-wise)\"></a>Build a DataFrame from mulfiple files (column-wise)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 축옵션만 넣어주면 된다</span></span><br><span class=\"line\">pd.concat((pd.read_csv(file) <span class=\"keyword\">for</span> file <span class=\"keyword\">in</span> drink_files), axis=<span class=\"string\">&#x27;columns&#x27;</span>).head()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"클립보드에서-df불러오기\"><a href=\"#클립보드에서-df불러오기\" class=\"headerlink\" title=\"클립보드에서 df불러오기\"></a>클립보드에서 df불러오기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_clipboard()</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"DF-subsetting-하기\"><a href=\"#DF-subsetting-하기\" class=\"headerlink\" title=\"DF subsetting 하기\"></a>DF subsetting 하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># frac으로 원db의 75% 할당</span></span><br><span class=\"line\">movies_1 = movies.sample(frac=<span class=\"number\">0.75</span>, random_state=<span class=\"number\">1234</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 나머지</span></span><br><span class=\"line\">movies_2 = movies.drop(movies_1.index)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"isin을-활용한-DF필터링\"><a href=\"#isin을-활용한-DF필터링\" class=\"headerlink\" title=\"isin을 활용한 DF필터링\"></a>isin을 활용한 DF필터링</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># inin을 사용해 특정열에 대해 값에 대한조건을 넣어줄 수 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 포함하고 뽑기</span></span><br><span class=\"line\">movies[movies.genre.isin([<span class=\"string\">&#x27;Action&#x27;</span>,<span class=\"string\">&#x27;Drama&#x27;</span>,<span class=\"string\">&#x27;Western&#x27;</span>])].head()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 제외하고 뽑기</span></span><br><span class=\"line\">movies[~movies.genre.isin([<span class=\"string\">&#x27;Action&#x27;</span>, <span class=\"string\">&#x27;Drama&#x27;</span>, <span class=\"string\">&#x27;Western&#x27;</span>])].head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"value-counts-를-관측값-구하기\"><a href=\"#value-counts-를-관측값-구하기\" class=\"headerlink\" title=\"value_counts()를 관측값 구하기\"></a>value_counts()를 관측값 구하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 우선 카테고리(장르)별 관측값를 구한다</span></span><br><span class=\"line\">counts = movies.genre.value_counts()</span><br><span class=\"line\">counts</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># count에서 상위3개를 구한다.</span></span><br><span class=\"line\">counts.nlargest(<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"결측값-처리하기\"><a href=\"#결측값-처리하기\" class=\"headerlink\" title=\"결측값 처리하기\"></a>결측값 처리하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 결측값 조건걸기</span></span><br><span class=\"line\">ufo = dropna(thresh = <span class=\"built_in\">len</span>(ufo)*<span class=\"number\">0.9</span>, axis = <span class=\"string\">&#x27;columns&#x27;</span>) <span class=\"comment\"># 90% 이상 값이 있는 컬럼만 유지</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 열별로 결측값의 수 세기</span></span><br><span class=\"line\">ufo.isna().<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># NA가 하나라도 있는 열 삭제</span></span><br><span class=\"line\">ufo.dropna(axis=<span class=\"string\">&#x27;columns&#x27;</span>).head()</span><br></pre></td></tr></table></figure>\n<h3 id=\"split를-활용한-문자열-나누기\"><a href=\"#split를-활용한-문자열-나누기\" class=\"headerlink\" title=\".split를 활용한 문자열 나누기\"></a>.split를 활용한 문자열 나누기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;name&#x27;</span>:[<span class=\"string\">&#x27;John Arthur Doe&#x27;</span>, <span class=\"string\">&#x27;Jane Ann Smith&#x27;</span>],</span><br><span class=\"line\">                   <span class=\"string\">&#x27;location&#x27;</span>:[<span class=\"string\">&#x27;Los Angeles, CA&#x27;</span>, <span class=\"string\">&#x27;Washington, DC&#x27;</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[[<span class=\"string\">&#x27;first&#x27;</span>, <span class=\"string\">&#x27;middle&#x27;</span>, <span class=\"string\">&#x27;last&#x27;</span>]] = df.name.<span class=\"built_in\">str</span>.split(<span class=\"string\">&#x27; &#x27;</span>, expand=<span class=\"literal\">True</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<p><img src=\"/image/output.png\"></p>\n<h3 id=\"리스트를-DF로-변환하기\"><a href=\"#리스트를-DF로-변환하기\" class=\"headerlink\" title=\"리스트를 DF로 변환하기\"></a>리스트를 DF로 변환하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">&#x27;col_one&#x27;</span>:[<span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>, <span class=\"string\">&#x27;c&#x27;</span>], <span class=\"string\">&#x27;col_two&#x27;</span>:[[<span class=\"number\">10</span>, <span class=\"number\">40</span>], [<span class=\"number\">20</span>, <span class=\"number\">50</span>], [<span class=\"number\">30</span>, <span class=\"number\">60</span>]]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<p><img src=\"/image/output2.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_new = df.col_two.apply(pd.Series) <span class=\"comment\"># apply를 활용한 df 생성</span></span><br><span class=\"line\">df_new</span><br></pre></td></tr></table></figure>\n<h3 id=\"Aggregate-by-multiple-funtions\"><a href=\"#Aggregate-by-multiple-funtions\" class=\"headerlink\" title=\"Aggregate by multiple funtions\"></a>Aggregate by multiple funtions</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># aggregate를 활용한 요약통계량 산출하기</span></span><br><span class=\"line\">orders.groupby(<span class=\"string\">&#x27;order_id&#x27;</span>).item_price.agg([<span class=\"string\">&#x27;sum&#x27;</span>, <span class=\"string\">&#x27;count&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Combine-the-output-of-an-aggregation-by-multiple-funtions\"><a href=\"#Combine-the-output-of-an-aggregation-by-multiple-funtions\" class=\"headerlink\" title=\"Combine the output of an aggregation by multiple funtions\"></a>Combine the output of an aggregation by multiple funtions</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># transform()은 입력된 개체와 동일하게 인덱스된 객체를 반환하며 다중연산에 쓰인다.</span></span><br><span class=\"line\">total_price = orders.groupby(<span class=\"string\">&#x27;order_id&#x27;</span>).item_price.transform(<span class=\"string\">&#x27;sum&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># transform() 관련레퍼런스</span></span><br><span class=\"line\"><span class=\"comment\"># https://kongdols-room.tistory.com/169 </span></span><br></pre></td></tr></table></figure>\n<h3 id=\"loc를-활용한-행열-슬라이싱\"><a href=\"#loc를-활용한-행열-슬라이싱\" class=\"headerlink\" title=\".loc를 활용한 행열 슬라이싱\"></a>.loc를 활용한 행열 슬라이싱</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">titanic.describe().loc[<span class=\"string\">&#x27;min&#x27;</span>:<span class=\"string\">&#x27;max&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">titanic.describe().loc[<span class=\"string\">&#x27;min&#x27;</span>:<span class=\"string\">&#x27;max&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>:<span class=\"string\">&#x27;Parch&#x27;</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"계층적-index를-가지는-Series-DF로-변환하기\"><a href=\"#계층적-index를-가지는-Series-DF로-변환하기\" class=\"headerlink\" title=\"계층적 index를 가지는 Series DF로 변환하기\"></a>계층적 index를 가지는 Series DF로 변환하기</h3><ul>\n<li>부모자식 노드처럼 계층이 있는 인덱스를 가지는 DF를 만들 수있다</li>\n<li>잘 쓰진 않는 것 같다<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 계층적</span></span><br><span class=\"line\"><span class=\"comment\"># https://nittaku.tistory.com/122</span></span><br><span class=\"line\"></span><br><span class=\"line\">titanic.groupby([<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>]).Survived.mean()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># changing multiple Series into a DF</span></span><br><span class=\"line\">titanic.groupby([<span class=\"string\">&#x27;Sex&#x27;</span>, <span class=\"string\">&#x27;Pclass&#x27;</span>]).Survived.mean().unstack()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"피벗테이블-만들기\"><a href=\"#피벗테이블-만들기\" class=\"headerlink\" title=\"피벗테이블 만들기\"></a>피벗테이블 만들기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">titanic.pivot_table(index=<span class=\"string\">&#x27;Sex&#x27;</span>, columns=<span class=\"string\">&#x27;Pclass&#x27;</span>, values=<span class=\"string\">&#x27;Survived&#x27;</span>, aggfunc=<span class=\"string\">&#x27;mean&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># pivot_table에서 aggfunc 파라미터를 &#x27;count&#x27; 으로 바꿀 경우 단순 crosstable을 반환한다</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># margins = True option으로 행열합을 DF에 추가한다</span></span><br><span class=\"line\">titanic.pivot_table(index=<span class=\"string\">&#x27;Sex&#x27;</span>, columns=<span class=\"string\">&#x27;Pclass&#x27;</span>, values=<span class=\"string\">&#x27;Survived&#x27;</span>, aggfunc=<span class=\"string\">&#x27;mean&#x27;</span>,</span><br><span class=\"line\">                    margins=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기\"><a href=\"#bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기\" class=\"headerlink\" title=\"bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기\"></a>bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기</h3></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># use bin with the labels</span></span><br><span class=\"line\">pd.cut(titanic.Age, bins=[<span class=\"number\">0</span>, <span class=\"number\">18</span>, <span class=\"number\">25</span>, <span class=\"number\">99</span>], labels=[<span class=\"string\">&#x27;child&#x27;</span>, <span class=\"string\">&#x27;young adult&#x27;</span>, <span class=\"string\">&#x27;adult&#x27;</span>]).head(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-표시형식-바꾸기\"><a href=\"#DF-표시형식-바꾸기\" class=\"headerlink\" title=\"DF 표시형식 바꾸기\"></a>DF 표시형식 바꾸기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># set_option을 통해 표시형식 바꾸기</span></span><br><span class=\"line\">pd.set_option(<span class=\"string\">&#x27;display.float_format&#x27;</span>, <span class=\"string\">&#x27;&#123;:.2f&#125;&#x27;</span>.<span class=\"built_in\">format</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"DF-꾸미기-Style-a-DataFrame\"><a href=\"#DF-꾸미기-Style-a-DataFrame\" class=\"headerlink\" title=\"DF 꾸미기 (Style a DataFrame)\"></a>DF 꾸미기 (Style a DataFrame)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">format_dict = &#123;<span class=\"string\">&#x27;Date&#x27;</span>:<span class=\"string\">&#x27;&#123;:%m/%d/%y&#125;&#x27;</span>, <span class=\"string\">&#x27;Close&#x27;</span>:<span class=\"string\">&#x27;$&#123;:.2f&#125;&#x27;</span>, <span class=\"string\">&#x27;Volume&#x27;</span>:<span class=\"string\">&#x27;&#123;:,&#125;&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">df.style.<span class=\"built_in\">format</span>(format_dict) <span class=\"comment\"># 스타일 바꾸기</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"ProfileReport를-통해-DF-구조-통계량-한번에-확인하기\"><a href=\"#ProfileReport를-통해-DF-구조-통계량-한번에-확인하기\" class=\"headerlink\" title=\"ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기\"></a>ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i</span><br><span class=\"line\">mport pandas_profiling</span><br><span class=\"line\">pandas_profiliing.PrifileReport(titanic)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"glob을-사용해-여러-csv파일을-하나의-df로-합치기\"><a href=\"#glob을-사용해-여러-csv파일을-하나의-df로-합치기\" class=\"headerlink\" title=\"glob을 사용해 여러 csv파일을 하나의 df로 합치기\"></a>glob을 사용해 여러 csv파일을 하나의 df로 합치기</h3><ul>\n<li><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> glob</span><br><span class=\"line\"></span><br><span class=\"line\">path = <span class=\"string\">r&#x27;C:\\DRO\\DCL_rawdata_files&#x27;</span> <span class=\"comment\"># use your path</span></span><br><span class=\"line\">all_files = glob.glob(path + <span class=\"string\">&quot;/*.csv&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">li = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> filename <span class=\"keyword\">in</span> all_files:</span><br><span class=\"line\">    df = pd.read_csv(filename, index_col=<span class=\"literal\">None</span>, header=<span class=\"number\">0</span>)</span><br><span class=\"line\">    li.append(df)</span><br><span class=\"line\"></span><br><span class=\"line\">frame = pd.concat(li, axis=<span class=\"number\">0</span>, ignore_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"DF에-컬럼-추가하기\"><a href=\"#DF에-컬럼-추가하기\" class=\"headerlink\" title=\"DF에 컬럼 추가하기\"></a>DF에 컬럼 추가하기</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">dic = &#123;<span class=\"string\">&#x27;Name&#x27;</span>: [<span class=\"string\">&#x27;Jai&#x27;</span>, <span class=\"string\">&#x27;Princi&#x27;</span>, <span class=\"string\">&#x27;Gaurav&#x27;</span>, <span class=\"string\">&#x27;Anuj&#x27;</span>],</span><br><span class=\"line\">        <span class=\"string\">&#x27;Height&#x27;</span>: [<span class=\"number\">5.1</span>, <span class=\"number\">6.2</span>, <span class=\"number\">5.1</span>, <span class=\"number\">5.2</span>],</span><br><span class=\"line\">        <span class=\"string\">&#x27;Qualification&#x27;</span>: [<span class=\"string\">&#x27;Msc&#x27;</span>, <span class=\"string\">&#x27;MA&#x27;</span>, <span class=\"string\">&#x27;Msc&#x27;</span>, <span class=\"string\">&#x27;Msc&#x27;</span>]&#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Convert the dictionary into DataFrame</span></span><br><span class=\"line\">df = pd.DataFrame(data)</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Declare a list that is to be converted into a column</span></span><br><span class=\"line\">address = [<span class=\"string\">&#x27;Delhi&#x27;</span>, <span class=\"string\">&#x27;Bangalore&#x27;</span>, <span class=\"string\">&#x27;Chennai&#x27;</span>, <span class=\"string\">&#x27;Patna&#x27;</span>]</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># Using &#x27;Address&#x27; as the column name</span></span><br><span class=\"line\"><span class=\"comment\"># and equating it to the list</span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;Address&#x27;</span>] = address</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"apply-등을-활용한-파생변수-생성하기\"><a href=\"#apply-등을-활용한-파생변수-생성하기\" class=\"headerlink\" title=\"apply 등을 활용한 파생변수 생성하기\"></a>apply 등을 활용한 파생변수 생성하기</h3></li>\n</ul>\n<p>-DF전체에 적용하거나 DF일부에 적용할 수 있다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"lambda를-활용한-함수-적용\"><a href=\"#lambda를-활용한-함수-적용\" class=\"headerlink\" title=\"lambda를 활용한 함수 적용\"></a>lambda를 활용한 함수 적용</h3><h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h3 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/\">http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/</a></p>\n<p><a href=\"https://wikidocs.net/46758\">https://wikidocs.net/46758</a></p>\n<p><a href=\"https://data-make.tistory.com/123\">https://data-make.tistory.com/123</a></p>\n<h2 id=\"3-References\"><a href=\"#3-References\" class=\"headerlink\" title=\"3. References\"></a>3. References</h2><ul>\n<li><a href=\"https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\">https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=RlIiVeig3hc\">https://www.youtube.com/watch?v=RlIiVeig3hc</a></li>\n<li><a href=\"https://kongdols-room.tistory.com/169\">https://kongdols-room.tistory.com/169</a> </li>\n<li><a href=\"https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/\">https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"/image/output.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[pandas]pandas 함수와 기초용법들","path":"2022/06/13/Preprocessing-pandas_tricks/","eyeCatchImage":"/image/output.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["pandas"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Sampling]Class Imbalance 다루기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n-->\n\n# Dealing with Class Imbalance(클래스 불균형 다루기)\n---\n\n<!--\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n\n-->\n\n**여기서 해결하는 문제**\n\n- Biased predictions\n- Misleading accuracy\n\n**보통 고려하는 해결방법**\n\n- 데이터 합성(Synthesisis of new minority class instances)\n- Over-sampling \n- Under-sampling \n- class weight 조정하기(상향/하향가중치 적용)\n- cost function 조정\n\n## Random Under-Sampling\n---\n\n- **Advantages**\n  + It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n\n- **Disadvantages**\n  + It can discard potentially useful information which could be important for building rule classifiers.\n  + The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.\n\n### Tomeck Links\n\nTomek Links란 두 샘플 사이에 다른 관측치가 없는 경우를 말한다.\n\n![](https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg)\n\nTomek Links 방법은 Tomeck links 중에 major에 속하는 데이터포인트를 제거하는 undersampling 기법의 일종이다. 이 경우 데이터 불균형을 해결하면서 클래스 간 거리가 확보 되지만 여전히 정보 자체를 잃어버린다는 단점은 남는다.\n\n```python\nfrom imblearn.under_sampling import TomekLinks\n\ntomek = TomekLinks(random_state = 123)\n\nX_tm, y_tm = tomek.fit_sample(X, y)\n```\n\n## Random Over-Sampling\n---\n\nminor class의 데이터를 반복적으로 replace하는 것\n\n단순히 부트스트래핑을 통한 업샘플링의 변형이다.\n\n- **Advantages**\n  + no information loss\n- **Disadvantages**\n  + prone to overfitting due to copying same information \n\n```python\nX_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n\nplt.subplot(121)\nclassification_result2(X_imb, y_imb)\nplt.subplot(122)\nmodel_samp = classification_result2(X_samp, y_samp)\n\n\n```\n\n- 부트스트래핑을 직접 구현할 경우\n\n```python\n\ndef bootstrap(X, n = None, iterations = 1):\n    if n == None:\n        n = len(X)\n        X_resampled = np.random.choice(X, size = (iterations, n), replace = True)\n    return X_resampled\n\n```\n\n### SMOTE(Synthetic Minority Oversample Technique)\n\n\n임의의 마이너 클래스 데이터 포인트와 근접한 마이너 클래스 데이터 포인트 사이에 새로운 데이터 포인트를 생성하는 것\n\n**반드시 training set에 대해서만 SMOTE 시행. 이는 data leakage 문제와 관련이 있다.**\n\n$$syntetic = x_{minor} + u * (x_{nn}-x_{minor})$$\n\nsynthetic 합성 값은 minor class의 데이터 포인트와 근접한 minor class의 데이터포인트의 차이에 uniform distribution을 곱한 뒤 minor class의 데이터포인트를 더해준 값이다.\n\n\n<!--\n- Process\n  + Identify the feature vectore and its nearest neighbor\n  + take the the difference between the two\n  + multiply the difference with a random number between 0 and 1\n  + identify a new point on the line segment by adding the randomg number to feature vector\n  + repeat the process of identified feature vectors\n\n- 절차\n-->\n\n- numpy로 SMOTE 구현하기\n\n알고리즘을 구현하는 것 자체는 어렵지 않지만 실제로 작업을 할때는 `imblearn` 모듈에서 제공하는 SMOTE함수를 사용하는 것이 훨씬 낫다.\n\n```python\nimport random\nimport numpy as np\n\n# SMOTE\n\ndef euclidean_dist(x1,x2):\n    return np.sqrt(sum((x1-x2)**2))\n\n\ndef get_neighbors(X, x, k):\n  \"\"\"\n  minor 클레스 데이터에 대해서 k개의 nearest neighbor를 구한다\n  \"\"\"\n    X_len = len(X)\n    euclidean_dist = [euclidean_dist(X[i],x) for i in range(X_len)]\n    euclidean_dist = np.sort(euclidean_dist)\n    neighbors = euclidean_dist[:k]\n    \n    return neighbors\n\ndef SMOTE(X,k):\n  \"\"\"\n  smote algorithm 적용한 합성 데이터 생성\n  \"\"\"\n    X_len = len(X)\n    synthetic = []\n    for i in range(0,X_len):\n        w = 0\n        while w == 0:\n            w = np.random.uniform(0,1)\n        add = get_neighbors(X,X[i],k)\n        rand_idx = random.randint(0,k-1)\n        add = add[rand_idx]\n        \n        diff = X[i] - add\n        \n        synthetic.append(X[i] + w*diff)\n\n    return np.array(synthetic)\n\n```\n\n- imblearn을 활용한 target resampling\n\n\n```python\nfrom imblearn.over_sampling import SMOTE\n\nrs = SMOTE(random_state=123)\n\nX_new, y_new = rs.fit_sample(X, y)\n\n```\n\n### Borderline-SMOTE\n\n: major와 minor를 구분하는 경계선에 있는 Borderline에 속하는 데이터데 대해 SMOTE을 적용하는 것\n\nMinor class data X와 근접한 K개의 데이터포인트의 클래스의 수에 따라 SMOTE 적용 여부를 결정\n\n- 0 <= K' <= K/2 : Safe\n\n- K = K' : Noise\n\n- K/2 < K' < K : Danger : 이 경우에 SMOTE을 적용한다.\n\n```python\n# Borderline-SMOTE\n\nbsmote = BorderlineSMOTE(random_state = 1234, k_neighbors=3, m_neighbors=10)\nX, y_new = bsmote.fit_resample(X, y)\n\nprint('Original_y %s' % Counter(y))\nprint('BorderlineSMOTE_y %s' % Counter(y_new))\n```\n\n### ADASYN\n\n: Adaptive Synthetic Sampling\n\n- 가중치를 적용해 SMOTE을 다르게 진행\n- 인접한 major class의 비율에 따라 SMOTE을 다르게 적용하는 것\n\n```python\nX_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n\n```\n\n## 모델링과 평가 단계에서 Class Imbalance 다루기\n---\n\n샘플링 단계가 아니라 모델링과 평가단계에서 Class Imbalance 문제를 처리한다.\n\n### Change the performance metric\n\nclass weight에 영향을 덜 받게끔 평가지표 자체를 바꿀 수 있다.\n\n다른 방법보다 품이 덜 들어서 의외로 괜찮은 방법이다.\n\n\n- **Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n\n- **Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n\n- **Recall**: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n\n- **F1**: Score: the weighted average of precision and recall.\n\n- **Area Under ROC Curve (AUROC)**: AUROC represents the likelihood of your model distinguishing observations from two classes.\nIn other words, if you randomly select one observation from each class, what’s the probability that your model will be able to “rank” them correctly?\n\n\n### Penalize Algorithms(class_weight)\n\n- Cost-Sensitive Training\n- minority class로의 오분류에 대한 패널티를 크게 만듦\n\n```python\n# load library\nfrom sklearn.svm import SVC\n\n# class weight \nsvc_model = SVC(class_weight='balanced', probability=True)\n\nsvc_model.fit(x_train, y_train)\n\nsvc_predict = svc_model.predict(x_test)# check performance\nprint('ROCAUC score:',roc_auc_score(y_test, svc_predict))\nprint('Accuracy score:',accuracy_score(y_test, svc_predict))\nprint('F1 score:',f1_score(y_test, svc_predict))\n\n```\n- sklearn를 활용한 구현\n\n```python\n\n# Classweight  계산 \nfrom sklearn.utils.class_weight import compute_class_weight\nclasses = np.unique(y_train)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\nclass_weights = dict(zip(classes, weights)) # 모델의 인수로 들어간다.\n```\n\n- R 을 활용한 구현\n\n대출연체가 minor이기에 연제에 대한 가중치를 1/p로 적용.\np는 연체의 확률값\n\n\n```R\n# wt 가중치 벡터 만들기\nwt <- ifelse(loan_all_data$outcome == 'default',\n             1/mean(loan_all_data$outcome == 'default'),1)\n\nclf <- glm(outcome ~ payment_inc_ratio+purpose_+home_+emp_len,\n           data= loan_all_data,\n           weight =wt, family=\"binomial\")\n\n```\n\n### Novelty Detection(단일클래스 분류기법)\n\n- 단일클래스 분류기법\n- Minor를 무시하고 Major class 에 속하는 데이터를 결정하는 일종의 바운더리를 생성하고 그 바운더리에 들어가냐 들어가지 않냐의 boolen으로 클래스를 결정한다.\n- outlier 를 판별하는 알고리즘\n\n\n**Reference & annotation**\n---\n- https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n- **Class weight를 적용하는 방식이 minor를 oversampling하거나 major를 undersampling하는 방법을 대체할 수 있다.(Practical Statistics for Data Scientist)**\n- https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n","source":"_posts/Preprocessing-sampling-imbalance-data.md","raw":"---\ntitle: '[Sampling]Class Imbalance 다루기'\ncategories:\n  - Preprocessing\ndate:\nupdated:\ntags: \n  - Sampling\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n-->\n\n# Dealing with Class Imbalance(클래스 불균형 다루기)\n---\n\n<!--\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n\n-->\n\n**여기서 해결하는 문제**\n\n- Biased predictions\n- Misleading accuracy\n\n**보통 고려하는 해결방법**\n\n- 데이터 합성(Synthesisis of new minority class instances)\n- Over-sampling \n- Under-sampling \n- class weight 조정하기(상향/하향가중치 적용)\n- cost function 조정\n\n## Random Under-Sampling\n---\n\n- **Advantages**\n  + It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n\n- **Disadvantages**\n  + It can discard potentially useful information which could be important for building rule classifiers.\n  + The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.\n\n### Tomeck Links\n\nTomek Links란 두 샘플 사이에 다른 관측치가 없는 경우를 말한다.\n\n![](https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg)\n\nTomek Links 방법은 Tomeck links 중에 major에 속하는 데이터포인트를 제거하는 undersampling 기법의 일종이다. 이 경우 데이터 불균형을 해결하면서 클래스 간 거리가 확보 되지만 여전히 정보 자체를 잃어버린다는 단점은 남는다.\n\n```python\nfrom imblearn.under_sampling import TomekLinks\n\ntomek = TomekLinks(random_state = 123)\n\nX_tm, y_tm = tomek.fit_sample(X, y)\n```\n\n## Random Over-Sampling\n---\n\nminor class의 데이터를 반복적으로 replace하는 것\n\n단순히 부트스트래핑을 통한 업샘플링의 변형이다.\n\n- **Advantages**\n  + no information loss\n- **Disadvantages**\n  + prone to overfitting due to copying same information \n\n```python\nX_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n\nplt.subplot(121)\nclassification_result2(X_imb, y_imb)\nplt.subplot(122)\nmodel_samp = classification_result2(X_samp, y_samp)\n\n\n```\n\n- 부트스트래핑을 직접 구현할 경우\n\n```python\n\ndef bootstrap(X, n = None, iterations = 1):\n    if n == None:\n        n = len(X)\n        X_resampled = np.random.choice(X, size = (iterations, n), replace = True)\n    return X_resampled\n\n```\n\n### SMOTE(Synthetic Minority Oversample Technique)\n\n\n임의의 마이너 클래스 데이터 포인트와 근접한 마이너 클래스 데이터 포인트 사이에 새로운 데이터 포인트를 생성하는 것\n\n**반드시 training set에 대해서만 SMOTE 시행. 이는 data leakage 문제와 관련이 있다.**\n\n$$syntetic = x_{minor} + u * (x_{nn}-x_{minor})$$\n\nsynthetic 합성 값은 minor class의 데이터 포인트와 근접한 minor class의 데이터포인트의 차이에 uniform distribution을 곱한 뒤 minor class의 데이터포인트를 더해준 값이다.\n\n\n<!--\n- Process\n  + Identify the feature vectore and its nearest neighbor\n  + take the the difference between the two\n  + multiply the difference with a random number between 0 and 1\n  + identify a new point on the line segment by adding the randomg number to feature vector\n  + repeat the process of identified feature vectors\n\n- 절차\n-->\n\n- numpy로 SMOTE 구현하기\n\n알고리즘을 구현하는 것 자체는 어렵지 않지만 실제로 작업을 할때는 `imblearn` 모듈에서 제공하는 SMOTE함수를 사용하는 것이 훨씬 낫다.\n\n```python\nimport random\nimport numpy as np\n\n# SMOTE\n\ndef euclidean_dist(x1,x2):\n    return np.sqrt(sum((x1-x2)**2))\n\n\ndef get_neighbors(X, x, k):\n  \"\"\"\n  minor 클레스 데이터에 대해서 k개의 nearest neighbor를 구한다\n  \"\"\"\n    X_len = len(X)\n    euclidean_dist = [euclidean_dist(X[i],x) for i in range(X_len)]\n    euclidean_dist = np.sort(euclidean_dist)\n    neighbors = euclidean_dist[:k]\n    \n    return neighbors\n\ndef SMOTE(X,k):\n  \"\"\"\n  smote algorithm 적용한 합성 데이터 생성\n  \"\"\"\n    X_len = len(X)\n    synthetic = []\n    for i in range(0,X_len):\n        w = 0\n        while w == 0:\n            w = np.random.uniform(0,1)\n        add = get_neighbors(X,X[i],k)\n        rand_idx = random.randint(0,k-1)\n        add = add[rand_idx]\n        \n        diff = X[i] - add\n        \n        synthetic.append(X[i] + w*diff)\n\n    return np.array(synthetic)\n\n```\n\n- imblearn을 활용한 target resampling\n\n\n```python\nfrom imblearn.over_sampling import SMOTE\n\nrs = SMOTE(random_state=123)\n\nX_new, y_new = rs.fit_sample(X, y)\n\n```\n\n### Borderline-SMOTE\n\n: major와 minor를 구분하는 경계선에 있는 Borderline에 속하는 데이터데 대해 SMOTE을 적용하는 것\n\nMinor class data X와 근접한 K개의 데이터포인트의 클래스의 수에 따라 SMOTE 적용 여부를 결정\n\n- 0 <= K' <= K/2 : Safe\n\n- K = K' : Noise\n\n- K/2 < K' < K : Danger : 이 경우에 SMOTE을 적용한다.\n\n```python\n# Borderline-SMOTE\n\nbsmote = BorderlineSMOTE(random_state = 1234, k_neighbors=3, m_neighbors=10)\nX, y_new = bsmote.fit_resample(X, y)\n\nprint('Original_y %s' % Counter(y))\nprint('BorderlineSMOTE_y %s' % Counter(y_new))\n```\n\n### ADASYN\n\n: Adaptive Synthetic Sampling\n\n- 가중치를 적용해 SMOTE을 다르게 진행\n- 인접한 major class의 비율에 따라 SMOTE을 다르게 적용하는 것\n\n```python\nX_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n\n```\n\n## 모델링과 평가 단계에서 Class Imbalance 다루기\n---\n\n샘플링 단계가 아니라 모델링과 평가단계에서 Class Imbalance 문제를 처리한다.\n\n### Change the performance metric\n\nclass weight에 영향을 덜 받게끔 평가지표 자체를 바꿀 수 있다.\n\n다른 방법보다 품이 덜 들어서 의외로 괜찮은 방법이다.\n\n\n- **Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n\n- **Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n\n- **Recall**: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n\n- **F1**: Score: the weighted average of precision and recall.\n\n- **Area Under ROC Curve (AUROC)**: AUROC represents the likelihood of your model distinguishing observations from two classes.\nIn other words, if you randomly select one observation from each class, what’s the probability that your model will be able to “rank” them correctly?\n\n\n### Penalize Algorithms(class_weight)\n\n- Cost-Sensitive Training\n- minority class로의 오분류에 대한 패널티를 크게 만듦\n\n```python\n# load library\nfrom sklearn.svm import SVC\n\n# class weight \nsvc_model = SVC(class_weight='balanced', probability=True)\n\nsvc_model.fit(x_train, y_train)\n\nsvc_predict = svc_model.predict(x_test)# check performance\nprint('ROCAUC score:',roc_auc_score(y_test, svc_predict))\nprint('Accuracy score:',accuracy_score(y_test, svc_predict))\nprint('F1 score:',f1_score(y_test, svc_predict))\n\n```\n- sklearn를 활용한 구현\n\n```python\n\n# Classweight  계산 \nfrom sklearn.utils.class_weight import compute_class_weight\nclasses = np.unique(y_train)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\nclass_weights = dict(zip(classes, weights)) # 모델의 인수로 들어간다.\n```\n\n- R 을 활용한 구현\n\n대출연체가 minor이기에 연제에 대한 가중치를 1/p로 적용.\np는 연체의 확률값\n\n\n```R\n# wt 가중치 벡터 만들기\nwt <- ifelse(loan_all_data$outcome == 'default',\n             1/mean(loan_all_data$outcome == 'default'),1)\n\nclf <- glm(outcome ~ payment_inc_ratio+purpose_+home_+emp_len,\n           data= loan_all_data,\n           weight =wt, family=\"binomial\")\n\n```\n\n### Novelty Detection(단일클래스 분류기법)\n\n- 단일클래스 분류기법\n- Minor를 무시하고 Major class 에 속하는 데이터를 결정하는 일종의 바운더리를 생성하고 그 바운더리에 들어가냐 들어가지 않냐의 boolen으로 클래스를 결정한다.\n- outlier 를 판별하는 알고리즘\n\n\n**Reference & annotation**\n---\n- https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n- **Class weight를 적용하는 방식이 minor를 oversampling하거나 major를 undersampling하는 방법을 대체할 수 있다.(Practical Statistics for Data Scientist)**\n- https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n","slug":"Preprocessing-sampling-imbalance-data","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsci002mb36q4u0c2ibj","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n-->\n\n<h1 id=\"Dealing-with-Class-Imbalance-클래스-불균형-다루기\"><a href=\"#Dealing-with-Class-Imbalance-클래스-불균형-다루기\" class=\"headerlink\" title=\"Dealing with Class Imbalance(클래스 불균형 다루기)\"></a>Dealing with Class Imbalance(클래스 불균형 다루기)</h1><hr>\n<!--\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n\n-->\n\n<p><strong>여기서 해결하는 문제</strong></p>\n<ul>\n<li>Biased predictions</li>\n<li>Misleading accuracy</li>\n</ul>\n<p><strong>보통 고려하는 해결방법</strong></p>\n<ul>\n<li>데이터 합성(Synthesisis of new minority class instances)</li>\n<li>Over-sampling </li>\n<li>Under-sampling </li>\n<li>class weight 조정하기(상향&#x2F;하향가중치 적용)</li>\n<li>cost function 조정</li>\n</ul>\n<h2 id=\"Random-Under-Sampling\"><a href=\"#Random-Under-Sampling\" class=\"headerlink\" title=\"Random Under-Sampling\"></a>Random Under-Sampling</h2><hr>\n<ul>\n<li><p><strong>Advantages</strong></p>\n<ul>\n<li>It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.</li>\n</ul>\n</li>\n<li><p><strong>Disadvantages</strong></p>\n<ul>\n<li>It can discard potentially useful information which could be important for building rule classifiers.</li>\n<li>The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Tomeck-Links\"><a href=\"#Tomeck-Links\" class=\"headerlink\" title=\"Tomeck Links\"></a>Tomeck Links</h3><p>Tomek Links란 두 샘플 사이에 다른 관측치가 없는 경우를 말한다.</p>\n<p><img src=\"https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg\"></p>\n<p>Tomek Links 방법은 Tomeck links 중에 major에 속하는 데이터포인트를 제거하는 undersampling 기법의 일종이다. 이 경우 데이터 불균형을 해결하면서 클래스 간 거리가 확보 되지만 여전히 정보 자체를 잃어버린다는 단점은 남는다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> imblearn.under_sampling <span class=\"keyword\">import</span> TomekLinks</span><br><span class=\"line\"></span><br><span class=\"line\">tomek = TomekLinks(random_state = <span class=\"number\">123</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X_tm, y_tm = tomek.fit_sample(X, y)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Random-Over-Sampling\"><a href=\"#Random-Over-Sampling\" class=\"headerlink\" title=\"Random Over-Sampling\"></a>Random Over-Sampling</h2><hr>\n<p>minor class의 데이터를 반복적으로 replace하는 것</p>\n<p>단순히 부트스트래핑을 통한 업샘플링의 변형이다.</p>\n<ul>\n<li><strong>Advantages</strong><ul>\n<li>no information loss</li>\n</ul>\n</li>\n<li><strong>Disadvantages</strong><ul>\n<li>prone to overfitting due to copying same information</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_samp, y_samp = RandomOverSampler(random_state=<span class=\"number\">0</span>).fit_sample(X_imb, y_imb)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">121</span>)</span><br><span class=\"line\">classification_result2(X_imb, y_imb)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">122</span>)</span><br><span class=\"line\">model_samp = classification_result2(X_samp, y_samp)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>부트스트래핑을 직접 구현할 경우</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bootstrap</span>(<span class=\"params\">X, n = <span class=\"literal\">None</span>, iterations = <span class=\"number\">1</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"literal\">None</span>:</span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">        X_resampled = np.random.choice(X, size = (iterations, n), replace = <span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_resampled</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SMOTE-Synthetic-Minority-Oversample-Technique\"><a href=\"#SMOTE-Synthetic-Minority-Oversample-Technique\" class=\"headerlink\" title=\"SMOTE(Synthetic Minority Oversample Technique)\"></a>SMOTE(Synthetic Minority Oversample Technique)</h3><p>임의의 마이너 클래스 데이터 포인트와 근접한 마이너 클래스 데이터 포인트 사이에 새로운 데이터 포인트를 생성하는 것</p>\n<p><strong>반드시 training set에 대해서만 SMOTE 시행. 이는 data leakage 문제와 관련이 있다.</strong></p>\n<p>$$syntetic &#x3D; x_{minor} + u * (x_{nn}-x_{minor})$$</p>\n<p>synthetic 합성 값은 minor class의 데이터 포인트와 근접한 minor class의 데이터포인트의 차이에 uniform distribution을 곱한 뒤 minor class의 데이터포인트를 더해준 값이다.</p>\n<!--\n- Process\n  + Identify the feature vectore and its nearest neighbor\n  + take the the difference between the two\n  + multiply the difference with a random number between 0 and 1\n  + identify a new point on the line segment by adding the randomg number to feature vector\n  + repeat the process of identified feature vectors\n\n- 절차\n-->\n\n<ul>\n<li>numpy로 SMOTE 구현하기</li>\n</ul>\n<p>알고리즘을 구현하는 것 자체는 어렵지 않지만 실제로 작업을 할때는 <code>imblearn</code> 모듈에서 제공하는 SMOTE함수를 사용하는 것이 훨씬 낫다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># SMOTE</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">euclidean_dist</span>(<span class=\"params\">x1,x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(<span class=\"built_in\">sum</span>((x1-x2)**<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_neighbors</span>(<span class=\"params\">X, x, k</span>):</span><br><span class=\"line\">  <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  minor 클레스 데이터에 대해서 k개의 nearest neighbor를 구한다</span></span><br><span class=\"line\"><span class=\"string\">  &quot;&quot;&quot;</span></span><br><span class=\"line\">    X_len = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">    euclidean_dist = [euclidean_dist(X[i],x) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(X_len)]</span><br><span class=\"line\">    euclidean_dist = np.sort(euclidean_dist)</span><br><span class=\"line\">    neighbors = euclidean_dist[:k]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> neighbors</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">SMOTE</span>(<span class=\"params\">X,k</span>):</span><br><span class=\"line\">  <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  smote algorithm 적용한 합성 데이터 생성</span></span><br><span class=\"line\"><span class=\"string\">  &quot;&quot;&quot;</span></span><br><span class=\"line\">    X_len = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">    synthetic = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>,X_len):</span><br><span class=\"line\">        w = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> w == <span class=\"number\">0</span>:</span><br><span class=\"line\">            w = np.random.uniform(<span class=\"number\">0</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">        add = get_neighbors(X,X[i],k)</span><br><span class=\"line\">        rand_idx = random.randint(<span class=\"number\">0</span>,k-<span class=\"number\">1</span>)</span><br><span class=\"line\">        add = add[rand_idx]</span><br><span class=\"line\">        </span><br><span class=\"line\">        diff = X[i] - add</span><br><span class=\"line\">        </span><br><span class=\"line\">        synthetic.append(X[i] + w*diff)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.array(synthetic)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>imblearn을 활용한 target resampling</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> imblearn.over_sampling <span class=\"keyword\">import</span> SMOTE</span><br><span class=\"line\"></span><br><span class=\"line\">rs = SMOTE(random_state=<span class=\"number\">123</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X_new, y_new = rs.fit_sample(X, y)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Borderline-SMOTE\"><a href=\"#Borderline-SMOTE\" class=\"headerlink\" title=\"Borderline-SMOTE\"></a>Borderline-SMOTE</h3><p>: major와 minor를 구분하는 경계선에 있는 Borderline에 속하는 데이터데 대해 SMOTE을 적용하는 것</p>\n<p>Minor class data X와 근접한 K개의 데이터포인트의 클래스의 수에 따라 SMOTE 적용 여부를 결정</p>\n<ul>\n<li><p>0 &lt;&#x3D; K’ &lt;&#x3D; K&#x2F;2 : Safe</p>\n</li>\n<li><p>K &#x3D; K’ : Noise</p>\n</li>\n<li><p>K&#x2F;2 &lt; K’ &lt; K : Danger : 이 경우에 SMOTE을 적용한다.</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Borderline-SMOTE</span></span><br><span class=\"line\"></span><br><span class=\"line\">bsmote = BorderlineSMOTE(random_state = <span class=\"number\">1234</span>, k_neighbors=<span class=\"number\">3</span>, m_neighbors=<span class=\"number\">10</span>)</span><br><span class=\"line\">X, y_new = bsmote.fit_resample(X, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Original_y %s&#x27;</span> % Counter(y))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;BorderlineSMOTE_y %s&#x27;</span> % Counter(y_new))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ADASYN\"><a href=\"#ADASYN\" class=\"headerlink\" title=\"ADASYN\"></a>ADASYN</h3><p>: Adaptive Synthetic Sampling</p>\n<ul>\n<li>가중치를 적용해 SMOTE을 다르게 진행</li>\n<li>인접한 major class의 비율에 따라 SMOTE을 다르게 적용하는 것</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_samp, y_samp = ADASYN(random_state=<span class=\"number\">0</span>).fit_sample(X_imb, y_imb)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"모델링과-평가-단계에서-Class-Imbalance-다루기\"><a href=\"#모델링과-평가-단계에서-Class-Imbalance-다루기\" class=\"headerlink\" title=\"모델링과 평가 단계에서 Class Imbalance 다루기\"></a>모델링과 평가 단계에서 Class Imbalance 다루기</h2><hr>\n<p>샘플링 단계가 아니라 모델링과 평가단계에서 Class Imbalance 문제를 처리한다.</p>\n<h3 id=\"Change-the-performance-metric\"><a href=\"#Change-the-performance-metric\" class=\"headerlink\" title=\"Change the performance metric\"></a>Change the performance metric</h3><p>class weight에 영향을 덜 받게끔 평가지표 자체를 바꿀 수 있다.</p>\n<p>다른 방법보다 품이 덜 들어서 의외로 괜찮은 방법이다.</p>\n<ul>\n<li><p><strong>Confusion Matrix</strong>: a table showing correct predictions and types of incorrect predictions.</p>\n</li>\n<li><p><strong>Precision</strong>: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.</p>\n</li>\n<li><p><strong>Recall</strong>: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.</p>\n</li>\n<li><p><strong>F1</strong>: Score: the weighted average of precision and recall.</p>\n</li>\n<li><p><strong>Area Under ROC Curve (AUROC)</strong>: AUROC represents the likelihood of your model distinguishing observations from two classes.<br>In other words, if you randomly select one observation from each class, what’s the probability that your model will be able to “rank” them correctly?</p>\n</li>\n</ul>\n<h3 id=\"Penalize-Algorithms-class-weight\"><a href=\"#Penalize-Algorithms-class-weight\" class=\"headerlink\" title=\"Penalize Algorithms(class_weight)\"></a>Penalize Algorithms(class_weight)</h3><ul>\n<li>Cost-Sensitive Training</li>\n<li>minority class로의 오분류에 대한 패널티를 크게 만듦</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># load library</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># class weight </span></span><br><span class=\"line\">svc_model = SVC(class_weight=<span class=\"string\">&#x27;balanced&#x27;</span>, probability=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">svc_model.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">svc_predict = svc_model.predict(x_test)<span class=\"comment\"># check performance</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ROCAUC score:&#x27;</span>,roc_auc_score(y_test, svc_predict))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy score:&#x27;</span>,accuracy_score(y_test, svc_predict))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;F1 score:&#x27;</span>,f1_score(y_test, svc_predict))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>sklearn를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Classweight  계산 </span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.utils.class_weight <span class=\"keyword\">import</span> compute_class_weight</span><br><span class=\"line\">classes = np.unique(y_train)</span><br><span class=\"line\">weights = compute_class_weight(class_weight=<span class=\"string\">&#x27;balanced&#x27;</span>, classes=classes, y=y_train)</span><br><span class=\"line\">class_weights = <span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(classes, weights)) <span class=\"comment\"># 모델의 인수로 들어간다.</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>R 을 활용한 구현</li>\n</ul>\n<p>대출연체가 minor이기에 연제에 대한 가중치를 1&#x2F;p로 적용.<br>p는 연체의 확률값</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># wt 가중치 벡터 만들기</span></span><br><span class=\"line\">wt <span class=\"operator\">&lt;-</span> ifelse<span class=\"punctuation\">(</span>loan_all_data<span class=\"operator\">$</span>outcome <span class=\"operator\">==</span> <span class=\"string\">&#x27;default&#x27;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">             <span class=\"number\">1</span><span class=\"operator\">/</span>mean<span class=\"punctuation\">(</span>loan_all_data<span class=\"operator\">$</span>outcome <span class=\"operator\">==</span> <span class=\"string\">&#x27;default&#x27;</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf <span class=\"operator\">&lt;-</span> glm<span class=\"punctuation\">(</span>outcome <span class=\"operator\">~</span> payment_inc_ratio<span class=\"operator\">+</span>purpose_<span class=\"operator\">+</span>home_<span class=\"operator\">+</span>emp_len<span class=\"punctuation\">,</span></span><br><span class=\"line\">           data<span class=\"operator\">=</span> loan_all_data<span class=\"punctuation\">,</span></span><br><span class=\"line\">           weight <span class=\"operator\">=</span>wt<span class=\"punctuation\">,</span> family<span class=\"operator\">=</span><span class=\"string\">&quot;binomial&quot;</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Novelty-Detection-단일클래스-분류기법\"><a href=\"#Novelty-Detection-단일클래스-분류기법\" class=\"headerlink\" title=\"Novelty Detection(단일클래스 분류기법)\"></a>Novelty Detection(단일클래스 분류기법)</h3><ul>\n<li>단일클래스 분류기법</li>\n<li>Minor를 무시하고 Major class 에 속하는 데이터를 결정하는 일종의 바운더리를 생성하고 그 바운더리에 들어가냐 들어가지 않냐의 boolen으로 클래스를 결정한다.</li>\n<li>outlier 를 판별하는 알고리즘</li>\n</ul>\n<h2 id=\"Reference-amp-annotation\"><a href=\"#Reference-amp-annotation\" class=\"headerlink\" title=\"Reference &amp; annotation\"></a><strong>Reference &amp; annotation</strong></h2><ul>\n<li><a href=\"https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\">https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18</a></li>\n<li><strong>Class weight를 적용하는 방식이 minor를 oversampling하거나 major를 undersampling하는 방법을 대체할 수 있다.(Practical Statistics for Data Scientist)</strong></li>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\">https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n-->\n\n<h1 id=\"Dealing-with-Class-Imbalance-클래스-불균형-다루기\"><a href=\"#Dealing-with-Class-Imbalance-클래스-불균형-다루기\" class=\"headerlink\" title=\"Dealing with Class Imbalance(클래스 불균형 다루기)\"></a>Dealing with Class Imbalance(클래스 불균형 다루기)</h1><hr>\n<!--\n오버 샘플링 렉카\nhttps://wyatt37.tistory.com/10\n\n-->\n\n<p><strong>여기서 해결하는 문제</strong></p>\n<ul>\n<li>Biased predictions</li>\n<li>Misleading accuracy</li>\n</ul>\n<p><strong>보통 고려하는 해결방법</strong></p>\n<ul>\n<li>데이터 합성(Synthesisis of new minority class instances)</li>\n<li>Over-sampling </li>\n<li>Under-sampling </li>\n<li>class weight 조정하기(상향&#x2F;하향가중치 적용)</li>\n<li>cost function 조정</li>\n</ul>\n<h2 id=\"Random-Under-Sampling\"><a href=\"#Random-Under-Sampling\" class=\"headerlink\" title=\"Random Under-Sampling\"></a>Random Under-Sampling</h2><hr>\n<ul>\n<li><p><strong>Advantages</strong></p>\n<ul>\n<li>It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.</li>\n</ul>\n</li>\n<li><p><strong>Disadvantages</strong></p>\n<ul>\n<li>It can discard potentially useful information which could be important for building rule classifiers.</li>\n<li>The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Tomeck-Links\"><a href=\"#Tomeck-Links\" class=\"headerlink\" title=\"Tomeck Links\"></a>Tomeck Links</h3><p>Tomek Links란 두 샘플 사이에 다른 관측치가 없는 경우를 말한다.</p>\n<p><img src=\"https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg\"></p>\n<p>Tomek Links 방법은 Tomeck links 중에 major에 속하는 데이터포인트를 제거하는 undersampling 기법의 일종이다. 이 경우 데이터 불균형을 해결하면서 클래스 간 거리가 확보 되지만 여전히 정보 자체를 잃어버린다는 단점은 남는다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> imblearn.under_sampling <span class=\"keyword\">import</span> TomekLinks</span><br><span class=\"line\"></span><br><span class=\"line\">tomek = TomekLinks(random_state = <span class=\"number\">123</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X_tm, y_tm = tomek.fit_sample(X, y)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Random-Over-Sampling\"><a href=\"#Random-Over-Sampling\" class=\"headerlink\" title=\"Random Over-Sampling\"></a>Random Over-Sampling</h2><hr>\n<p>minor class의 데이터를 반복적으로 replace하는 것</p>\n<p>단순히 부트스트래핑을 통한 업샘플링의 변형이다.</p>\n<ul>\n<li><strong>Advantages</strong><ul>\n<li>no information loss</li>\n</ul>\n</li>\n<li><strong>Disadvantages</strong><ul>\n<li>prone to overfitting due to copying same information</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_samp, y_samp = RandomOverSampler(random_state=<span class=\"number\">0</span>).fit_sample(X_imb, y_imb)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">121</span>)</span><br><span class=\"line\">classification_result2(X_imb, y_imb)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">122</span>)</span><br><span class=\"line\">model_samp = classification_result2(X_samp, y_samp)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>부트스트래핑을 직접 구현할 경우</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bootstrap</span>(<span class=\"params\">X, n = <span class=\"literal\">None</span>, iterations = <span class=\"number\">1</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"literal\">None</span>:</span><br><span class=\"line\">        n = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">        X_resampled = np.random.choice(X, size = (iterations, n), replace = <span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_resampled</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"SMOTE-Synthetic-Minority-Oversample-Technique\"><a href=\"#SMOTE-Synthetic-Minority-Oversample-Technique\" class=\"headerlink\" title=\"SMOTE(Synthetic Minority Oversample Technique)\"></a>SMOTE(Synthetic Minority Oversample Technique)</h3><p>임의의 마이너 클래스 데이터 포인트와 근접한 마이너 클래스 데이터 포인트 사이에 새로운 데이터 포인트를 생성하는 것</p>\n<p><strong>반드시 training set에 대해서만 SMOTE 시행. 이는 data leakage 문제와 관련이 있다.</strong></p>\n<p>$$syntetic &#x3D; x_{minor} + u * (x_{nn}-x_{minor})$$</p>\n<p>synthetic 합성 값은 minor class의 데이터 포인트와 근접한 minor class의 데이터포인트의 차이에 uniform distribution을 곱한 뒤 minor class의 데이터포인트를 더해준 값이다.</p>\n<!--\n- Process\n  + Identify the feature vectore and its nearest neighbor\n  + take the the difference between the two\n  + multiply the difference with a random number between 0 and 1\n  + identify a new point on the line segment by adding the randomg number to feature vector\n  + repeat the process of identified feature vectors\n\n- 절차\n-->\n\n<ul>\n<li>numpy로 SMOTE 구현하기</li>\n</ul>\n<p>알고리즘을 구현하는 것 자체는 어렵지 않지만 실제로 작업을 할때는 <code>imblearn</code> 모듈에서 제공하는 SMOTE함수를 사용하는 것이 훨씬 낫다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># SMOTE</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">euclidean_dist</span>(<span class=\"params\">x1,x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.sqrt(<span class=\"built_in\">sum</span>((x1-x2)**<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_neighbors</span>(<span class=\"params\">X, x, k</span>):</span><br><span class=\"line\">  <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  minor 클레스 데이터에 대해서 k개의 nearest neighbor를 구한다</span></span><br><span class=\"line\"><span class=\"string\">  &quot;&quot;&quot;</span></span><br><span class=\"line\">    X_len = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">    euclidean_dist = [euclidean_dist(X[i],x) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(X_len)]</span><br><span class=\"line\">    euclidean_dist = np.sort(euclidean_dist)</span><br><span class=\"line\">    neighbors = euclidean_dist[:k]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> neighbors</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">SMOTE</span>(<span class=\"params\">X,k</span>):</span><br><span class=\"line\">  <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">  smote algorithm 적용한 합성 데이터 생성</span></span><br><span class=\"line\"><span class=\"string\">  &quot;&quot;&quot;</span></span><br><span class=\"line\">    X_len = <span class=\"built_in\">len</span>(X)</span><br><span class=\"line\">    synthetic = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>,X_len):</span><br><span class=\"line\">        w = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> w == <span class=\"number\">0</span>:</span><br><span class=\"line\">            w = np.random.uniform(<span class=\"number\">0</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">        add = get_neighbors(X,X[i],k)</span><br><span class=\"line\">        rand_idx = random.randint(<span class=\"number\">0</span>,k-<span class=\"number\">1</span>)</span><br><span class=\"line\">        add = add[rand_idx]</span><br><span class=\"line\">        </span><br><span class=\"line\">        diff = X[i] - add</span><br><span class=\"line\">        </span><br><span class=\"line\">        synthetic.append(X[i] + w*diff)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.array(synthetic)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>imblearn을 활용한 target resampling</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> imblearn.over_sampling <span class=\"keyword\">import</span> SMOTE</span><br><span class=\"line\"></span><br><span class=\"line\">rs = SMOTE(random_state=<span class=\"number\">123</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X_new, y_new = rs.fit_sample(X, y)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Borderline-SMOTE\"><a href=\"#Borderline-SMOTE\" class=\"headerlink\" title=\"Borderline-SMOTE\"></a>Borderline-SMOTE</h3><p>: major와 minor를 구분하는 경계선에 있는 Borderline에 속하는 데이터데 대해 SMOTE을 적용하는 것</p>\n<p>Minor class data X와 근접한 K개의 데이터포인트의 클래스의 수에 따라 SMOTE 적용 여부를 결정</p>\n<ul>\n<li><p>0 &lt;&#x3D; K’ &lt;&#x3D; K&#x2F;2 : Safe</p>\n</li>\n<li><p>K &#x3D; K’ : Noise</p>\n</li>\n<li><p>K&#x2F;2 &lt; K’ &lt; K : Danger : 이 경우에 SMOTE을 적용한다.</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Borderline-SMOTE</span></span><br><span class=\"line\"></span><br><span class=\"line\">bsmote = BorderlineSMOTE(random_state = <span class=\"number\">1234</span>, k_neighbors=<span class=\"number\">3</span>, m_neighbors=<span class=\"number\">10</span>)</span><br><span class=\"line\">X, y_new = bsmote.fit_resample(X, y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Original_y %s&#x27;</span> % Counter(y))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;BorderlineSMOTE_y %s&#x27;</span> % Counter(y_new))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ADASYN\"><a href=\"#ADASYN\" class=\"headerlink\" title=\"ADASYN\"></a>ADASYN</h3><p>: Adaptive Synthetic Sampling</p>\n<ul>\n<li>가중치를 적용해 SMOTE을 다르게 진행</li>\n<li>인접한 major class의 비율에 따라 SMOTE을 다르게 적용하는 것</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X_samp, y_samp = ADASYN(random_state=<span class=\"number\">0</span>).fit_sample(X_imb, y_imb)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"모델링과-평가-단계에서-Class-Imbalance-다루기\"><a href=\"#모델링과-평가-단계에서-Class-Imbalance-다루기\" class=\"headerlink\" title=\"모델링과 평가 단계에서 Class Imbalance 다루기\"></a>모델링과 평가 단계에서 Class Imbalance 다루기</h2><hr>\n<p>샘플링 단계가 아니라 모델링과 평가단계에서 Class Imbalance 문제를 처리한다.</p>\n<h3 id=\"Change-the-performance-metric\"><a href=\"#Change-the-performance-metric\" class=\"headerlink\" title=\"Change the performance metric\"></a>Change the performance metric</h3><p>class weight에 영향을 덜 받게끔 평가지표 자체를 바꿀 수 있다.</p>\n<p>다른 방법보다 품이 덜 들어서 의외로 괜찮은 방법이다.</p>\n<ul>\n<li><p><strong>Confusion Matrix</strong>: a table showing correct predictions and types of incorrect predictions.</p>\n</li>\n<li><p><strong>Precision</strong>: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.</p>\n</li>\n<li><p><strong>Recall</strong>: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.</p>\n</li>\n<li><p><strong>F1</strong>: Score: the weighted average of precision and recall.</p>\n</li>\n<li><p><strong>Area Under ROC Curve (AUROC)</strong>: AUROC represents the likelihood of your model distinguishing observations from two classes.<br>In other words, if you randomly select one observation from each class, what’s the probability that your model will be able to “rank” them correctly?</p>\n</li>\n</ul>\n<h3 id=\"Penalize-Algorithms-class-weight\"><a href=\"#Penalize-Algorithms-class-weight\" class=\"headerlink\" title=\"Penalize Algorithms(class_weight)\"></a>Penalize Algorithms(class_weight)</h3><ul>\n<li>Cost-Sensitive Training</li>\n<li>minority class로의 오분류에 대한 패널티를 크게 만듦</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># load library</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># class weight </span></span><br><span class=\"line\">svc_model = SVC(class_weight=<span class=\"string\">&#x27;balanced&#x27;</span>, probability=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">svc_model.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">svc_predict = svc_model.predict(x_test)<span class=\"comment\"># check performance</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;ROCAUC score:&#x27;</span>,roc_auc_score(y_test, svc_predict))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy score:&#x27;</span>,accuracy_score(y_test, svc_predict))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;F1 score:&#x27;</span>,f1_score(y_test, svc_predict))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>sklearn를 활용한 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Classweight  계산 </span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.utils.class_weight <span class=\"keyword\">import</span> compute_class_weight</span><br><span class=\"line\">classes = np.unique(y_train)</span><br><span class=\"line\">weights = compute_class_weight(class_weight=<span class=\"string\">&#x27;balanced&#x27;</span>, classes=classes, y=y_train)</span><br><span class=\"line\">class_weights = <span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(classes, weights)) <span class=\"comment\"># 모델의 인수로 들어간다.</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>R 을 활용한 구현</li>\n</ul>\n<p>대출연체가 minor이기에 연제에 대한 가중치를 1&#x2F;p로 적용.<br>p는 연체의 확률값</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># wt 가중치 벡터 만들기</span></span><br><span class=\"line\">wt <span class=\"operator\">&lt;-</span> ifelse<span class=\"punctuation\">(</span>loan_all_data<span class=\"operator\">$</span>outcome <span class=\"operator\">==</span> <span class=\"string\">&#x27;default&#x27;</span><span class=\"punctuation\">,</span></span><br><span class=\"line\">             <span class=\"number\">1</span><span class=\"operator\">/</span>mean<span class=\"punctuation\">(</span>loan_all_data<span class=\"operator\">$</span>outcome <span class=\"operator\">==</span> <span class=\"string\">&#x27;default&#x27;</span><span class=\"punctuation\">)</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">clf <span class=\"operator\">&lt;-</span> glm<span class=\"punctuation\">(</span>outcome <span class=\"operator\">~</span> payment_inc_ratio<span class=\"operator\">+</span>purpose_<span class=\"operator\">+</span>home_<span class=\"operator\">+</span>emp_len<span class=\"punctuation\">,</span></span><br><span class=\"line\">           data<span class=\"operator\">=</span> loan_all_data<span class=\"punctuation\">,</span></span><br><span class=\"line\">           weight <span class=\"operator\">=</span>wt<span class=\"punctuation\">,</span> family<span class=\"operator\">=</span><span class=\"string\">&quot;binomial&quot;</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Novelty-Detection-단일클래스-분류기법\"><a href=\"#Novelty-Detection-단일클래스-분류기법\" class=\"headerlink\" title=\"Novelty Detection(단일클래스 분류기법)\"></a>Novelty Detection(단일클래스 분류기법)</h3><ul>\n<li>단일클래스 분류기법</li>\n<li>Minor를 무시하고 Major class 에 속하는 데이터를 결정하는 일종의 바운더리를 생성하고 그 바운더리에 들어가냐 들어가지 않냐의 boolen으로 클래스를 결정한다.</li>\n<li>outlier 를 판별하는 알고리즘</li>\n</ul>\n<h2 id=\"Reference-amp-annotation\"><a href=\"#Reference-amp-annotation\" class=\"headerlink\" title=\"Reference &amp; annotation\"></a><strong>Reference &amp; annotation</strong></h2><ul>\n<li><a href=\"https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\">https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18</a></li>\n<li><strong>Class weight를 적용하는 방식이 minor를 oversampling하거나 major를 undersampling하는 방법을 대체할 수 있다.(Practical Statistics for Data Scientist)</strong></li>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\">https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Sampling]Class Imbalance 다루기","path":"2022/06/13/Preprocessing-sampling-imbalance-data/","eyeCatchImage":"https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Preprocessing","tags":["Sampling"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]Iterator,Generator,yield에 대한 정리","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**Iterator,Generator,yield에 대한 정리**\n\n---\n\n### Iterator\n\n- **Iterators are objects that allow you to traverse through all the elements of a collection and return one element at a time.**\n- iterator는 iterable로 생성되는 값을 순서대로 꺼낼 수 있는 객체이다,\n- iter(collections) : returns unmodified iterator\n- iter(<function>, to_exclusive) : A sequence of return values until 'to_exclusive'\n- next(<iter>,default) :Raises StopIteration or returns 'default' on end.\n- <list> = list(<iter>) : Return a list of iterator's remaining elements\n\n```python\ntemp = (\"apple\", \"banana\", \"cherry\")\nmyit = iter(temp)\n\nprint(next(myit))\nprint(next(myit))\nprint(next(myit))\n\n```\n\n```python\nIn [30]: iv = list(range(0,5))\n\nIn [31]: io = iter(iv)\n\nIn [32]: while True:\n    ...:     try:\n    ...:\n    ...:         item = next(io)\n    ...:         print(item)\n    ...:     except StopIteration:\n    ...:         break\n    ...:\n    ...:\n0\n1\n2\n3\n4\n```\n\n#### itertools\n\n\n```python\nfrom itertools import count, repeat, cycle, chain, islice\n```\n\n- `count` :  count(시작, [step]) 의 함수로 시작 숫자부터 step만큼(없으면 1) 씩 무한히 증가하는 generator 반환\n- `islice` : islice(iterable객체, [시작], 정지[,step])의 함수로, iterable한 객체를 특정 범위로 슬라이싱하고 iterator로 반환.\n- `chain` : chain(**iterable**)은 iterable한 객체들을 인수로 받아 하나의 iterator로 반환\n\n```python\n# chain\nfrom itertools import chain\ne1 = ['Happiness','Caring','Energy']\ne2 = ['Fear','Hurt','Tired']\nemotions = chain(e1, e2)\n\nnext(emotions) >>> 'Happiness'\nnext(emotions) >>> 'Caring'\nnext(emotions) >>> 'Energy'\n\n```\n\n- [itertools](https://www.geeksforgeeks.org/python-itertools/\n)\n- https://realpython.com/python-itertools/\n- https://hamait.tistory.com/803\n\n**itertools.product를 활용한 이중 반복문 변형**\n\n```python\n# 기존 반복문\nfor i in i_ex:\n    for j in j_ex:\n        print(i,j)\n\n# itertools활용\nimport itertools\nfor i, j in itertools.product(i_ex, j_ex):\n    print(i, j)\n\n\n```\n\n### Generator\n\n- Any function that contains a yield statement returns a generator.\n- Generators and iterators are interchangeable.\n- **Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly**\n\n- Lazy-evaluation : 값을 미리 생성하여 메모리에 저장하고 있는게 아니며, 요청이 있을 때마다  함수를 실행하고 값을 공급(yield)해 줌\n\n```python\nIn [1]: my_gen = (x*x for x in range(3))\n\nIn [2]: type(my_gen)\nOut[2]: generator\n\nIn [3]: for i in my_gen:\n   ...:     print(i)\n   ...:\n0\n1\n4\n\n```\nIt is just the same except you used () instead of []. BUT, you cannot perform for i in generator a second time since **generators can only be used once**: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one\n\n\n#### yield form\n\n- `yield`는 `return`과 유사하지만 generator를 반환한다.\n\n```python\nIn [6]: def gen_count(start,step):\n   ...:     while True:\n   ...:         yield start\n   ...:         start += step\n   ...:\n\nIn [7]: counter = gen_count(10,2)\n\nIn [8]: next(counter)\nOut[8]: 10\n\nIn [9]: next(counter)\nOut[9]: 12\n\nIn [10]: next(counter)\nOut[10]: 14\n\nIn [11]: next(counter)\nOut[11]: 16\n```\n\n- `yield from a`를 통해 `iterable`의 전체 요소들을 반환할 수 있다.\n\n```python\n>>> def three_generator():\n...     a = [1, 2, 3]\n...     yield from a\n... \n>>> gen = three_generator()\n>>> list(gen)\n[1, 2, 3]\n```\n\n**References & annotation**\n---\n\n- [핵심 stackoverflow ref](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do)\n- Python Comprehensive Cheat Sheet","source":"_posts/Programming-Python-Generator.md","raw":"---\ntitle: '[Python]Iterator,Generator,yield에 대한 정리'\ncategories:\n    - Programming\ntags:\n   - Python\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**Iterator,Generator,yield에 대한 정리**\n\n---\n\n### Iterator\n\n- **Iterators are objects that allow you to traverse through all the elements of a collection and return one element at a time.**\n- iterator는 iterable로 생성되는 값을 순서대로 꺼낼 수 있는 객체이다,\n- iter(collections) : returns unmodified iterator\n- iter(<function>, to_exclusive) : A sequence of return values until 'to_exclusive'\n- next(<iter>,default) :Raises StopIteration or returns 'default' on end.\n- <list> = list(<iter>) : Return a list of iterator's remaining elements\n\n```python\ntemp = (\"apple\", \"banana\", \"cherry\")\nmyit = iter(temp)\n\nprint(next(myit))\nprint(next(myit))\nprint(next(myit))\n\n```\n\n```python\nIn [30]: iv = list(range(0,5))\n\nIn [31]: io = iter(iv)\n\nIn [32]: while True:\n    ...:     try:\n    ...:\n    ...:         item = next(io)\n    ...:         print(item)\n    ...:     except StopIteration:\n    ...:         break\n    ...:\n    ...:\n0\n1\n2\n3\n4\n```\n\n#### itertools\n\n\n```python\nfrom itertools import count, repeat, cycle, chain, islice\n```\n\n- `count` :  count(시작, [step]) 의 함수로 시작 숫자부터 step만큼(없으면 1) 씩 무한히 증가하는 generator 반환\n- `islice` : islice(iterable객체, [시작], 정지[,step])의 함수로, iterable한 객체를 특정 범위로 슬라이싱하고 iterator로 반환.\n- `chain` : chain(**iterable**)은 iterable한 객체들을 인수로 받아 하나의 iterator로 반환\n\n```python\n# chain\nfrom itertools import chain\ne1 = ['Happiness','Caring','Energy']\ne2 = ['Fear','Hurt','Tired']\nemotions = chain(e1, e2)\n\nnext(emotions) >>> 'Happiness'\nnext(emotions) >>> 'Caring'\nnext(emotions) >>> 'Energy'\n\n```\n\n- [itertools](https://www.geeksforgeeks.org/python-itertools/\n)\n- https://realpython.com/python-itertools/\n- https://hamait.tistory.com/803\n\n**itertools.product를 활용한 이중 반복문 변형**\n\n```python\n# 기존 반복문\nfor i in i_ex:\n    for j in j_ex:\n        print(i,j)\n\n# itertools활용\nimport itertools\nfor i, j in itertools.product(i_ex, j_ex):\n    print(i, j)\n\n\n```\n\n### Generator\n\n- Any function that contains a yield statement returns a generator.\n- Generators and iterators are interchangeable.\n- **Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly**\n\n- Lazy-evaluation : 값을 미리 생성하여 메모리에 저장하고 있는게 아니며, 요청이 있을 때마다  함수를 실행하고 값을 공급(yield)해 줌\n\n```python\nIn [1]: my_gen = (x*x for x in range(3))\n\nIn [2]: type(my_gen)\nOut[2]: generator\n\nIn [3]: for i in my_gen:\n   ...:     print(i)\n   ...:\n0\n1\n4\n\n```\nIt is just the same except you used () instead of []. BUT, you cannot perform for i in generator a second time since **generators can only be used once**: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one\n\n\n#### yield form\n\n- `yield`는 `return`과 유사하지만 generator를 반환한다.\n\n```python\nIn [6]: def gen_count(start,step):\n   ...:     while True:\n   ...:         yield start\n   ...:         start += step\n   ...:\n\nIn [7]: counter = gen_count(10,2)\n\nIn [8]: next(counter)\nOut[8]: 10\n\nIn [9]: next(counter)\nOut[9]: 12\n\nIn [10]: next(counter)\nOut[10]: 14\n\nIn [11]: next(counter)\nOut[11]: 16\n```\n\n- `yield from a`를 통해 `iterable`의 전체 요소들을 반환할 수 있다.\n\n```python\n>>> def three_generator():\n...     a = [1, 2, 3]\n...     yield from a\n... \n>>> gen = three_generator()\n>>> list(gen)\n[1, 2, 3]\n```\n\n**References & annotation**\n---\n\n- [핵심 stackoverflow ref](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do)\n- Python Comprehensive Cheat Sheet","slug":"Programming-Python-Generator","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsci002pb36q4mcu3uiy","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>Iterator,Generator,yield에 대한 정리</strong></p>\n<hr>\n<h3 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h3><ul>\n<li><strong>Iterators are objects that allow you to traverse through all the elements of a collection and return one element at a time.</strong></li>\n<li>iterator는 iterable로 생성되는 값을 순서대로 꺼낼 수 있는 객체이다,</li>\n<li>iter(collections) : returns unmodified iterator</li>\n<li>iter(<function>, to_exclusive) : A sequence of return values until ‘to_exclusive’</li>\n<li>next(<iter>,default) :Raises StopIteration or returns ‘default’ on end.</li>\n<li><list> &#x3D; list(<iter>) : Return a list of iterator’s remaining elements</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">temp = (<span class=\"string\">&quot;apple&quot;</span>, <span class=\"string\">&quot;banana&quot;</span>, <span class=\"string\">&quot;cherry&quot;</span>)</span><br><span class=\"line\">myit = <span class=\"built_in\">iter</span>(temp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">30</span>]: iv = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">31</span>]: io = <span class=\"built_in\">iter</span>(iv)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">32</span>]: <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">    ...:     <span class=\"keyword\">try</span>:</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...:         item = <span class=\"built_in\">next</span>(io)</span><br><span class=\"line\">    ...:         <span class=\"built_in\">print</span>(item)</span><br><span class=\"line\">    ...:     <span class=\"keyword\">except</span> StopIteration:</span><br><span class=\"line\">    ...:         <span class=\"keyword\">break</span></span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...:</span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"itertools\"><a href=\"#itertools\" class=\"headerlink\" title=\"itertools\"></a>itertools</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> count, repeat, cycle, chain, islice</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>count</code> :  count(시작, [step]) 의 함수로 시작 숫자부터 step만큼(없으면 1) 씩 무한히 증가하는 generator 반환</li>\n<li><code>islice</code> : islice(iterable객체, [시작], 정지[,step])의 함수로, iterable한 객체를 특정 범위로 슬라이싱하고 iterator로 반환.</li>\n<li><code>chain</code> : chain(<strong>iterable</strong>)은 iterable한 객체들을 인수로 받아 하나의 iterator로 반환</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># chain</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\">e1 = [<span class=\"string\">&#x27;Happiness&#x27;</span>,<span class=\"string\">&#x27;Caring&#x27;</span>,<span class=\"string\">&#x27;Energy&#x27;</span>]</span><br><span class=\"line\">e2 = [<span class=\"string\">&#x27;Fear&#x27;</span>,<span class=\"string\">&#x27;Hurt&#x27;</span>,<span class=\"string\">&#x27;Tired&#x27;</span>]</span><br><span class=\"line\">emotions = chain(e1, e2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Happiness&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Caring&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Energy&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><a href=\"https://www.geeksforgeeks.org/python-itertools/\">itertools</a></li>\n<li><a href=\"https://realpython.com/python-itertools/\">https://realpython.com/python-itertools/</a></li>\n<li><a href=\"https://hamait.tistory.com/803\">https://hamait.tistory.com/803</a></li>\n</ul>\n<p><strong>itertools.product를 활용한 이중 반복문 변형</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기존 반복문</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> i_ex:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> j_ex:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># itertools활용</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> itertools</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, j <span class=\"keyword\">in</span> itertools.product(i_ex, j_ex):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, j)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Generator\"><a href=\"#Generator\" class=\"headerlink\" title=\"Generator\"></a>Generator</h3><ul>\n<li><p>Any function that contains a yield statement returns a generator.</p>\n</li>\n<li><p>Generators and iterators are interchangeable.</p>\n</li>\n<li><p><strong>Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly</strong></p>\n</li>\n<li><p>Lazy-evaluation : 값을 미리 생성하여 메모리에 저장하고 있는게 아니며, 요청이 있을 때마다  함수를 실행하고 값을 공급(yield)해 줌</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">1</span>]: my_gen = (x*x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: <span class=\"built_in\">type</span>(my_gen)</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: generator</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> my_gen:</span><br><span class=\"line\">   ...:     <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>It is just the same except you used () instead of []. BUT, you cannot perform for i in generator a second time since <strong>generators can only be used once</strong>: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one</p>\n<h4 id=\"yield-form\"><a href=\"#yield-form\" class=\"headerlink\" title=\"yield form\"></a>yield form</h4><ul>\n<li><code>yield</code>는 <code>return</code>과 유사하지만 generator를 반환한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"keyword\">def</span> <span class=\"title function_\">gen_count</span>(<span class=\"params\">start,step</span>):</span><br><span class=\"line\">   ...:     <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">   ...:         <span class=\"keyword\">yield</span> start</span><br><span class=\"line\">   ...:         start += step</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: counter = gen_count(<span class=\"number\">10</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">12</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">14</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">11</span>]: <span class=\"number\">16</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>yield from a</code>를 통해 <code>iterable</code>의 전체 요소들을 반환할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">def</span> <span class=\"title function_\">three_generator</span>():</span><br><span class=\"line\"><span class=\"meta\">... </span>    a = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">yield</span> <span class=\"keyword\">from</span> a</span><br><span class=\"line\"><span class=\"meta\">... </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>gen = three_generator()</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">list</span>(gen)</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\">핵심 stackoverflow ref</a></li>\n<li>Python Comprehensive Cheat Sheet</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>Iterator,Generator,yield에 대한 정리</strong></p>\n<hr>\n<h3 id=\"Iterator\"><a href=\"#Iterator\" class=\"headerlink\" title=\"Iterator\"></a>Iterator</h3><ul>\n<li><strong>Iterators are objects that allow you to traverse through all the elements of a collection and return one element at a time.</strong></li>\n<li>iterator는 iterable로 생성되는 값을 순서대로 꺼낼 수 있는 객체이다,</li>\n<li>iter(collections) : returns unmodified iterator</li>\n<li>iter(<function>, to_exclusive) : A sequence of return values until ‘to_exclusive’</li>\n<li>next(<iter>,default) :Raises StopIteration or returns ‘default’ on end.</li>\n<li><list> &#x3D; list(<iter>) : Return a list of iterator’s remaining elements</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">temp = (<span class=\"string\">&quot;apple&quot;</span>, <span class=\"string\">&quot;banana&quot;</span>, <span class=\"string\">&quot;cherry&quot;</span>)</span><br><span class=\"line\">myit = <span class=\"built_in\">iter</span>(temp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">next</span>(myit))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">30</span>]: iv = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">31</span>]: io = <span class=\"built_in\">iter</span>(iv)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">32</span>]: <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">    ...:     <span class=\"keyword\">try</span>:</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...:         item = <span class=\"built_in\">next</span>(io)</span><br><span class=\"line\">    ...:         <span class=\"built_in\">print</span>(item)</span><br><span class=\"line\">    ...:     <span class=\"keyword\">except</span> StopIteration:</span><br><span class=\"line\">    ...:         <span class=\"keyword\">break</span></span><br><span class=\"line\">    ...:</span><br><span class=\"line\">    ...:</span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"itertools\"><a href=\"#itertools\" class=\"headerlink\" title=\"itertools\"></a>itertools</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> count, repeat, cycle, chain, islice</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>count</code> :  count(시작, [step]) 의 함수로 시작 숫자부터 step만큼(없으면 1) 씩 무한히 증가하는 generator 반환</li>\n<li><code>islice</code> : islice(iterable객체, [시작], 정지[,step])의 함수로, iterable한 객체를 특정 범위로 슬라이싱하고 iterator로 반환.</li>\n<li><code>chain</code> : chain(<strong>iterable</strong>)은 iterable한 객체들을 인수로 받아 하나의 iterator로 반환</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># chain</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\">e1 = [<span class=\"string\">&#x27;Happiness&#x27;</span>,<span class=\"string\">&#x27;Caring&#x27;</span>,<span class=\"string\">&#x27;Energy&#x27;</span>]</span><br><span class=\"line\">e2 = [<span class=\"string\">&#x27;Fear&#x27;</span>,<span class=\"string\">&#x27;Hurt&#x27;</span>,<span class=\"string\">&#x27;Tired&#x27;</span>]</span><br><span class=\"line\">emotions = chain(e1, e2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Happiness&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Caring&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">next</span>(emotions) &gt;&gt;&gt; <span class=\"string\">&#x27;Energy&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><a href=\"https://www.geeksforgeeks.org/python-itertools/\">itertools</a></li>\n<li><a href=\"https://realpython.com/python-itertools/\">https://realpython.com/python-itertools/</a></li>\n<li><a href=\"https://hamait.tistory.com/803\">https://hamait.tistory.com/803</a></li>\n</ul>\n<p><strong>itertools.product를 활용한 이중 반복문 변형</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기존 반복문</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> i_ex:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> j_ex:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># itertools활용</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> itertools</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, j <span class=\"keyword\">in</span> itertools.product(i_ex, j_ex):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, j)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Generator\"><a href=\"#Generator\" class=\"headerlink\" title=\"Generator\"></a>Generator</h3><ul>\n<li><p>Any function that contains a yield statement returns a generator.</p>\n</li>\n<li><p>Generators and iterators are interchangeable.</p>\n</li>\n<li><p><strong>Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly</strong></p>\n</li>\n<li><p>Lazy-evaluation : 값을 미리 생성하여 메모리에 저장하고 있는게 아니며, 요청이 있을 때마다  함수를 실행하고 값을 공급(yield)해 줌</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">1</span>]: my_gen = (x*x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: <span class=\"built_in\">type</span>(my_gen)</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: generator</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> my_gen:</span><br><span class=\"line\">   ...:     <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"><span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>It is just the same except you used () instead of []. BUT, you cannot perform for i in generator a second time since <strong>generators can only be used once</strong>: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one</p>\n<h4 id=\"yield-form\"><a href=\"#yield-form\" class=\"headerlink\" title=\"yield form\"></a>yield form</h4><ul>\n<li><code>yield</code>는 <code>return</code>과 유사하지만 generator를 반환한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">6</span>]: <span class=\"keyword\">def</span> <span class=\"title function_\">gen_count</span>(<span class=\"params\">start,step</span>):</span><br><span class=\"line\">   ...:     <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">   ...:         <span class=\"keyword\">yield</span> start</span><br><span class=\"line\">   ...:         start += step</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: counter = gen_count(<span class=\"number\">10</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">12</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">14</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: <span class=\"built_in\">next</span>(counter)</span><br><span class=\"line\">Out[<span class=\"number\">11</span>]: <span class=\"number\">16</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>yield from a</code>를 통해 <code>iterable</code>의 전체 요소들을 반환할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">def</span> <span class=\"title function_\">three_generator</span>():</span><br><span class=\"line\"><span class=\"meta\">... </span>    a = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">yield</span> <span class=\"keyword\">from</span> a</span><br><span class=\"line\"><span class=\"meta\">... </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>gen = three_generator()</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">list</span>(gen)</span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\">핵심 stackoverflow ref</a></li>\n<li>Python Comprehensive Cheat Sheet</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]Iterator,Generator,yield에 대한 정리","path":"2022/06/13/Programming-Python-Generator/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]Graph와 Graph Representation의 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## 그래프\n\n### Graph Concept\n---\n**_Concept_**\n\n(용어정리)\n- **그래프** : node(점)와 edge(간선)으로 이루어진 자료구조 . 추상자료형에 해당한다.\n- **Directed Graph** : edge가 방향성이 있을 경우. 기본적으로 엣지가 순서가 있는 쌍으로 표현된다. \n  - leaf : Directed Graph는 순서가 있으므로 마지막 노드가 있다. leaf는 마지막 노드를 뜻한다.\n- **Undirected Graph** : edge가 방향성이 없을 경우. \n  - 관계의 목적이 **상호교환** 일 경우 `Undirected Graph`가 가장 적합하다.\n  - 항상 동일한 노드에 재방문 가능하기 때문에 순환 그래프에 속한다.\n  - adjacency(인접) : 간선이 연결된 것\n  - neighbor : 간선이 연결된 노드들을 이웃(neighbor) 이라고 한다.  \n- **Cyclic  Graph** :그래프에 루프가 있을 경우\n  - loop : 방문한 노드에 재방문 가능할 경우\n- **Weighted Graph** : edge에 값(가중치)가 있을 경우. 최단거리 문제에 사용\n  - 그래프에서 경로의 총 가중치가 높을 경우 비용이 늘어난다.\n- **Directed Acyclic Graph** : 그래프가 순환되지 않고 단방향일 경우. 선형정렬 가능\n- Adjacency List : 배열로 표현된 그래프 자료구조(연결리스트)\n- Adjacency Matrix : 2차원 배열로 표현된 그래프 자료구조 \n\n---\n\n\n### Graph Representation\n\n- 기본적으로 인접리스트와 인접행렬 두 가지 방법을 사용한다.\n\n#### 인접리스트(adjacency list)\n\n\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png)\n\n- 인접리스트는 그래프를 배열로 나타내는 방식이다.\n- 인접리스트에서 그래프는 `전체 노드 목록`을 저장한다,\n  - 배열의 크기는 노드의 수와 같다.\n  - 배열의 i 번째 엔트리는 i번째 노드와 인접노드의 값을 리스트로 저장한다.\n  - Weighted Graph 일 경우 리스트에 값이 아닌 값과 가중치의 페어들이 저장된다.\n\n\n\n![](https://i.imgur.com/GiStmNh.jpg)\n\n- Dictionarys로 인접리스트 구현하기\n  - vertices는 **O(1)상수시간에 각 edge(간선)에 접근**할 수 있다.\n  - edge가 set에 포함되어 있기 때문에 O(1) 상수 시간에 edge가 있는지 확인할 수 있다.\n \n\n```python\n# 위 그림에 대해 딕셔너리를 사용한 인접리스트 예시\n# 노드가 키가 되고, 인접노드가 값이 되는 딕셔너리이다.\n# 가장자리 노드들은 set으로 구현되어 있다.\n\nclass Graph:\n    def __init__(self):\n        self.vertices = {\n                            \"A\": {\"B\"},   # 여기서 {\"B\"}가 set의 형태이다.\n                            \"B\": {\"C\", \"D\"}, # {\"B\" : {}}의 형태는 딕셔너리\n                            \"C\": {\"E\"},     # 즉, 딕셔너리 안에 set이 있는 것이다.\n                            \"D\": {\"F\", \"G\"},\n                            \"E\": {\"C\"},\n                            \"F\": {\"C\"},\n                            \"G\": {\"A\", \"F\"}\n                        }\n```\n\n- List로 인접리스트 구현\n\n```python\n# 이웃노드를 반복적으로 접근해 탐색\n# 시간복잡도(N)\n\na,b,c,d,e,f = range(6) # 6개노드\n\nN = [[b,c,d,f],[a,d,f],[a,b,c],[a,e],[b,c,d,e]]\n\n```\n\n- class로 인접리스트 구현\n\n```python\n\n# 기본적으로 연결리스트처럼 초기화 클래스가 필요하다.\nclass adjnode:\n  def __init__(self,data):\n    self.node = data\n    self.next = None\n\nclass Graph:\n  def __init__(self,vertices):\n    self.V = vertices\n    self.gragh = [None] * self.V\n\n\n\n  def add_edge(self,src,dest):\n    # 시작지점(src)에 노드 추가\n    # 기본적으로 Undirected Graph를 만든다.\n    node = adjnode(dest)\n    # 아래 코드로 두 노드를 연결시킨다.\n    node.next = self.graph[src]\n    self.graph[src] = node\n\n  def print_graph(self):\n    for i in range(self.V):\n      print(f\"노드 {i}의 인접리스트 \\n\")\n      temp = self.graph[i]\n      while temp:\n        print(f\"-> {temp.node}\",end=\"\" )\n        temp = temp.next\n      print(\"\\n\")\n\ngraph = Graph(5)\n\ngraph.add_edge(0,1) # \ngraph.add_edge(0,4)\ngraph.add_edge(1,2)\ngraph.add_edge(1,3)\ngraph.add_edge(1,4)\ngraph.add_edge(3,1)\ngraph.add_edge(2,3)\ngraph.add_edge(3,4)\n\n\n\n```\n```bash\n노드 0의 인접리스트 \n\n-> 4-> 1\n\n노드 1의 인접리스트 \n\n-> 4-> 3-> 2\n\n노드 2의 인접리스트 \n\n-> 3\n\n노드 3의 인접리스트 \n\n-> 4-> 1\n\n노드 4의 인접리스트 \n\n```\n\n#### 인접행렬(adjacency matrix)\n\n- N * N 크기의 2차원 배열로 나타낸다.\n  - N 은 노드의 개수이다.\n- 인접행렬은 2차원 배열을 활용해 그래프를 표현한 것이다.\n  - `adj[i][j] : 노드 i에서 노드 j로 가는 간선이 있으면 1, 아니면 0`\n  \n- 파이썬에서는 중첩리스트로 구현한다.\n- Undirected Graph의 인접행렬은 항상 대칭이다.\n- 인접행령의 가중치를 추가할 경우 1과 0 값을 다른 숫자로 바꾼다.\n- 인접행렬로 나타낸 그래프 구조들\n<img src=\"https://www.researchgate.net/publication/347300725/figure/fig1/AS:969208926044162@1608088823984/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm\" alt=\"700\"/>\n\n\n![https://i.imgur.com/GiStmNh.jpg](https://i.imgur.com/GiStmNh.jpg)\n\n- 위 그래프를 인접행렬로 만들 경우 우선 아래와 같은 그림을 만들 수 있다.\n\n![https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true](https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true)\n\n- List로 구현한 인접행렬\n```python\n# 리스트로 구현한 인접행렬\n# 아래 코드처럼 위의 간선 가중치는 1이다.\n\nclass Graph:\n    def __init__(self):\n        self.edges = [[0,1,0,0,0,0,0],\n                      [0,0,1,1,0,0,0],\n                      [0,0,0,0,1,0,0],\n                      [0,0,0,0,0,1,1],\n                      [0,0,1,0,0,0,0],\n                      [0,0,1,0,0,0,0],\n                      [1,0,0,0,0,1,0]]\n\n```\n\n\n\n- 위에서 행렬은 리스트 안에 리스트가 있는 2차원 배열로 표현된다.\n    - 구현을 통해 기본 제공되는 행렬 간에 **edge weights(간선 가중치)**를 알 수 있다.\n    - **0은 관계가 없음**을 나타내지만 다른 값은 **edge label 또는 edge weight**를 나타낸다.\n    - 인접행렬의 단점은 **노드 값과 해당 인덱스 사이에 연관성이 없다**는 것이다.\n- 실제로 인접리스트와 인접행렬을 모두 구현하면 Vertex(정점) 및 Edge(간선) 클래스를 포함하여 더 많은 정보를 파악할 수 있다.\n\n#### Weighted Graph 구현하기\n\n- 리스트로 그래프 가중치를 표현하는 것 보다 행렬로 구현하는 것이 쉽다.\n\n```python\n\n# 인접리스트 구현\n\nclass Graph:\n    def __init__(self):\n        self.vertices = {\n                            \"A\": {\"B\": 1},  # 가중치 부여\n                            \"B\": {\"C\": 3, \"D\": 2},  # 가중치 부여\n                            \"C\": {},\n                            \"D\": {},\n                            \"E\": {\"D\": 1}   # 가중치 부여\n                        }\n```\n\n\n```python\n# 인접행렬 구현\n\nclass Graph:\n    def __init__(self):\n        self.edges = [[0,1,0,0,0],\n                      [0,0,3,2,0],\n                      [0,0,0,0,0],\n                      [0,0,0,0,0],\n                      [0,0,0,1,0]]\n```\n\n\n### 그래프에서의 복잡도 계산\n\n- 인접행렬은 특징은 **구현이 쉽다**는 것이다.\n    - 때문에 인접행렬의 가장 큰 단점은 **특정노드에 방문한 노드들을 알기 위해서는 모든 노드를 확인**해야 한다는 것이다. (시간복잡도 O(N))\n    - 이러한 단점을 위해 인접리스트로 표현방식이 생겼다.\n- 인접리스트는 실제 연결된 관계만을 저장해주기 때문에 실행시간에 영향을 적게 준다.\n    - 인접리스트의 단점은 **특정 노드간의 연결관계를 확인하기 위해서는 반복문이 활용되어야 하며 따라서 O(N) 이상의 시간복잡도** 가 발생한다는 것이다.\n\n**References & annotation**\n---\n- [Graph Concept](https://youtu.be/ofykE5elSfI)\n- [Graph Data Structure](https://ratsgo.github.io/data%20structure&algorithm/2017/11/18/graph/)\n- [Graph Representaion](https://www.geeksforgeeks.org/graph-and-its-representations/)\n\n","source":"_posts/Programming-Python-Graph-basic.md","raw":"---\ntitle: '[Python]Graph와 Graph Representation의 이해'\ncategories:\n  - - Programming\ntags:\n  - Python\n  - Algorithm\n  - Data Structure\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n## 그래프\n\n### Graph Concept\n---\n**_Concept_**\n\n(용어정리)\n- **그래프** : node(점)와 edge(간선)으로 이루어진 자료구조 . 추상자료형에 해당한다.\n- **Directed Graph** : edge가 방향성이 있을 경우. 기본적으로 엣지가 순서가 있는 쌍으로 표현된다. \n  - leaf : Directed Graph는 순서가 있으므로 마지막 노드가 있다. leaf는 마지막 노드를 뜻한다.\n- **Undirected Graph** : edge가 방향성이 없을 경우. \n  - 관계의 목적이 **상호교환** 일 경우 `Undirected Graph`가 가장 적합하다.\n  - 항상 동일한 노드에 재방문 가능하기 때문에 순환 그래프에 속한다.\n  - adjacency(인접) : 간선이 연결된 것\n  - neighbor : 간선이 연결된 노드들을 이웃(neighbor) 이라고 한다.  \n- **Cyclic  Graph** :그래프에 루프가 있을 경우\n  - loop : 방문한 노드에 재방문 가능할 경우\n- **Weighted Graph** : edge에 값(가중치)가 있을 경우. 최단거리 문제에 사용\n  - 그래프에서 경로의 총 가중치가 높을 경우 비용이 늘어난다.\n- **Directed Acyclic Graph** : 그래프가 순환되지 않고 단방향일 경우. 선형정렬 가능\n- Adjacency List : 배열로 표현된 그래프 자료구조(연결리스트)\n- Adjacency Matrix : 2차원 배열로 표현된 그래프 자료구조 \n\n---\n\n\n### Graph Representation\n\n- 기본적으로 인접리스트와 인접행렬 두 가지 방법을 사용한다.\n\n#### 인접리스트(adjacency list)\n\n\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png)\n\n- 인접리스트는 그래프를 배열로 나타내는 방식이다.\n- 인접리스트에서 그래프는 `전체 노드 목록`을 저장한다,\n  - 배열의 크기는 노드의 수와 같다.\n  - 배열의 i 번째 엔트리는 i번째 노드와 인접노드의 값을 리스트로 저장한다.\n  - Weighted Graph 일 경우 리스트에 값이 아닌 값과 가중치의 페어들이 저장된다.\n\n\n\n![](https://i.imgur.com/GiStmNh.jpg)\n\n- Dictionarys로 인접리스트 구현하기\n  - vertices는 **O(1)상수시간에 각 edge(간선)에 접근**할 수 있다.\n  - edge가 set에 포함되어 있기 때문에 O(1) 상수 시간에 edge가 있는지 확인할 수 있다.\n \n\n```python\n# 위 그림에 대해 딕셔너리를 사용한 인접리스트 예시\n# 노드가 키가 되고, 인접노드가 값이 되는 딕셔너리이다.\n# 가장자리 노드들은 set으로 구현되어 있다.\n\nclass Graph:\n    def __init__(self):\n        self.vertices = {\n                            \"A\": {\"B\"},   # 여기서 {\"B\"}가 set의 형태이다.\n                            \"B\": {\"C\", \"D\"}, # {\"B\" : {}}의 형태는 딕셔너리\n                            \"C\": {\"E\"},     # 즉, 딕셔너리 안에 set이 있는 것이다.\n                            \"D\": {\"F\", \"G\"},\n                            \"E\": {\"C\"},\n                            \"F\": {\"C\"},\n                            \"G\": {\"A\", \"F\"}\n                        }\n```\n\n- List로 인접리스트 구현\n\n```python\n# 이웃노드를 반복적으로 접근해 탐색\n# 시간복잡도(N)\n\na,b,c,d,e,f = range(6) # 6개노드\n\nN = [[b,c,d,f],[a,d,f],[a,b,c],[a,e],[b,c,d,e]]\n\n```\n\n- class로 인접리스트 구현\n\n```python\n\n# 기본적으로 연결리스트처럼 초기화 클래스가 필요하다.\nclass adjnode:\n  def __init__(self,data):\n    self.node = data\n    self.next = None\n\nclass Graph:\n  def __init__(self,vertices):\n    self.V = vertices\n    self.gragh = [None] * self.V\n\n\n\n  def add_edge(self,src,dest):\n    # 시작지점(src)에 노드 추가\n    # 기본적으로 Undirected Graph를 만든다.\n    node = adjnode(dest)\n    # 아래 코드로 두 노드를 연결시킨다.\n    node.next = self.graph[src]\n    self.graph[src] = node\n\n  def print_graph(self):\n    for i in range(self.V):\n      print(f\"노드 {i}의 인접리스트 \\n\")\n      temp = self.graph[i]\n      while temp:\n        print(f\"-> {temp.node}\",end=\"\" )\n        temp = temp.next\n      print(\"\\n\")\n\ngraph = Graph(5)\n\ngraph.add_edge(0,1) # \ngraph.add_edge(0,4)\ngraph.add_edge(1,2)\ngraph.add_edge(1,3)\ngraph.add_edge(1,4)\ngraph.add_edge(3,1)\ngraph.add_edge(2,3)\ngraph.add_edge(3,4)\n\n\n\n```\n```bash\n노드 0의 인접리스트 \n\n-> 4-> 1\n\n노드 1의 인접리스트 \n\n-> 4-> 3-> 2\n\n노드 2의 인접리스트 \n\n-> 3\n\n노드 3의 인접리스트 \n\n-> 4-> 1\n\n노드 4의 인접리스트 \n\n```\n\n#### 인접행렬(adjacency matrix)\n\n- N * N 크기의 2차원 배열로 나타낸다.\n  - N 은 노드의 개수이다.\n- 인접행렬은 2차원 배열을 활용해 그래프를 표현한 것이다.\n  - `adj[i][j] : 노드 i에서 노드 j로 가는 간선이 있으면 1, 아니면 0`\n  \n- 파이썬에서는 중첩리스트로 구현한다.\n- Undirected Graph의 인접행렬은 항상 대칭이다.\n- 인접행령의 가중치를 추가할 경우 1과 0 값을 다른 숫자로 바꾼다.\n- 인접행렬로 나타낸 그래프 구조들\n<img src=\"https://www.researchgate.net/publication/347300725/figure/fig1/AS:969208926044162@1608088823984/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm\" alt=\"700\"/>\n\n\n![https://i.imgur.com/GiStmNh.jpg](https://i.imgur.com/GiStmNh.jpg)\n\n- 위 그래프를 인접행렬로 만들 경우 우선 아래와 같은 그림을 만들 수 있다.\n\n![https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true](https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true)\n\n- List로 구현한 인접행렬\n```python\n# 리스트로 구현한 인접행렬\n# 아래 코드처럼 위의 간선 가중치는 1이다.\n\nclass Graph:\n    def __init__(self):\n        self.edges = [[0,1,0,0,0,0,0],\n                      [0,0,1,1,0,0,0],\n                      [0,0,0,0,1,0,0],\n                      [0,0,0,0,0,1,1],\n                      [0,0,1,0,0,0,0],\n                      [0,0,1,0,0,0,0],\n                      [1,0,0,0,0,1,0]]\n\n```\n\n\n\n- 위에서 행렬은 리스트 안에 리스트가 있는 2차원 배열로 표현된다.\n    - 구현을 통해 기본 제공되는 행렬 간에 **edge weights(간선 가중치)**를 알 수 있다.\n    - **0은 관계가 없음**을 나타내지만 다른 값은 **edge label 또는 edge weight**를 나타낸다.\n    - 인접행렬의 단점은 **노드 값과 해당 인덱스 사이에 연관성이 없다**는 것이다.\n- 실제로 인접리스트와 인접행렬을 모두 구현하면 Vertex(정점) 및 Edge(간선) 클래스를 포함하여 더 많은 정보를 파악할 수 있다.\n\n#### Weighted Graph 구현하기\n\n- 리스트로 그래프 가중치를 표현하는 것 보다 행렬로 구현하는 것이 쉽다.\n\n```python\n\n# 인접리스트 구현\n\nclass Graph:\n    def __init__(self):\n        self.vertices = {\n                            \"A\": {\"B\": 1},  # 가중치 부여\n                            \"B\": {\"C\": 3, \"D\": 2},  # 가중치 부여\n                            \"C\": {},\n                            \"D\": {},\n                            \"E\": {\"D\": 1}   # 가중치 부여\n                        }\n```\n\n\n```python\n# 인접행렬 구현\n\nclass Graph:\n    def __init__(self):\n        self.edges = [[0,1,0,0,0],\n                      [0,0,3,2,0],\n                      [0,0,0,0,0],\n                      [0,0,0,0,0],\n                      [0,0,0,1,0]]\n```\n\n\n### 그래프에서의 복잡도 계산\n\n- 인접행렬은 특징은 **구현이 쉽다**는 것이다.\n    - 때문에 인접행렬의 가장 큰 단점은 **특정노드에 방문한 노드들을 알기 위해서는 모든 노드를 확인**해야 한다는 것이다. (시간복잡도 O(N))\n    - 이러한 단점을 위해 인접리스트로 표현방식이 생겼다.\n- 인접리스트는 실제 연결된 관계만을 저장해주기 때문에 실행시간에 영향을 적게 준다.\n    - 인접리스트의 단점은 **특정 노드간의 연결관계를 확인하기 위해서는 반복문이 활용되어야 하며 따라서 O(N) 이상의 시간복잡도** 가 발생한다는 것이다.\n\n**References & annotation**\n---\n- [Graph Concept](https://youtu.be/ofykE5elSfI)\n- [Graph Data Structure](https://ratsgo.github.io/data%20structure&algorithm/2017/11/18/graph/)\n- [Graph Representaion](https://www.geeksforgeeks.org/graph-and-its-representations/)\n\n","slug":"Programming-Python-Graph-basic","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscj002rb36q8xy0eb8w","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"그래프\"><a href=\"#그래프\" class=\"headerlink\" title=\"그래프\"></a>그래프</h2><h3 id=\"Graph-Concept\"><a href=\"#Graph-Concept\" class=\"headerlink\" title=\"Graph Concept\"></a>Graph Concept</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<p>(용어정리)</p>\n<ul>\n<li><strong>그래프</strong> : node(점)와 edge(간선)으로 이루어진 자료구조 . 추상자료형에 해당한다.</li>\n<li><strong>Directed Graph</strong> : edge가 방향성이 있을 경우. 기본적으로 엣지가 순서가 있는 쌍으로 표현된다. <ul>\n<li>leaf : Directed Graph는 순서가 있으므로 마지막 노드가 있다. leaf는 마지막 노드를 뜻한다.</li>\n</ul>\n</li>\n<li><strong>Undirected Graph</strong> : edge가 방향성이 없을 경우. <ul>\n<li>관계의 목적이 <strong>상호교환</strong> 일 경우 <code>Undirected Graph</code>가 가장 적합하다.</li>\n<li>항상 동일한 노드에 재방문 가능하기 때문에 순환 그래프에 속한다.</li>\n<li>adjacency(인접) : 간선이 연결된 것</li>\n<li>neighbor : 간선이 연결된 노드들을 이웃(neighbor) 이라고 한다.</li>\n</ul>\n</li>\n<li><strong>Cyclic  Graph</strong> :그래프에 루프가 있을 경우<ul>\n<li>loop : 방문한 노드에 재방문 가능할 경우</li>\n</ul>\n</li>\n<li><strong>Weighted Graph</strong> : edge에 값(가중치)가 있을 경우. 최단거리 문제에 사용<ul>\n<li>그래프에서 경로의 총 가중치가 높을 경우 비용이 늘어난다.</li>\n</ul>\n</li>\n<li><strong>Directed Acyclic Graph</strong> : 그래프가 순환되지 않고 단방향일 경우. 선형정렬 가능</li>\n<li>Adjacency List : 배열로 표현된 그래프 자료구조(연결리스트)</li>\n<li>Adjacency Matrix : 2차원 배열로 표현된 그래프 자료구조</li>\n</ul>\n<hr>\n<h3 id=\"Graph-Representation\"><a href=\"#Graph-Representation\" class=\"headerlink\" title=\"Graph Representation\"></a>Graph Representation</h3><ul>\n<li>기본적으로 인접리스트와 인접행렬 두 가지 방법을 사용한다.</li>\n</ul>\n<h4 id=\"인접리스트-adjacency-list\"><a href=\"#인접리스트-adjacency-list\" class=\"headerlink\" title=\"인접리스트(adjacency list)\"></a>인접리스트(adjacency list)</h4><p><img src=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png\"></p>\n<ul>\n<li>인접리스트는 그래프를 배열로 나타내는 방식이다.</li>\n<li>인접리스트에서 그래프는 <code>전체 노드 목록</code>을 저장한다,<ul>\n<li>배열의 크기는 노드의 수와 같다.</li>\n<li>배열의 i 번째 엔트리는 i번째 노드와 인접노드의 값을 리스트로 저장한다.</li>\n<li>Weighted Graph 일 경우 리스트에 값이 아닌 값과 가중치의 페어들이 저장된다.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/GiStmNh.jpg\"></p>\n<ul>\n<li>Dictionarys로 인접리스트 구현하기<ul>\n<li>vertices는 <strong>O(1)상수시간에 각 edge(간선)에 접근</strong>할 수 있다.</li>\n<li>edge가 set에 포함되어 있기 때문에 O(1) 상수 시간에 edge가 있는지 확인할 수 있다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 위 그림에 대해 딕셔너리를 사용한 인접리스트 예시</span></span><br><span class=\"line\"><span class=\"comment\"># 노드가 키가 되고, 인접노드가 값이 되는 딕셔너리이다.</span></span><br><span class=\"line\"><span class=\"comment\"># 가장자리 노드들은 set으로 구현되어 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.vertices = &#123;</span><br><span class=\"line\">                            <span class=\"string\">&quot;A&quot;</span>: &#123;<span class=\"string\">&quot;B&quot;</span>&#125;,   <span class=\"comment\"># 여기서 &#123;&quot;B&quot;&#125;가 set의 형태이다.</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;B&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>, <span class=\"string\">&quot;D&quot;</span>&#125;, <span class=\"comment\"># &#123;&quot;B&quot; : &#123;&#125;&#125;의 형태는 딕셔너리</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;C&quot;</span>: &#123;<span class=\"string\">&quot;E&quot;</span>&#125;,     <span class=\"comment\"># 즉, 딕셔너리 안에 set이 있는 것이다.</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;D&quot;</span>: &#123;<span class=\"string\">&quot;F&quot;</span>, <span class=\"string\">&quot;G&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;E&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;F&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;G&quot;</span>: &#123;<span class=\"string\">&quot;A&quot;</span>, <span class=\"string\">&quot;F&quot;</span>&#125;</span><br><span class=\"line\">                        &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>List로 인접리스트 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 이웃노드를 반복적으로 접근해 탐색</span></span><br><span class=\"line\"><span class=\"comment\"># 시간복잡도(N)</span></span><br><span class=\"line\"></span><br><span class=\"line\">a,b,c,d,e,f = <span class=\"built_in\">range</span>(<span class=\"number\">6</span>) <span class=\"comment\"># 6개노드</span></span><br><span class=\"line\"></span><br><span class=\"line\">N = [[b,c,d,f],[a,d,f],[a,b,c],[a,e],[b,c,d,e]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>class로 인접리스트 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 기본적으로 연결리스트처럼 초기화 클래스가 필요하다.</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">adjnode</span>:</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,data</span>):</span><br><span class=\"line\">    self.node = data</span><br><span class=\"line\">    self.<span class=\"built_in\">next</span> = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,vertices</span>):</span><br><span class=\"line\">    self.V = vertices</span><br><span class=\"line\">    self.gragh = [<span class=\"literal\">None</span>] * self.V</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">add_edge</span>(<span class=\"params\">self,src,dest</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 시작지점(src)에 노드 추가</span></span><br><span class=\"line\">    <span class=\"comment\"># 기본적으로 Undirected Graph를 만든다.</span></span><br><span class=\"line\">    node = adjnode(dest)</span><br><span class=\"line\">    <span class=\"comment\"># 아래 코드로 두 노드를 연결시킨다.</span></span><br><span class=\"line\">    node.<span class=\"built_in\">next</span> = self.graph[src]</span><br><span class=\"line\">    self.graph[src] = node</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">print_graph</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.V):</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;노드 <span class=\"subst\">&#123;i&#125;</span>의 인접리스트 \\n&quot;</span>)</span><br><span class=\"line\">      temp = self.graph[i]</span><br><span class=\"line\">      <span class=\"keyword\">while</span> temp:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;-&gt; <span class=\"subst\">&#123;temp.node&#125;</span>&quot;</span>,end=<span class=\"string\">&quot;&quot;</span> )</span><br><span class=\"line\">        temp = temp.<span class=\"built_in\">next</span></span><br><span class=\"line\">      <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph = Graph(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph.add_edge(<span class=\"number\">0</span>,<span class=\"number\">1</span>) <span class=\"comment\"># </span></span><br><span class=\"line\">graph.add_edge(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">3</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">노드 0의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">노드 1의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 3-&gt; 2</span><br><span class=\"line\"></span><br><span class=\"line\">노드 2의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 3</span><br><span class=\"line\"></span><br><span class=\"line\">노드 3의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">노드 4의 인접리스트 </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"인접행렬-adjacency-matrix\"><a href=\"#인접행렬-adjacency-matrix\" class=\"headerlink\" title=\"인접행렬(adjacency matrix)\"></a>인접행렬(adjacency matrix)</h4><ul>\n<li><p>N * N 크기의 2차원 배열로 나타낸다.</p>\n<ul>\n<li>N 은 노드의 개수이다.</li>\n</ul>\n</li>\n<li><p>인접행렬은 2차원 배열을 활용해 그래프를 표현한 것이다.</p>\n<ul>\n<li><code>adj[i][j] : 노드 i에서 노드 j로 가는 간선이 있으면 1, 아니면 0</code></li>\n</ul>\n</li>\n<li><p>파이썬에서는 중첩리스트로 구현한다.</p>\n</li>\n<li><p>Undirected Graph의 인접행렬은 항상 대칭이다.</p>\n</li>\n<li><p>인접행령의 가중치를 추가할 경우 1과 0 값을 다른 숫자로 바꾼다.</p>\n</li>\n<li><p>인접행렬로 나타낸 그래프 구조들</p>\n<img src=\"https://www.researchgate.net/publication/347300725/figure/fig1/AS:969208926044162@1608088823984/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm\" alt=\"700\"/></li>\n</ul>\n<p><img src=\"https://i.imgur.com/GiStmNh.jpg\" alt=\"https://i.imgur.com/GiStmNh.jpg\"></p>\n<ul>\n<li>위 그래프를 인접행렬로 만들 경우 우선 아래와 같은 그림을 만들 수 있다.</li>\n</ul>\n<p><img src=\"https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true\" alt=\"https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true\"></p>\n<ul>\n<li><p>List로 구현한 인접행렬</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 리스트로 구현한 인접행렬</span></span><br><span class=\"line\"><span class=\"comment\"># 아래 코드처럼 위의 간선 가중치는 1이다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.edges = [[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>위에서 행렬은 리스트 안에 리스트가 있는 2차원 배열로 표현된다.</p>\n<ul>\n<li>구현을 통해 기본 제공되는 행렬 간에 **edge weights(간선 가중치)**를 알 수 있다.</li>\n<li><strong>0은 관계가 없음</strong>을 나타내지만 다른 값은 <strong>edge label 또는 edge weight</strong>를 나타낸다.</li>\n<li>인접행렬의 단점은 <strong>노드 값과 해당 인덱스 사이에 연관성이 없다</strong>는 것이다.</li>\n</ul>\n</li>\n<li><p>실제로 인접리스트와 인접행렬을 모두 구현하면 Vertex(정점) 및 Edge(간선) 클래스를 포함하여 더 많은 정보를 파악할 수 있다.</p>\n</li>\n</ul>\n<h4 id=\"Weighted-Graph-구현하기\"><a href=\"#Weighted-Graph-구현하기\" class=\"headerlink\" title=\"Weighted Graph 구현하기\"></a>Weighted Graph 구현하기</h4><ul>\n<li>리스트로 그래프 가중치를 표현하는 것 보다 행렬로 구현하는 것이 쉽다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 인접리스트 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.vertices = &#123;</span><br><span class=\"line\">                            <span class=\"string\">&quot;A&quot;</span>: &#123;<span class=\"string\">&quot;B&quot;</span>: <span class=\"number\">1</span>&#125;,  <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;B&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>: <span class=\"number\">3</span>, <span class=\"string\">&quot;D&quot;</span>: <span class=\"number\">2</span>&#125;,  <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;C&quot;</span>: &#123;&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;D&quot;</span>: &#123;&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;E&quot;</span>: &#123;<span class=\"string\">&quot;D&quot;</span>: <span class=\"number\">1</span>&#125;   <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                        &#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 인접행렬 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.edges = [[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]]</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"그래프에서의-복잡도-계산\"><a href=\"#그래프에서의-복잡도-계산\" class=\"headerlink\" title=\"그래프에서의 복잡도 계산\"></a>그래프에서의 복잡도 계산</h3><ul>\n<li>인접행렬은 특징은 <strong>구현이 쉽다</strong>는 것이다.<ul>\n<li>때문에 인접행렬의 가장 큰 단점은 <strong>특정노드에 방문한 노드들을 알기 위해서는 모든 노드를 확인</strong>해야 한다는 것이다. (시간복잡도 O(N))</li>\n<li>이러한 단점을 위해 인접리스트로 표현방식이 생겼다.</li>\n</ul>\n</li>\n<li>인접리스트는 실제 연결된 관계만을 저장해주기 때문에 실행시간에 영향을 적게 준다.<ul>\n<li>인접리스트의 단점은 <strong>특정 노드간의 연결관계를 확인하기 위해서는 반복문이 활용되어야 하며 따라서 O(N) 이상의 시간복잡도</strong> 가 발생한다는 것이다.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/ofykE5elSfI\">Graph Concept</a></li>\n<li><a href=\"https://ratsgo.github.io/data%20structure&algorithm/2017/11/18/graph/\">Graph Data Structure</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/graph-and-its-representations/\">Graph Representaion</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<h2 id=\"그래프\"><a href=\"#그래프\" class=\"headerlink\" title=\"그래프\"></a>그래프</h2><h3 id=\"Graph-Concept\"><a href=\"#Graph-Concept\" class=\"headerlink\" title=\"Graph Concept\"></a>Graph Concept</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<p>(용어정리)</p>\n<ul>\n<li><strong>그래프</strong> : node(점)와 edge(간선)으로 이루어진 자료구조 . 추상자료형에 해당한다.</li>\n<li><strong>Directed Graph</strong> : edge가 방향성이 있을 경우. 기본적으로 엣지가 순서가 있는 쌍으로 표현된다. <ul>\n<li>leaf : Directed Graph는 순서가 있으므로 마지막 노드가 있다. leaf는 마지막 노드를 뜻한다.</li>\n</ul>\n</li>\n<li><strong>Undirected Graph</strong> : edge가 방향성이 없을 경우. <ul>\n<li>관계의 목적이 <strong>상호교환</strong> 일 경우 <code>Undirected Graph</code>가 가장 적합하다.</li>\n<li>항상 동일한 노드에 재방문 가능하기 때문에 순환 그래프에 속한다.</li>\n<li>adjacency(인접) : 간선이 연결된 것</li>\n<li>neighbor : 간선이 연결된 노드들을 이웃(neighbor) 이라고 한다.</li>\n</ul>\n</li>\n<li><strong>Cyclic  Graph</strong> :그래프에 루프가 있을 경우<ul>\n<li>loop : 방문한 노드에 재방문 가능할 경우</li>\n</ul>\n</li>\n<li><strong>Weighted Graph</strong> : edge에 값(가중치)가 있을 경우. 최단거리 문제에 사용<ul>\n<li>그래프에서 경로의 총 가중치가 높을 경우 비용이 늘어난다.</li>\n</ul>\n</li>\n<li><strong>Directed Acyclic Graph</strong> : 그래프가 순환되지 않고 단방향일 경우. 선형정렬 가능</li>\n<li>Adjacency List : 배열로 표현된 그래프 자료구조(연결리스트)</li>\n<li>Adjacency Matrix : 2차원 배열로 표현된 그래프 자료구조</li>\n</ul>\n<hr>\n<h3 id=\"Graph-Representation\"><a href=\"#Graph-Representation\" class=\"headerlink\" title=\"Graph Representation\"></a>Graph Representation</h3><ul>\n<li>기본적으로 인접리스트와 인접행렬 두 가지 방법을 사용한다.</li>\n</ul>\n<h4 id=\"인접리스트-adjacency-list\"><a href=\"#인접리스트-adjacency-list\" class=\"headerlink\" title=\"인접리스트(adjacency list)\"></a>인접리스트(adjacency list)</h4><p><img src=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png\"></p>\n<ul>\n<li>인접리스트는 그래프를 배열로 나타내는 방식이다.</li>\n<li>인접리스트에서 그래프는 <code>전체 노드 목록</code>을 저장한다,<ul>\n<li>배열의 크기는 노드의 수와 같다.</li>\n<li>배열의 i 번째 엔트리는 i번째 노드와 인접노드의 값을 리스트로 저장한다.</li>\n<li>Weighted Graph 일 경우 리스트에 값이 아닌 값과 가중치의 페어들이 저장된다.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/GiStmNh.jpg\"></p>\n<ul>\n<li>Dictionarys로 인접리스트 구현하기<ul>\n<li>vertices는 <strong>O(1)상수시간에 각 edge(간선)에 접근</strong>할 수 있다.</li>\n<li>edge가 set에 포함되어 있기 때문에 O(1) 상수 시간에 edge가 있는지 확인할 수 있다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 위 그림에 대해 딕셔너리를 사용한 인접리스트 예시</span></span><br><span class=\"line\"><span class=\"comment\"># 노드가 키가 되고, 인접노드가 값이 되는 딕셔너리이다.</span></span><br><span class=\"line\"><span class=\"comment\"># 가장자리 노드들은 set으로 구현되어 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.vertices = &#123;</span><br><span class=\"line\">                            <span class=\"string\">&quot;A&quot;</span>: &#123;<span class=\"string\">&quot;B&quot;</span>&#125;,   <span class=\"comment\"># 여기서 &#123;&quot;B&quot;&#125;가 set의 형태이다.</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;B&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>, <span class=\"string\">&quot;D&quot;</span>&#125;, <span class=\"comment\"># &#123;&quot;B&quot; : &#123;&#125;&#125;의 형태는 딕셔너리</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;C&quot;</span>: &#123;<span class=\"string\">&quot;E&quot;</span>&#125;,     <span class=\"comment\"># 즉, 딕셔너리 안에 set이 있는 것이다.</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;D&quot;</span>: &#123;<span class=\"string\">&quot;F&quot;</span>, <span class=\"string\">&quot;G&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;E&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;F&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;G&quot;</span>: &#123;<span class=\"string\">&quot;A&quot;</span>, <span class=\"string\">&quot;F&quot;</span>&#125;</span><br><span class=\"line\">                        &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>List로 인접리스트 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 이웃노드를 반복적으로 접근해 탐색</span></span><br><span class=\"line\"><span class=\"comment\"># 시간복잡도(N)</span></span><br><span class=\"line\"></span><br><span class=\"line\">a,b,c,d,e,f = <span class=\"built_in\">range</span>(<span class=\"number\">6</span>) <span class=\"comment\"># 6개노드</span></span><br><span class=\"line\"></span><br><span class=\"line\">N = [[b,c,d,f],[a,d,f],[a,b,c],[a,e],[b,c,d,e]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>class로 인접리스트 구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 기본적으로 연결리스트처럼 초기화 클래스가 필요하다.</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">adjnode</span>:</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,data</span>):</span><br><span class=\"line\">    self.node = data</span><br><span class=\"line\">    self.<span class=\"built_in\">next</span> = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,vertices</span>):</span><br><span class=\"line\">    self.V = vertices</span><br><span class=\"line\">    self.gragh = [<span class=\"literal\">None</span>] * self.V</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">add_edge</span>(<span class=\"params\">self,src,dest</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 시작지점(src)에 노드 추가</span></span><br><span class=\"line\">    <span class=\"comment\"># 기본적으로 Undirected Graph를 만든다.</span></span><br><span class=\"line\">    node = adjnode(dest)</span><br><span class=\"line\">    <span class=\"comment\"># 아래 코드로 두 노드를 연결시킨다.</span></span><br><span class=\"line\">    node.<span class=\"built_in\">next</span> = self.graph[src]</span><br><span class=\"line\">    self.graph[src] = node</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">print_graph</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.V):</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;노드 <span class=\"subst\">&#123;i&#125;</span>의 인접리스트 \\n&quot;</span>)</span><br><span class=\"line\">      temp = self.graph[i]</span><br><span class=\"line\">      <span class=\"keyword\">while</span> temp:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;-&gt; <span class=\"subst\">&#123;temp.node&#125;</span>&quot;</span>,end=<span class=\"string\">&quot;&quot;</span> )</span><br><span class=\"line\">        temp = temp.<span class=\"built_in\">next</span></span><br><span class=\"line\">      <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph = Graph(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">graph.add_edge(<span class=\"number\">0</span>,<span class=\"number\">1</span>) <span class=\"comment\"># </span></span><br><span class=\"line\">graph.add_edge(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">1</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">3</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">graph.add_edge(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">노드 0의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">노드 1의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 3-&gt; 2</span><br><span class=\"line\"></span><br><span class=\"line\">노드 2의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 3</span><br><span class=\"line\"></span><br><span class=\"line\">노드 3의 인접리스트 </span><br><span class=\"line\"></span><br><span class=\"line\">-&gt; 4-&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">노드 4의 인접리스트 </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"인접행렬-adjacency-matrix\"><a href=\"#인접행렬-adjacency-matrix\" class=\"headerlink\" title=\"인접행렬(adjacency matrix)\"></a>인접행렬(adjacency matrix)</h4><ul>\n<li><p>N * N 크기의 2차원 배열로 나타낸다.</p>\n<ul>\n<li>N 은 노드의 개수이다.</li>\n</ul>\n</li>\n<li><p>인접행렬은 2차원 배열을 활용해 그래프를 표현한 것이다.</p>\n<ul>\n<li><code>adj[i][j] : 노드 i에서 노드 j로 가는 간선이 있으면 1, 아니면 0</code></li>\n</ul>\n</li>\n<li><p>파이썬에서는 중첩리스트로 구현한다.</p>\n</li>\n<li><p>Undirected Graph의 인접행렬은 항상 대칭이다.</p>\n</li>\n<li><p>인접행령의 가중치를 추가할 경우 1과 0 값을 다른 숫자로 바꾼다.</p>\n</li>\n<li><p>인접행렬로 나타낸 그래프 구조들</p>\n<img src=\"https://www.researchgate.net/publication/347300725/figure/fig1/AS:969208926044162@1608088823984/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The.ppm\" alt=\"700\"/></li>\n</ul>\n<p><img src=\"https://i.imgur.com/GiStmNh.jpg\" alt=\"https://i.imgur.com/GiStmNh.jpg\"></p>\n<ul>\n<li>위 그래프를 인접행렬로 만들 경우 우선 아래와 같은 그림을 만들 수 있다.</li>\n</ul>\n<p><img src=\"https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true\" alt=\"https://github.com/Maiven/data-science/blob/main/%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%8C%E1%85%A5%E1%86%B8%E1%84%92%E1%85%A2%E1%86%BC%E1%84%85%E1%85%A7%E1%86%AF.png?raw=true\"></p>\n<ul>\n<li><p>List로 구현한 인접행렬</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 리스트로 구현한 인접행렬</span></span><br><span class=\"line\"><span class=\"comment\"># 아래 코드처럼 위의 간선 가중치는 1이다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.edges = [[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n</li>\n<li><p>위에서 행렬은 리스트 안에 리스트가 있는 2차원 배열로 표현된다.</p>\n<ul>\n<li>구현을 통해 기본 제공되는 행렬 간에 **edge weights(간선 가중치)**를 알 수 있다.</li>\n<li><strong>0은 관계가 없음</strong>을 나타내지만 다른 값은 <strong>edge label 또는 edge weight</strong>를 나타낸다.</li>\n<li>인접행렬의 단점은 <strong>노드 값과 해당 인덱스 사이에 연관성이 없다</strong>는 것이다.</li>\n</ul>\n</li>\n<li><p>실제로 인접리스트와 인접행렬을 모두 구현하면 Vertex(정점) 및 Edge(간선) 클래스를 포함하여 더 많은 정보를 파악할 수 있다.</p>\n</li>\n</ul>\n<h4 id=\"Weighted-Graph-구현하기\"><a href=\"#Weighted-Graph-구현하기\" class=\"headerlink\" title=\"Weighted Graph 구현하기\"></a>Weighted Graph 구현하기</h4><ul>\n<li>리스트로 그래프 가중치를 표현하는 것 보다 행렬로 구현하는 것이 쉽다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 인접리스트 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.vertices = &#123;</span><br><span class=\"line\">                            <span class=\"string\">&quot;A&quot;</span>: &#123;<span class=\"string\">&quot;B&quot;</span>: <span class=\"number\">1</span>&#125;,  <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;B&quot;</span>: &#123;<span class=\"string\">&quot;C&quot;</span>: <span class=\"number\">3</span>, <span class=\"string\">&quot;D&quot;</span>: <span class=\"number\">2</span>&#125;,  <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                            <span class=\"string\">&quot;C&quot;</span>: &#123;&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;D&quot;</span>: &#123;&#125;,</span><br><span class=\"line\">                            <span class=\"string\">&quot;E&quot;</span>: &#123;<span class=\"string\">&quot;D&quot;</span>: <span class=\"number\">1</span>&#125;   <span class=\"comment\"># 가중치 부여</span></span><br><span class=\"line\">                        &#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 인접행렬 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Graph</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.edges = [[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>],</span><br><span class=\"line\">                      [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]]</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"그래프에서의-복잡도-계산\"><a href=\"#그래프에서의-복잡도-계산\" class=\"headerlink\" title=\"그래프에서의 복잡도 계산\"></a>그래프에서의 복잡도 계산</h3><ul>\n<li>인접행렬은 특징은 <strong>구현이 쉽다</strong>는 것이다.<ul>\n<li>때문에 인접행렬의 가장 큰 단점은 <strong>특정노드에 방문한 노드들을 알기 위해서는 모든 노드를 확인</strong>해야 한다는 것이다. (시간복잡도 O(N))</li>\n<li>이러한 단점을 위해 인접리스트로 표현방식이 생겼다.</li>\n</ul>\n</li>\n<li>인접리스트는 실제 연결된 관계만을 저장해주기 때문에 실행시간에 영향을 적게 준다.<ul>\n<li>인접리스트의 단점은 <strong>특정 노드간의 연결관계를 확인하기 위해서는 반복문이 활용되어야 하며 따라서 O(N) 이상의 시간복잡도</strong> 가 발생한다는 것이다.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/ofykE5elSfI\">Graph Concept</a></li>\n<li><a href=\"https://ratsgo.github.io/data%20structure&algorithm/2017/11/18/graph/\">Graph Data Structure</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/graph-and-its-representations/\">Graph Representaion</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]Graph와 Graph Representation의 이해","path":"2022/06/13/Programming-Python-Graph-basic/","eyeCatchImage":"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python","Algorithm","Data Structure"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Algorithm]Hash Table과 Hash에 대한 이해","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n### Hash table\n\n\n---\n**_Concept_**\n\n- hash function : 데이터의 효율적 관리를 목적으로 임의의 길의의 데이터를 고정된 길이의 데이터로 매핑하는 함수.\n  - key : 매핑 전 원래 데이터의 값\n  - hashing value : 매핑 후 데이터의 값 \n- hash table : 해시함수를 이용해 키를 해시값으로 매핑하고 이 해시값을 index로 해서 데이터의 값을 키와 함께 빠르게 저장 및 검색할 수 있는 **테이블 형태의 자료구조**\n- hashing : 매핑하는 과정 자체를 뜻한다.해싱은 기본적으로 다 흩뜨려놓고, 키와 매칭되는 값을 검색하는 과정이다.\n- hash collision(해시충돌) : 서로 다른 두개의 키에 대해 동일한 해시값을 내는 것\n- Load Factor : 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값\n\n---\n\n:hash table은 기본적으로 **키를 활용하여 값에 직접 접근이 가능한 구조** 이다.\n\n- key : value system을 활용해 자료를 정리함\n  - -> 데이터 양에 영향을 덜 받으며 성능이 빠르다.\n- 해싱의 목적은 기본적으로 **검색**이다. -> 해시 테이블은 검색알고리즘의 역할도 한다.\n- Python Dictionary는 내부적으로 해시테이블 구조로 구현되어 있다.\n  - hash table은 검색을 위한 역할도 하고 딕셔너리를 위한 자료구조의 역할도 한다.\n\n- **hash table 사용이유**\n  - 기본적으로 적은 리소르로 많은 데이터를 효율적으로 관리할 수 있다\n    - 하드디스크나 클라우드에 존재하는 데이터(키) 들을 유한한 개수의 해시값으로 매핑함으로써 작은 크기의 캐쉬메모리로도 프로세스를 관리할 수 있게 된다.\n    - **index에 해시값을 사용함으로서 모든 데이터를 살피지 않아도 검색과 삽입/삭제를 빠르게 수행할 수 있습니다.**\n    - 해시함수는 언제나 동일한 해시값을 리턴하고 해당 index만 알면 해시테이블 크기에 상관 없이 데이터에 빠르게 접근할 수 있다.\n      - index는 계산이 간단한 함수로 작동하기 때무에 매우 효율적이다.\n\n- hash table in python\n\n```python\n\n# case 1 - 딕셔너리로 활용되는 hash table\n\ntest_code = {2.5: 'A' ,'2.0': 'B', '1.0': 'C'}\nprint(test_code[2.5]) \nprint(test_code['1.0'])\nprint(test_code['2.0'])\n\n```\n\n```python\n\n# case 2 - 리스트와 튜플을 활욯한 hash table\n# 데이터는 튜플로 저장\n\ntest_code = [(2.5, 'A'), ('2.0', 'B'), ('1.0', 'C')]\n \ndef insert(item_list, key, value):\n    item_list.append((key, value))\n\n\ndef search(item_list, key):\n    for item in item_list:    \n        if item[0] == key:\n            return item[1]      \n    print('not matching')       \n    \nprint(search(test_code, '2.0'))\nprint(search(test_code, 2.5))\nsearch(test_code, 2.5)    \n\n```\n\n- 딕셔너리를 활용한  hash table의 이해\n\n```python\n# 테이블에 값 할당\n\ndict = {}\n\ndict['a'] = 1\ndict['b'] = 2\ndict['c'] = 3\n\ndict\n\n```\n\n```python\n\n# hash table에 반복문 적용\n\nfor key in dict.keys():\n  print(dict[key])\n\n# {키, 쌍} 출력\n\nfor k, v in dict.items():\n  print(f\"key : {k} , value : {v}\")\n\n\n```\n\n#### hash function\n\n- 해시함수는 보통 문자열 입력값에 정수형 출력값을 반환한다.\n- 정수형에서 문자열로 변환하기 위해 해시함수는 문자열에 해당하는 개별 단어들을 활용한\n- 삽입, 검색, 삭제 무엇을 하든지 해시함수는 키를 통해 저장된 값에 연관된 인덱스를 반환한다.(키와 인덱스가 매칭되어야 한다.)\n  - -> 만약 해시테이블이 하나의 요소를 갖고 잇다면, 해시테이블 인덱스 갯수에 관계 겂이 프로그래밍 수행시간이 비슷하다\n\n```python\n\n# 굳이 리스트로 hash를 구현할 경우\n# 파이썬의 hash table 은 Dictionary이다.\n# Dictionary method로 삽입, 삭제, 검색을 수행할 수 있다.\n\ndef hash_func(str,list_size):\n    bytes_repr = str.encode()\n\n    #print(f\"str : {str}\")\n    #print(f\"str_encode : {str.encode()}\")\n    #print(f\"byte_repr : {bytes_repr}\")\n\n    sum = 0\n\n    for byte in bytes_repr:\n        sum += byte\n\n    return sum % list_size\n\n\nmy_list = [None] * 5 # 리스트 초기화: 중요 테크닉\n\n\nmy_list[hash_func(\"aqua\",len(my_list))] = \"#00FFFF\" # 삽입\n\n\n#print(my_list[hash_func(\"aqua\",len(my_list))]) # 리스트 값 출력\nprint(my_list[hash_func(\"aqua\",len(my_list))])\nprint(my_list)\n# print(hash_func(\"aqua\",len(my_list)))\n\n```\n- Python Hash table 구현하기\n\n```python\n# python hash table 구현\nclass hash_table:\n    # 키에 따른 값 초기화\n    def __init__(self):\n        self.table = [None] * 5\n    \n    # 기능3) name에 따라 특정값을 반환해주는 해시함수\n    def hash_function(self, name):\n        table_sum = 0\n\n        encoded = name.encode() # 문자열을 \n        for byte in encoded:\n            table_sum += byte\n\n        return table_sum % len(self.table) # 반환된 정수 값이 리스틔 인덱스(키) 가 된다\n\n    # name에 따라 num이 매칭되게끔 삽입\n    def hash_insert(self, name, num):\n        hash_key = self.hash_function(name) #\n        self.table[hash_key] = num # \n\n    # name에 따라 매칭되는 num 검색\n    def hash_search(self, name):\n        return self.table[self.hash_function(name)]\n\nht = hash_table()\n\nht.hash_insert('Kim', 1234)\nht.hash_insert('Johne', 5678)\nht.hash_insert('Smith', 1526)\nht.hash_insert('Michael', 3748)\n\n\nprint(ht.hash_search('Johne'))\n\n```\n\n\n#### hash 충돌\n\n- 해시함수가 서로 다른 두 개의 키에 대해 동일한 해시값을 내는 것을 해시 충돌이라고 한다.\n- 해시충돌은 보통 해쉬값의 개수보다 많은 키값을 해쉬값으로 변환하는 일대다 대응 때문에 발생한다.\n  - 키가 들어갈 자리(버킷)이 없는 경우에 발생한다.\n\n- 아래 그림의 경우 Sandra와 Jonn의 키가 같아 버킷 152에서 충돌이 발생한다.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png\" alt=\"700\"/>\n\n\n#### hash 충돌 방지 - chaining 활용\n\n- chaining은 충돌이 발생한 위 그림처럼 해시테이블에서 동일한 해시 값에 대해 충돌이 일어나면, 해당 위치에 있던 버킷에 키값을 뒤이어 연결하는 것이다.\n- 이때 데이터는 **해시값이 같은 노드를 연결하는** 연결리스트의 형태를 가진다.\n  - 따라서 특정 해시값에 대해 충돌이 발생하여도, 체이닝을 통해 값을 찾을 수 있다.\n\n```python\n\n# python hashtable chaining 구현\n\n\nchain_hash_table = [[] for _ in range(20)]\n\ndef chain_hash_func():\n  return key % len(chain_hash_table)\n\n# 키 값 쌍을 해시테이블에 삽입\n\ndef chain_insert_func(chain_hash_table, key, value):\n  hash_key = chain_hash_func(key)\n  chain_hash_table[hash_key].extend(value)\n\n\nchain_insert_func(chain_hash_table,20,\"C\")\nprint(chain_hash_table)\n\n```\n\n#### hash 충돌 방지 - open addressing\n\n- 하나의 버켓에 하나의 entry만 들어갈 수 있는 형태이다.(저장공간이 정해져있다.)\n- 기본적인 로직은 비어있는 배열 슬롯이 발견될 때까지 배열의 위치를 검색하는 것이다.\n- Chaining은 연결문제를 해결하여 충돌을 해결하고 Open Addressing은 내부적으로 공간이 정해진 배열을 활용하여 빈공간을 찾는 식으로 충돌을 해결한다.\n- close hashing이라고도 불린다.\n- 파이썬 자료형으로 구현된 hash table이 Dictionary이다.\n-   Dictioanary는 내부적으로 open addressing 방식을 활용한다.\n\n- **로드 팩터** : (Number of items in hash table) / (Total Number of Slots)\n  - 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값\n  - open addressing을 사용하면 최대 로드 팩터는 1정도 나온다.\n  - 체이닝을 사용할 경우 로드 팩터는 open addressing보다 좋은 성능을 보인다.\n  - 로드 팩터를 낮추면 해시에 대한 성능이 올라간다.\n\n```python\n# 파이썬으로 구현한 open addressing\nclass open_address:\n    def __init__(self, table_size):\n        self.size = table_size\n        self.hash_table = [0 for a in range(self.size)]\n        \n    def getKey(self, data):\n        self.key = ord(data[0])\n        return self.key\n    \n    def hashFunction(self, key):\n        return key % self.size\n\n    def getAddress(self, key):\n        myKey = self.getKey(key)\n        hash_address = self.hashFunction(myKey)\n        return hash_address\n    \n    def save(self, key, value):\n        hash_address = self.getAddress(key)\n        \n        if self.hash_table[hash_address] != 0:\n            for a in range(hash_address, len(self.hash_table)):\n                if self.hash_table[a] == 0:\n                    self.hash_table[a] = [key, value]\n                    return\n                elif self.hash_table[a][0] == key:\n                    self.hash_table[a] = [key, value]\n                    return\n            return None\n        else:\n            self.hash_table[hash_address] = [key, value]\n            \n    def read(self, key):\n        hash_address = self.getAddress(key)\n        \n        for a in range(hash_address, len(self.hash_table)):\n            if self.hash_table[a][0] == key:\n                return self.hash_table[a][1]\n        return None\n    \n    def delete(self, key):\n        hash_address = self.getAddress(key)\n        \n        for a in range(hash_address, len(self.hash_table)):\n            if self.hash_table[a] == 0:\n                continue\n            if self.hash_table[a][0] == key:\n                self.hash_table[a] = 0\n                return\n        return False\n        \n        \n#Test Code\n#h_table = CloseHash(8)\n\nh_table = open_address(8)\n\ndata1 = 'aa'\ndata2 = 'ad'\nprint(ord(data1[0]), ord(data2[0]))\n\nh_table.save('aa', '3333')\nh_table.save('ad', '9999')\nprint(h_table.hash_table)\n\nh_table.read('ad')\n\nh_table.delete('aa')\nprint(h_table.hash_table)\n\nh_table.delete('ad')\nprint(h_table.hash_table)\n\n```\n\n**References & annotation**\n---\n\n- [hash table wikipedia](https://en.wikipedia.org/wiki/Hash_table)\n- [참조 블로그](https://ratsgo.github.io/data%20structure&algorithm/2017/10/25/hash/)\n- [hashnet hash](http://wiki.hash.kr/index.php/%ED%95%B4%EC%8B%9C_%ED%85%8C%EC%9D%B4%EB%B8%94)\n\n","source":"_posts/Programming-Python-hash-table.md","raw":"---\ntitle: '[Algorithm]Hash Table과 Hash에 대한 이해'\ncategories:\n  - Programming\ntags:\n  - Python\n  - Algorithm\n  - Data Structure\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n### Hash table\n\n\n---\n**_Concept_**\n\n- hash function : 데이터의 효율적 관리를 목적으로 임의의 길의의 데이터를 고정된 길이의 데이터로 매핑하는 함수.\n  - key : 매핑 전 원래 데이터의 값\n  - hashing value : 매핑 후 데이터의 값 \n- hash table : 해시함수를 이용해 키를 해시값으로 매핑하고 이 해시값을 index로 해서 데이터의 값을 키와 함께 빠르게 저장 및 검색할 수 있는 **테이블 형태의 자료구조**\n- hashing : 매핑하는 과정 자체를 뜻한다.해싱은 기본적으로 다 흩뜨려놓고, 키와 매칭되는 값을 검색하는 과정이다.\n- hash collision(해시충돌) : 서로 다른 두개의 키에 대해 동일한 해시값을 내는 것\n- Load Factor : 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값\n\n---\n\n:hash table은 기본적으로 **키를 활용하여 값에 직접 접근이 가능한 구조** 이다.\n\n- key : value system을 활용해 자료를 정리함\n  - -> 데이터 양에 영향을 덜 받으며 성능이 빠르다.\n- 해싱의 목적은 기본적으로 **검색**이다. -> 해시 테이블은 검색알고리즘의 역할도 한다.\n- Python Dictionary는 내부적으로 해시테이블 구조로 구현되어 있다.\n  - hash table은 검색을 위한 역할도 하고 딕셔너리를 위한 자료구조의 역할도 한다.\n\n- **hash table 사용이유**\n  - 기본적으로 적은 리소르로 많은 데이터를 효율적으로 관리할 수 있다\n    - 하드디스크나 클라우드에 존재하는 데이터(키) 들을 유한한 개수의 해시값으로 매핑함으로써 작은 크기의 캐쉬메모리로도 프로세스를 관리할 수 있게 된다.\n    - **index에 해시값을 사용함으로서 모든 데이터를 살피지 않아도 검색과 삽입/삭제를 빠르게 수행할 수 있습니다.**\n    - 해시함수는 언제나 동일한 해시값을 리턴하고 해당 index만 알면 해시테이블 크기에 상관 없이 데이터에 빠르게 접근할 수 있다.\n      - index는 계산이 간단한 함수로 작동하기 때무에 매우 효율적이다.\n\n- hash table in python\n\n```python\n\n# case 1 - 딕셔너리로 활용되는 hash table\n\ntest_code = {2.5: 'A' ,'2.0': 'B', '1.0': 'C'}\nprint(test_code[2.5]) \nprint(test_code['1.0'])\nprint(test_code['2.0'])\n\n```\n\n```python\n\n# case 2 - 리스트와 튜플을 활욯한 hash table\n# 데이터는 튜플로 저장\n\ntest_code = [(2.5, 'A'), ('2.0', 'B'), ('1.0', 'C')]\n \ndef insert(item_list, key, value):\n    item_list.append((key, value))\n\n\ndef search(item_list, key):\n    for item in item_list:    \n        if item[0] == key:\n            return item[1]      \n    print('not matching')       \n    \nprint(search(test_code, '2.0'))\nprint(search(test_code, 2.5))\nsearch(test_code, 2.5)    \n\n```\n\n- 딕셔너리를 활용한  hash table의 이해\n\n```python\n# 테이블에 값 할당\n\ndict = {}\n\ndict['a'] = 1\ndict['b'] = 2\ndict['c'] = 3\n\ndict\n\n```\n\n```python\n\n# hash table에 반복문 적용\n\nfor key in dict.keys():\n  print(dict[key])\n\n# {키, 쌍} 출력\n\nfor k, v in dict.items():\n  print(f\"key : {k} , value : {v}\")\n\n\n```\n\n#### hash function\n\n- 해시함수는 보통 문자열 입력값에 정수형 출력값을 반환한다.\n- 정수형에서 문자열로 변환하기 위해 해시함수는 문자열에 해당하는 개별 단어들을 활용한\n- 삽입, 검색, 삭제 무엇을 하든지 해시함수는 키를 통해 저장된 값에 연관된 인덱스를 반환한다.(키와 인덱스가 매칭되어야 한다.)\n  - -> 만약 해시테이블이 하나의 요소를 갖고 잇다면, 해시테이블 인덱스 갯수에 관계 겂이 프로그래밍 수행시간이 비슷하다\n\n```python\n\n# 굳이 리스트로 hash를 구현할 경우\n# 파이썬의 hash table 은 Dictionary이다.\n# Dictionary method로 삽입, 삭제, 검색을 수행할 수 있다.\n\ndef hash_func(str,list_size):\n    bytes_repr = str.encode()\n\n    #print(f\"str : {str}\")\n    #print(f\"str_encode : {str.encode()}\")\n    #print(f\"byte_repr : {bytes_repr}\")\n\n    sum = 0\n\n    for byte in bytes_repr:\n        sum += byte\n\n    return sum % list_size\n\n\nmy_list = [None] * 5 # 리스트 초기화: 중요 테크닉\n\n\nmy_list[hash_func(\"aqua\",len(my_list))] = \"#00FFFF\" # 삽입\n\n\n#print(my_list[hash_func(\"aqua\",len(my_list))]) # 리스트 값 출력\nprint(my_list[hash_func(\"aqua\",len(my_list))])\nprint(my_list)\n# print(hash_func(\"aqua\",len(my_list)))\n\n```\n- Python Hash table 구현하기\n\n```python\n# python hash table 구현\nclass hash_table:\n    # 키에 따른 값 초기화\n    def __init__(self):\n        self.table = [None] * 5\n    \n    # 기능3) name에 따라 특정값을 반환해주는 해시함수\n    def hash_function(self, name):\n        table_sum = 0\n\n        encoded = name.encode() # 문자열을 \n        for byte in encoded:\n            table_sum += byte\n\n        return table_sum % len(self.table) # 반환된 정수 값이 리스틔 인덱스(키) 가 된다\n\n    # name에 따라 num이 매칭되게끔 삽입\n    def hash_insert(self, name, num):\n        hash_key = self.hash_function(name) #\n        self.table[hash_key] = num # \n\n    # name에 따라 매칭되는 num 검색\n    def hash_search(self, name):\n        return self.table[self.hash_function(name)]\n\nht = hash_table()\n\nht.hash_insert('Kim', 1234)\nht.hash_insert('Johne', 5678)\nht.hash_insert('Smith', 1526)\nht.hash_insert('Michael', 3748)\n\n\nprint(ht.hash_search('Johne'))\n\n```\n\n\n#### hash 충돌\n\n- 해시함수가 서로 다른 두 개의 키에 대해 동일한 해시값을 내는 것을 해시 충돌이라고 한다.\n- 해시충돌은 보통 해쉬값의 개수보다 많은 키값을 해쉬값으로 변환하는 일대다 대응 때문에 발생한다.\n  - 키가 들어갈 자리(버킷)이 없는 경우에 발생한다.\n\n- 아래 그림의 경우 Sandra와 Jonn의 키가 같아 버킷 152에서 충돌이 발생한다.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png\" alt=\"700\"/>\n\n\n#### hash 충돌 방지 - chaining 활용\n\n- chaining은 충돌이 발생한 위 그림처럼 해시테이블에서 동일한 해시 값에 대해 충돌이 일어나면, 해당 위치에 있던 버킷에 키값을 뒤이어 연결하는 것이다.\n- 이때 데이터는 **해시값이 같은 노드를 연결하는** 연결리스트의 형태를 가진다.\n  - 따라서 특정 해시값에 대해 충돌이 발생하여도, 체이닝을 통해 값을 찾을 수 있다.\n\n```python\n\n# python hashtable chaining 구현\n\n\nchain_hash_table = [[] for _ in range(20)]\n\ndef chain_hash_func():\n  return key % len(chain_hash_table)\n\n# 키 값 쌍을 해시테이블에 삽입\n\ndef chain_insert_func(chain_hash_table, key, value):\n  hash_key = chain_hash_func(key)\n  chain_hash_table[hash_key].extend(value)\n\n\nchain_insert_func(chain_hash_table,20,\"C\")\nprint(chain_hash_table)\n\n```\n\n#### hash 충돌 방지 - open addressing\n\n- 하나의 버켓에 하나의 entry만 들어갈 수 있는 형태이다.(저장공간이 정해져있다.)\n- 기본적인 로직은 비어있는 배열 슬롯이 발견될 때까지 배열의 위치를 검색하는 것이다.\n- Chaining은 연결문제를 해결하여 충돌을 해결하고 Open Addressing은 내부적으로 공간이 정해진 배열을 활용하여 빈공간을 찾는 식으로 충돌을 해결한다.\n- close hashing이라고도 불린다.\n- 파이썬 자료형으로 구현된 hash table이 Dictionary이다.\n-   Dictioanary는 내부적으로 open addressing 방식을 활용한다.\n\n- **로드 팩터** : (Number of items in hash table) / (Total Number of Slots)\n  - 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값\n  - open addressing을 사용하면 최대 로드 팩터는 1정도 나온다.\n  - 체이닝을 사용할 경우 로드 팩터는 open addressing보다 좋은 성능을 보인다.\n  - 로드 팩터를 낮추면 해시에 대한 성능이 올라간다.\n\n```python\n# 파이썬으로 구현한 open addressing\nclass open_address:\n    def __init__(self, table_size):\n        self.size = table_size\n        self.hash_table = [0 for a in range(self.size)]\n        \n    def getKey(self, data):\n        self.key = ord(data[0])\n        return self.key\n    \n    def hashFunction(self, key):\n        return key % self.size\n\n    def getAddress(self, key):\n        myKey = self.getKey(key)\n        hash_address = self.hashFunction(myKey)\n        return hash_address\n    \n    def save(self, key, value):\n        hash_address = self.getAddress(key)\n        \n        if self.hash_table[hash_address] != 0:\n            for a in range(hash_address, len(self.hash_table)):\n                if self.hash_table[a] == 0:\n                    self.hash_table[a] = [key, value]\n                    return\n                elif self.hash_table[a][0] == key:\n                    self.hash_table[a] = [key, value]\n                    return\n            return None\n        else:\n            self.hash_table[hash_address] = [key, value]\n            \n    def read(self, key):\n        hash_address = self.getAddress(key)\n        \n        for a in range(hash_address, len(self.hash_table)):\n            if self.hash_table[a][0] == key:\n                return self.hash_table[a][1]\n        return None\n    \n    def delete(self, key):\n        hash_address = self.getAddress(key)\n        \n        for a in range(hash_address, len(self.hash_table)):\n            if self.hash_table[a] == 0:\n                continue\n            if self.hash_table[a][0] == key:\n                self.hash_table[a] = 0\n                return\n        return False\n        \n        \n#Test Code\n#h_table = CloseHash(8)\n\nh_table = open_address(8)\n\ndata1 = 'aa'\ndata2 = 'ad'\nprint(ord(data1[0]), ord(data2[0]))\n\nh_table.save('aa', '3333')\nh_table.save('ad', '9999')\nprint(h_table.hash_table)\n\nh_table.read('ad')\n\nh_table.delete('aa')\nprint(h_table.hash_table)\n\nh_table.delete('ad')\nprint(h_table.hash_table)\n\n```\n\n**References & annotation**\n---\n\n- [hash table wikipedia](https://en.wikipedia.org/wiki/Hash_table)\n- [참조 블로그](https://ratsgo.github.io/data%20structure&algorithm/2017/10/25/hash/)\n- [hashnet hash](http://wiki.hash.kr/index.php/%ED%95%B4%EC%8B%9C_%ED%85%8C%EC%9D%B4%EB%B8%94)\n\n","slug":"Programming-Python-hash-table","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsck002ub36q8e2w3b0y","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h3 id=\"Hash-table\"><a href=\"#Hash-table\" class=\"headerlink\" title=\"Hash table\"></a>Hash table</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li>hash function : 데이터의 효율적 관리를 목적으로 임의의 길의의 데이터를 고정된 길이의 데이터로 매핑하는 함수.<ul>\n<li>key : 매핑 전 원래 데이터의 값</li>\n<li>hashing value : 매핑 후 데이터의 값</li>\n</ul>\n</li>\n<li>hash table : 해시함수를 이용해 키를 해시값으로 매핑하고 이 해시값을 index로 해서 데이터의 값을 키와 함께 빠르게 저장 및 검색할 수 있는 <strong>테이블 형태의 자료구조</strong></li>\n<li>hashing : 매핑하는 과정 자체를 뜻한다.해싱은 기본적으로 다 흩뜨려놓고, 키와 매칭되는 값을 검색하는 과정이다.</li>\n<li>hash collision(해시충돌) : 서로 다른 두개의 키에 대해 동일한 해시값을 내는 것</li>\n<li>Load Factor : 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값</li>\n</ul>\n<hr>\n<p>:hash table은 기본적으로 <strong>키를 활용하여 값에 직접 접근이 가능한 구조</strong> 이다.</p>\n<ul>\n<li><p>key : value system을 활용해 자료를 정리함</p>\n<ul>\n<li>-&gt; 데이터 양에 영향을 덜 받으며 성능이 빠르다.</li>\n</ul>\n</li>\n<li><p>해싱의 목적은 기본적으로 <strong>검색</strong>이다. -&gt; 해시 테이블은 검색알고리즘의 역할도 한다.</p>\n</li>\n<li><p>Python Dictionary는 내부적으로 해시테이블 구조로 구현되어 있다.</p>\n<ul>\n<li>hash table은 검색을 위한 역할도 하고 딕셔너리를 위한 자료구조의 역할도 한다.</li>\n</ul>\n</li>\n<li><p><strong>hash table 사용이유</strong></p>\n<ul>\n<li>기본적으로 적은 리소르로 많은 데이터를 효율적으로 관리할 수 있다<ul>\n<li>하드디스크나 클라우드에 존재하는 데이터(키) 들을 유한한 개수의 해시값으로 매핑함으로써 작은 크기의 캐쉬메모리로도 프로세스를 관리할 수 있게 된다.</li>\n<li><strong>index에 해시값을 사용함으로서 모든 데이터를 살피지 않아도 검색과 삽입&#x2F;삭제를 빠르게 수행할 수 있습니다.</strong></li>\n<li>해시함수는 언제나 동일한 해시값을 리턴하고 해당 index만 알면 해시테이블 크기에 상관 없이 데이터에 빠르게 접근할 수 있다.<ul>\n<li>index는 계산이 간단한 함수로 작동하기 때무에 매우 효율적이다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>hash table in python</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># case 1 - 딕셔너리로 활용되는 hash table</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_code = &#123;<span class=\"number\">2.5</span>: <span class=\"string\">&#x27;A&#x27;</span> ,<span class=\"string\">&#x27;2.0&#x27;</span>: <span class=\"string\">&#x27;B&#x27;</span>, <span class=\"string\">&#x27;1.0&#x27;</span>: <span class=\"string\">&#x27;C&#x27;</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"number\">2.5</span>]) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"string\">&#x27;1.0&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"string\">&#x27;2.0&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># case 2 - 리스트와 튜플을 활욯한 hash table</span></span><br><span class=\"line\"><span class=\"comment\"># 데이터는 튜플로 저장</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_code = [(<span class=\"number\">2.5</span>, <span class=\"string\">&#x27;A&#x27;</span>), (<span class=\"string\">&#x27;2.0&#x27;</span>, <span class=\"string\">&#x27;B&#x27;</span>), (<span class=\"string\">&#x27;1.0&#x27;</span>, <span class=\"string\">&#x27;C&#x27;</span>)]</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">item_list, key, value</span>):</span><br><span class=\"line\">    item_list.append((key, value))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">item_list, key</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> item_list:    </span><br><span class=\"line\">        <span class=\"keyword\">if</span> item[<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> item[<span class=\"number\">1</span>]      </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;not matching&#x27;</span>)       </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(search(test_code, <span class=\"string\">&#x27;2.0&#x27;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(search(test_code, <span class=\"number\">2.5</span>))</span><br><span class=\"line\">search(test_code, <span class=\"number\">2.5</span>)    </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>딕셔너리를 활용한  hash table의 이해</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 테이블에 값 할당</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span> = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;a&#x27;</span>] = <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;b&#x27;</span>] = <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;c&#x27;</span>] = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># hash table에 반복문 적용</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"built_in\">dict</span>.keys():</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"built_in\">dict</span>[key])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># &#123;키, 쌍&#125; 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> <span class=\"built_in\">dict</span>.items():</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;key : <span class=\"subst\">&#123;k&#125;</span> , value : <span class=\"subst\">&#123;v&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"hash-function\"><a href=\"#hash-function\" class=\"headerlink\" title=\"hash function\"></a>hash function</h4><ul>\n<li>해시함수는 보통 문자열 입력값에 정수형 출력값을 반환한다.</li>\n<li>정수형에서 문자열로 변환하기 위해 해시함수는 문자열에 해당하는 개별 단어들을 활용한</li>\n<li>삽입, 검색, 삭제 무엇을 하든지 해시함수는 키를 통해 저장된 값에 연관된 인덱스를 반환한다.(키와 인덱스가 매칭되어야 한다.)<ul>\n<li>-&gt; 만약 해시테이블이 하나의 요소를 갖고 잇다면, 해시테이블 인덱스 갯수에 관계 겂이 프로그래밍 수행시간이 비슷하다</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 굳이 리스트로 hash를 구현할 경우</span></span><br><span class=\"line\"><span class=\"comment\"># 파이썬의 hash table 은 Dictionary이다.</span></span><br><span class=\"line\"><span class=\"comment\"># Dictionary method로 삽입, 삭제, 검색을 수행할 수 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">hash_func</span>(<span class=\"params\"><span class=\"built_in\">str</span>,list_size</span>):</span><br><span class=\"line\">    bytes_repr = <span class=\"built_in\">str</span>.encode()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;str : &#123;str&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;str_encode : &#123;str.encode()&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;byte_repr : &#123;bytes_repr&#125;&quot;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">sum</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> byte <span class=\"keyword\">in</span> bytes_repr:</span><br><span class=\"line\">        <span class=\"built_in\">sum</span> += byte</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span> % list_size</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">my_list = [<span class=\"literal\">None</span>] * <span class=\"number\">5</span> <span class=\"comment\"># 리스트 초기화: 중요 테크닉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">my_list[hash_func(<span class=\"string\">&quot;aqua&quot;</span>,<span class=\"built_in\">len</span>(my_list))] = <span class=\"string\">&quot;#00FFFF&quot;</span> <span class=\"comment\"># 삽입</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#print(my_list[hash_func(&quot;aqua&quot;,len(my_list))]) # 리스트 값 출력</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(my_list[hash_func(<span class=\"string\">&quot;aqua&quot;</span>,<span class=\"built_in\">len</span>(my_list))])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(my_list)</span><br><span class=\"line\"><span class=\"comment\"># print(hash_func(&quot;aqua&quot;,len(my_list)))</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Python Hash table 구현하기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># python hash table 구현</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">hash_table</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 키에 따른 값 초기화</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.table = [<span class=\"literal\">None</span>] * <span class=\"number\">5</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 기능3) name에 따라 특정값을 반환해주는 해시함수</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_function</span>(<span class=\"params\">self, name</span>):</span><br><span class=\"line\">        table_sum = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        encoded = name.encode() <span class=\"comment\"># 문자열을 </span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> byte <span class=\"keyword\">in</span> encoded:</span><br><span class=\"line\">            table_sum += byte</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> table_sum % <span class=\"built_in\">len</span>(self.table) <span class=\"comment\"># 반환된 정수 값이 리스틔 인덱스(키) 가 된다</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># name에 따라 num이 매칭되게끔 삽입</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_insert</span>(<span class=\"params\">self, name, num</span>):</span><br><span class=\"line\">        hash_key = self.hash_function(name) <span class=\"comment\">#</span></span><br><span class=\"line\">        self.table[hash_key] = num <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># name에 따라 매칭되는 num 검색</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_search</span>(<span class=\"params\">self, name</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.table[self.hash_function(name)]</span><br><span class=\"line\"></span><br><span class=\"line\">ht = hash_table()</span><br><span class=\"line\"></span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Kim&#x27;</span>, <span class=\"number\">1234</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Johne&#x27;</span>, <span class=\"number\">5678</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Smith&#x27;</span>, <span class=\"number\">1526</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Michael&#x27;</span>, <span class=\"number\">3748</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(ht.hash_search(<span class=\"string\">&#x27;Johne&#x27;</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"hash-충돌\"><a href=\"#hash-충돌\" class=\"headerlink\" title=\"hash 충돌\"></a>hash 충돌</h4><ul>\n<li><p>해시함수가 서로 다른 두 개의 키에 대해 동일한 해시값을 내는 것을 해시 충돌이라고 한다.</p>\n</li>\n<li><p>해시충돌은 보통 해쉬값의 개수보다 많은 키값을 해쉬값으로 변환하는 일대다 대응 때문에 발생한다.</p>\n<ul>\n<li>키가 들어갈 자리(버킷)이 없는 경우에 발생한다.</li>\n</ul>\n</li>\n<li><p>아래 그림의 경우 Sandra와 Jonn의 키가 같아 버킷 152에서 충돌이 발생한다.</p>\n</li>\n</ul>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png\" alt=\"700\"/>\n\n\n<h4 id=\"hash-충돌-방지-chaining-활용\"><a href=\"#hash-충돌-방지-chaining-활용\" class=\"headerlink\" title=\"hash 충돌 방지 - chaining 활용\"></a>hash 충돌 방지 - chaining 활용</h4><ul>\n<li>chaining은 충돌이 발생한 위 그림처럼 해시테이블에서 동일한 해시 값에 대해 충돌이 일어나면, 해당 위치에 있던 버킷에 키값을 뒤이어 연결하는 것이다.</li>\n<li>이때 데이터는 <strong>해시값이 같은 노드를 연결하는</strong> 연결리스트의 형태를 가진다.<ul>\n<li>따라서 특정 해시값에 대해 충돌이 발생하여도, 체이닝을 통해 값을 찾을 수 있다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># python hashtable chaining 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">chain_hash_table = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">chain_hash_func</span>():</span><br><span class=\"line\">  <span class=\"keyword\">return</span> key % <span class=\"built_in\">len</span>(chain_hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 키 값 쌍을 해시테이블에 삽입</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">chain_insert_func</span>(<span class=\"params\">chain_hash_table, key, value</span>):</span><br><span class=\"line\">  hash_key = chain_hash_func(key)</span><br><span class=\"line\">  chain_hash_table[hash_key].extend(value)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">chain_insert_func(chain_hash_table,<span class=\"number\">20</span>,<span class=\"string\">&quot;C&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_hash_table)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"hash-충돌-방지-open-addressing\"><a href=\"#hash-충돌-방지-open-addressing\" class=\"headerlink\" title=\"hash 충돌 방지 - open addressing\"></a>hash 충돌 방지 - open addressing</h4><ul>\n<li><p>하나의 버켓에 하나의 entry만 들어갈 수 있는 형태이다.(저장공간이 정해져있다.)</p>\n</li>\n<li><p>기본적인 로직은 비어있는 배열 슬롯이 발견될 때까지 배열의 위치를 검색하는 것이다.</p>\n</li>\n<li><p>Chaining은 연결문제를 해결하여 충돌을 해결하고 Open Addressing은 내부적으로 공간이 정해진 배열을 활용하여 빈공간을 찾는 식으로 충돌을 해결한다.</p>\n</li>\n<li><p>close hashing이라고도 불린다.</p>\n</li>\n<li><p>파이썬 자료형으로 구현된 hash table이 Dictionary이다.</p>\n</li>\n<li><p>Dictioanary는 내부적으로 open addressing 방식을 활용한다.</p>\n</li>\n<li><p><strong>로드 팩터</strong> : (Number of items in hash table) &#x2F; (Total Number of Slots)</p>\n<ul>\n<li>해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값</li>\n<li>open addressing을 사용하면 최대 로드 팩터는 1정도 나온다.</li>\n<li>체이닝을 사용할 경우 로드 팩터는 open addressing보다 좋은 성능을 보인다.</li>\n<li>로드 팩터를 낮추면 해시에 대한 성능이 올라간다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 파이썬으로 구현한 open addressing</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">open_address</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, table_size</span>):</span><br><span class=\"line\">        self.size = table_size</span><br><span class=\"line\">        self.hash_table = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.size)]</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">getKey</span>(<span class=\"params\">self, data</span>):</span><br><span class=\"line\">        self.key = <span class=\"built_in\">ord</span>(data[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.key</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hashFunction</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> key % self.size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">getAddress</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        myKey = self.getKey(key)</span><br><span class=\"line\">        hash_address = self.hashFunction(myKey)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hash_address</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">save</span>(<span class=\"params\">self, key, value</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.hash_table[hash_address] != <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> self.hash_table[a] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                    self.hash_table[a] = [key, value]</span><br><span class=\"line\">                    <span class=\"keyword\">return</span></span><br><span class=\"line\">                <span class=\"keyword\">elif</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                    self.hash_table[a] = [key, value]</span><br><span class=\"line\">                    <span class=\"keyword\">return</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.hash_table[hash_address] = [key, value]</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">read</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.hash_table[a][<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">delete</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                self.hash_table[a] = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"comment\">#Test Code</span></span><br><span class=\"line\"><span class=\"comment\">#h_table = CloseHash(8)</span></span><br><span class=\"line\"></span><br><span class=\"line\">h_table = open_address(<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data1 = <span class=\"string\">&#x27;aa&#x27;</span></span><br><span class=\"line\">data2 = <span class=\"string\">&#x27;ad&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">ord</span>(data1[<span class=\"number\">0</span>]), <span class=\"built_in\">ord</span>(data2[<span class=\"number\">0</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.save(<span class=\"string\">&#x27;aa&#x27;</span>, <span class=\"string\">&#x27;3333&#x27;</span>)</span><br><span class=\"line\">h_table.save(<span class=\"string\">&#x27;ad&#x27;</span>, <span class=\"string\">&#x27;9999&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.read(<span class=\"string\">&#x27;ad&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.delete(<span class=\"string\">&#x27;aa&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.delete(<span class=\"string\">&#x27;ad&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Hash_table\">hash table wikipedia</a></li>\n<li><a href=\"https://ratsgo.github.io/data%20structure&algorithm/2017/10/25/hash/\">참조 블로그</a></li>\n<li><a href=\"http://wiki.hash.kr/index.php/%ED%95%B4%EC%8B%9C_%ED%85%8C%EC%9D%B4%EB%B8%94\">hashnet hash</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h3 id=\"Hash-table\"><a href=\"#Hash-table\" class=\"headerlink\" title=\"Hash table\"></a>Hash table</h3><hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li>hash function : 데이터의 효율적 관리를 목적으로 임의의 길의의 데이터를 고정된 길이의 데이터로 매핑하는 함수.<ul>\n<li>key : 매핑 전 원래 데이터의 값</li>\n<li>hashing value : 매핑 후 데이터의 값</li>\n</ul>\n</li>\n<li>hash table : 해시함수를 이용해 키를 해시값으로 매핑하고 이 해시값을 index로 해서 데이터의 값을 키와 함께 빠르게 저장 및 검색할 수 있는 <strong>테이블 형태의 자료구조</strong></li>\n<li>hashing : 매핑하는 과정 자체를 뜻한다.해싱은 기본적으로 다 흩뜨려놓고, 키와 매칭되는 값을 검색하는 과정이다.</li>\n<li>hash collision(해시충돌) : 서로 다른 두개의 키에 대해 동일한 해시값을 내는 것</li>\n<li>Load Factor : 해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값</li>\n</ul>\n<hr>\n<p>:hash table은 기본적으로 <strong>키를 활용하여 값에 직접 접근이 가능한 구조</strong> 이다.</p>\n<ul>\n<li><p>key : value system을 활용해 자료를 정리함</p>\n<ul>\n<li>-&gt; 데이터 양에 영향을 덜 받으며 성능이 빠르다.</li>\n</ul>\n</li>\n<li><p>해싱의 목적은 기본적으로 <strong>검색</strong>이다. -&gt; 해시 테이블은 검색알고리즘의 역할도 한다.</p>\n</li>\n<li><p>Python Dictionary는 내부적으로 해시테이블 구조로 구현되어 있다.</p>\n<ul>\n<li>hash table은 검색을 위한 역할도 하고 딕셔너리를 위한 자료구조의 역할도 한다.</li>\n</ul>\n</li>\n<li><p><strong>hash table 사용이유</strong></p>\n<ul>\n<li>기본적으로 적은 리소르로 많은 데이터를 효율적으로 관리할 수 있다<ul>\n<li>하드디스크나 클라우드에 존재하는 데이터(키) 들을 유한한 개수의 해시값으로 매핑함으로써 작은 크기의 캐쉬메모리로도 프로세스를 관리할 수 있게 된다.</li>\n<li><strong>index에 해시값을 사용함으로서 모든 데이터를 살피지 않아도 검색과 삽입&#x2F;삭제를 빠르게 수행할 수 있습니다.</strong></li>\n<li>해시함수는 언제나 동일한 해시값을 리턴하고 해당 index만 알면 해시테이블 크기에 상관 없이 데이터에 빠르게 접근할 수 있다.<ul>\n<li>index는 계산이 간단한 함수로 작동하기 때무에 매우 효율적이다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>hash table in python</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># case 1 - 딕셔너리로 활용되는 hash table</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_code = &#123;<span class=\"number\">2.5</span>: <span class=\"string\">&#x27;A&#x27;</span> ,<span class=\"string\">&#x27;2.0&#x27;</span>: <span class=\"string\">&#x27;B&#x27;</span>, <span class=\"string\">&#x27;1.0&#x27;</span>: <span class=\"string\">&#x27;C&#x27;</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"number\">2.5</span>]) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"string\">&#x27;1.0&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(test_code[<span class=\"string\">&#x27;2.0&#x27;</span>])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># case 2 - 리스트와 튜플을 활욯한 hash table</span></span><br><span class=\"line\"><span class=\"comment\"># 데이터는 튜플로 저장</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_code = [(<span class=\"number\">2.5</span>, <span class=\"string\">&#x27;A&#x27;</span>), (<span class=\"string\">&#x27;2.0&#x27;</span>, <span class=\"string\">&#x27;B&#x27;</span>), (<span class=\"string\">&#x27;1.0&#x27;</span>, <span class=\"string\">&#x27;C&#x27;</span>)]</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">item_list, key, value</span>):</span><br><span class=\"line\">    item_list.append((key, value))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">item_list, key</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> item <span class=\"keyword\">in</span> item_list:    </span><br><span class=\"line\">        <span class=\"keyword\">if</span> item[<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> item[<span class=\"number\">1</span>]      </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;not matching&#x27;</span>)       </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(search(test_code, <span class=\"string\">&#x27;2.0&#x27;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(search(test_code, <span class=\"number\">2.5</span>))</span><br><span class=\"line\">search(test_code, <span class=\"number\">2.5</span>)    </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>딕셔너리를 활용한  hash table의 이해</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 테이블에 값 할당</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span> = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;a&#x27;</span>] = <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;b&#x27;</span>] = <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"built_in\">dict</span>[<span class=\"string\">&#x27;c&#x27;</span>] = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">dict</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># hash table에 반복문 적용</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"built_in\">dict</span>.keys():</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"built_in\">dict</span>[key])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># &#123;키, 쌍&#125; 출력</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> <span class=\"built_in\">dict</span>.items():</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;key : <span class=\"subst\">&#123;k&#125;</span> , value : <span class=\"subst\">&#123;v&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"hash-function\"><a href=\"#hash-function\" class=\"headerlink\" title=\"hash function\"></a>hash function</h4><ul>\n<li>해시함수는 보통 문자열 입력값에 정수형 출력값을 반환한다.</li>\n<li>정수형에서 문자열로 변환하기 위해 해시함수는 문자열에 해당하는 개별 단어들을 활용한</li>\n<li>삽입, 검색, 삭제 무엇을 하든지 해시함수는 키를 통해 저장된 값에 연관된 인덱스를 반환한다.(키와 인덱스가 매칭되어야 한다.)<ul>\n<li>-&gt; 만약 해시테이블이 하나의 요소를 갖고 잇다면, 해시테이블 인덱스 갯수에 관계 겂이 프로그래밍 수행시간이 비슷하다</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 굳이 리스트로 hash를 구현할 경우</span></span><br><span class=\"line\"><span class=\"comment\"># 파이썬의 hash table 은 Dictionary이다.</span></span><br><span class=\"line\"><span class=\"comment\"># Dictionary method로 삽입, 삭제, 검색을 수행할 수 있다.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">hash_func</span>(<span class=\"params\"><span class=\"built_in\">str</span>,list_size</span>):</span><br><span class=\"line\">    bytes_repr = <span class=\"built_in\">str</span>.encode()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;str : &#123;str&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;str_encode : &#123;str.encode()&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\">#print(f&quot;byte_repr : &#123;bytes_repr&#125;&quot;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">sum</span> = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> byte <span class=\"keyword\">in</span> bytes_repr:</span><br><span class=\"line\">        <span class=\"built_in\">sum</span> += byte</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span> % list_size</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">my_list = [<span class=\"literal\">None</span>] * <span class=\"number\">5</span> <span class=\"comment\"># 리스트 초기화: 중요 테크닉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">my_list[hash_func(<span class=\"string\">&quot;aqua&quot;</span>,<span class=\"built_in\">len</span>(my_list))] = <span class=\"string\">&quot;#00FFFF&quot;</span> <span class=\"comment\"># 삽입</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#print(my_list[hash_func(&quot;aqua&quot;,len(my_list))]) # 리스트 값 출력</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(my_list[hash_func(<span class=\"string\">&quot;aqua&quot;</span>,<span class=\"built_in\">len</span>(my_list))])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(my_list)</span><br><span class=\"line\"><span class=\"comment\"># print(hash_func(&quot;aqua&quot;,len(my_list)))</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Python Hash table 구현하기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># python hash table 구현</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">hash_table</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 키에 따른 값 초기화</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.table = [<span class=\"literal\">None</span>] * <span class=\"number\">5</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 기능3) name에 따라 특정값을 반환해주는 해시함수</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_function</span>(<span class=\"params\">self, name</span>):</span><br><span class=\"line\">        table_sum = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        encoded = name.encode() <span class=\"comment\"># 문자열을 </span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> byte <span class=\"keyword\">in</span> encoded:</span><br><span class=\"line\">            table_sum += byte</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> table_sum % <span class=\"built_in\">len</span>(self.table) <span class=\"comment\"># 반환된 정수 값이 리스틔 인덱스(키) 가 된다</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># name에 따라 num이 매칭되게끔 삽입</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_insert</span>(<span class=\"params\">self, name, num</span>):</span><br><span class=\"line\">        hash_key = self.hash_function(name) <span class=\"comment\">#</span></span><br><span class=\"line\">        self.table[hash_key] = num <span class=\"comment\"># </span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># name에 따라 매칭되는 num 검색</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hash_search</span>(<span class=\"params\">self, name</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.table[self.hash_function(name)]</span><br><span class=\"line\"></span><br><span class=\"line\">ht = hash_table()</span><br><span class=\"line\"></span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Kim&#x27;</span>, <span class=\"number\">1234</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Johne&#x27;</span>, <span class=\"number\">5678</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Smith&#x27;</span>, <span class=\"number\">1526</span>)</span><br><span class=\"line\">ht.hash_insert(<span class=\"string\">&#x27;Michael&#x27;</span>, <span class=\"number\">3748</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(ht.hash_search(<span class=\"string\">&#x27;Johne&#x27;</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"hash-충돌\"><a href=\"#hash-충돌\" class=\"headerlink\" title=\"hash 충돌\"></a>hash 충돌</h4><ul>\n<li><p>해시함수가 서로 다른 두 개의 키에 대해 동일한 해시값을 내는 것을 해시 충돌이라고 한다.</p>\n</li>\n<li><p>해시충돌은 보통 해쉬값의 개수보다 많은 키값을 해쉬값으로 변환하는 일대다 대응 때문에 발생한다.</p>\n<ul>\n<li>키가 들어갈 자리(버킷)이 없는 경우에 발생한다.</li>\n</ul>\n</li>\n<li><p>아래 그림의 경우 Sandra와 Jonn의 키가 같아 버킷 152에서 충돌이 발생한다.</p>\n</li>\n</ul>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png\" alt=\"700\"/>\n\n\n<h4 id=\"hash-충돌-방지-chaining-활용\"><a href=\"#hash-충돌-방지-chaining-활용\" class=\"headerlink\" title=\"hash 충돌 방지 - chaining 활용\"></a>hash 충돌 방지 - chaining 활용</h4><ul>\n<li>chaining은 충돌이 발생한 위 그림처럼 해시테이블에서 동일한 해시 값에 대해 충돌이 일어나면, 해당 위치에 있던 버킷에 키값을 뒤이어 연결하는 것이다.</li>\n<li>이때 데이터는 <strong>해시값이 같은 노드를 연결하는</strong> 연결리스트의 형태를 가진다.<ul>\n<li>따라서 특정 해시값에 대해 충돌이 발생하여도, 체이닝을 통해 값을 찾을 수 있다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># python hashtable chaining 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">chain_hash_table = [[] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">chain_hash_func</span>():</span><br><span class=\"line\">  <span class=\"keyword\">return</span> key % <span class=\"built_in\">len</span>(chain_hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 키 값 쌍을 해시테이블에 삽입</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">chain_insert_func</span>(<span class=\"params\">chain_hash_table, key, value</span>):</span><br><span class=\"line\">  hash_key = chain_hash_func(key)</span><br><span class=\"line\">  chain_hash_table[hash_key].extend(value)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">chain_insert_func(chain_hash_table,<span class=\"number\">20</span>,<span class=\"string\">&quot;C&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_hash_table)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"hash-충돌-방지-open-addressing\"><a href=\"#hash-충돌-방지-open-addressing\" class=\"headerlink\" title=\"hash 충돌 방지 - open addressing\"></a>hash 충돌 방지 - open addressing</h4><ul>\n<li><p>하나의 버켓에 하나의 entry만 들어갈 수 있는 형태이다.(저장공간이 정해져있다.)</p>\n</li>\n<li><p>기본적인 로직은 비어있는 배열 슬롯이 발견될 때까지 배열의 위치를 검색하는 것이다.</p>\n</li>\n<li><p>Chaining은 연결문제를 해결하여 충돌을 해결하고 Open Addressing은 내부적으로 공간이 정해진 배열을 활용하여 빈공간을 찾는 식으로 충돌을 해결한다.</p>\n</li>\n<li><p>close hashing이라고도 불린다.</p>\n</li>\n<li><p>파이썬 자료형으로 구현된 hash table이 Dictionary이다.</p>\n</li>\n<li><p>Dictioanary는 내부적으로 open addressing 방식을 활용한다.</p>\n</li>\n<li><p><strong>로드 팩터</strong> : (Number of items in hash table) &#x2F; (Total Number of Slots)</p>\n<ul>\n<li>해시테이블에 저장된 항목 수(테이블에 입력된 키 갯수)를 슬롯 수 (해시테이블 전체 인덱스 갯수)로 나눈 값</li>\n<li>open addressing을 사용하면 최대 로드 팩터는 1정도 나온다.</li>\n<li>체이닝을 사용할 경우 로드 팩터는 open addressing보다 좋은 성능을 보인다.</li>\n<li>로드 팩터를 낮추면 해시에 대한 성능이 올라간다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 파이썬으로 구현한 open addressing</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">open_address</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, table_size</span>):</span><br><span class=\"line\">        self.size = table_size</span><br><span class=\"line\">        self.hash_table = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.size)]</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">getKey</span>(<span class=\"params\">self, data</span>):</span><br><span class=\"line\">        self.key = <span class=\"built_in\">ord</span>(data[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.key</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">hashFunction</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> key % self.size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">getAddress</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        myKey = self.getKey(key)</span><br><span class=\"line\">        hash_address = self.hashFunction(myKey)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> hash_address</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">save</span>(<span class=\"params\">self, key, value</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.hash_table[hash_address] != <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> self.hash_table[a] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                    self.hash_table[a] = [key, value]</span><br><span class=\"line\">                    <span class=\"keyword\">return</span></span><br><span class=\"line\">                <span class=\"keyword\">elif</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                    self.hash_table[a] = [key, value]</span><br><span class=\"line\">                    <span class=\"keyword\">return</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.hash_table[hash_address] = [key, value]</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">read</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> self.hash_table[a][<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">delete</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        hash_address = self.getAddress(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hash_address, <span class=\"built_in\">len</span>(self.hash_table)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">continue</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.hash_table[a][<span class=\"number\">0</span>] == key:</span><br><span class=\"line\">                self.hash_table[a] = <span class=\"number\">0</span></span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"comment\">#Test Code</span></span><br><span class=\"line\"><span class=\"comment\">#h_table = CloseHash(8)</span></span><br><span class=\"line\"></span><br><span class=\"line\">h_table = open_address(<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data1 = <span class=\"string\">&#x27;aa&#x27;</span></span><br><span class=\"line\">data2 = <span class=\"string\">&#x27;ad&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">ord</span>(data1[<span class=\"number\">0</span>]), <span class=\"built_in\">ord</span>(data2[<span class=\"number\">0</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.save(<span class=\"string\">&#x27;aa&#x27;</span>, <span class=\"string\">&#x27;3333&#x27;</span>)</span><br><span class=\"line\">h_table.save(<span class=\"string\">&#x27;ad&#x27;</span>, <span class=\"string\">&#x27;9999&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.read(<span class=\"string\">&#x27;ad&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.delete(<span class=\"string\">&#x27;aa&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br><span class=\"line\">h_table.delete(<span class=\"string\">&#x27;ad&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(h_table.hash_table)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Hash_table\">hash table wikipedia</a></li>\n<li><a href=\"https://ratsgo.github.io/data%20structure&algorithm/2017/10/25/hash/\">참조 블로그</a></li>\n<li><a href=\"http://wiki.hash.kr/index.php/%ED%95%B4%EC%8B%9C_%ED%85%8C%EC%9D%B4%EB%B8%94\">hashnet hash</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Algorithm]Hash Table과 Hash에 대한 이해","path":"2022/06/13/Programming-Python-hash-table/","eyeCatchImage":"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Hash_table_5_0_1_1_1_1_1_LL.svg/675px-Hash_table_5_0_1_1_1_1_1_LL.svg.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python","Algorithm","Data Structure"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]자주쓰는 High Order 함수 정리(lambda,map,filter,apply..)","ate":null,"updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\nfor loop과 조건문 만으로도 동일한 결과를 낼 수 있지만 **가독성, 속도, 재사용성** 때문애 고차함수를 사용해 프로그래밍을 하는 경우가 많다.\n특히 lambda는 데이터 전처리시 자주 사용한다.\n\n---\n\n### High order function\n\n\nHigher-order Function(고차 함수)\n\n**함수의 매개변수의 인수로 전달이 될수 있고 함수로 결과를 반환할 수 있는 함수를 말한다.**\n\n(First-class Function이 성립되는 3조건 중 2개만 만족한다.)\n\n대표적인 함수로는 map, filter, reduce, lambda 등이 있다.\n\n\n#### lambda\n\n- `lambda 인자 : 표현식` 형태로 사용한다.\n- lambda 함수 자체가 파이썬에서 정의된 함수처럼 기능한다고 생각하면 편하다.\n- 간단한 기능의 함수가 컨테이너의 요소로 들어가거나 다른 함수(high order function)의 인자로 함수를 넘길때 주로 사용한다.\n\n```python\n# lambda 예시\n\n\nIn [1]: double = lambda x : x+x\n   ...: print(double(2))\n   ...:\n4\n\n```\n\n- 단일 값에 lambda 적용하기\n\n\n- lambda 자체를 변수에 바인딩하여 사용 가능하다.\n  + `PEP8`에 어긋나기 때문에 별로 권장하지 않는다.\n\n```python\n\nIn [8]: temp = lambda x,y : x*y\n\nIn [9]: temp(2,3)\nOut[9]: 6\n\nIn [10]: temp(10,50)\nOut[10]: 500\n\n```\n\n- lamda로 필터링한 파생변수 만들기\n\n```python\n#Conditional Lambda statement\ndf['Gender'] = df['Status'].map(lambda x: 'Male' if x=='father' or x=='son' else 'Female')\n```\n#### filter\n\n- `filter(조건함수,iterable`)\n- 특정 조건에 따라 필터된 요소들로 `iterator` 객체를 만들어 반환한다.\n  + `map`과 마찬가지로 list형태로 만들어 결과를 볼 수 있다.\n- 조건에 따라 iterable에서 일부를 뽑을 때 사용한다.\n\n```python\n# lamdas in filter\n\nIn [2]: even = list(filter(lambda x : x%2 == 0 ,range(10)))\n   ...: print(even)\n[0, 2, 4, 6, 8]\n\n# lamdas in filter2 \nsequences = [10,2,8,7,5,4,3,11,0,1]\nfiltered_result = filter (lambda x: x > 4, sequences) \nprint(list(filtered_result))\n\n```\n\n\n- 보통은 filter 대신 `list comprehension` 같은 보다 pythonic한 방법을 사용해 iterable에서 일부를 필터링 한다.\n\n\n```python\n\n# list comprehension을 활용한 filter\n\n\nIn [3]: even = [x for x in range(10) if x %2 == 0]\n   ...: print(even)\n   ...:\n[0, 2, 4, 6, 8]\n\n\n\n```\n\n- `dataframe`에서 파생변수를 만들때 쓰기 좋다.\n\n```python\n\nlist(filter(lambda x: x>18, df['age']))\n\n\n```\n\n#### map\n\n- `map(func,list)` 형태로 사욯한다.\n- `iterable`의 각 원소에 대해 함수를 적용시킨다.\n- map함수 자체는 `map` 타입으로 결과를 리턴하기에 함수가 적용된 결과를 리스트로 받으려면 내장함수 `list()`를 사용해야 한다.\n\n```python\n\nIn [2]: map(lambda x: x+x, range(5))\nOut[2]: <map at 0x2446499e640>\n\nIn [3]: list(map(lambda x: x+x,range(5)))\nOut[3]: [0, 2, 4, 6, 8]\n\n```\n\n- **map with lambda**\n\n```python\nIn [4]: sequences = [10,2,8,7,5,4,3,11,0, 1]\n   ...: filtered_result = map (lambda x: x*x, sequences)\n   ...: print(list(filtered_result))\n   ...:\n[100, 4, 64, 49, 25, 16, 9, 121, 0, 1]\n\n```\n\n- `dataframe`의 `series` 객체에 적용해 파생변수를 생성힐때 유용하다.\n\n```python\n\n#Double the age \ndf['double_age'] = df['age'].map(lambda x: x*2)\n\n```\n\n#### reduce\n\n- `reduce(func,sequence)` 형태로 사용한다.\n- iterable의 요소들을 함수에 누적해서 적용 후 반환한다.\n- **`iterable`의 순회가 끝날때까지 재귀적으로 함수를 적용한다고 생각하면 이해가 쉽다.**\n\n```python\nIn [9]:  def my_add(a, b):\n   ...:      result = a + b\n   ...:      print(f\"{a} + {b} = {result}\")\n   ...:      return result\n\nIn [10]: from functools import reduce\n\nIn [11]: numbers = list(range(0,11))\n\nIn [12]: reduce(my_add,numbers)\n0 + 1 = 1\n1 + 2 = 3\n3 + 3 = 6\n6 + 4 = 10\n10 + 5 = 15\n15 + 6 = 21\n21 + 7 = 28\n28 + 8 = 36\n36 + 9 = 45\n45 + 10 = 55\nOut[12]: 55\n```\n\n- 초기값(함수를 적용하기 시직할 지점)을 추가 인자로 넣어줄 수 있다.\n\n- **reduce에 lambda 적용**\n  + 코드 가독성을 해치기 때문에 그다지 권장되지 않는다.\n\n```python\nIn [13]: numbers = list(range(1,6))\n\nIn [14]: reduce(lambda x,y :x*y,numbers)\nOut[14]: 120\n```\n\n\n#### apply\n\n- pandas의 dataframe의 행이나 열 단위로 함수를 적용하는 함수이다. `map` 과 유사하지만 df의 메소드로 보다 쉽게 쓸 수 있다.\n- `df.apply(func, axis = 0 or 1 )` 형태로 사용한다\n  + axis = 0 . 열단위로 함수 적용. default 옵션\n  + axis = 1 . 행단위로 함수적용\n\n\n```python\nIn [3]: df\nOut[3]:\n    A   B\n0  16  81\n1  16  81\n2  16  81\n3  16  81\n\n\n# 열단위 함수적용\nIn [6]: df.apply(np.sum,axis=0)\nOut[6]:\nA     64\nB    324\ndtype: int64\n\n# 행단위 함수적용\nIn [7]: df.apply(np.sum,axis=1)\nOut[7]:\n0    97\n1    97\n2    97\n3    97\ndtype: int64\n\n```\n\n- **lambda with apply 예시**\n  + df의 파생변수 생성에 유용하다.\n\n```python\n\ndf['age'] = df['Birthyear'].apply(lambda x: 2021-x)\n\n```\n\n\n#### applymap\n\n- `df.applymap(func)` 형태로 사용항다.\n- **df의 모든 요소에 인자로 주어진 함수를 적용한다.**\n- na_action='ignore' 옵션을 적용할 경우 null값에 대해서는 함수를 적용하지 않는다.\n\n```python\n\nIn [8]: import pandas as pd\n   ...:\n   ...: df = pd.DataFrame({\n   ...:     'Col 1': [30,40,50,60],\n   ...:     'Col 2': [23,35,65,45],\n   ...:     'Col 3': [85,87,90,89],\n   ...:\n   ...: },index=[\"A\",\"B\",\"C\",\"D\"])\n   ...:\n   ...: print(\"Initial DF:\")\n   ...: print(df,\"\\n\")\n   ...:\n   ...: scaled_df=df.applymap(lambda a: a*10)\n   ...:\n   ...: print(\"Scaled DF:\")\n   ...: print(scaled_df,\"\\n\")\nInitial DF:\n   Col 1  Col 2  Col 3\nA     30     23     85\nB     40     35     87\nC     50     65     90\nD     60     45     89\n\nScaled DF:\n   Col 1  Col 2  Col 3\nA    300    230    850\nB    400    350    870\nC    500    650    900\nD    600    450    890\n\n```\n\n\n### References\n\n- [pandas 공식문서](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.applymap.html#pandas.DataFrame.applymap)\n- [lambda](https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\n)\n- [reduce](https://realpython.com/python-reduce-function/)\n","source":"_posts/Programming-Python-lambda-highorder.md","raw":"---\ntitle: '[Python]자주쓰는 High Order 함수 정리(lambda,map,filter,apply..)'\ncategories:\n  - Programming\ntags:\n   - Python\nate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\nfor loop과 조건문 만으로도 동일한 결과를 낼 수 있지만 **가독성, 속도, 재사용성** 때문애 고차함수를 사용해 프로그래밍을 하는 경우가 많다.\n특히 lambda는 데이터 전처리시 자주 사용한다.\n\n---\n\n### High order function\n\n\nHigher-order Function(고차 함수)\n\n**함수의 매개변수의 인수로 전달이 될수 있고 함수로 결과를 반환할 수 있는 함수를 말한다.**\n\n(First-class Function이 성립되는 3조건 중 2개만 만족한다.)\n\n대표적인 함수로는 map, filter, reduce, lambda 등이 있다.\n\n\n#### lambda\n\n- `lambda 인자 : 표현식` 형태로 사용한다.\n- lambda 함수 자체가 파이썬에서 정의된 함수처럼 기능한다고 생각하면 편하다.\n- 간단한 기능의 함수가 컨테이너의 요소로 들어가거나 다른 함수(high order function)의 인자로 함수를 넘길때 주로 사용한다.\n\n```python\n# lambda 예시\n\n\nIn [1]: double = lambda x : x+x\n   ...: print(double(2))\n   ...:\n4\n\n```\n\n- 단일 값에 lambda 적용하기\n\n\n- lambda 자체를 변수에 바인딩하여 사용 가능하다.\n  + `PEP8`에 어긋나기 때문에 별로 권장하지 않는다.\n\n```python\n\nIn [8]: temp = lambda x,y : x*y\n\nIn [9]: temp(2,3)\nOut[9]: 6\n\nIn [10]: temp(10,50)\nOut[10]: 500\n\n```\n\n- lamda로 필터링한 파생변수 만들기\n\n```python\n#Conditional Lambda statement\ndf['Gender'] = df['Status'].map(lambda x: 'Male' if x=='father' or x=='son' else 'Female')\n```\n#### filter\n\n- `filter(조건함수,iterable`)\n- 특정 조건에 따라 필터된 요소들로 `iterator` 객체를 만들어 반환한다.\n  + `map`과 마찬가지로 list형태로 만들어 결과를 볼 수 있다.\n- 조건에 따라 iterable에서 일부를 뽑을 때 사용한다.\n\n```python\n# lamdas in filter\n\nIn [2]: even = list(filter(lambda x : x%2 == 0 ,range(10)))\n   ...: print(even)\n[0, 2, 4, 6, 8]\n\n# lamdas in filter2 \nsequences = [10,2,8,7,5,4,3,11,0,1]\nfiltered_result = filter (lambda x: x > 4, sequences) \nprint(list(filtered_result))\n\n```\n\n\n- 보통은 filter 대신 `list comprehension` 같은 보다 pythonic한 방법을 사용해 iterable에서 일부를 필터링 한다.\n\n\n```python\n\n# list comprehension을 활용한 filter\n\n\nIn [3]: even = [x for x in range(10) if x %2 == 0]\n   ...: print(even)\n   ...:\n[0, 2, 4, 6, 8]\n\n\n\n```\n\n- `dataframe`에서 파생변수를 만들때 쓰기 좋다.\n\n```python\n\nlist(filter(lambda x: x>18, df['age']))\n\n\n```\n\n#### map\n\n- `map(func,list)` 형태로 사욯한다.\n- `iterable`의 각 원소에 대해 함수를 적용시킨다.\n- map함수 자체는 `map` 타입으로 결과를 리턴하기에 함수가 적용된 결과를 리스트로 받으려면 내장함수 `list()`를 사용해야 한다.\n\n```python\n\nIn [2]: map(lambda x: x+x, range(5))\nOut[2]: <map at 0x2446499e640>\n\nIn [3]: list(map(lambda x: x+x,range(5)))\nOut[3]: [0, 2, 4, 6, 8]\n\n```\n\n- **map with lambda**\n\n```python\nIn [4]: sequences = [10,2,8,7,5,4,3,11,0, 1]\n   ...: filtered_result = map (lambda x: x*x, sequences)\n   ...: print(list(filtered_result))\n   ...:\n[100, 4, 64, 49, 25, 16, 9, 121, 0, 1]\n\n```\n\n- `dataframe`의 `series` 객체에 적용해 파생변수를 생성힐때 유용하다.\n\n```python\n\n#Double the age \ndf['double_age'] = df['age'].map(lambda x: x*2)\n\n```\n\n#### reduce\n\n- `reduce(func,sequence)` 형태로 사용한다.\n- iterable의 요소들을 함수에 누적해서 적용 후 반환한다.\n- **`iterable`의 순회가 끝날때까지 재귀적으로 함수를 적용한다고 생각하면 이해가 쉽다.**\n\n```python\nIn [9]:  def my_add(a, b):\n   ...:      result = a + b\n   ...:      print(f\"{a} + {b} = {result}\")\n   ...:      return result\n\nIn [10]: from functools import reduce\n\nIn [11]: numbers = list(range(0,11))\n\nIn [12]: reduce(my_add,numbers)\n0 + 1 = 1\n1 + 2 = 3\n3 + 3 = 6\n6 + 4 = 10\n10 + 5 = 15\n15 + 6 = 21\n21 + 7 = 28\n28 + 8 = 36\n36 + 9 = 45\n45 + 10 = 55\nOut[12]: 55\n```\n\n- 초기값(함수를 적용하기 시직할 지점)을 추가 인자로 넣어줄 수 있다.\n\n- **reduce에 lambda 적용**\n  + 코드 가독성을 해치기 때문에 그다지 권장되지 않는다.\n\n```python\nIn [13]: numbers = list(range(1,6))\n\nIn [14]: reduce(lambda x,y :x*y,numbers)\nOut[14]: 120\n```\n\n\n#### apply\n\n- pandas의 dataframe의 행이나 열 단위로 함수를 적용하는 함수이다. `map` 과 유사하지만 df의 메소드로 보다 쉽게 쓸 수 있다.\n- `df.apply(func, axis = 0 or 1 )` 형태로 사용한다\n  + axis = 0 . 열단위로 함수 적용. default 옵션\n  + axis = 1 . 행단위로 함수적용\n\n\n```python\nIn [3]: df\nOut[3]:\n    A   B\n0  16  81\n1  16  81\n2  16  81\n3  16  81\n\n\n# 열단위 함수적용\nIn [6]: df.apply(np.sum,axis=0)\nOut[6]:\nA     64\nB    324\ndtype: int64\n\n# 행단위 함수적용\nIn [7]: df.apply(np.sum,axis=1)\nOut[7]:\n0    97\n1    97\n2    97\n3    97\ndtype: int64\n\n```\n\n- **lambda with apply 예시**\n  + df의 파생변수 생성에 유용하다.\n\n```python\n\ndf['age'] = df['Birthyear'].apply(lambda x: 2021-x)\n\n```\n\n\n#### applymap\n\n- `df.applymap(func)` 형태로 사용항다.\n- **df의 모든 요소에 인자로 주어진 함수를 적용한다.**\n- na_action='ignore' 옵션을 적용할 경우 null값에 대해서는 함수를 적용하지 않는다.\n\n```python\n\nIn [8]: import pandas as pd\n   ...:\n   ...: df = pd.DataFrame({\n   ...:     'Col 1': [30,40,50,60],\n   ...:     'Col 2': [23,35,65,45],\n   ...:     'Col 3': [85,87,90,89],\n   ...:\n   ...: },index=[\"A\",\"B\",\"C\",\"D\"])\n   ...:\n   ...: print(\"Initial DF:\")\n   ...: print(df,\"\\n\")\n   ...:\n   ...: scaled_df=df.applymap(lambda a: a*10)\n   ...:\n   ...: print(\"Scaled DF:\")\n   ...: print(scaled_df,\"\\n\")\nInitial DF:\n   Col 1  Col 2  Col 3\nA     30     23     85\nB     40     35     87\nC     50     65     90\nD     60     45     89\n\nScaled DF:\n   Col 1  Col 2  Col 3\nA    300    230    850\nB    400    350    870\nC    500    650    900\nD    600    450    890\n\n```\n\n\n### References\n\n- [pandas 공식문서](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.applymap.html#pandas.DataFrame.applymap)\n- [lambda](https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\n)\n- [reduce](https://realpython.com/python-reduce-function/)\n","slug":"Programming-Python-lambda-highorder","published":1,"date":"2022-06-13T14:23:32.350Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsck002xb36qhy033yoa","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>for loop과 조건문 만으로도 동일한 결과를 낼 수 있지만 <strong>가독성, 속도, 재사용성</strong> 때문애 고차함수를 사용해 프로그래밍을 하는 경우가 많다.<br>특히 lambda는 데이터 전처리시 자주 사용한다.</p>\n<hr>\n<h3 id=\"High-order-function\"><a href=\"#High-order-function\" class=\"headerlink\" title=\"High order function\"></a>High order function</h3><p>Higher-order Function(고차 함수)</p>\n<p><strong>함수의 매개변수의 인수로 전달이 될수 있고 함수로 결과를 반환할 수 있는 함수를 말한다.</strong></p>\n<p>(First-class Function이 성립되는 3조건 중 2개만 만족한다.)</p>\n<p>대표적인 함수로는 map, filter, reduce, lambda 등이 있다.</p>\n<h4 id=\"lambda\"><a href=\"#lambda\" class=\"headerlink\" title=\"lambda\"></a>lambda</h4><ul>\n<li><code>lambda 인자 : 표현식</code> 형태로 사용한다.</li>\n<li>lambda 함수 자체가 파이썬에서 정의된 함수처럼 기능한다고 생각하면 편하다.</li>\n<li>간단한 기능의 함수가 컨테이너의 요소로 들어가거나 다른 함수(high order function)의 인자로 함수를 넘길때 주로 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># lambda 예시</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: double = <span class=\"keyword\">lambda</span> x : x+x</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(double(<span class=\"number\">2</span>))</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>단일 값에 lambda 적용하기</p>\n</li>\n<li><p>lambda 자체를 변수에 바인딩하여 사용 가능하다.</p>\n<ul>\n<li><code>PEP8</code>에 어긋나기 때문에 별로 권장하지 않는다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: temp = <span class=\"keyword\">lambda</span> x,y : x*y</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: temp(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">6</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: temp(<span class=\"number\">10</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">500</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>lamda로 필터링한 파생변수 만들기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Conditional Lambda statement</span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;Gender&#x27;</span>] = df[<span class=\"string\">&#x27;Status&#x27;</span>].<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: <span class=\"string\">&#x27;Male&#x27;</span> <span class=\"keyword\">if</span> x==<span class=\"string\">&#x27;father&#x27;</span> <span class=\"keyword\">or</span> x==<span class=\"string\">&#x27;son&#x27;</span> <span class=\"keyword\">else</span> <span class=\"string\">&#x27;Female&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h4><ul>\n<li><code>filter(조건함수,iterable</code>)</li>\n<li>특정 조건에 따라 필터된 요소들로 <code>iterator</code> 객체를 만들어 반환한다.<ul>\n<li><code>map</code>과 마찬가지로 list형태로 만들어 결과를 볼 수 있다.</li>\n</ul>\n</li>\n<li>조건에 따라 iterable에서 일부를 뽑을 때 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># lamdas in filter</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: even = <span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x : x%<span class=\"number\">2</span> == <span class=\"number\">0</span> ,<span class=\"built_in\">range</span>(<span class=\"number\">10</span>)))</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(even)</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># lamdas in filter2 </span></span><br><span class=\"line\">sequences = [<span class=\"number\">10</span>,<span class=\"number\">2</span>,<span class=\"number\">8</span>,<span class=\"number\">7</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">11</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">filtered_result = <span class=\"built_in\">filter</span> (<span class=\"keyword\">lambda</span> x: x &gt; <span class=\"number\">4</span>, sequences) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(filtered_result))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>보통은 filter 대신 <code>list comprehension</code> 같은 보다 pythonic한 방법을 사용해 iterable에서 일부를 필터링 한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># list comprehension을 활용한 filter</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: even = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>) <span class=\"keyword\">if</span> x %<span class=\"number\">2</span> == <span class=\"number\">0</span>]</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(even)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>dataframe</code>에서 파생변수를 만들때 쓰기 좋다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x: x&gt;<span class=\"number\">18</span>, df[<span class=\"string\">&#x27;age&#x27;</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a>map</h4><ul>\n<li><code>map(func,list)</code> 형태로 사욯한다.</li>\n<li><code>iterable</code>의 각 원소에 대해 함수를 적용시킨다.</li>\n<li>map함수 자체는 <code>map</code> 타입으로 결과를 리턴하기에 함수가 적용된 결과를 리스트로 받으려면 내장함수 <code>list()</code>를 사용해야 한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: <span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x+x, <span class=\"built_in\">range</span>(<span class=\"number\">5</span>))</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: &lt;<span class=\"built_in\">map</span> at <span class=\"number\">0x2446499e640</span>&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"built_in\">list</span>(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x+x,<span class=\"built_in\">range</span>(<span class=\"number\">5</span>)))</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: [<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>map with lambda</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">4</span>]: sequences = [<span class=\"number\">10</span>,<span class=\"number\">2</span>,<span class=\"number\">8</span>,<span class=\"number\">7</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">11</span>,<span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">   ...: filtered_result = <span class=\"built_in\">map</span> (<span class=\"keyword\">lambda</span> x: x*x, sequences)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(filtered_result))</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">[<span class=\"number\">100</span>, <span class=\"number\">4</span>, <span class=\"number\">64</span>, <span class=\"number\">49</span>, <span class=\"number\">25</span>, <span class=\"number\">16</span>, <span class=\"number\">9</span>, <span class=\"number\">121</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>dataframe</code>의 <code>series</code> 객체에 적용해 파생변수를 생성힐때 유용하다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Double the age </span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;double_age&#x27;</span>] = df[<span class=\"string\">&#x27;age&#x27;</span>].<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x*<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a>reduce</h4><ul>\n<li><code>reduce(func,sequence)</code> 형태로 사용한다.</li>\n<li>iterable의 요소들을 함수에 누적해서 적용 후 반환한다.</li>\n<li><strong><code>iterable</code>의 순회가 끝날때까지 재귀적으로 함수를 적용한다고 생각하면 이해가 쉽다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">9</span>]:  <span class=\"keyword\">def</span> <span class=\"title function_\">my_add</span>(<span class=\"params\">a, b</span>):</span><br><span class=\"line\">   ...:      result = a + b</span><br><span class=\"line\">   ...:      <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;<span class=\"subst\">&#123;a&#125;</span> + <span class=\"subst\">&#123;b&#125;</span> = <span class=\"subst\">&#123;result&#125;</span>&quot;</span>)</span><br><span class=\"line\">   ...:      <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: <span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> reduce</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: reduce(my_add,numbers)</span><br><span class=\"line\"><span class=\"number\">0</span> + <span class=\"number\">1</span> = <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">1</span> + <span class=\"number\">2</span> = <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">3</span> + <span class=\"number\">3</span> = <span class=\"number\">6</span></span><br><span class=\"line\"><span class=\"number\">6</span> + <span class=\"number\">4</span> = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"number\">10</span> + <span class=\"number\">5</span> = <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">15</span> + <span class=\"number\">6</span> = <span class=\"number\">21</span></span><br><span class=\"line\"><span class=\"number\">21</span> + <span class=\"number\">7</span> = <span class=\"number\">28</span></span><br><span class=\"line\"><span class=\"number\">28</span> + <span class=\"number\">8</span> = <span class=\"number\">36</span></span><br><span class=\"line\"><span class=\"number\">36</span> + <span class=\"number\">9</span> = <span class=\"number\">45</span></span><br><span class=\"line\"><span class=\"number\">45</span> + <span class=\"number\">10</span> = <span class=\"number\">55</span></span><br><span class=\"line\">Out[<span class=\"number\">12</span>]: <span class=\"number\">55</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>초기값(함수를 적용하기 시직할 지점)을 추가 인자로 넣어줄 수 있다.</p>\n</li>\n<li><p><strong>reduce에 lambda 적용</strong></p>\n<ul>\n<li>코드 가독성을 해치기 때문에 그다지 권장되지 않는다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">13</span>]: numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: reduce(<span class=\"keyword\">lambda</span> x,y :x*y,numbers)</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]: <span class=\"number\">120</span></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"apply\"><a href=\"#apply\" class=\"headerlink\" title=\"apply\"></a>apply</h4><ul>\n<li>pandas의 dataframe의 행이나 열 단위로 함수를 적용하는 함수이다. <code>map</code> 과 유사하지만 df의 메소드로 보다 쉽게 쓸 수 있다.</li>\n<li><code>df.apply(func, axis = 0 or 1 )</code> 형태로 사용한다<ul>\n<li>axis &#x3D; 0 . 열단위로 함수 적용. default 옵션</li>\n<li>axis &#x3D; 1 . 행단위로 함수적용</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">3</span>]: df</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]:</span><br><span class=\"line\">    A   B</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 열단위 함수적용</span></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: df.apply(np.<span class=\"built_in\">sum</span>,axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]:</span><br><span class=\"line\">A     <span class=\"number\">64</span></span><br><span class=\"line\">B    <span class=\"number\">324</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행단위 함수적용</span></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: df.apply(np.<span class=\"built_in\">sum</span>,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">Out[<span class=\"number\">7</span>]:</span><br><span class=\"line\"><span class=\"number\">0</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">1</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">2</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">3</span>    <span class=\"number\">97</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>lambda with apply 예시</strong><ul>\n<li>df의 파생변수 생성에 유용하다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">&#x27;age&#x27;</span>] = df[<span class=\"string\">&#x27;Birthyear&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x: <span class=\"number\">2021</span>-x)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"applymap\"><a href=\"#applymap\" class=\"headerlink\" title=\"applymap\"></a>applymap</h4><ul>\n<li><code>df.applymap(func)</code> 형태로 사용항다.</li>\n<li><strong>df의 모든 요소에 인자로 주어진 함수를 적용한다.</strong></li>\n<li>na_action&#x3D;’ignore’ 옵션을 적용할 경우 null값에 대해서는 함수를 적용하지 않는다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: <span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: df = pd.DataFrame(&#123;</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 1&#x27;</span>: [<span class=\"number\">30</span>,<span class=\"number\">40</span>,<span class=\"number\">50</span>,<span class=\"number\">60</span>],</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 2&#x27;</span>: [<span class=\"number\">23</span>,<span class=\"number\">35</span>,<span class=\"number\">65</span>,<span class=\"number\">45</span>],</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 3&#x27;</span>: [<span class=\"number\">85</span>,<span class=\"number\">87</span>,<span class=\"number\">90</span>,<span class=\"number\">89</span>],</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: &#125;,index=[<span class=\"string\">&quot;A&quot;</span>,<span class=\"string\">&quot;B&quot;</span>,<span class=\"string\">&quot;C&quot;</span>,<span class=\"string\">&quot;D&quot;</span>])</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Initial DF:&quot;</span>)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(df,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: scaled_df=df.applymap(<span class=\"keyword\">lambda</span> a: a*<span class=\"number\">10</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Scaled DF:&quot;</span>)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(scaled_df,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">Initial DF:</span><br><span class=\"line\">   Col <span class=\"number\">1</span>  Col <span class=\"number\">2</span>  Col <span class=\"number\">3</span></span><br><span class=\"line\">A     <span class=\"number\">30</span>     <span class=\"number\">23</span>     <span class=\"number\">85</span></span><br><span class=\"line\">B     <span class=\"number\">40</span>     <span class=\"number\">35</span>     <span class=\"number\">87</span></span><br><span class=\"line\">C     <span class=\"number\">50</span>     <span class=\"number\">65</span>     <span class=\"number\">90</span></span><br><span class=\"line\">D     <span class=\"number\">60</span>     <span class=\"number\">45</span>     <span class=\"number\">89</span></span><br><span class=\"line\"></span><br><span class=\"line\">Scaled DF:</span><br><span class=\"line\">   Col <span class=\"number\">1</span>  Col <span class=\"number\">2</span>  Col <span class=\"number\">3</span></span><br><span class=\"line\">A    <span class=\"number\">300</span>    <span class=\"number\">230</span>    <span class=\"number\">850</span></span><br><span class=\"line\">B    <span class=\"number\">400</span>    <span class=\"number\">350</span>    <span class=\"number\">870</span></span><br><span class=\"line\">C    <span class=\"number\">500</span>    <span class=\"number\">650</span>    <span class=\"number\">900</span></span><br><span class=\"line\">D    <span class=\"number\">600</span>    <span class=\"number\">450</span>    <span class=\"number\">890</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><ul>\n<li><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.applymap.html#pandas.DataFrame.applymap\">pandas 공식문서</a></li>\n<li><a href=\"https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\">lambda</a></li>\n<li><a href=\"https://realpython.com/python-reduce-function/\">reduce</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>for loop과 조건문 만으로도 동일한 결과를 낼 수 있지만 <strong>가독성, 속도, 재사용성</strong> 때문애 고차함수를 사용해 프로그래밍을 하는 경우가 많다.<br>특히 lambda는 데이터 전처리시 자주 사용한다.</p>\n<hr>\n<h3 id=\"High-order-function\"><a href=\"#High-order-function\" class=\"headerlink\" title=\"High order function\"></a>High order function</h3><p>Higher-order Function(고차 함수)</p>\n<p><strong>함수의 매개변수의 인수로 전달이 될수 있고 함수로 결과를 반환할 수 있는 함수를 말한다.</strong></p>\n<p>(First-class Function이 성립되는 3조건 중 2개만 만족한다.)</p>\n<p>대표적인 함수로는 map, filter, reduce, lambda 등이 있다.</p>\n<h4 id=\"lambda\"><a href=\"#lambda\" class=\"headerlink\" title=\"lambda\"></a>lambda</h4><ul>\n<li><code>lambda 인자 : 표현식</code> 형태로 사용한다.</li>\n<li>lambda 함수 자체가 파이썬에서 정의된 함수처럼 기능한다고 생각하면 편하다.</li>\n<li>간단한 기능의 함수가 컨테이너의 요소로 들어가거나 다른 함수(high order function)의 인자로 함수를 넘길때 주로 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># lambda 예시</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: double = <span class=\"keyword\">lambda</span> x : x+x</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(double(<span class=\"number\">2</span>))</span><br><span class=\"line\">   ...:</span><br><span class=\"line\"><span class=\"number\">4</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>단일 값에 lambda 적용하기</p>\n</li>\n<li><p>lambda 자체를 변수에 바인딩하여 사용 가능하다.</p>\n<ul>\n<li><code>PEP8</code>에 어긋나기 때문에 별로 권장하지 않는다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: temp = <span class=\"keyword\">lambda</span> x,y : x*y</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: temp(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"number\">6</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: temp(<span class=\"number\">10</span>,<span class=\"number\">50</span>)</span><br><span class=\"line\">Out[<span class=\"number\">10</span>]: <span class=\"number\">500</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>lamda로 필터링한 파생변수 만들기</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Conditional Lambda statement</span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;Gender&#x27;</span>] = df[<span class=\"string\">&#x27;Status&#x27;</span>].<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: <span class=\"string\">&#x27;Male&#x27;</span> <span class=\"keyword\">if</span> x==<span class=\"string\">&#x27;father&#x27;</span> <span class=\"keyword\">or</span> x==<span class=\"string\">&#x27;son&#x27;</span> <span class=\"keyword\">else</span> <span class=\"string\">&#x27;Female&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h4><ul>\n<li><code>filter(조건함수,iterable</code>)</li>\n<li>특정 조건에 따라 필터된 요소들로 <code>iterator</code> 객체를 만들어 반환한다.<ul>\n<li><code>map</code>과 마찬가지로 list형태로 만들어 결과를 볼 수 있다.</li>\n</ul>\n</li>\n<li>조건에 따라 iterable에서 일부를 뽑을 때 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># lamdas in filter</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: even = <span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x : x%<span class=\"number\">2</span> == <span class=\"number\">0</span> ,<span class=\"built_in\">range</span>(<span class=\"number\">10</span>)))</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(even)</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># lamdas in filter2 </span></span><br><span class=\"line\">sequences = [<span class=\"number\">10</span>,<span class=\"number\">2</span>,<span class=\"number\">8</span>,<span class=\"number\">7</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">11</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>]</span><br><span class=\"line\">filtered_result = <span class=\"built_in\">filter</span> (<span class=\"keyword\">lambda</span> x: x &gt; <span class=\"number\">4</span>, sequences) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(filtered_result))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>보통은 filter 대신 <code>list comprehension</code> 같은 보다 pythonic한 방법을 사용해 iterable에서 일부를 필터링 한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># list comprehension을 활용한 filter</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: even = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>) <span class=\"keyword\">if</span> x %<span class=\"number\">2</span> == <span class=\"number\">0</span>]</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(even)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>dataframe</code>에서 파생변수를 만들때 쓰기 좋다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> x: x&gt;<span class=\"number\">18</span>, df[<span class=\"string\">&#x27;age&#x27;</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a>map</h4><ul>\n<li><code>map(func,list)</code> 형태로 사욯한다.</li>\n<li><code>iterable</code>의 각 원소에 대해 함수를 적용시킨다.</li>\n<li>map함수 자체는 <code>map</code> 타입으로 결과를 리턴하기에 함수가 적용된 결과를 리스트로 받으려면 내장함수 <code>list()</code>를 사용해야 한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: <span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x+x, <span class=\"built_in\">range</span>(<span class=\"number\">5</span>))</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: &lt;<span class=\"built_in\">map</span> at <span class=\"number\">0x2446499e640</span>&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: <span class=\"built_in\">list</span>(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x+x,<span class=\"built_in\">range</span>(<span class=\"number\">5</span>)))</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: [<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>map with lambda</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">4</span>]: sequences = [<span class=\"number\">10</span>,<span class=\"number\">2</span>,<span class=\"number\">8</span>,<span class=\"number\">7</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">11</span>,<span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">   ...: filtered_result = <span class=\"built_in\">map</span> (<span class=\"keyword\">lambda</span> x: x*x, sequences)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>(filtered_result))</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">[<span class=\"number\">100</span>, <span class=\"number\">4</span>, <span class=\"number\">64</span>, <span class=\"number\">49</span>, <span class=\"number\">25</span>, <span class=\"number\">16</span>, <span class=\"number\">9</span>, <span class=\"number\">121</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>dataframe</code>의 <code>series</code> 객체에 적용해 파생변수를 생성힐때 유용하다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Double the age </span></span><br><span class=\"line\">df[<span class=\"string\">&#x27;double_age&#x27;</span>] = df[<span class=\"string\">&#x27;age&#x27;</span>].<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: x*<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a>reduce</h4><ul>\n<li><code>reduce(func,sequence)</code> 형태로 사용한다.</li>\n<li>iterable의 요소들을 함수에 누적해서 적용 후 반환한다.</li>\n<li><strong><code>iterable</code>의 순회가 끝날때까지 재귀적으로 함수를 적용한다고 생각하면 이해가 쉽다.</strong></li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">9</span>]:  <span class=\"keyword\">def</span> <span class=\"title function_\">my_add</span>(<span class=\"params\">a, b</span>):</span><br><span class=\"line\">   ...:      result = a + b</span><br><span class=\"line\">   ...:      <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;<span class=\"subst\">&#123;a&#125;</span> + <span class=\"subst\">&#123;b&#125;</span> = <span class=\"subst\">&#123;result&#125;</span>&quot;</span>)</span><br><span class=\"line\">   ...:      <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">10</span>]: <span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> reduce</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">11</span>]: numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">0</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: reduce(my_add,numbers)</span><br><span class=\"line\"><span class=\"number\">0</span> + <span class=\"number\">1</span> = <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"number\">1</span> + <span class=\"number\">2</span> = <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">3</span> + <span class=\"number\">3</span> = <span class=\"number\">6</span></span><br><span class=\"line\"><span class=\"number\">6</span> + <span class=\"number\">4</span> = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"number\">10</span> + <span class=\"number\">5</span> = <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"number\">15</span> + <span class=\"number\">6</span> = <span class=\"number\">21</span></span><br><span class=\"line\"><span class=\"number\">21</span> + <span class=\"number\">7</span> = <span class=\"number\">28</span></span><br><span class=\"line\"><span class=\"number\">28</span> + <span class=\"number\">8</span> = <span class=\"number\">36</span></span><br><span class=\"line\"><span class=\"number\">36</span> + <span class=\"number\">9</span> = <span class=\"number\">45</span></span><br><span class=\"line\"><span class=\"number\">45</span> + <span class=\"number\">10</span> = <span class=\"number\">55</span></span><br><span class=\"line\">Out[<span class=\"number\">12</span>]: <span class=\"number\">55</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>초기값(함수를 적용하기 시직할 지점)을 추가 인자로 넣어줄 수 있다.</p>\n</li>\n<li><p><strong>reduce에 lambda 적용</strong></p>\n<ul>\n<li>코드 가독성을 해치기 때문에 그다지 권장되지 않는다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">13</span>]: numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: reduce(<span class=\"keyword\">lambda</span> x,y :x*y,numbers)</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]: <span class=\"number\">120</span></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"apply\"><a href=\"#apply\" class=\"headerlink\" title=\"apply\"></a>apply</h4><ul>\n<li>pandas의 dataframe의 행이나 열 단위로 함수를 적용하는 함수이다. <code>map</code> 과 유사하지만 df의 메소드로 보다 쉽게 쓸 수 있다.</li>\n<li><code>df.apply(func, axis = 0 or 1 )</code> 형태로 사용한다<ul>\n<li>axis &#x3D; 0 . 열단위로 함수 적용. default 옵션</li>\n<li>axis &#x3D; 1 . 행단위로 함수적용</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">3</span>]: df</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]:</span><br><span class=\"line\">    A   B</span><br><span class=\"line\"><span class=\"number\">0</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">1</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">2</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"><span class=\"number\">3</span>  <span class=\"number\">16</span>  <span class=\"number\">81</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 열단위 함수적용</span></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: df.apply(np.<span class=\"built_in\">sum</span>,axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]:</span><br><span class=\"line\">A     <span class=\"number\">64</span></span><br><span class=\"line\">B    <span class=\"number\">324</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 행단위 함수적용</span></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: df.apply(np.<span class=\"built_in\">sum</span>,axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">Out[<span class=\"number\">7</span>]:</span><br><span class=\"line\"><span class=\"number\">0</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">1</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">2</span>    <span class=\"number\">97</span></span><br><span class=\"line\"><span class=\"number\">3</span>    <span class=\"number\">97</span></span><br><span class=\"line\">dtype: int64</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><strong>lambda with apply 예시</strong><ul>\n<li>df의 파생변수 생성에 유용하다.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">&#x27;age&#x27;</span>] = df[<span class=\"string\">&#x27;Birthyear&#x27;</span>].apply(<span class=\"keyword\">lambda</span> x: <span class=\"number\">2021</span>-x)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"applymap\"><a href=\"#applymap\" class=\"headerlink\" title=\"applymap\"></a>applymap</h4><ul>\n<li><code>df.applymap(func)</code> 형태로 사용항다.</li>\n<li><strong>df의 모든 요소에 인자로 주어진 함수를 적용한다.</strong></li>\n<li>na_action&#x3D;’ignore’ 옵션을 적용할 경우 null값에 대해서는 함수를 적용하지 않는다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">8</span>]: <span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: df = pd.DataFrame(&#123;</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 1&#x27;</span>: [<span class=\"number\">30</span>,<span class=\"number\">40</span>,<span class=\"number\">50</span>,<span class=\"number\">60</span>],</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 2&#x27;</span>: [<span class=\"number\">23</span>,<span class=\"number\">35</span>,<span class=\"number\">65</span>,<span class=\"number\">45</span>],</span><br><span class=\"line\">   ...:     <span class=\"string\">&#x27;Col 3&#x27;</span>: [<span class=\"number\">85</span>,<span class=\"number\">87</span>,<span class=\"number\">90</span>,<span class=\"number\">89</span>],</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: &#125;,index=[<span class=\"string\">&quot;A&quot;</span>,<span class=\"string\">&quot;B&quot;</span>,<span class=\"string\">&quot;C&quot;</span>,<span class=\"string\">&quot;D&quot;</span>])</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Initial DF:&quot;</span>)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(df,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: scaled_df=df.applymap(<span class=\"keyword\">lambda</span> a: a*<span class=\"number\">10</span>)</span><br><span class=\"line\">   ...:</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Scaled DF:&quot;</span>)</span><br><span class=\"line\">   ...: <span class=\"built_in\">print</span>(scaled_df,<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">Initial DF:</span><br><span class=\"line\">   Col <span class=\"number\">1</span>  Col <span class=\"number\">2</span>  Col <span class=\"number\">3</span></span><br><span class=\"line\">A     <span class=\"number\">30</span>     <span class=\"number\">23</span>     <span class=\"number\">85</span></span><br><span class=\"line\">B     <span class=\"number\">40</span>     <span class=\"number\">35</span>     <span class=\"number\">87</span></span><br><span class=\"line\">C     <span class=\"number\">50</span>     <span class=\"number\">65</span>     <span class=\"number\">90</span></span><br><span class=\"line\">D     <span class=\"number\">60</span>     <span class=\"number\">45</span>     <span class=\"number\">89</span></span><br><span class=\"line\"></span><br><span class=\"line\">Scaled DF:</span><br><span class=\"line\">   Col <span class=\"number\">1</span>  Col <span class=\"number\">2</span>  Col <span class=\"number\">3</span></span><br><span class=\"line\">A    <span class=\"number\">300</span>    <span class=\"number\">230</span>    <span class=\"number\">850</span></span><br><span class=\"line\">B    <span class=\"number\">400</span>    <span class=\"number\">350</span>    <span class=\"number\">870</span></span><br><span class=\"line\">C    <span class=\"number\">500</span>    <span class=\"number\">650</span>    <span class=\"number\">900</span></span><br><span class=\"line\">D    <span class=\"number\">600</span>    <span class=\"number\">450</span>    <span class=\"number\">890</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h3><ul>\n<li><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.applymap.html#pandas.DataFrame.applymap\">pandas 공식문서</a></li>\n<li><a href=\"https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\">lambda</a></li>\n<li><a href=\"https://realpython.com/python-reduce-function/\">reduce</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]자주쓰는 High Order 함수 정리(lambda,map,filter,apply..)","path":"2022/06/13/Programming-Python-lambda-highorder/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python] asterisk를 활용한 unpacking","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nience-interview-questions.html\n-->\n\n**asterisk를 활용한 Unpacking을 간단히 정리**\n\n\nPython에서 `*`(asterisk)를 쓰는 방법은 크게 3가지이다.\n\n- **function call 할때 인자를 unpacking 하기** \n  - `*` 연산자는 리스트 또는 튜플과 같은 iterable을 unpack한다\n  - `**` 연산자는 dictionary를 펑션에 필요한 인수로 unpack한다.\n\n- **Variadic Parameters(가변인자) 사용하기**\n  - positional arguments 나 keyword arguments(dictionary 형태)를 여러개 받고 싶을 때 사용한다\n\n- **곱셈, 거듭제곱의 연산자로 사용**\n\n여기서는 일단 iterable에 unpacking을 적용하는 것 중심으로 작성한다.\n\n---\n\n## unpacking parameters\n\n- 5개의 positional argument를 받는 함수가 있을 때 unpacking을 활용해 보다 간소화해서 실행할 수 있다.\n\n```python\ndef num_sum(num1,num2,num3,num4,num5):\n    return num1 + num2 + num3 + num4 + num5\n\nnum_list = list(range(1,6))\n\nnum_sum(*num_list) # 1+2+3+4+5\n\n```\n\n\n## iterable의 데이터를 unpacking하기\n\n- list unpacking\n```python\ntest = [1, 2, 3, 4]\nprint(*test) # 1 2 3 4\n```\n\n- tuple unpacking\n\n```python\ntest = (5, 6, 7, 8)\nprint(*test) # 5 6 7 8\n```\n\n- unpacking 을 활용해 iterable을 여러 부분으로 나눌 수 있다.\n\n```python\nn = [2, 3, 4, 5, 6 ,7]\n\n# unpacking의 좌변은 iterable의 형태를 가져야 한다,\n\n*a, = num\n# a = [2, 3, 4, 5, 6 ,7]\n\n*a, b = num\n# a = [2, 3, 4, 5, 6]\n# b = 7\n\na, *b, = num\n# a = 2\n# b = [3, 4, 5, 6, 7]\n\na, *b, c = num\n# a = 2\n# b = [3, 4, 5, 6]\n# c = 7\n\n```\n\n- dictionary unpacking 예시\n\n```python\n\ndct = {'a':3, 'b':3,'c':5,'d':3}\n\nlst = ['c', 'd', 'a', 'b', 'd']\n\nres = [*map(dct.get,lst)] # unpacking\n\nres2 = map(dct.get,lst)\n\n\nprint(res)\n\n# 각 인자를 unpacking해서 출력\nfor i in res2:\n      print(i) # 3, 3, 5 ,3 \n\n\n```\n\n## multiple list를 합치기\n\n```pyhton\n\nnum_list = [1,2,3,4,5]\nnum_list2 = [6,7,8,9,10]\n\nnew_list = [*num_list, *num_list_2]\n# [1,2,3,4,5,6,7,8,9,10]\n\n```\n\n## References\n\n- https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\n- https://mingrammer.com/understanding-the-asterisk-of-python/\n- https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480","source":"_posts/Programming-Python-unpacking.md","raw":"---\ntitle: '[Python] asterisk를 활용한 unpacking'\ncategories:\n  - Programming\ntags:\n  - Python\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nience-interview-questions.html\n-->\n\n**asterisk를 활용한 Unpacking을 간단히 정리**\n\n\nPython에서 `*`(asterisk)를 쓰는 방법은 크게 3가지이다.\n\n- **function call 할때 인자를 unpacking 하기** \n  - `*` 연산자는 리스트 또는 튜플과 같은 iterable을 unpack한다\n  - `**` 연산자는 dictionary를 펑션에 필요한 인수로 unpack한다.\n\n- **Variadic Parameters(가변인자) 사용하기**\n  - positional arguments 나 keyword arguments(dictionary 형태)를 여러개 받고 싶을 때 사용한다\n\n- **곱셈, 거듭제곱의 연산자로 사용**\n\n여기서는 일단 iterable에 unpacking을 적용하는 것 중심으로 작성한다.\n\n---\n\n## unpacking parameters\n\n- 5개의 positional argument를 받는 함수가 있을 때 unpacking을 활용해 보다 간소화해서 실행할 수 있다.\n\n```python\ndef num_sum(num1,num2,num3,num4,num5):\n    return num1 + num2 + num3 + num4 + num5\n\nnum_list = list(range(1,6))\n\nnum_sum(*num_list) # 1+2+3+4+5\n\n```\n\n\n## iterable의 데이터를 unpacking하기\n\n- list unpacking\n```python\ntest = [1, 2, 3, 4]\nprint(*test) # 1 2 3 4\n```\n\n- tuple unpacking\n\n```python\ntest = (5, 6, 7, 8)\nprint(*test) # 5 6 7 8\n```\n\n- unpacking 을 활용해 iterable을 여러 부분으로 나눌 수 있다.\n\n```python\nn = [2, 3, 4, 5, 6 ,7]\n\n# unpacking의 좌변은 iterable의 형태를 가져야 한다,\n\n*a, = num\n# a = [2, 3, 4, 5, 6 ,7]\n\n*a, b = num\n# a = [2, 3, 4, 5, 6]\n# b = 7\n\na, *b, = num\n# a = 2\n# b = [3, 4, 5, 6, 7]\n\na, *b, c = num\n# a = 2\n# b = [3, 4, 5, 6]\n# c = 7\n\n```\n\n- dictionary unpacking 예시\n\n```python\n\ndct = {'a':3, 'b':3,'c':5,'d':3}\n\nlst = ['c', 'd', 'a', 'b', 'd']\n\nres = [*map(dct.get,lst)] # unpacking\n\nres2 = map(dct.get,lst)\n\n\nprint(res)\n\n# 각 인자를 unpacking해서 출력\nfor i in res2:\n      print(i) # 3, 3, 5 ,3 \n\n\n```\n\n## multiple list를 합치기\n\n```pyhton\n\nnum_list = [1,2,3,4,5]\nnum_list2 = [6,7,8,9,10]\n\nnew_list = [*num_list, *num_list_2]\n# [1,2,3,4,5,6,7,8,9,10]\n\n```\n\n## References\n\n- https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\n- https://mingrammer.com/understanding-the-asterisk-of-python/\n- https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480","slug":"Programming-Python-unpacking","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscl0030b36qbvya1ny5","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nience-interview-questions.html\n-->\n\n<p><strong>asterisk를 활용한 Unpacking을 간단히 정리</strong></p>\n<p>Python에서 <code>*</code>(asterisk)를 쓰는 방법은 크게 3가지이다.</p>\n<ul>\n<li><p><strong>function call 할때 인자를 unpacking 하기</strong> </p>\n<ul>\n<li><code>*</code> 연산자는 리스트 또는 튜플과 같은 iterable을 unpack한다</li>\n<li><code>**</code> 연산자는 dictionary를 펑션에 필요한 인수로 unpack한다.</li>\n</ul>\n</li>\n<li><p><strong>Variadic Parameters(가변인자) 사용하기</strong></p>\n<ul>\n<li>positional arguments 나 keyword arguments(dictionary 형태)를 여러개 받고 싶을 때 사용한다</li>\n</ul>\n</li>\n<li><p><strong>곱셈, 거듭제곱의 연산자로 사용</strong></p>\n</li>\n</ul>\n<p>여기서는 일단 iterable에 unpacking을 적용하는 것 중심으로 작성한다.</p>\n<hr>\n<h2 id=\"unpacking-parameters\"><a href=\"#unpacking-parameters\" class=\"headerlink\" title=\"unpacking parameters\"></a>unpacking parameters</h2><ul>\n<li>5개의 positional argument를 받는 함수가 있을 때 unpacking을 활용해 보다 간소화해서 실행할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">num_sum</span>(<span class=\"params\">num1,num2,num3,num4,num5</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> num1 + num2 + num3 + num4 + num5</span><br><span class=\"line\"></span><br><span class=\"line\">num_list = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">num_sum(*num_list) <span class=\"comment\"># 1+2+3+4+5</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"iterable의-데이터를-unpacking하기\"><a href=\"#iterable의-데이터를-unpacking하기\" class=\"headerlink\" title=\"iterable의 데이터를 unpacking하기\"></a>iterable의 데이터를 unpacking하기</h2><ul>\n<li><p>list unpacking</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*test) <span class=\"comment\"># 1 2 3 4</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>tuple unpacking</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test = (<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*test) <span class=\"comment\"># 5 6 7 8</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>unpacking 을 활용해 iterable을 여러 부분으로 나눌 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n = [<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span> ,<span class=\"number\">7</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># unpacking의 좌변은 iterable의 형태를 가져야 한다,</span></span><br><span class=\"line\"></span><br><span class=\"line\">*a, = num</span><br><span class=\"line\"><span class=\"comment\"># a = [2, 3, 4, 5, 6 ,7]</span></span><br><span class=\"line\"></span><br><span class=\"line\">*a, b = num</span><br><span class=\"line\"><span class=\"comment\"># a = [2, 3, 4, 5, 6]</span></span><br><span class=\"line\"><span class=\"comment\"># b = 7</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, *b, = num</span><br><span class=\"line\"><span class=\"comment\"># a = 2</span></span><br><span class=\"line\"><span class=\"comment\"># b = [3, 4, 5, 6, 7]</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, *b, c = num</span><br><span class=\"line\"><span class=\"comment\"># a = 2</span></span><br><span class=\"line\"><span class=\"comment\"># b = [3, 4, 5, 6]</span></span><br><span class=\"line\"><span class=\"comment\"># c = 7</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>dictionary unpacking 예시</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">dct = &#123;<span class=\"string\">&#x27;a&#x27;</span>:<span class=\"number\">3</span>, <span class=\"string\">&#x27;b&#x27;</span>:<span class=\"number\">3</span>,<span class=\"string\">&#x27;c&#x27;</span>:<span class=\"number\">5</span>,<span class=\"string\">&#x27;d&#x27;</span>:<span class=\"number\">3</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">lst = [<span class=\"string\">&#x27;c&#x27;</span>, <span class=\"string\">&#x27;d&#x27;</span>, <span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>, <span class=\"string\">&#x27;d&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">res = [*<span class=\"built_in\">map</span>(dct.get,lst)] <span class=\"comment\"># unpacking</span></span><br><span class=\"line\"></span><br><span class=\"line\">res2 = <span class=\"built_in\">map</span>(dct.get,lst)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 각 인자를 unpacking해서 출력</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> res2:</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(i) <span class=\"comment\"># 3, 3, 5 ,3 </span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"multiple-list를-합치기\"><a href=\"#multiple-list를-합치기\" class=\"headerlink\" title=\"multiple list를 합치기\"></a>multiple list를 합치기</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">num_list = [1,2,3,4,5]</span><br><span class=\"line\">num_list2 = [6,7,8,9,10]</span><br><span class=\"line\"></span><br><span class=\"line\">new_list = [*num_list, *num_list_2]</span><br><span class=\"line\"># [1,2,3,4,5,6,7,8,9,10]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\">https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/</a></li>\n<li><a href=\"https://mingrammer.com/understanding-the-asterisk-of-python/\">https://mingrammer.com/understanding-the-asterisk-of-python/</a></li>\n<li><a href=\"https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480\">https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\nience-interview-questions.html\n-->\n\n<p><strong>asterisk를 활용한 Unpacking을 간단히 정리</strong></p>\n<p>Python에서 <code>*</code>(asterisk)를 쓰는 방법은 크게 3가지이다.</p>\n<ul>\n<li><p><strong>function call 할때 인자를 unpacking 하기</strong> </p>\n<ul>\n<li><code>*</code> 연산자는 리스트 또는 튜플과 같은 iterable을 unpack한다</li>\n<li><code>**</code> 연산자는 dictionary를 펑션에 필요한 인수로 unpack한다.</li>\n</ul>\n</li>\n<li><p><strong>Variadic Parameters(가변인자) 사용하기</strong></p>\n<ul>\n<li>positional arguments 나 keyword arguments(dictionary 형태)를 여러개 받고 싶을 때 사용한다</li>\n</ul>\n</li>\n<li><p><strong>곱셈, 거듭제곱의 연산자로 사용</strong></p>\n</li>\n</ul>\n<p>여기서는 일단 iterable에 unpacking을 적용하는 것 중심으로 작성한다.</p>\n<hr>\n<h2 id=\"unpacking-parameters\"><a href=\"#unpacking-parameters\" class=\"headerlink\" title=\"unpacking parameters\"></a>unpacking parameters</h2><ul>\n<li>5개의 positional argument를 받는 함수가 있을 때 unpacking을 활용해 보다 간소화해서 실행할 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">num_sum</span>(<span class=\"params\">num1,num2,num3,num4,num5</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> num1 + num2 + num3 + num4 + num5</span><br><span class=\"line\"></span><br><span class=\"line\">num_list = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">num_sum(*num_list) <span class=\"comment\"># 1+2+3+4+5</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"iterable의-데이터를-unpacking하기\"><a href=\"#iterable의-데이터를-unpacking하기\" class=\"headerlink\" title=\"iterable의 데이터를 unpacking하기\"></a>iterable의 데이터를 unpacking하기</h2><ul>\n<li><p>list unpacking</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*test) <span class=\"comment\"># 1 2 3 4</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>tuple unpacking</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test = (<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*test) <span class=\"comment\"># 5 6 7 8</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>unpacking 을 활용해 iterable을 여러 부분으로 나눌 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n = [<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span> ,<span class=\"number\">7</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># unpacking의 좌변은 iterable의 형태를 가져야 한다,</span></span><br><span class=\"line\"></span><br><span class=\"line\">*a, = num</span><br><span class=\"line\"><span class=\"comment\"># a = [2, 3, 4, 5, 6 ,7]</span></span><br><span class=\"line\"></span><br><span class=\"line\">*a, b = num</span><br><span class=\"line\"><span class=\"comment\"># a = [2, 3, 4, 5, 6]</span></span><br><span class=\"line\"><span class=\"comment\"># b = 7</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, *b, = num</span><br><span class=\"line\"><span class=\"comment\"># a = 2</span></span><br><span class=\"line\"><span class=\"comment\"># b = [3, 4, 5, 6, 7]</span></span><br><span class=\"line\"></span><br><span class=\"line\">a, *b, c = num</span><br><span class=\"line\"><span class=\"comment\"># a = 2</span></span><br><span class=\"line\"><span class=\"comment\"># b = [3, 4, 5, 6]</span></span><br><span class=\"line\"><span class=\"comment\"># c = 7</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>dictionary unpacking 예시</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">dct = &#123;<span class=\"string\">&#x27;a&#x27;</span>:<span class=\"number\">3</span>, <span class=\"string\">&#x27;b&#x27;</span>:<span class=\"number\">3</span>,<span class=\"string\">&#x27;c&#x27;</span>:<span class=\"number\">5</span>,<span class=\"string\">&#x27;d&#x27;</span>:<span class=\"number\">3</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">lst = [<span class=\"string\">&#x27;c&#x27;</span>, <span class=\"string\">&#x27;d&#x27;</span>, <span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>, <span class=\"string\">&#x27;d&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">res = [*<span class=\"built_in\">map</span>(dct.get,lst)] <span class=\"comment\"># unpacking</span></span><br><span class=\"line\"></span><br><span class=\"line\">res2 = <span class=\"built_in\">map</span>(dct.get,lst)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 각 인자를 unpacking해서 출력</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> res2:</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(i) <span class=\"comment\"># 3, 3, 5 ,3 </span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"multiple-list를-합치기\"><a href=\"#multiple-list를-합치기\" class=\"headerlink\" title=\"multiple list를 합치기\"></a>multiple list를 합치기</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">num_list = [1,2,3,4,5]</span><br><span class=\"line\">num_list2 = [6,7,8,9,10]</span><br><span class=\"line\"></span><br><span class=\"line\">new_list = [*num_list, *num_list_2]</span><br><span class=\"line\"># [1,2,3,4,5,6,7,8,9,10]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/\">https://treyhunner.com/2018/10/asterisks-in-python-what-they-are-and-how-to-use-them/</a></li>\n<li><a href=\"https://mingrammer.com/understanding-the-asterisk-of-python/\">https://mingrammer.com/understanding-the-asterisk-of-python/</a></li>\n<li><a href=\"https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480\">https://towardsdatascience.com/unpacking-operators-in-python-306ae44cd480</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python] asterisk를 활용한 unpacking","path":"2022/06/13/Programming-Python-unpacking/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]for-loop관련 함수들","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n- 대부분의 문제는 반복문과 제어문을 잘 쓰면 어떻게든 해결이 된다.\n- for loop에 자주쓰이는 유용한 내장함수로 zip과 enumerate가 있다.\n- itertools를 활욯해 반복문의 코드 가독성을 높일 수 있다.\n\n---\n\n## for-loop\n\n\n#### zip\n\n- 같은 크기의 여러 `iterable`를 한 쌍으로 묶은 뒤 tuple의 형태로 접근할 수 있는 `iterator`를 반환한다.\n- 2개 이상의 인자를 넘겨서 병렬처리가 가능하다(가변인자를 받는다.)\n\n```python\n\nnum = [1, 2, 3]\nname = [\"A\", \"B\", \"C\"]\nfor i in zip(num, name):\n   print(i)\n\n(1, 'A')\n(2, 'B')\n(3, 'C')\n\n```\n\n- zip으로 쉽게 dictionary를 만들 수 있다.\n\n```python\n\nfood = ['beef', 'chicken']\ncount = [5, 10]\n\nstock = dict(zip(food, count))\n\n# dictionary comprehension을 사욯할 경우\n\n\nstock2 = {k:v*2 for k, v in zip(food, count)} # stock이 2배 늘어남\n\nprint(stock)\nprint(stock2)\n\n```\n\n- `*` 연산자를 사용해 unzip이 가능하다\n- `*` 은 iterable의 각 요소를 분리하는 역할을 한다.\n  + * (a, b, c, d) 는 a,b,c,d 각각을 분리한 것과 같다.\n- zip(* zipped) 는 배열의 각 요소들을 분리한 다음 페어로 다시 묶은 것이다.\n\n```python\na = [(1, 2, 3), (4, 5, 6)]\n\n\nb,c,d=zip(*a) # 배열을 페어링\n\nIn [12]: b\nOut[12]: (1, 4)\n\nIn [13]: c\nOut[13]: (2, 5)\n\nIn [14]: d\nOut[14]: (3, 6)\n\n\n```\n\n\n#### enumerate\n\n- **Get the element and index from a list**\n- `iterable`에 사용한다. `iterable`의 인덱스와 원소를 튜플형태로 반환한다.\n- `zip`과 다른 것은 배열을 묶는게 아니라 배열의 인덱스를 원소와 함께 묶은 `iterator`를 반환한다는 것.\n\n```python\n\n#\n\nlst = ['a','b','c','d']\nfor i in enumerate(lst):\n    print(i)\n\n(0,'a')\n(1,'b')\n(2,'c')\n(3,'d')\n```\n\n**zip()과 enumerate() 활용**\n\n- 인덱스와 배열을 묶은 값을 모두 반환해야 할경우 zip과 enumerate를 같이 사용한다.\n\n```python\nnames = ['Alice', 'Bob', 'Chris']\nages = [18, 20, 24]\n\nfor i, (name, age) in enumerate(zip(names, ages)):\n    print(i, name, age)\n# 0 Alice 18\n# 1 Bob 20\n# 2 Chris 14\n\n```\n\n### itertools를 활용한 반복문 응용\n\n**itertools.product를 활용한 이중 반복문 변형**\n\n```python\n# 기존 반복문\nfor i in i_ex:\n    for j in j_ex:\n        print(i,j)\n\n# itertools활용\nimport itertools\nfor i, j in itertools.product(i_ex, j_ex):\n    print(i, j)\n\n\n```\n\n**References & annotation**\n\n\n- [unzipping 연산자](https://stackoverflow.com/questions/5917522/unzipping-and-the-operator)\n- [itertools](https://www.geeksforgeeks.org/python-itertools/\n)\n","source":"_posts/Programming-Python-zip-enumerate.md","raw":"---\ntitle: '[Python]for-loop관련 함수들'\ncategories:\n  - - Programming\ntags:\n    - Python\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n- 대부분의 문제는 반복문과 제어문을 잘 쓰면 어떻게든 해결이 된다.\n- for loop에 자주쓰이는 유용한 내장함수로 zip과 enumerate가 있다.\n- itertools를 활욯해 반복문의 코드 가독성을 높일 수 있다.\n\n---\n\n## for-loop\n\n\n#### zip\n\n- 같은 크기의 여러 `iterable`를 한 쌍으로 묶은 뒤 tuple의 형태로 접근할 수 있는 `iterator`를 반환한다.\n- 2개 이상의 인자를 넘겨서 병렬처리가 가능하다(가변인자를 받는다.)\n\n```python\n\nnum = [1, 2, 3]\nname = [\"A\", \"B\", \"C\"]\nfor i in zip(num, name):\n   print(i)\n\n(1, 'A')\n(2, 'B')\n(3, 'C')\n\n```\n\n- zip으로 쉽게 dictionary를 만들 수 있다.\n\n```python\n\nfood = ['beef', 'chicken']\ncount = [5, 10]\n\nstock = dict(zip(food, count))\n\n# dictionary comprehension을 사욯할 경우\n\n\nstock2 = {k:v*2 for k, v in zip(food, count)} # stock이 2배 늘어남\n\nprint(stock)\nprint(stock2)\n\n```\n\n- `*` 연산자를 사용해 unzip이 가능하다\n- `*` 은 iterable의 각 요소를 분리하는 역할을 한다.\n  + * (a, b, c, d) 는 a,b,c,d 각각을 분리한 것과 같다.\n- zip(* zipped) 는 배열의 각 요소들을 분리한 다음 페어로 다시 묶은 것이다.\n\n```python\na = [(1, 2, 3), (4, 5, 6)]\n\n\nb,c,d=zip(*a) # 배열을 페어링\n\nIn [12]: b\nOut[12]: (1, 4)\n\nIn [13]: c\nOut[13]: (2, 5)\n\nIn [14]: d\nOut[14]: (3, 6)\n\n\n```\n\n\n#### enumerate\n\n- **Get the element and index from a list**\n- `iterable`에 사용한다. `iterable`의 인덱스와 원소를 튜플형태로 반환한다.\n- `zip`과 다른 것은 배열을 묶는게 아니라 배열의 인덱스를 원소와 함께 묶은 `iterator`를 반환한다는 것.\n\n```python\n\n#\n\nlst = ['a','b','c','d']\nfor i in enumerate(lst):\n    print(i)\n\n(0,'a')\n(1,'b')\n(2,'c')\n(3,'d')\n```\n\n**zip()과 enumerate() 활용**\n\n- 인덱스와 배열을 묶은 값을 모두 반환해야 할경우 zip과 enumerate를 같이 사용한다.\n\n```python\nnames = ['Alice', 'Bob', 'Chris']\nages = [18, 20, 24]\n\nfor i, (name, age) in enumerate(zip(names, ages)):\n    print(i, name, age)\n# 0 Alice 18\n# 1 Bob 20\n# 2 Chris 14\n\n```\n\n### itertools를 활용한 반복문 응용\n\n**itertools.product를 활용한 이중 반복문 변형**\n\n```python\n# 기존 반복문\nfor i in i_ex:\n    for j in j_ex:\n        print(i,j)\n\n# itertools활용\nimport itertools\nfor i, j in itertools.product(i_ex, j_ex):\n    print(i, j)\n\n\n```\n\n**References & annotation**\n\n\n- [unzipping 연산자](https://stackoverflow.com/questions/5917522/unzipping-and-the-operator)\n- [itertools](https://www.geeksforgeeks.org/python-itertools/\n)\n","slug":"Programming-Python-zip-enumerate","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscl0032b36qg6fp5amg","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<ul>\n<li>대부분의 문제는 반복문과 제어문을 잘 쓰면 어떻게든 해결이 된다.</li>\n<li>for loop에 자주쓰이는 유용한 내장함수로 zip과 enumerate가 있다.</li>\n<li>itertools를 활욯해 반복문의 코드 가독성을 높일 수 있다.</li>\n</ul>\n<hr>\n<h2 id=\"for-loop\"><a href=\"#for-loop\" class=\"headerlink\" title=\"for-loop\"></a>for-loop</h2><h4 id=\"zip\"><a href=\"#zip\" class=\"headerlink\" title=\"zip\"></a>zip</h4><ul>\n<li>같은 크기의 여러 <code>iterable</code>를 한 쌍으로 묶은 뒤 tuple의 형태로 접근할 수 있는 <code>iterator</code>를 반환한다.</li>\n<li>2개 이상의 인자를 넘겨서 병렬처리가 가능하다(가변인자를 받는다.)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">num = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">name = [<span class=\"string\">&quot;A&quot;</span>, <span class=\"string\">&quot;B&quot;</span>, <span class=\"string\">&quot;C&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(num, name):</span><br><span class=\"line\">   <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">1</span>, <span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">2</span>, <span class=\"string\">&#x27;B&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">3</span>, <span class=\"string\">&#x27;C&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>zip으로 쉽게 dictionary를 만들 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">food = [<span class=\"string\">&#x27;beef&#x27;</span>, <span class=\"string\">&#x27;chicken&#x27;</span>]</span><br><span class=\"line\">count = [<span class=\"number\">5</span>, <span class=\"number\">10</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">stock = <span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(food, count))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># dictionary comprehension을 사욯할 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">stock2 = &#123;k:v*<span class=\"number\">2</span> <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(food, count)&#125; <span class=\"comment\"># stock이 2배 늘어남</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(stock)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(stock2)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>*</code> 연산자를 사용해 unzip이 가능하다</li>\n<li><code>*</code> 은 iterable의 각 요소를 분리하는 역할을 한다.<ul>\n<li><ul>\n<li>(a, b, c, d) 는 a,b,c,d 각각을 분리한 것과 같다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>zip(* zipped) 는 배열의 각 요소들을 분리한 다음 페어로 다시 묶은 것이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = [(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>), (<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">b,c,d=<span class=\"built_in\">zip</span>(*a) <span class=\"comment\"># 배열을 페어링</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: b</span><br><span class=\"line\">Out[<span class=\"number\">12</span>]: (<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">13</span>]: c</span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: (<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: d</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]: (<span class=\"number\">3</span>, <span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h4><ul>\n<li><strong>Get the element and index from a list</strong></li>\n<li><code>iterable</code>에 사용한다. <code>iterable</code>의 인덱스와 원소를 튜플형태로 반환한다.</li>\n<li><code>zip</code>과 다른 것은 배열을 묶는게 아니라 배열의 인덱스를 원소와 함께 묶은 <code>iterator</code>를 반환한다는 것.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\">lst = [<span class=\"string\">&#x27;a&#x27;</span>,<span class=\"string\">&#x27;b&#x27;</span>,<span class=\"string\">&#x27;c&#x27;</span>,<span class=\"string\">&#x27;d&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(lst):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">0</span>,<span class=\"string\">&#x27;a&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">1</span>,<span class=\"string\">&#x27;b&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">2</span>,<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">3</span>,<span class=\"string\">&#x27;d&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>zip()과 enumerate() 활용</strong></p>\n<ul>\n<li>인덱스와 배열을 묶은 값을 모두 반환해야 할경우 zip과 enumerate를 같이 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">names = [<span class=\"string\">&#x27;Alice&#x27;</span>, <span class=\"string\">&#x27;Bob&#x27;</span>, <span class=\"string\">&#x27;Chris&#x27;</span>]</span><br><span class=\"line\">ages = [<span class=\"number\">18</span>, <span class=\"number\">20</span>, <span class=\"number\">24</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, (name, age) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(names, ages)):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, name, age)</span><br><span class=\"line\"><span class=\"comment\"># 0 Alice 18</span></span><br><span class=\"line\"><span class=\"comment\"># 1 Bob 20</span></span><br><span class=\"line\"><span class=\"comment\"># 2 Chris 14</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"itertools를-활용한-반복문-응용\"><a href=\"#itertools를-활용한-반복문-응용\" class=\"headerlink\" title=\"itertools를 활용한 반복문 응용\"></a>itertools를 활용한 반복문 응용</h3><p><strong>itertools.product를 활용한 이중 반복문 변형</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기존 반복문</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> i_ex:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> j_ex:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># itertools활용</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> itertools</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, j <span class=\"keyword\">in</span> itertools.product(i_ex, j_ex):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, j)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>References &amp; annotation</strong></p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/5917522/unzipping-and-the-operator\">unzipping 연산자</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/python-itertools/\">itertools</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<ul>\n<li>대부분의 문제는 반복문과 제어문을 잘 쓰면 어떻게든 해결이 된다.</li>\n<li>for loop에 자주쓰이는 유용한 내장함수로 zip과 enumerate가 있다.</li>\n<li>itertools를 활욯해 반복문의 코드 가독성을 높일 수 있다.</li>\n</ul>\n<hr>\n<h2 id=\"for-loop\"><a href=\"#for-loop\" class=\"headerlink\" title=\"for-loop\"></a>for-loop</h2><h4 id=\"zip\"><a href=\"#zip\" class=\"headerlink\" title=\"zip\"></a>zip</h4><ul>\n<li>같은 크기의 여러 <code>iterable</code>를 한 쌍으로 묶은 뒤 tuple의 형태로 접근할 수 있는 <code>iterator</code>를 반환한다.</li>\n<li>2개 이상의 인자를 넘겨서 병렬처리가 가능하다(가변인자를 받는다.)</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">num = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">name = [<span class=\"string\">&quot;A&quot;</span>, <span class=\"string\">&quot;B&quot;</span>, <span class=\"string\">&quot;C&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(num, name):</span><br><span class=\"line\">   <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">1</span>, <span class=\"string\">&#x27;A&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">2</span>, <span class=\"string\">&#x27;B&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">3</span>, <span class=\"string\">&#x27;C&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>zip으로 쉽게 dictionary를 만들 수 있다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">food = [<span class=\"string\">&#x27;beef&#x27;</span>, <span class=\"string\">&#x27;chicken&#x27;</span>]</span><br><span class=\"line\">count = [<span class=\"number\">5</span>, <span class=\"number\">10</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">stock = <span class=\"built_in\">dict</span>(<span class=\"built_in\">zip</span>(food, count))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># dictionary comprehension을 사욯할 경우</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">stock2 = &#123;k:v*<span class=\"number\">2</span> <span class=\"keyword\">for</span> k, v <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(food, count)&#125; <span class=\"comment\"># stock이 2배 늘어남</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(stock)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(stock2)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>*</code> 연산자를 사용해 unzip이 가능하다</li>\n<li><code>*</code> 은 iterable의 각 요소를 분리하는 역할을 한다.<ul>\n<li><ul>\n<li>(a, b, c, d) 는 a,b,c,d 각각을 분리한 것과 같다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>zip(* zipped) 는 배열의 각 요소들을 분리한 다음 페어로 다시 묶은 것이다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = [(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>), (<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">b,c,d=<span class=\"built_in\">zip</span>(*a) <span class=\"comment\"># 배열을 페어링</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">12</span>]: b</span><br><span class=\"line\">Out[<span class=\"number\">12</span>]: (<span class=\"number\">1</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">13</span>]: c</span><br><span class=\"line\">Out[<span class=\"number\">13</span>]: (<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">14</span>]: d</span><br><span class=\"line\">Out[<span class=\"number\">14</span>]: (<span class=\"number\">3</span>, <span class=\"number\">6</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h4><ul>\n<li><strong>Get the element and index from a list</strong></li>\n<li><code>iterable</code>에 사용한다. <code>iterable</code>의 인덱스와 원소를 튜플형태로 반환한다.</li>\n<li><code>zip</code>과 다른 것은 배열을 묶는게 아니라 배열의 인덱스를 원소와 함께 묶은 <code>iterator</code>를 반환한다는 것.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"></span><br><span class=\"line\">lst = [<span class=\"string\">&#x27;a&#x27;</span>,<span class=\"string\">&#x27;b&#x27;</span>,<span class=\"string\">&#x27;c&#x27;</span>,<span class=\"string\">&#x27;d&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(lst):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">0</span>,<span class=\"string\">&#x27;a&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">1</span>,<span class=\"string\">&#x27;b&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">2</span>,<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">(<span class=\"number\">3</span>,<span class=\"string\">&#x27;d&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>zip()과 enumerate() 활용</strong></p>\n<ul>\n<li>인덱스와 배열을 묶은 값을 모두 반환해야 할경우 zip과 enumerate를 같이 사용한다.</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">names = [<span class=\"string\">&#x27;Alice&#x27;</span>, <span class=\"string\">&#x27;Bob&#x27;</span>, <span class=\"string\">&#x27;Chris&#x27;</span>]</span><br><span class=\"line\">ages = [<span class=\"number\">18</span>, <span class=\"number\">20</span>, <span class=\"number\">24</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, (name, age) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(<span class=\"built_in\">zip</span>(names, ages)):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, name, age)</span><br><span class=\"line\"><span class=\"comment\"># 0 Alice 18</span></span><br><span class=\"line\"><span class=\"comment\"># 1 Bob 20</span></span><br><span class=\"line\"><span class=\"comment\"># 2 Chris 14</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"itertools를-활용한-반복문-응용\"><a href=\"#itertools를-활용한-반복문-응용\" class=\"headerlink\" title=\"itertools를 활용한 반복문 응용\"></a>itertools를 활용한 반복문 응용</h3><p><strong>itertools.product를 활용한 이중 반복문 변형</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 기존 반복문</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> i_ex:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> j_ex:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i,j)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># itertools활용</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> itertools</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, j <span class=\"keyword\">in</span> itertools.product(i_ex, j_ex):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i, j)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p><strong>References &amp; annotation</strong></p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/5917522/unzipping-and-the-operator\">unzipping 연산자</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/python-itertools/\">itertools</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]for-loop관련 함수들","path":"2022/06/13/Programming-Python-zip-enumerate/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Python]Regex 정리","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","source":"_posts/Programming-python-regex.md","raw":"---\ntitle: '[Python]Regex 정리'\ncategories:\n  - Programming \ntags:\n  - regex\n  - Python\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**굵은 글씨로 뭔가 쓴다.**\n\n---\n\n## References","slug":"Programming-python-regex","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscm0036b36q20bhfccx","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>굵은 글씨로 뭔가 쓴다.</strong></p>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Python]Regex 정리","path":"2022/06/13/Programming-python-regex/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["Python","regex"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[R]특별한 R 연산자들(Binary Operators)","date":"2021-03-02T05:11:05.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"## Intro\n\nR로 분석을 하다보면 자연스럽게 손에 익게 되는 몇가지 연산자들이 있습니다. 이 포스팅에서는 R에서 사용되는 특별한 연산자들과 특정 연산자를 단축키로 Rstudio에 추가하는 법을 다룹니다.\n\n---\n## 연산자들\n\n### %in% (matching 연산자)\n**특정 vector 내에 원하는 요소가 있는지 확인하고 이를 반환할 때 사용합니다.**\nx%in%y 일 경우 x 기준으로 y와 매칭되는 값에 대한 논리값을 반환합니다.\n~~~r\n\na <- seq(1, 5)\nb <- seq(3, 12)\n\n\n> b %in% a  \n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n> a %in% b\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n~~~\n\n\n### %>% (pipe)\ntidyverse에 포함되어 있어 아마도 가장 유명할 연산자인 pipe operator입니다. \n\n자주 쓰이기 때문에 Rstudio에 ctrl+shift+m으로 단축키가 지정되어 있습니다.\n\n**기능은 함수들을 연결해 직관적으로 전달하는 것입니다.**\nf(g(x)) 와 같은 합성함수를 R 코드 상에서 직관적으로 구현한 것이라고 이해하면 될 것 같습니다.\n\n이 연산자는 특히 데이터 분석에 유용한데 파이프 연산자를 사용하면 코드 가독성을 저해하는 분석 과정상에서의 중간 객체를 만들 필요가 없어지기 때문입니다. \n~~~r\n\niris %>% # df에 연속적으로 함수를 전달함\n  subset(Sepal.Length > 3) %>%\n  aggregate(. ~ Species, ., mean)\n\n~~~\n\n### %<-%  (unpacking)\n생각보다 자주 사용하게 되는 unpacking 연산자 입니다.\n**기능은 list나 vector를 분해해서 이름을 할당하는 것입니다.**\nlist object인 선형회귀 모형의 특정 요소들을 불러와 이름을 할당해 독립적인 객체로 만들 수 있습니다.\n~~~r\nlibrary(zeallot)\n\nm<- lm(hp ~ gear, data = mtcars)\nc(mcall,...,mdf,mstat) %<-% summary(m)\n\nmcall # 모델식\nlm(formula = hp ~ gear, data = mtcars)\n\n> mdf # 자유도\n     value      numdf      dendf \n 0.4816578  1.0000000 30.0000000 \n> mstat # 통계량\n            (Intercept)        gear\n(Intercept)   0.8370370 -0.21851852\ngear         -0.2185185  0.05925926\n\n~~~\n\n### %x% (행렬곱-벡터의 내적)\n**%x%는 벡터의 내적을 반환합니다.**\n~~~r\nc <- matrix(1:4,nrow = 2)\n\n> c %*% c\n     [,1] [,2]\n[1,]    7   15\n[2,]   10   22\n\n\n~~~\n\n\n### %o% (행렬곱-벡터의 외적)\n**%o%는 벡터의 외적을 반환합니다.**\n~~~r\n> d <- 1:3\n> d %o% d\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    3    6    9\n\n~~~\n\n\n### %$% (값 할당하기)\n**magrittr** 패키지의 %$% 연산자는 데이터 프레임이 중심이 되는 분석을 할때 사용하는 단순하지만 유용한 연산자입니다. \n**기능은 데이터 프레임에서 단순히 특정 변수를 추출하는 것입니다.**\n~~~r\nlibrary(magrittr)\n\nmtcars %$%\n  cor(disp, mpg)\n\n~~~\n\n---\n## 특정 단축키를 addin으로 Rstudio에 추가하기\n\n\n특별한 연산자들은 유용하지만 %>%와 같이 Rstudio에서 미리 단축키로 지정해놓지 않은 연산자들을 매번 타이핑해서 사용하는 것은 귀찮은 일입니다.  \n따라서 자주 사용하는 연산자의 경우 Rstudio addin을 사용해 %in% 처럼 단축키를 만들어 주는 것을 고려해볼 수 있습니다.\n\n~~~r\n # 사용자 정의 단축키를 추가하는 Rstudio addin 설치\n\ndevtools::install_github(\"rstudio/addinexamples\", type = \"source\")\n\n~~~\n\n\n**References & annotation**\n---\n\n  - https://www.datamentor.io/r-programming/infix-operator/\n  - https://github.com/r-lib/zeallot\n  - https://rfriend.tistory.com/35\n  - https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them # Binary operator에 대한 자세한 설명이 나와있습니다.","source":"_posts/R-Programming-operators.md","raw":"---\ntitle: \"[R]특별한 R 연산자들(Binary Operators)\"\ndate: 2021-03-02 14:11:05\nupdated:\ncategories: \n        - [Programming]\ntags:\n  - [R]\n---\n## Intro\n\nR로 분석을 하다보면 자연스럽게 손에 익게 되는 몇가지 연산자들이 있습니다. 이 포스팅에서는 R에서 사용되는 특별한 연산자들과 특정 연산자를 단축키로 Rstudio에 추가하는 법을 다룹니다.\n\n---\n## 연산자들\n\n### %in% (matching 연산자)\n**특정 vector 내에 원하는 요소가 있는지 확인하고 이를 반환할 때 사용합니다.**\nx%in%y 일 경우 x 기준으로 y와 매칭되는 값에 대한 논리값을 반환합니다.\n~~~r\n\na <- seq(1, 5)\nb <- seq(3, 12)\n\n\n> b %in% a  \n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n> a %in% b\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n~~~\n\n\n### %>% (pipe)\ntidyverse에 포함되어 있어 아마도 가장 유명할 연산자인 pipe operator입니다. \n\n자주 쓰이기 때문에 Rstudio에 ctrl+shift+m으로 단축키가 지정되어 있습니다.\n\n**기능은 함수들을 연결해 직관적으로 전달하는 것입니다.**\nf(g(x)) 와 같은 합성함수를 R 코드 상에서 직관적으로 구현한 것이라고 이해하면 될 것 같습니다.\n\n이 연산자는 특히 데이터 분석에 유용한데 파이프 연산자를 사용하면 코드 가독성을 저해하는 분석 과정상에서의 중간 객체를 만들 필요가 없어지기 때문입니다. \n~~~r\n\niris %>% # df에 연속적으로 함수를 전달함\n  subset(Sepal.Length > 3) %>%\n  aggregate(. ~ Species, ., mean)\n\n~~~\n\n### %<-%  (unpacking)\n생각보다 자주 사용하게 되는 unpacking 연산자 입니다.\n**기능은 list나 vector를 분해해서 이름을 할당하는 것입니다.**\nlist object인 선형회귀 모형의 특정 요소들을 불러와 이름을 할당해 독립적인 객체로 만들 수 있습니다.\n~~~r\nlibrary(zeallot)\n\nm<- lm(hp ~ gear, data = mtcars)\nc(mcall,...,mdf,mstat) %<-% summary(m)\n\nmcall # 모델식\nlm(formula = hp ~ gear, data = mtcars)\n\n> mdf # 자유도\n     value      numdf      dendf \n 0.4816578  1.0000000 30.0000000 \n> mstat # 통계량\n            (Intercept)        gear\n(Intercept)   0.8370370 -0.21851852\ngear         -0.2185185  0.05925926\n\n~~~\n\n### %x% (행렬곱-벡터의 내적)\n**%x%는 벡터의 내적을 반환합니다.**\n~~~r\nc <- matrix(1:4,nrow = 2)\n\n> c %*% c\n     [,1] [,2]\n[1,]    7   15\n[2,]   10   22\n\n\n~~~\n\n\n### %o% (행렬곱-벡터의 외적)\n**%o%는 벡터의 외적을 반환합니다.**\n~~~r\n> d <- 1:3\n> d %o% d\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    3    6    9\n\n~~~\n\n\n### %$% (값 할당하기)\n**magrittr** 패키지의 %$% 연산자는 데이터 프레임이 중심이 되는 분석을 할때 사용하는 단순하지만 유용한 연산자입니다. \n**기능은 데이터 프레임에서 단순히 특정 변수를 추출하는 것입니다.**\n~~~r\nlibrary(magrittr)\n\nmtcars %$%\n  cor(disp, mpg)\n\n~~~\n\n---\n## 특정 단축키를 addin으로 Rstudio에 추가하기\n\n\n특별한 연산자들은 유용하지만 %>%와 같이 Rstudio에서 미리 단축키로 지정해놓지 않은 연산자들을 매번 타이핑해서 사용하는 것은 귀찮은 일입니다.  \n따라서 자주 사용하는 연산자의 경우 Rstudio addin을 사용해 %in% 처럼 단축키를 만들어 주는 것을 고려해볼 수 있습니다.\n\n~~~r\n # 사용자 정의 단축키를 추가하는 Rstudio addin 설치\n\ndevtools::install_github(\"rstudio/addinexamples\", type = \"source\")\n\n~~~\n\n\n**References & annotation**\n---\n\n  - https://www.datamentor.io/r-programming/infix-operator/\n  - https://github.com/r-lib/zeallot\n  - https://rfriend.tistory.com/35\n  - https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them # Binary operator에 대한 자세한 설명이 나와있습니다.","slug":"R-Programming-operators","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscm0039b36qh9pjdqsa","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>R로 분석을 하다보면 자연스럽게 손에 익게 되는 몇가지 연산자들이 있습니다. 이 포스팅에서는 R에서 사용되는 특별한 연산자들과 특정 연산자를 단축키로 Rstudio에 추가하는 법을 다룹니다.</p>\n<hr>\n<h2 id=\"연산자들\"><a href=\"#연산자들\" class=\"headerlink\" title=\"연산자들\"></a>연산자들</h2><h3 id=\"in-matching-연산자\"><a href=\"#in-matching-연산자\" class=\"headerlink\" title=\"%in% (matching 연산자)\"></a>%in% (matching 연산자)</h3><p><strong>특정 vector 내에 원하는 요소가 있는지 확인하고 이를 반환할 때 사용합니다.</strong><br>x%in%y 일 경우 x 기준으로 y와 매칭되는 값에 대한 논리값을 반환합니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">a <span class=\"operator\">&lt;-</span> seq<span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">5</span><span class=\"punctuation\">)</span></span><br><span class=\"line\">b <span class=\"operator\">&lt;-</span> seq<span class=\"punctuation\">(</span><span class=\"number\">3</span><span class=\"punctuation\">,</span> <span class=\"number\">12</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> b <span class=\"operator\">%in%</span> a  </span><br><span class=\"line\"> <span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> a <span class=\"operator\">%in%</span> b</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"gt-pipe\"><a href=\"#gt-pipe\" class=\"headerlink\" title=\"%&gt;% (pipe)\"></a>%&gt;% (pipe)</h3><p>tidyverse에 포함되어 있어 아마도 가장 유명할 연산자인 pipe operator입니다. </p>\n<p>자주 쓰이기 때문에 Rstudio에 ctrl+shift+m으로 단축키가 지정되어 있습니다.</p>\n<p><strong>기능은 함수들을 연결해 직관적으로 전달하는 것입니다.</strong><br>f(g(x)) 와 같은 합성함수를 R 코드 상에서 직관적으로 구현한 것이라고 이해하면 될 것 같습니다.</p>\n<p>이 연산자는 특히 데이터 분석에 유용한데 파이프 연산자를 사용하면 코드 가독성을 저해하는 분석 과정상에서의 중간 객체를 만들 필요가 없어지기 때문입니다. </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">iris <span class=\"operator\">%&gt;%</span> <span class=\"comment\"># df에 연속적으로 함수를 전달함</span></span><br><span class=\"line\">  subset<span class=\"punctuation\">(</span>Sepal.Length <span class=\"operator\">&gt;</span> <span class=\"number\">3</span><span class=\"punctuation\">)</span> <span class=\"operator\">%&gt;%</span></span><br><span class=\"line\">  aggregate<span class=\"punctuation\">(</span>. <span class=\"operator\">~</span> Species<span class=\"punctuation\">,</span> .<span class=\"punctuation\">,</span> mean<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"lt-unpacking\"><a href=\"#lt-unpacking\" class=\"headerlink\" title=\"%&lt;-%  (unpacking)\"></a>%&lt;-%  (unpacking)</h3><p>생각보다 자주 사용하게 되는 unpacking 연산자 입니다.<br><strong>기능은 list나 vector를 분해해서 이름을 할당하는 것입니다.</strong><br>list object인 선형회귀 모형의 특정 요소들을 불러와 이름을 할당해 독립적인 객체로 만들 수 있습니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">library<span class=\"punctuation\">(</span>zeallot<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">m<span class=\"operator\">&lt;-</span> lm<span class=\"punctuation\">(</span>hp <span class=\"operator\">~</span> gear<span class=\"punctuation\">,</span> data <span class=\"operator\">=</span> mtcars<span class=\"punctuation\">)</span></span><br><span class=\"line\"><span class=\"built_in\">c</span><span class=\"punctuation\">(</span>mcall<span class=\"punctuation\">,</span>...<span class=\"punctuation\">,</span>mdf<span class=\"punctuation\">,</span>mstat<span class=\"punctuation\">)</span> <span class=\"operator\">%&lt;-%</span> summary<span class=\"punctuation\">(</span>m<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mcall <span class=\"comment\"># 모델식</span></span><br><span class=\"line\">lm<span class=\"punctuation\">(</span>formula <span class=\"operator\">=</span> hp <span class=\"operator\">~</span> gear<span class=\"punctuation\">,</span> data <span class=\"operator\">=</span> mtcars<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> mdf <span class=\"comment\"># 자유도</span></span><br><span class=\"line\">     value      numdf      dendf </span><br><span class=\"line\"> <span class=\"number\">0.4816578</span>  <span class=\"number\">1.0000000</span> <span class=\"number\">30.0000000</span> </span><br><span class=\"line\"><span class=\"operator\">&gt;</span> mstat <span class=\"comment\"># 통계량</span></span><br><span class=\"line\">            <span class=\"punctuation\">(</span>Intercept<span class=\"punctuation\">)</span>        gear</span><br><span class=\"line\"><span class=\"punctuation\">(</span>Intercept<span class=\"punctuation\">)</span>   <span class=\"number\">0.8370370</span> <span class=\"operator\">-</span><span class=\"number\">0.21851852</span></span><br><span class=\"line\">gear         <span class=\"operator\">-</span><span class=\"number\">0.2185185</span>  <span class=\"number\">0.05925926</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"x-행렬곱-벡터의-내적\"><a href=\"#x-행렬곱-벡터의-내적\" class=\"headerlink\" title=\"%x% (행렬곱-벡터의 내적)\"></a>%x% (행렬곱-벡터의 내적)</h3><p><strong>%x%는 벡터의 내적을 반환합니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">c</span> <span class=\"operator\">&lt;-</span> matrix<span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"operator\">:</span><span class=\"number\">4</span><span class=\"punctuation\">,</span>nrow <span class=\"operator\">=</span> <span class=\"number\">2</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> <span class=\"built_in\">c</span> <span class=\"operator\">%*%</span> <span class=\"built_in\">c</span></span><br><span class=\"line\">     <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">7</span>   <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>   <span class=\"number\">10</span>   <span class=\"number\">22</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"o-행렬곱-벡터의-외적\"><a href=\"#o-행렬곱-벡터의-외적\" class=\"headerlink\" title=\"%o% (행렬곱-벡터의 외적)\"></a>%o% (행렬곱-벡터의 외적)</h3><p><strong>%o%는 벡터의 외적을 반환합니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"operator\">&gt;</span> d <span class=\"operator\">&lt;-</span> 1<span class=\"operator\">:</span><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> d <span class=\"operator\">%o%</span> d</span><br><span class=\"line\">     <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">3</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">2</span>    <span class=\"number\">4</span>    <span class=\"number\">6</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">3</span>    <span class=\"number\">6</span>    <span class=\"number\">9</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"값-할당하기\"><a href=\"#값-할당하기\" class=\"headerlink\" title=\"%$% (값 할당하기)\"></a>%$% (값 할당하기)</h3><p><strong>magrittr</strong> 패키지의 %$% 연산자는 데이터 프레임이 중심이 되는 분석을 할때 사용하는 단순하지만 유용한 연산자입니다.<br><strong>기능은 데이터 프레임에서 단순히 특정 변수를 추출하는 것입니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">library<span class=\"punctuation\">(</span>magrittr<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mtcars <span class=\"operator\">%$%</span></span><br><span class=\"line\">  cor<span class=\"punctuation\">(</span>disp<span class=\"punctuation\">,</span> mpg<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"특정-단축키를-addin으로-Rstudio에-추가하기\"><a href=\"#특정-단축키를-addin으로-Rstudio에-추가하기\" class=\"headerlink\" title=\"특정 단축키를 addin으로 Rstudio에 추가하기\"></a>특정 단축키를 addin으로 Rstudio에 추가하기</h2><p>특별한 연산자들은 유용하지만 %&gt;%와 같이 Rstudio에서 미리 단축키로 지정해놓지 않은 연산자들을 매번 타이핑해서 사용하는 것은 귀찮은 일입니다.<br>따라서 자주 사용하는 연산자의 경우 Rstudio addin을 사용해 %in% 처럼 단축키를 만들어 주는 것을 고려해볼 수 있습니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\"># 사용자 정의 단축키를 추가하는 Rstudio addin 설치</span></span><br><span class=\"line\"></span><br><span class=\"line\">devtools<span class=\"operator\">::</span>install_github<span class=\"punctuation\">(</span><span class=\"string\">&quot;rstudio/addinexamples&quot;</span><span class=\"punctuation\">,</span> type <span class=\"operator\">=</span> <span class=\"string\">&quot;source&quot;</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.datamentor.io/r-programming/infix-operator/\">https://www.datamentor.io/r-programming/infix-operator/</a></li>\n<li><a href=\"https://github.com/r-lib/zeallot\">https://github.com/r-lib/zeallot</a></li>\n<li><a href=\"https://rfriend.tistory.com/35\">https://rfriend.tistory.com/35</a></li>\n<li><a href=\"https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them\">https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them</a> # Binary operator에 대한 자세한 설명이 나와있습니다.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>R로 분석을 하다보면 자연스럽게 손에 익게 되는 몇가지 연산자들이 있습니다. 이 포스팅에서는 R에서 사용되는 특별한 연산자들과 특정 연산자를 단축키로 Rstudio에 추가하는 법을 다룹니다.</p>\n<hr>\n<h2 id=\"연산자들\"><a href=\"#연산자들\" class=\"headerlink\" title=\"연산자들\"></a>연산자들</h2><h3 id=\"in-matching-연산자\"><a href=\"#in-matching-연산자\" class=\"headerlink\" title=\"%in% (matching 연산자)\"></a>%in% (matching 연산자)</h3><p><strong>특정 vector 내에 원하는 요소가 있는지 확인하고 이를 반환할 때 사용합니다.</strong><br>x%in%y 일 경우 x 기준으로 y와 매칭되는 값에 대한 논리값을 반환합니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">a <span class=\"operator\">&lt;-</span> seq<span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">5</span><span class=\"punctuation\">)</span></span><br><span class=\"line\">b <span class=\"operator\">&lt;-</span> seq<span class=\"punctuation\">(</span><span class=\"number\">3</span><span class=\"punctuation\">,</span> <span class=\"number\">12</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> b <span class=\"operator\">%in%</span> a  </span><br><span class=\"line\"> <span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> a <span class=\"operator\">%in%</span> b</span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"literal\">FALSE</span> <span class=\"literal\">FALSE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span>  <span class=\"literal\">TRUE</span></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"gt-pipe\"><a href=\"#gt-pipe\" class=\"headerlink\" title=\"%&gt;% (pipe)\"></a>%&gt;% (pipe)</h3><p>tidyverse에 포함되어 있어 아마도 가장 유명할 연산자인 pipe operator입니다. </p>\n<p>자주 쓰이기 때문에 Rstudio에 ctrl+shift+m으로 단축키가 지정되어 있습니다.</p>\n<p><strong>기능은 함수들을 연결해 직관적으로 전달하는 것입니다.</strong><br>f(g(x)) 와 같은 합성함수를 R 코드 상에서 직관적으로 구현한 것이라고 이해하면 될 것 같습니다.</p>\n<p>이 연산자는 특히 데이터 분석에 유용한데 파이프 연산자를 사용하면 코드 가독성을 저해하는 분석 과정상에서의 중간 객체를 만들 필요가 없어지기 때문입니다. </p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">iris <span class=\"operator\">%&gt;%</span> <span class=\"comment\"># df에 연속적으로 함수를 전달함</span></span><br><span class=\"line\">  subset<span class=\"punctuation\">(</span>Sepal.Length <span class=\"operator\">&gt;</span> <span class=\"number\">3</span><span class=\"punctuation\">)</span> <span class=\"operator\">%&gt;%</span></span><br><span class=\"line\">  aggregate<span class=\"punctuation\">(</span>. <span class=\"operator\">~</span> Species<span class=\"punctuation\">,</span> .<span class=\"punctuation\">,</span> mean<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"lt-unpacking\"><a href=\"#lt-unpacking\" class=\"headerlink\" title=\"%&lt;-%  (unpacking)\"></a>%&lt;-%  (unpacking)</h3><p>생각보다 자주 사용하게 되는 unpacking 연산자 입니다.<br><strong>기능은 list나 vector를 분해해서 이름을 할당하는 것입니다.</strong><br>list object인 선형회귀 모형의 특정 요소들을 불러와 이름을 할당해 독립적인 객체로 만들 수 있습니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">library<span class=\"punctuation\">(</span>zeallot<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">m<span class=\"operator\">&lt;-</span> lm<span class=\"punctuation\">(</span>hp <span class=\"operator\">~</span> gear<span class=\"punctuation\">,</span> data <span class=\"operator\">=</span> mtcars<span class=\"punctuation\">)</span></span><br><span class=\"line\"><span class=\"built_in\">c</span><span class=\"punctuation\">(</span>mcall<span class=\"punctuation\">,</span>...<span class=\"punctuation\">,</span>mdf<span class=\"punctuation\">,</span>mstat<span class=\"punctuation\">)</span> <span class=\"operator\">%&lt;-%</span> summary<span class=\"punctuation\">(</span>m<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mcall <span class=\"comment\"># 모델식</span></span><br><span class=\"line\">lm<span class=\"punctuation\">(</span>formula <span class=\"operator\">=</span> hp <span class=\"operator\">~</span> gear<span class=\"punctuation\">,</span> data <span class=\"operator\">=</span> mtcars<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> mdf <span class=\"comment\"># 자유도</span></span><br><span class=\"line\">     value      numdf      dendf </span><br><span class=\"line\"> <span class=\"number\">0.4816578</span>  <span class=\"number\">1.0000000</span> <span class=\"number\">30.0000000</span> </span><br><span class=\"line\"><span class=\"operator\">&gt;</span> mstat <span class=\"comment\"># 통계량</span></span><br><span class=\"line\">            <span class=\"punctuation\">(</span>Intercept<span class=\"punctuation\">)</span>        gear</span><br><span class=\"line\"><span class=\"punctuation\">(</span>Intercept<span class=\"punctuation\">)</span>   <span class=\"number\">0.8370370</span> <span class=\"operator\">-</span><span class=\"number\">0.21851852</span></span><br><span class=\"line\">gear         <span class=\"operator\">-</span><span class=\"number\">0.2185185</span>  <span class=\"number\">0.05925926</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"x-행렬곱-벡터의-내적\"><a href=\"#x-행렬곱-벡터의-내적\" class=\"headerlink\" title=\"%x% (행렬곱-벡터의 내적)\"></a>%x% (행렬곱-벡터의 내적)</h3><p><strong>%x%는 벡터의 내적을 반환합니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">c</span> <span class=\"operator\">&lt;-</span> matrix<span class=\"punctuation\">(</span><span class=\"number\">1</span><span class=\"operator\">:</span><span class=\"number\">4</span><span class=\"punctuation\">,</span>nrow <span class=\"operator\">=</span> <span class=\"number\">2</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> <span class=\"built_in\">c</span> <span class=\"operator\">%*%</span> <span class=\"built_in\">c</span></span><br><span class=\"line\">     <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">7</span>   <span class=\"number\">15</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>   <span class=\"number\">10</span>   <span class=\"number\">22</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"o-행렬곱-벡터의-외적\"><a href=\"#o-행렬곱-벡터의-외적\" class=\"headerlink\" title=\"%o% (행렬곱-벡터의 외적)\"></a>%o% (행렬곱-벡터의 외적)</h3><p><strong>%o%는 벡터의 외적을 반환합니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"operator\">&gt;</span> d <span class=\"operator\">&lt;-</span> 1<span class=\"operator\">:</span><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"operator\">&gt;</span> d <span class=\"operator\">%o%</span> d</span><br><span class=\"line\">     <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">1</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">2</span><span class=\"punctuation\">]</span> <span class=\"punctuation\">[</span><span class=\"punctuation\">,</span><span class=\"number\">3</span><span class=\"punctuation\">]</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">1</span>    <span class=\"number\">2</span>    <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">2</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">2</span>    <span class=\"number\">4</span>    <span class=\"number\">6</span></span><br><span class=\"line\"><span class=\"punctuation\">[</span><span class=\"number\">3</span><span class=\"punctuation\">,</span><span class=\"punctuation\">]</span>    <span class=\"number\">3</span>    <span class=\"number\">6</span>    <span class=\"number\">9</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"값-할당하기\"><a href=\"#값-할당하기\" class=\"headerlink\" title=\"%$% (값 할당하기)\"></a>%$% (값 할당하기)</h3><p><strong>magrittr</strong> 패키지의 %$% 연산자는 데이터 프레임이 중심이 되는 분석을 할때 사용하는 단순하지만 유용한 연산자입니다.<br><strong>기능은 데이터 프레임에서 단순히 특정 변수를 추출하는 것입니다.</strong></p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">library<span class=\"punctuation\">(</span>magrittr<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mtcars <span class=\"operator\">%$%</span></span><br><span class=\"line\">  cor<span class=\"punctuation\">(</span>disp<span class=\"punctuation\">,</span> mpg<span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"특정-단축키를-addin으로-Rstudio에-추가하기\"><a href=\"#특정-단축키를-addin으로-Rstudio에-추가하기\" class=\"headerlink\" title=\"특정 단축키를 addin으로 Rstudio에 추가하기\"></a>특정 단축키를 addin으로 Rstudio에 추가하기</h2><p>특별한 연산자들은 유용하지만 %&gt;%와 같이 Rstudio에서 미리 단축키로 지정해놓지 않은 연산자들을 매번 타이핑해서 사용하는 것은 귀찮은 일입니다.<br>따라서 자주 사용하는 연산자의 경우 Rstudio addin을 사용해 %in% 처럼 단축키를 만들어 주는 것을 고려해볼 수 있습니다.</p>\n<figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\"># 사용자 정의 단축키를 추가하는 Rstudio addin 설치</span></span><br><span class=\"line\"></span><br><span class=\"line\">devtools<span class=\"operator\">::</span>install_github<span class=\"punctuation\">(</span><span class=\"string\">&quot;rstudio/addinexamples&quot;</span><span class=\"punctuation\">,</span> type <span class=\"operator\">=</span> <span class=\"string\">&quot;source&quot;</span><span class=\"punctuation\">)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.datamentor.io/r-programming/infix-operator/\">https://www.datamentor.io/r-programming/infix-operator/</a></li>\n<li><a href=\"https://github.com/r-lib/zeallot\">https://github.com/r-lib/zeallot</a></li>\n<li><a href=\"https://rfriend.tistory.com/35\">https://rfriend.tistory.com/35</a></li>\n<li><a href=\"https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them\">https://stackoverflow.com/questions/25179457/r-what-are-operators-like-in-called-and-how-can-i-learn-about-them</a> # Binary operator에 대한 자세한 설명이 나와있습니다.</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[R]특별한 R 연산자들(Binary Operators)","path":"2021/03/02/R-Programming-operators/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-03-02T05:11:05.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-03-02T05:11:05.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Programming","tags":["R"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Math]미적분 기초개념","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n>미분의 정의, Data Science에서 미분이 필요한 이유, 미분 공식들에 대해 알아보자.\n\n## 미분\n\n- 함수의 input인 x에 대해서 나오는 결과값이 변화하는 정도를(0에 근사하는 부분을 탐색) 계산하는 것\n- 실제로 계산하는 것은 x의 변화량인 delta_x가 한없이 0에 가까워 질 때의 기울기이다.\n- **Data Science에서 미분이 필요한 이유는 기본적으로 모델의 오차함수가 최소화되는 지점(오차함수의 변화율이 0이 되는 지점)을 찾을 때 미분이 활용되기 때문이다.(최적화 문제)**\n\n## 미분 공식\n\nnumerical method만큼은 확실히 이해하고 넘어가자.\n\n### numerical  method\n\n실제로 0으로 나눌 수는 없기 때문에 Delta_x를 0에 근사한 값인 1e-5로 나눠준다.이를 numerical method라 한다.\n\n#### 미분 기본공식\n\n$$ f'(x) = {f(x + \\Delta x) - f(x) \\over \\Delta x}, \\Delta x \\rightarrow 0~ $$\n\n\n\n\n### **power rule**\n\n멱함수의 도함수를 구하는 그나마 익숙한 미분규칙\n\n$ \\frac{d} { {dx} }x^n=nx^{n-1} $ \n\n#### **chain rule**\n\n합성함수에 대한 미분규칙. 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.\n\n\n$$ \\frac{dy} {dx} = \\frac{dy} {du} \\times \\frac{du} {dx} $$\n\n\n좀 더 이해하기 쉽게 나타내면 아래와 같다.\n\n\n$$F(x) = f(g(x))$$\n\n$$F'(x) \\rightarrow f'((g(x)) \\cdot g'(x)$$\n\n\n#### **Exponential**\n\n지수함수에 대한 미분규칙. 지수함수의 경우 도함수도 지수함수이다.\n\n$$ f(x) = e^x \\rightarrow f'(x) = e^x $$\n\n#### **Logarithmic**\n\n자연로그에 대한 미분규칙.Logistic Regression이나 Section sigmoid 함수를 미분하는데 도움을 준다.\n\n$$f(x) = lnx \\rightarrow f'(x) = { {1} \\over {x} } $$\n\n#### **product rule**\n\n두 함수의 곱으로 이루어진 함수에 대한 미분규칙.\n\n$$\\frac{d}{ {dx} }\\left( {f\\left( x \\right)g\\left( x \\right)} \\right) = f\\left( x \\right)\\frac{d} { {dx} }g\\left( x \\right) + \\frac{d}{ {dx} }f\\left( x \\right)g\\left( x \\right)$$\n\n\n#### **quotinent rule**\n\n분수형태로 생긴 합성함수에 대한 미분규칙. 시그모이드 함수의 도함수를 구할 때 사용된다.\n\n$$\\frac{d}{ {dx} }\\left( {\\frac{ {f\\left( x \\right)} } { {g\\left( x \\right)} } } \\right) = \\frac{ {\\frac{d} { {dx} }f\\left( x \\right)g\\left( x \\right) - f\\left( x \\right)\\frac{d}{ {dx} }g\\left( x \\right)} } { {g^2 \\left( x \\right)} }$$\n\n\n### **편미분(Partial Derivtives)**\n\n- 편미분은 다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것이다.\n- 최적화 관점에서 보면 파라미터가 2개 이상인 Error 함수에 대해 **우선 1개의 파라미터에 대해서만 미분을 한다**는 것이다.\n- 편미분은 ${\\partial y} \\over {\\partial x}$ 와 같이 나타내며 이 경우 x에 대해 편미분한다 하며 x를 제외한 나머지 변수는 상수취급하고 미분한다.\n- **선형회귀에서 오차함수의 최소값을 유도할때 사용된다**.\n- ex) x에 대해 편미분할 경우\n\n$$ f(x,y) = x^2 + 2xy + y^2$$\n\n$${ {\\partial f(x,y)} \\over {\\partial x} } = { {\\partial {(x^2 + 2xy + y^2)} } \\over {\\partial x}} = 2x + 2y$$\n\n## sympy를 활욯한 미분계산\n\n실무에서 이런식으로 따로 미분을 할 일은 없지만 구현한다는 것에 의의를 두자.\n\n```python\n#import sympy\nfrom sympy import *\n \n# Symbol정의하기\nx = Symbol('x')\n \n# 함수 정의하기\nf = x**4\n \n#도함수 계산하기\nderivative_f = f.diff(x)\n \nderivative_f\n```\n\n## References\n\n- <https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant>.\n- <https://youtu.be/H-ybCx8gt-8>\n- <https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84>\n","source":"_posts/Statistics-Math-derivatives.md","raw":"---\ntitle: \"[Math]미적분 기초개념\"\ndate: \nupdated:\ncategories: \n        - [Statistics]\ntags:\nmathjax: true\n---\n\n>미분의 정의, Data Science에서 미분이 필요한 이유, 미분 공식들에 대해 알아보자.\n\n## 미분\n\n- 함수의 input인 x에 대해서 나오는 결과값이 변화하는 정도를(0에 근사하는 부분을 탐색) 계산하는 것\n- 실제로 계산하는 것은 x의 변화량인 delta_x가 한없이 0에 가까워 질 때의 기울기이다.\n- **Data Science에서 미분이 필요한 이유는 기본적으로 모델의 오차함수가 최소화되는 지점(오차함수의 변화율이 0이 되는 지점)을 찾을 때 미분이 활용되기 때문이다.(최적화 문제)**\n\n## 미분 공식\n\nnumerical method만큼은 확실히 이해하고 넘어가자.\n\n### numerical  method\n\n실제로 0으로 나눌 수는 없기 때문에 Delta_x를 0에 근사한 값인 1e-5로 나눠준다.이를 numerical method라 한다.\n\n#### 미분 기본공식\n\n$$ f'(x) = {f(x + \\Delta x) - f(x) \\over \\Delta x}, \\Delta x \\rightarrow 0~ $$\n\n\n\n\n### **power rule**\n\n멱함수의 도함수를 구하는 그나마 익숙한 미분규칙\n\n$ \\frac{d} { {dx} }x^n=nx^{n-1} $ \n\n#### **chain rule**\n\n합성함수에 대한 미분규칙. 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.\n\n\n$$ \\frac{dy} {dx} = \\frac{dy} {du} \\times \\frac{du} {dx} $$\n\n\n좀 더 이해하기 쉽게 나타내면 아래와 같다.\n\n\n$$F(x) = f(g(x))$$\n\n$$F'(x) \\rightarrow f'((g(x)) \\cdot g'(x)$$\n\n\n#### **Exponential**\n\n지수함수에 대한 미분규칙. 지수함수의 경우 도함수도 지수함수이다.\n\n$$ f(x) = e^x \\rightarrow f'(x) = e^x $$\n\n#### **Logarithmic**\n\n자연로그에 대한 미분규칙.Logistic Regression이나 Section sigmoid 함수를 미분하는데 도움을 준다.\n\n$$f(x) = lnx \\rightarrow f'(x) = { {1} \\over {x} } $$\n\n#### **product rule**\n\n두 함수의 곱으로 이루어진 함수에 대한 미분규칙.\n\n$$\\frac{d}{ {dx} }\\left( {f\\left( x \\right)g\\left( x \\right)} \\right) = f\\left( x \\right)\\frac{d} { {dx} }g\\left( x \\right) + \\frac{d}{ {dx} }f\\left( x \\right)g\\left( x \\right)$$\n\n\n#### **quotinent rule**\n\n분수형태로 생긴 합성함수에 대한 미분규칙. 시그모이드 함수의 도함수를 구할 때 사용된다.\n\n$$\\frac{d}{ {dx} }\\left( {\\frac{ {f\\left( x \\right)} } { {g\\left( x \\right)} } } \\right) = \\frac{ {\\frac{d} { {dx} }f\\left( x \\right)g\\left( x \\right) - f\\left( x \\right)\\frac{d}{ {dx} }g\\left( x \\right)} } { {g^2 \\left( x \\right)} }$$\n\n\n### **편미분(Partial Derivtives)**\n\n- 편미분은 다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것이다.\n- 최적화 관점에서 보면 파라미터가 2개 이상인 Error 함수에 대해 **우선 1개의 파라미터에 대해서만 미분을 한다**는 것이다.\n- 편미분은 ${\\partial y} \\over {\\partial x}$ 와 같이 나타내며 이 경우 x에 대해 편미분한다 하며 x를 제외한 나머지 변수는 상수취급하고 미분한다.\n- **선형회귀에서 오차함수의 최소값을 유도할때 사용된다**.\n- ex) x에 대해 편미분할 경우\n\n$$ f(x,y) = x^2 + 2xy + y^2$$\n\n$${ {\\partial f(x,y)} \\over {\\partial x} } = { {\\partial {(x^2 + 2xy + y^2)} } \\over {\\partial x}} = 2x + 2y$$\n\n## sympy를 활욯한 미분계산\n\n실무에서 이런식으로 따로 미분을 할 일은 없지만 구현한다는 것에 의의를 두자.\n\n```python\n#import sympy\nfrom sympy import *\n \n# Symbol정의하기\nx = Symbol('x')\n \n# 함수 정의하기\nf = x**4\n \n#도함수 계산하기\nderivative_f = f.diff(x)\n \nderivative_f\n```\n\n## References\n\n- <https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant>.\n- <https://youtu.be/H-ybCx8gt-8>\n- <https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84>\n","slug":"Statistics-Math-derivatives","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscn003cb36q26m0cgeq","content":"<blockquote>\n<p>미분의 정의, Data Science에서 미분이 필요한 이유, 미분 공식들에 대해 알아보자.</p>\n</blockquote>\n<h2 id=\"미분\"><a href=\"#미분\" class=\"headerlink\" title=\"미분\"></a>미분</h2><ul>\n<li>함수의 input인 x에 대해서 나오는 결과값이 변화하는 정도를(0에 근사하는 부분을 탐색) 계산하는 것</li>\n<li>실제로 계산하는 것은 x의 변화량인 delta_x가 한없이 0에 가까워 질 때의 기울기이다.</li>\n<li><strong>Data Science에서 미분이 필요한 이유는 기본적으로 모델의 오차함수가 최소화되는 지점(오차함수의 변화율이 0이 되는 지점)을 찾을 때 미분이 활용되기 때문이다.(최적화 문제)</strong></li>\n</ul>\n<h2 id=\"미분-공식\"><a href=\"#미분-공식\" class=\"headerlink\" title=\"미분 공식\"></a>미분 공식</h2><p>numerical method만큼은 확실히 이해하고 넘어가자.</p>\n<h3 id=\"numerical-method\"><a href=\"#numerical-method\" class=\"headerlink\" title=\"numerical  method\"></a>numerical  method</h3><p>실제로 0으로 나눌 수는 없기 때문에 Delta_x를 0에 근사한 값인 1e-5로 나눠준다.이를 numerical method라 한다.</p>\n<h4 id=\"미분-기본공식\"><a href=\"#미분-기본공식\" class=\"headerlink\" title=\"미분 기본공식\"></a>미분 기본공식</h4><p>$$ f’(x) &#x3D; {f(x + \\Delta x) - f(x) \\over \\Delta x}, \\Delta x \\rightarrow 0~ $$</p>\n<h3 id=\"power-rule\"><a href=\"#power-rule\" class=\"headerlink\" title=\"power rule\"></a><strong>power rule</strong></h3><p>멱함수의 도함수를 구하는 그나마 익숙한 미분규칙</p>\n<p>$ \\frac{d} { {dx} }x^n&#x3D;nx^{n-1} $ </p>\n<h4 id=\"chain-rule\"><a href=\"#chain-rule\" class=\"headerlink\" title=\"chain rule\"></a><strong>chain rule</strong></h4><p>합성함수에 대한 미분규칙. 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.</p>\n<p>$$ \\frac{dy} {dx} &#x3D; \\frac{dy} {du} \\times \\frac{du} {dx} $$</p>\n<p>좀 더 이해하기 쉽게 나타내면 아래와 같다.</p>\n<p>$$F(x) &#x3D; f(g(x))$$</p>\n<p>$$F’(x) \\rightarrow f’((g(x)) \\cdot g’(x)$$</p>\n<h4 id=\"Exponential\"><a href=\"#Exponential\" class=\"headerlink\" title=\"Exponential\"></a><strong>Exponential</strong></h4><p>지수함수에 대한 미분규칙. 지수함수의 경우 도함수도 지수함수이다.</p>\n<p>$$ f(x) &#x3D; e^x \\rightarrow f’(x) &#x3D; e^x $$</p>\n<h4 id=\"Logarithmic\"><a href=\"#Logarithmic\" class=\"headerlink\" title=\"Logarithmic\"></a><strong>Logarithmic</strong></h4><p>자연로그에 대한 미분규칙.Logistic Regression이나 Section sigmoid 함수를 미분하는데 도움을 준다.</p>\n<p>$$f(x) &#x3D; lnx \\rightarrow f’(x) &#x3D; { {1} \\over {x} } $$</p>\n<h4 id=\"product-rule\"><a href=\"#product-rule\" class=\"headerlink\" title=\"product rule\"></a><strong>product rule</strong></h4><p>두 함수의 곱으로 이루어진 함수에 대한 미분규칙.</p>\n<p>$$\\frac{d}{ {dx} }\\left( {f\\left( x \\right)g\\left( x \\right)} \\right) &#x3D; f\\left( x \\right)\\frac{d} { {dx} }g\\left( x \\right) + \\frac{d}{ {dx} }f\\left( x \\right)g\\left( x \\right)$$</p>\n<h4 id=\"quotinent-rule\"><a href=\"#quotinent-rule\" class=\"headerlink\" title=\"quotinent rule\"></a><strong>quotinent rule</strong></h4><p>분수형태로 생긴 합성함수에 대한 미분규칙. 시그모이드 함수의 도함수를 구할 때 사용된다.</p>\n<p>$$\\frac{d}{ {dx} }\\left( {\\frac{ {f\\left( x \\right)} } { {g\\left( x \\right)} } } \\right) &#x3D; \\frac{ {\\frac{d} { {dx} }f\\left( x \\right)g\\left( x \\right) - f\\left( x \\right)\\frac{d}{ {dx} }g\\left( x \\right)} } { {g^2 \\left( x \\right)} }$$</p>\n<h3 id=\"편미분-Partial-Derivtives\"><a href=\"#편미분-Partial-Derivtives\" class=\"headerlink\" title=\"편미분(Partial Derivtives)\"></a><strong>편미분(Partial Derivtives)</strong></h3><ul>\n<li>편미분은 다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것이다.</li>\n<li>최적화 관점에서 보면 파라미터가 2개 이상인 Error 함수에 대해 <strong>우선 1개의 파라미터에 대해서만 미분을 한다</strong>는 것이다.</li>\n<li>편미분은 ${\\partial y} \\over {\\partial x}$ 와 같이 나타내며 이 경우 x에 대해 편미분한다 하며 x를 제외한 나머지 변수는 상수취급하고 미분한다.</li>\n<li><strong>선형회귀에서 오차함수의 최소값을 유도할때 사용된다</strong>.</li>\n<li>ex) x에 대해 편미분할 경우</li>\n</ul>\n<p>$$ f(x,y) &#x3D; x^2 + 2xy + y^2$$</p>\n<p>$${ {\\partial f(x,y)} \\over {\\partial x} } &#x3D; { {\\partial {(x^2 + 2xy + y^2)} } \\over {\\partial x}} &#x3D; 2x + 2y$$</p>\n<h2 id=\"sympy를-활욯한-미분계산\"><a href=\"#sympy를-활욯한-미분계산\" class=\"headerlink\" title=\"sympy를 활욯한 미분계산\"></a>sympy를 활욯한 미분계산</h2><p>실무에서 이런식으로 따로 미분을 할 일은 없지만 구현한다는 것에 의의를 두자.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#import sympy</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sympy <span class=\"keyword\">import</span> *</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Symbol정의하기</span></span><br><span class=\"line\">x = Symbol(<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 함수 정의하기</span></span><br><span class=\"line\">f = x**<span class=\"number\">4</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">#도함수 계산하기</span></span><br><span class=\"line\">derivative_f = f.diff(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">derivative_f</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&amp;text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant\">https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&amp;text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant</a>.</li>\n<li><a href=\"https://youtu.be/H-ybCx8gt-8\">https://youtu.be/H-ybCx8gt-8</a></li>\n<li><a href=\"https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84\">https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>미분의 정의, Data Science에서 미분이 필요한 이유, 미분 공식들에 대해 알아보자.</p>\n</blockquote>\n<h2 id=\"미분\"><a href=\"#미분\" class=\"headerlink\" title=\"미분\"></a>미분</h2><ul>\n<li>함수의 input인 x에 대해서 나오는 결과값이 변화하는 정도를(0에 근사하는 부분을 탐색) 계산하는 것</li>\n<li>실제로 계산하는 것은 x의 변화량인 delta_x가 한없이 0에 가까워 질 때의 기울기이다.</li>\n<li><strong>Data Science에서 미분이 필요한 이유는 기본적으로 모델의 오차함수가 최소화되는 지점(오차함수의 변화율이 0이 되는 지점)을 찾을 때 미분이 활용되기 때문이다.(최적화 문제)</strong></li>\n</ul>\n<h2 id=\"미분-공식\"><a href=\"#미분-공식\" class=\"headerlink\" title=\"미분 공식\"></a>미분 공식</h2><p>numerical method만큼은 확실히 이해하고 넘어가자.</p>\n<h3 id=\"numerical-method\"><a href=\"#numerical-method\" class=\"headerlink\" title=\"numerical  method\"></a>numerical  method</h3><p>실제로 0으로 나눌 수는 없기 때문에 Delta_x를 0에 근사한 값인 1e-5로 나눠준다.이를 numerical method라 한다.</p>\n<h4 id=\"미분-기본공식\"><a href=\"#미분-기본공식\" class=\"headerlink\" title=\"미분 기본공식\"></a>미분 기본공식</h4><p>$$ f’(x) &#x3D; {f(x + \\Delta x) - f(x) \\over \\Delta x}, \\Delta x \\rightarrow 0~ $$</p>\n<h3 id=\"power-rule\"><a href=\"#power-rule\" class=\"headerlink\" title=\"power rule\"></a><strong>power rule</strong></h3><p>멱함수의 도함수를 구하는 그나마 익숙한 미분규칙</p>\n<p>$ \\frac{d} { {dx} }x^n&#x3D;nx^{n-1} $ </p>\n<h4 id=\"chain-rule\"><a href=\"#chain-rule\" class=\"headerlink\" title=\"chain rule\"></a><strong>chain rule</strong></h4><p>합성함수에 대한 미분규칙. 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.</p>\n<p>$$ \\frac{dy} {dx} &#x3D; \\frac{dy} {du} \\times \\frac{du} {dx} $$</p>\n<p>좀 더 이해하기 쉽게 나타내면 아래와 같다.</p>\n<p>$$F(x) &#x3D; f(g(x))$$</p>\n<p>$$F’(x) \\rightarrow f’((g(x)) \\cdot g’(x)$$</p>\n<h4 id=\"Exponential\"><a href=\"#Exponential\" class=\"headerlink\" title=\"Exponential\"></a><strong>Exponential</strong></h4><p>지수함수에 대한 미분규칙. 지수함수의 경우 도함수도 지수함수이다.</p>\n<p>$$ f(x) &#x3D; e^x \\rightarrow f’(x) &#x3D; e^x $$</p>\n<h4 id=\"Logarithmic\"><a href=\"#Logarithmic\" class=\"headerlink\" title=\"Logarithmic\"></a><strong>Logarithmic</strong></h4><p>자연로그에 대한 미분규칙.Logistic Regression이나 Section sigmoid 함수를 미분하는데 도움을 준다.</p>\n<p>$$f(x) &#x3D; lnx \\rightarrow f’(x) &#x3D; { {1} \\over {x} } $$</p>\n<h4 id=\"product-rule\"><a href=\"#product-rule\" class=\"headerlink\" title=\"product rule\"></a><strong>product rule</strong></h4><p>두 함수의 곱으로 이루어진 함수에 대한 미분규칙.</p>\n<p>$$\\frac{d}{ {dx} }\\left( {f\\left( x \\right)g\\left( x \\right)} \\right) &#x3D; f\\left( x \\right)\\frac{d} { {dx} }g\\left( x \\right) + \\frac{d}{ {dx} }f\\left( x \\right)g\\left( x \\right)$$</p>\n<h4 id=\"quotinent-rule\"><a href=\"#quotinent-rule\" class=\"headerlink\" title=\"quotinent rule\"></a><strong>quotinent rule</strong></h4><p>분수형태로 생긴 합성함수에 대한 미분규칙. 시그모이드 함수의 도함수를 구할 때 사용된다.</p>\n<p>$$\\frac{d}{ {dx} }\\left( {\\frac{ {f\\left( x \\right)} } { {g\\left( x \\right)} } } \\right) &#x3D; \\frac{ {\\frac{d} { {dx} }f\\left( x \\right)g\\left( x \\right) - f\\left( x \\right)\\frac{d}{ {dx} }g\\left( x \\right)} } { {g^2 \\left( x \\right)} }$$</p>\n<h3 id=\"편미분-Partial-Derivtives\"><a href=\"#편미분-Partial-Derivtives\" class=\"headerlink\" title=\"편미분(Partial Derivtives)\"></a><strong>편미분(Partial Derivtives)</strong></h3><ul>\n<li>편미분은 다변수 함수의 특정 변수를 제외한 나머지 변수를 상수로 간주하여 미분하는 것이다.</li>\n<li>최적화 관점에서 보면 파라미터가 2개 이상인 Error 함수에 대해 <strong>우선 1개의 파라미터에 대해서만 미분을 한다</strong>는 것이다.</li>\n<li>편미분은 ${\\partial y} \\over {\\partial x}$ 와 같이 나타내며 이 경우 x에 대해 편미분한다 하며 x를 제외한 나머지 변수는 상수취급하고 미분한다.</li>\n<li><strong>선형회귀에서 오차함수의 최소값을 유도할때 사용된다</strong>.</li>\n<li>ex) x에 대해 편미분할 경우</li>\n</ul>\n<p>$$ f(x,y) &#x3D; x^2 + 2xy + y^2$$</p>\n<p>$${ {\\partial f(x,y)} \\over {\\partial x} } &#x3D; { {\\partial {(x^2 + 2xy + y^2)} } \\over {\\partial x}} &#x3D; 2x + 2y$$</p>\n<h2 id=\"sympy를-활욯한-미분계산\"><a href=\"#sympy를-활욯한-미분계산\" class=\"headerlink\" title=\"sympy를 활욯한 미분계산\"></a>sympy를 활욯한 미분계산</h2><p>실무에서 이런식으로 따로 미분을 할 일은 없지만 구현한다는 것에 의의를 두자.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#import sympy</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sympy <span class=\"keyword\">import</span> *</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Symbol정의하기</span></span><br><span class=\"line\">x = Symbol(<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 함수 정의하기</span></span><br><span class=\"line\">f = x**<span class=\"number\">4</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">#도함수 계산하기</span></span><br><span class=\"line\">derivative_f = f.diff(x)</span><br><span class=\"line\"> </span><br><span class=\"line\">derivative_f</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&amp;text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant\">https://www.askpython.com/python/examples/derivatives-in-python-sympy#:~:text=Derivatives%20of%20Multivariable%20Functions%20using%20sympy&amp;text=Such%20derivatives%20are%20generally%20referred,all%20other%20variables%20held%20constant</a>.</li>\n<li><a href=\"https://youtu.be/H-ybCx8gt-8\">https://youtu.be/H-ybCx8gt-8</a></li>\n<li><a href=\"https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84\">https://ko.wikipedia.org/wiki/%ED%8E%B8%EB%AF%B8%EB%B6%84</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Math]미적분 기초개념","path":"2022/06/13/Statistics-Math-derivatives/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Statistics","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Probability]Python을 활용한 카이스퀘어 검정 구현","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n### 카이스퀘어 분포\n\n#### Note\n\n- 감마분포의 특수한 형태이다.\n- 표준정규분포로부터 얻은 랜덤 변수들을 제곱해서 더한 것이다.\n- 자유도 수 만큼 표준정규분포에서 변수를 뽑고 그 값들을 제곱해서 더한다.\n- **데이터의 분산이 퍼저있는 정도를 분포로 보여준다는 것이 핵심이다.**\n  + 모분산을 추정할 수 있다 -> goodness of fit\n  + 두 분포의 차이를 확인할 수 있다 -> Chi-square test of independence\n\n#### 공식\n\n- k개의 정규분포를 따르는 확률변수 X_1 ,..., X_k를 정의하면 아래와 같이 자유도 k의 카이스퀘어 분포를 나타낼 수 있다.\n\n$$Q = \\sum_{i=1}^{k} X_i^2$$\n\n\n- 독립성 검정(independence test)과 적합성 검정(goodness of fit)을 위하 사용하는 피어슨 카이스퀘어 통계량\n  + $\\frac{(O_i - E_i)^2}{E_i}$는 정규분포를 따르고 데이터가 충분히 많다면 이를 합한 피어슨 카이스퀘어 통계량은 카이스퀘어 분포를 따른다.\n\n$$\\sum_i \\frac{(O_i - E_i)^2}{E_i}$$\n\n\n#### 적합성 검정(goodness of fit) 구현\n\n적합성 검정은 기본적으로 샘플 데이터가 특정 분포를 따르는지(정규분포) 확인할 때 사용한다.\n이 검정이 중요한 이유는 회귀분석 모델링에서 요구하는 오차의 정규성 가정을 확인하는데 쓸 수 있기 때문이다.\n\n**관측한 데이터(샘플)의 분포가 기대되는 어떤 분포(보통 정규분포)를 따르는지 확인한다는 것이 핵심이고 로직 자체는 아래의 독립성 검정과 유사하다.**\n```python\nimport scipy.stats as stats\n\nobs = [20, 40, 30, 10, 50]\nexp = [20, 20, 20, 20, 20]\n\n#Goodness of Fit Test\n# f_exp 인자로 로 기대되는 데이터 value를 넣을 수 있다. \nstats.chisquare(f_obs=obs,f_exp=exp)\n\n>>>Power_divergenceResult(statistic=33.333333333333336, pvalue=1.020735571764047e-06)\n\n```\n\n#### 독립성 검정(Chi-square test of independence)\n\n- **두 범주형 변수 사이의 관계를 파악한다.**\n- 독립성 검정을 시행하려면 교차표의 각 셀의 기대빈도가 5 이상이여 한다는 조건이 붙는다.\n- 각 셀의 기대빈도가 5 이상일 경우 $\\chi^2$ 는 근사적으로 기대빈도가 n-1인 카이스퀘어 분포를 따른다.\n- 보통 범주형 독립변수와 범주형 종속변수의 관계가 있는지 확인할 때 사용한다.\n\n귀무가설은 두 범주형 변수가 독립적이라는 것이다.\n귀무가설이 성립하려면 교차표의 각 셀의 관측빈도와 기대빈도의 차이는 0에 가까워야 한다.\n두 범주형 변수의 빈도의 범주간 차이가 기댓값에서 유의미하게 벗어나는지 검정한다.\n\n카이스퀘어 통계량이 유의수준을 넘어서면 귀무가설을 기각한다.\n\n```python \nfrom scipy.stats import chi2_contingency\n\n# 교차표 자체는 pandas에서 제공하는 crostab함수를 통해 df로 만들 수도 있다.\ntable = [[20, 40, 30, 10], [20, 42 ,15,  30]]\n\n# 카이 , p-value, 자유도, 기대빈도\nstat, p, dof, expected = chi2_contingency(table)\n# 유의수준 설정\nalpha = 0.05\n# 가설수용/기각\n\nprint('significance=%.3f, p=%.3f' % (alpha, p))\nif p <= alpha:\n    print('Variables are associated (reject H0)')\nelse:\n    print('Variables are not associated(fail to reject H0)')\n\n```\n\n\n**References & annotation**\n---\n\n- https://youtu.be/2QeDRsxSF9M\n- https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\n- 기대빈도가 5미만일 경우 피셔의 적합검정을 고려한다.\n- 제대로 쓰려면 헬퍼함수를 따로 만들긴 해야할 것 같다.\n","source":"_posts/Statistics-Prob-chi-square-dist.md","raw":"---\ntitle: '[Probability]Python을 활용한 카이스퀘어 검정 구현'\ncategories:\n  - Statistics\ndate:\nupdated:\ntags:\n  - Probability\nmathjax: true\n---\n\n### 카이스퀘어 분포\n\n#### Note\n\n- 감마분포의 특수한 형태이다.\n- 표준정규분포로부터 얻은 랜덤 변수들을 제곱해서 더한 것이다.\n- 자유도 수 만큼 표준정규분포에서 변수를 뽑고 그 값들을 제곱해서 더한다.\n- **데이터의 분산이 퍼저있는 정도를 분포로 보여준다는 것이 핵심이다.**\n  + 모분산을 추정할 수 있다 -> goodness of fit\n  + 두 분포의 차이를 확인할 수 있다 -> Chi-square test of independence\n\n#### 공식\n\n- k개의 정규분포를 따르는 확률변수 X_1 ,..., X_k를 정의하면 아래와 같이 자유도 k의 카이스퀘어 분포를 나타낼 수 있다.\n\n$$Q = \\sum_{i=1}^{k} X_i^2$$\n\n\n- 독립성 검정(independence test)과 적합성 검정(goodness of fit)을 위하 사용하는 피어슨 카이스퀘어 통계량\n  + $\\frac{(O_i - E_i)^2}{E_i}$는 정규분포를 따르고 데이터가 충분히 많다면 이를 합한 피어슨 카이스퀘어 통계량은 카이스퀘어 분포를 따른다.\n\n$$\\sum_i \\frac{(O_i - E_i)^2}{E_i}$$\n\n\n#### 적합성 검정(goodness of fit) 구현\n\n적합성 검정은 기본적으로 샘플 데이터가 특정 분포를 따르는지(정규분포) 확인할 때 사용한다.\n이 검정이 중요한 이유는 회귀분석 모델링에서 요구하는 오차의 정규성 가정을 확인하는데 쓸 수 있기 때문이다.\n\n**관측한 데이터(샘플)의 분포가 기대되는 어떤 분포(보통 정규분포)를 따르는지 확인한다는 것이 핵심이고 로직 자체는 아래의 독립성 검정과 유사하다.**\n```python\nimport scipy.stats as stats\n\nobs = [20, 40, 30, 10, 50]\nexp = [20, 20, 20, 20, 20]\n\n#Goodness of Fit Test\n# f_exp 인자로 로 기대되는 데이터 value를 넣을 수 있다. \nstats.chisquare(f_obs=obs,f_exp=exp)\n\n>>>Power_divergenceResult(statistic=33.333333333333336, pvalue=1.020735571764047e-06)\n\n```\n\n#### 독립성 검정(Chi-square test of independence)\n\n- **두 범주형 변수 사이의 관계를 파악한다.**\n- 독립성 검정을 시행하려면 교차표의 각 셀의 기대빈도가 5 이상이여 한다는 조건이 붙는다.\n- 각 셀의 기대빈도가 5 이상일 경우 $\\chi^2$ 는 근사적으로 기대빈도가 n-1인 카이스퀘어 분포를 따른다.\n- 보통 범주형 독립변수와 범주형 종속변수의 관계가 있는지 확인할 때 사용한다.\n\n귀무가설은 두 범주형 변수가 독립적이라는 것이다.\n귀무가설이 성립하려면 교차표의 각 셀의 관측빈도와 기대빈도의 차이는 0에 가까워야 한다.\n두 범주형 변수의 빈도의 범주간 차이가 기댓값에서 유의미하게 벗어나는지 검정한다.\n\n카이스퀘어 통계량이 유의수준을 넘어서면 귀무가설을 기각한다.\n\n```python \nfrom scipy.stats import chi2_contingency\n\n# 교차표 자체는 pandas에서 제공하는 crostab함수를 통해 df로 만들 수도 있다.\ntable = [[20, 40, 30, 10], [20, 42 ,15,  30]]\n\n# 카이 , p-value, 자유도, 기대빈도\nstat, p, dof, expected = chi2_contingency(table)\n# 유의수준 설정\nalpha = 0.05\n# 가설수용/기각\n\nprint('significance=%.3f, p=%.3f' % (alpha, p))\nif p <= alpha:\n    print('Variables are associated (reject H0)')\nelse:\n    print('Variables are not associated(fail to reject H0)')\n\n```\n\n\n**References & annotation**\n---\n\n- https://youtu.be/2QeDRsxSF9M\n- https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\n- 기대빈도가 5미만일 경우 피셔의 적합검정을 고려한다.\n- 제대로 쓰려면 헬퍼함수를 따로 만들긴 해야할 것 같다.\n","slug":"Statistics-Prob-chi-square-dist","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscn003db36q5snp610w","content":"<h3 id=\"카이스퀘어-분포\"><a href=\"#카이스퀘어-분포\" class=\"headerlink\" title=\"카이스퀘어 분포\"></a>카이스퀘어 분포</h3><h4 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h4><ul>\n<li>감마분포의 특수한 형태이다.</li>\n<li>표준정규분포로부터 얻은 랜덤 변수들을 제곱해서 더한 것이다.</li>\n<li>자유도 수 만큼 표준정규분포에서 변수를 뽑고 그 값들을 제곱해서 더한다.</li>\n<li><strong>데이터의 분산이 퍼저있는 정도를 분포로 보여준다는 것이 핵심이다.</strong><ul>\n<li>모분산을 추정할 수 있다 -&gt; goodness of fit</li>\n<li>두 분포의 차이를 확인할 수 있다 -&gt; Chi-square test of independence</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"공식\"><a href=\"#공식\" class=\"headerlink\" title=\"공식\"></a>공식</h4><ul>\n<li>k개의 정규분포를 따르는 확률변수 X_1 ,…, X_k를 정의하면 아래와 같이 자유도 k의 카이스퀘어 분포를 나타낼 수 있다.</li>\n</ul>\n<p>$$Q &#x3D; \\sum_{i&#x3D;1}^{k} X_i^2$$</p>\n<ul>\n<li>독립성 검정(independence test)과 적합성 검정(goodness of fit)을 위하 사용하는 피어슨 카이스퀘어 통계량<ul>\n<li>$\\frac{(O_i - E_i)^2}{E_i}$는 정규분포를 따르고 데이터가 충분히 많다면 이를 합한 피어슨 카이스퀘어 통계량은 카이스퀘어 분포를 따른다.</li>\n</ul>\n</li>\n</ul>\n<p>$$\\sum_i \\frac{(O_i - E_i)^2}{E_i}$$</p>\n<h4 id=\"적합성-검정-goodness-of-fit-구현\"><a href=\"#적합성-검정-goodness-of-fit-구현\" class=\"headerlink\" title=\"적합성 검정(goodness of fit) 구현\"></a>적합성 검정(goodness of fit) 구현</h4><p>적합성 검정은 기본적으로 샘플 데이터가 특정 분포를 따르는지(정규분포) 확인할 때 사용한다.<br>이 검정이 중요한 이유는 회귀분석 모델링에서 요구하는 오차의 정규성 가정을 확인하는데 쓸 수 있기 때문이다.</p>\n<p><strong>관측한 데이터(샘플)의 분포가 기대되는 어떤 분포(보통 정규분포)를 따르는지 확인한다는 것이 핵심이고 로직 자체는 아래의 독립성 검정과 유사하다.</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scipy.stats <span class=\"keyword\">as</span> stats</span><br><span class=\"line\"></span><br><span class=\"line\">obs = [<span class=\"number\">20</span>, <span class=\"number\">40</span>, <span class=\"number\">30</span>, <span class=\"number\">10</span>, <span class=\"number\">50</span>]</span><br><span class=\"line\">exp = [<span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Goodness of Fit Test</span></span><br><span class=\"line\"><span class=\"comment\"># f_exp 인자로 로 기대되는 데이터 value를 넣을 수 있다. </span></span><br><span class=\"line\">stats.chisquare(f_obs=obs,f_exp=exp)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;Power_divergenceResult(statistic=<span class=\"number\">33.333333333333336</span>, pvalue=<span class=\"number\">1.020735571764047e-06</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"독립성-검정-Chi-square-test-of-independence\"><a href=\"#독립성-검정-Chi-square-test-of-independence\" class=\"headerlink\" title=\"독립성 검정(Chi-square test of independence)\"></a>독립성 검정(Chi-square test of independence)</h4><ul>\n<li><strong>두 범주형 변수 사이의 관계를 파악한다.</strong></li>\n<li>독립성 검정을 시행하려면 교차표의 각 셀의 기대빈도가 5 이상이여 한다는 조건이 붙는다.</li>\n<li>각 셀의 기대빈도가 5 이상일 경우 $\\chi^2$ 는 근사적으로 기대빈도가 n-1인 카이스퀘어 분포를 따른다.</li>\n<li>보통 범주형 독립변수와 범주형 종속변수의 관계가 있는지 확인할 때 사용한다.</li>\n</ul>\n<p>귀무가설은 두 범주형 변수가 독립적이라는 것이다.<br>귀무가설이 성립하려면 교차표의 각 셀의 관측빈도와 기대빈도의 차이는 0에 가까워야 한다.<br>두 범주형 변수의 빈도의 범주간 차이가 기댓값에서 유의미하게 벗어나는지 검정한다.</p>\n<p>카이스퀘어 통계량이 유의수준을 넘어서면 귀무가설을 기각한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy.stats <span class=\"keyword\">import</span> chi2_contingency</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 교차표 자체는 pandas에서 제공하는 crostab함수를 통해 df로 만들 수도 있다.</span></span><br><span class=\"line\">table = [[<span class=\"number\">20</span>, <span class=\"number\">40</span>, <span class=\"number\">30</span>, <span class=\"number\">10</span>], [<span class=\"number\">20</span>, <span class=\"number\">42</span> ,<span class=\"number\">15</span>,  <span class=\"number\">30</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 카이 , p-value, 자유도, 기대빈도</span></span><br><span class=\"line\">stat, p, dof, expected = chi2_contingency(table)</span><br><span class=\"line\"><span class=\"comment\"># 유의수준 설정</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.05</span></span><br><span class=\"line\"><span class=\"comment\"># 가설수용/기각</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;significance=%.3f, p=%.3f&#x27;</span> % (alpha, p))</span><br><span class=\"line\"><span class=\"keyword\">if</span> p &lt;= alpha:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Variables are associated (reject H0)&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Variables are not associated(fail to reject H0)&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/2QeDRsxSF9M\">https://youtu.be/2QeDRsxSF9M</a></li>\n<li><a href=\"https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\">https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db</a></li>\n<li>기대빈도가 5미만일 경우 피셔의 적합검정을 고려한다.</li>\n<li>제대로 쓰려면 헬퍼함수를 따로 만들긴 해야할 것 같다.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"카이스퀘어-분포\"><a href=\"#카이스퀘어-분포\" class=\"headerlink\" title=\"카이스퀘어 분포\"></a>카이스퀘어 분포</h3><h4 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h4><ul>\n<li>감마분포의 특수한 형태이다.</li>\n<li>표준정규분포로부터 얻은 랜덤 변수들을 제곱해서 더한 것이다.</li>\n<li>자유도 수 만큼 표준정규분포에서 변수를 뽑고 그 값들을 제곱해서 더한다.</li>\n<li><strong>데이터의 분산이 퍼저있는 정도를 분포로 보여준다는 것이 핵심이다.</strong><ul>\n<li>모분산을 추정할 수 있다 -&gt; goodness of fit</li>\n<li>두 분포의 차이를 확인할 수 있다 -&gt; Chi-square test of independence</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"공식\"><a href=\"#공식\" class=\"headerlink\" title=\"공식\"></a>공식</h4><ul>\n<li>k개의 정규분포를 따르는 확률변수 X_1 ,…, X_k를 정의하면 아래와 같이 자유도 k의 카이스퀘어 분포를 나타낼 수 있다.</li>\n</ul>\n<p>$$Q &#x3D; \\sum_{i&#x3D;1}^{k} X_i^2$$</p>\n<ul>\n<li>독립성 검정(independence test)과 적합성 검정(goodness of fit)을 위하 사용하는 피어슨 카이스퀘어 통계량<ul>\n<li>$\\frac{(O_i - E_i)^2}{E_i}$는 정규분포를 따르고 데이터가 충분히 많다면 이를 합한 피어슨 카이스퀘어 통계량은 카이스퀘어 분포를 따른다.</li>\n</ul>\n</li>\n</ul>\n<p>$$\\sum_i \\frac{(O_i - E_i)^2}{E_i}$$</p>\n<h4 id=\"적합성-검정-goodness-of-fit-구현\"><a href=\"#적합성-검정-goodness-of-fit-구현\" class=\"headerlink\" title=\"적합성 검정(goodness of fit) 구현\"></a>적합성 검정(goodness of fit) 구현</h4><p>적합성 검정은 기본적으로 샘플 데이터가 특정 분포를 따르는지(정규분포) 확인할 때 사용한다.<br>이 검정이 중요한 이유는 회귀분석 모델링에서 요구하는 오차의 정규성 가정을 확인하는데 쓸 수 있기 때문이다.</p>\n<p><strong>관측한 데이터(샘플)의 분포가 기대되는 어떤 분포(보통 정규분포)를 따르는지 확인한다는 것이 핵심이고 로직 자체는 아래의 독립성 검정과 유사하다.</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scipy.stats <span class=\"keyword\">as</span> stats</span><br><span class=\"line\"></span><br><span class=\"line\">obs = [<span class=\"number\">20</span>, <span class=\"number\">40</span>, <span class=\"number\">30</span>, <span class=\"number\">10</span>, <span class=\"number\">50</span>]</span><br><span class=\"line\">exp = [<span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Goodness of Fit Test</span></span><br><span class=\"line\"><span class=\"comment\"># f_exp 인자로 로 기대되는 데이터 value를 넣을 수 있다. </span></span><br><span class=\"line\">stats.chisquare(f_obs=obs,f_exp=exp)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;Power_divergenceResult(statistic=<span class=\"number\">33.333333333333336</span>, pvalue=<span class=\"number\">1.020735571764047e-06</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"독립성-검정-Chi-square-test-of-independence\"><a href=\"#독립성-검정-Chi-square-test-of-independence\" class=\"headerlink\" title=\"독립성 검정(Chi-square test of independence)\"></a>독립성 검정(Chi-square test of independence)</h4><ul>\n<li><strong>두 범주형 변수 사이의 관계를 파악한다.</strong></li>\n<li>독립성 검정을 시행하려면 교차표의 각 셀의 기대빈도가 5 이상이여 한다는 조건이 붙는다.</li>\n<li>각 셀의 기대빈도가 5 이상일 경우 $\\chi^2$ 는 근사적으로 기대빈도가 n-1인 카이스퀘어 분포를 따른다.</li>\n<li>보통 범주형 독립변수와 범주형 종속변수의 관계가 있는지 확인할 때 사용한다.</li>\n</ul>\n<p>귀무가설은 두 범주형 변수가 독립적이라는 것이다.<br>귀무가설이 성립하려면 교차표의 각 셀의 관측빈도와 기대빈도의 차이는 0에 가까워야 한다.<br>두 범주형 변수의 빈도의 범주간 차이가 기댓값에서 유의미하게 벗어나는지 검정한다.</p>\n<p>카이스퀘어 통계량이 유의수준을 넘어서면 귀무가설을 기각한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy.stats <span class=\"keyword\">import</span> chi2_contingency</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 교차표 자체는 pandas에서 제공하는 crostab함수를 통해 df로 만들 수도 있다.</span></span><br><span class=\"line\">table = [[<span class=\"number\">20</span>, <span class=\"number\">40</span>, <span class=\"number\">30</span>, <span class=\"number\">10</span>], [<span class=\"number\">20</span>, <span class=\"number\">42</span> ,<span class=\"number\">15</span>,  <span class=\"number\">30</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 카이 , p-value, 자유도, 기대빈도</span></span><br><span class=\"line\">stat, p, dof, expected = chi2_contingency(table)</span><br><span class=\"line\"><span class=\"comment\"># 유의수준 설정</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.05</span></span><br><span class=\"line\"><span class=\"comment\"># 가설수용/기각</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;significance=%.3f, p=%.3f&#x27;</span> % (alpha, p))</span><br><span class=\"line\"><span class=\"keyword\">if</span> p &lt;= alpha:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Variables are associated (reject H0)&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Variables are not associated(fail to reject H0)&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/2QeDRsxSF9M\">https://youtu.be/2QeDRsxSF9M</a></li>\n<li><a href=\"https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\">https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db</a></li>\n<li>기대빈도가 5미만일 경우 피셔의 적합검정을 고려한다.</li>\n<li>제대로 쓰려면 헬퍼함수를 따로 만들긴 해야할 것 같다.</li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Probability]Python을 활용한 카이스퀘어 검정 구현","path":"2022/06/13/Statistics-Prob-chi-square-dist/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Statistics","tags":["Probability"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Probability]numpy와 scipy로 다항분포 간단하게 구현하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","mathjax":true,"_content":"\n<!--\n\n진짜 ref\nhttps://www.statology.org/multinomial-distribution-in-python/\n\nhttps://boxnwhis.kr/2015/06/04/multinomial_dist_for_gachas.html\n\n다항로지스틱 머신러닝\nhttps://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n\n확륳함수로부터 나오는 확률들의 패턴을 확률분포라 한다\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n#### Note\n\n- 다항분포는 이항분포의 보다 일반화된 버전이다.(다항 분포에서 차원이 2인 경우 이항 분포가 된다.)\n- **다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다.**\n다항분포의 가장 쉬운 예시 중 하나는 주사위를 N 번 던저 각 면이 나오는 횟수 집합의 분포를 구하는 것이다.\n\n- **특정 확률변수 X가 다음의 조건을 충족할 경우 다항분포를 따른다.**\n  + k개의 class(발생가능한 결과, 카테고리 등)\n  + 각 trial은 독립적이다.\n  + 독립적인 각각의 trial에서 i번째 class가 나타날 확률읜 $p_{i}$로 고정한다.\n\n#### 다항분포 공식\n\n- 다항분포의 PMF는 다음과 같이 정의된다. 식이 복잡해보이는 것은 단순히 나올 수 있는 결과의 조합이 복잡하기 때문이다.\n$$f(x)=\\frac{n !}{x_{1} ! \\cdots x_{k} !} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}}$$\n\n- 다항분포의 기댓값 :\n$$E\\{X_{i}\\}=np_{i}$$\n\n- 다항분포의 분산 :\n$\\operatorname{Var}\\left(X_{i}\\right)=n p_{i}\\left(1-p_{i}\\right)$\n\n#### 구현\n\n- numpy를 활용한 시뮬레이션\n\n두번째 인수에 tuple 형태로 각 class의 확률이 들어간다.\n두번째 인수의 확률의 합은 반드시 1이여야 한다.\n\n```python\n# 주사위 10번 던지는 시뮬레이션\nnp.random.multinomial(10, [1/6.]*6, size=1)\narray([[4, 3, 6, 5, 2, 1]]) # random\n\n```\n\n- scipy 예제\n\n주머니에 6개의 노란 구슬,2개의 빨간 구슬, 2개의 파란구슬이 있을 때\n복원추출로 4개의 구슬을 뽑을 경우 모든 구슬이 빨간 색일 확률은 무엇인가?\n\n\n```python\nfrom scipy import multinomial\n\nmultinomial.pmf(x=[0,4,0],n=4,p = [.6,.2,.2])\n\n>>>0.1295999999999999\n\n```\n\n\n\n\n**References & annotation**\n---\n- https://youtu.be/nMsCHfrt3Cw\n- https://www.statisticshowto.com/multinomial-distribution/","source":"_posts/Statistics-Prob-multinomial-dist.md","raw":"---\ntitle: \"[Probability]numpy와 scipy로 다항분포 간단하게 구현하기\"\ndate: \nupdated:\ncategories: \n        - [Statistics]\ntags:\n  - Probability\nmathjax: true\n---\n\n<!--\n\n진짜 ref\nhttps://www.statology.org/multinomial-distribution-in-python/\n\nhttps://boxnwhis.kr/2015/06/04/multinomial_dist_for_gachas.html\n\n다항로지스틱 머신러닝\nhttps://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n\n확륳함수로부터 나오는 확률들의 패턴을 확률분포라 한다\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n#### Note\n\n- 다항분포는 이항분포의 보다 일반화된 버전이다.(다항 분포에서 차원이 2인 경우 이항 분포가 된다.)\n- **다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다.**\n다항분포의 가장 쉬운 예시 중 하나는 주사위를 N 번 던저 각 면이 나오는 횟수 집합의 분포를 구하는 것이다.\n\n- **특정 확률변수 X가 다음의 조건을 충족할 경우 다항분포를 따른다.**\n  + k개의 class(발생가능한 결과, 카테고리 등)\n  + 각 trial은 독립적이다.\n  + 독립적인 각각의 trial에서 i번째 class가 나타날 확률읜 $p_{i}$로 고정한다.\n\n#### 다항분포 공식\n\n- 다항분포의 PMF는 다음과 같이 정의된다. 식이 복잡해보이는 것은 단순히 나올 수 있는 결과의 조합이 복잡하기 때문이다.\n$$f(x)=\\frac{n !}{x_{1} ! \\cdots x_{k} !} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}}$$\n\n- 다항분포의 기댓값 :\n$$E\\{X_{i}\\}=np_{i}$$\n\n- 다항분포의 분산 :\n$\\operatorname{Var}\\left(X_{i}\\right)=n p_{i}\\left(1-p_{i}\\right)$\n\n#### 구현\n\n- numpy를 활용한 시뮬레이션\n\n두번째 인수에 tuple 형태로 각 class의 확률이 들어간다.\n두번째 인수의 확률의 합은 반드시 1이여야 한다.\n\n```python\n# 주사위 10번 던지는 시뮬레이션\nnp.random.multinomial(10, [1/6.]*6, size=1)\narray([[4, 3, 6, 5, 2, 1]]) # random\n\n```\n\n- scipy 예제\n\n주머니에 6개의 노란 구슬,2개의 빨간 구슬, 2개의 파란구슬이 있을 때\n복원추출로 4개의 구슬을 뽑을 경우 모든 구슬이 빨간 색일 확률은 무엇인가?\n\n\n```python\nfrom scipy import multinomial\n\nmultinomial.pmf(x=[0,4,0],n=4,p = [.6,.2,.2])\n\n>>>0.1295999999999999\n\n```\n\n\n\n\n**References & annotation**\n---\n- https://youtu.be/nMsCHfrt3Cw\n- https://www.statisticshowto.com/multinomial-distribution/","slug":"Statistics-Prob-multinomial-dist","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsco003hb36q4i952wj0","content":"<!--\n\n진짜 ref\nhttps://www.statology.org/multinomial-distribution-in-python/\n\nhttps://boxnwhis.kr/2015/06/04/multinomial_dist_for_gachas.html\n\n다항로지스틱 머신러닝\nhttps://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n\n확륳함수로부터 나오는 확률들의 패턴을 확률분포라 한다\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h4 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h4><ul>\n<li><p>다항분포는 이항분포의 보다 일반화된 버전이다.(다항 분포에서 차원이 2인 경우 이항 분포가 된다.)</p>\n</li>\n<li><p><strong>다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다.</strong><br>다항분포의 가장 쉬운 예시 중 하나는 주사위를 N 번 던저 각 면이 나오는 횟수 집합의 분포를 구하는 것이다.</p>\n</li>\n<li><p><strong>특정 확률변수 X가 다음의 조건을 충족할 경우 다항분포를 따른다.</strong></p>\n<ul>\n<li>k개의 class(발생가능한 결과, 카테고리 등)</li>\n<li>각 trial은 독립적이다.</li>\n<li>독립적인 각각의 trial에서 i번째 class가 나타날 확률읜 $p_{i}$로 고정한다.</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"다항분포-공식\"><a href=\"#다항분포-공식\" class=\"headerlink\" title=\"다항분포 공식\"></a>다항분포 공식</h4><ul>\n<li><p>다항분포의 PMF는 다음과 같이 정의된다. 식이 복잡해보이는 것은 단순히 나올 수 있는 결과의 조합이 복잡하기 때문이다.<br>$$f(x)&#x3D;\\frac{n !}{x_{1} ! \\cdots x_{k} !} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}}$$</p>\n</li>\n<li><p>다항분포의 기댓값 :<br>$$E{X_{i}}&#x3D;np_{i}$$</p>\n</li>\n<li><p>다항분포의 분산 :<br>$\\operatorname{Var}\\left(X_{i}\\right)&#x3D;n p_{i}\\left(1-p_{i}\\right)$</p>\n</li>\n</ul>\n<h4 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h4><ul>\n<li>numpy를 활용한 시뮬레이션</li>\n</ul>\n<p>두번째 인수에 tuple 형태로 각 class의 확률이 들어간다.<br>두번째 인수의 확률의 합은 반드시 1이여야 한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 주사위 10번 던지는 시뮬레이션</span></span><br><span class=\"line\">np.random.multinomial(<span class=\"number\">10</span>, [<span class=\"number\">1</span>/<span class=\"number\">6.</span>]*<span class=\"number\">6</span>, size=<span class=\"number\">1</span>)</span><br><span class=\"line\">array([[<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]]) <span class=\"comment\"># random</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>scipy 예제</li>\n</ul>\n<p>주머니에 6개의 노란 구슬,2개의 빨간 구슬, 2개의 파란구슬이 있을 때<br>복원추출로 4개의 구슬을 뽑을 경우 모든 구슬이 빨간 색일 확률은 무엇인가?</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> multinomial</span><br><span class=\"line\"></span><br><span class=\"line\">multinomial.pmf(x=[<span class=\"number\">0</span>,<span class=\"number\">4</span>,<span class=\"number\">0</span>],n=<span class=\"number\">4</span>,p = [<span class=\"number\">.6</span>,<span class=\"number\">.2</span>,<span class=\"number\">.2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"number\">0.1295999999999999</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/nMsCHfrt3Cw\">https://youtu.be/nMsCHfrt3Cw</a></li>\n<li><a href=\"https://www.statisticshowto.com/multinomial-distribution/\">https://www.statisticshowto.com/multinomial-distribution/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n진짜 ref\nhttps://www.statology.org/multinomial-distribution-in-python/\n\nhttps://boxnwhis.kr/2015/06/04/multinomial_dist_for_gachas.html\n\n다항로지스틱 머신러닝\nhttps://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n\n확륳함수로부터 나오는 확률들의 패턴을 확률분포라 한다\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<h4 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h4><ul>\n<li><p>다항분포는 이항분포의 보다 일반화된 버전이다.(다항 분포에서 차원이 2인 경우 이항 분포가 된다.)</p>\n</li>\n<li><p><strong>다항 분포는 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의한다.</strong><br>다항분포의 가장 쉬운 예시 중 하나는 주사위를 N 번 던저 각 면이 나오는 횟수 집합의 분포를 구하는 것이다.</p>\n</li>\n<li><p><strong>특정 확률변수 X가 다음의 조건을 충족할 경우 다항분포를 따른다.</strong></p>\n<ul>\n<li>k개의 class(발생가능한 결과, 카테고리 등)</li>\n<li>각 trial은 독립적이다.</li>\n<li>독립적인 각각의 trial에서 i번째 class가 나타날 확률읜 $p_{i}$로 고정한다.</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"다항분포-공식\"><a href=\"#다항분포-공식\" class=\"headerlink\" title=\"다항분포 공식\"></a>다항분포 공식</h4><ul>\n<li><p>다항분포의 PMF는 다음과 같이 정의된다. 식이 복잡해보이는 것은 단순히 나올 수 있는 결과의 조합이 복잡하기 때문이다.<br>$$f(x)&#x3D;\\frac{n !}{x_{1} ! \\cdots x_{k} !} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}}$$</p>\n</li>\n<li><p>다항분포의 기댓값 :<br>$$E{X_{i}}&#x3D;np_{i}$$</p>\n</li>\n<li><p>다항분포의 분산 :<br>$\\operatorname{Var}\\left(X_{i}\\right)&#x3D;n p_{i}\\left(1-p_{i}\\right)$</p>\n</li>\n</ul>\n<h4 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h4><ul>\n<li>numpy를 활용한 시뮬레이션</li>\n</ul>\n<p>두번째 인수에 tuple 형태로 각 class의 확률이 들어간다.<br>두번째 인수의 확률의 합은 반드시 1이여야 한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 주사위 10번 던지는 시뮬레이션</span></span><br><span class=\"line\">np.random.multinomial(<span class=\"number\">10</span>, [<span class=\"number\">1</span>/<span class=\"number\">6.</span>]*<span class=\"number\">6</span>, size=<span class=\"number\">1</span>)</span><br><span class=\"line\">array([[<span class=\"number\">4</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]]) <span class=\"comment\"># random</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>scipy 예제</li>\n</ul>\n<p>주머니에 6개의 노란 구슬,2개의 빨간 구슬, 2개의 파란구슬이 있을 때<br>복원추출로 4개의 구슬을 뽑을 경우 모든 구슬이 빨간 색일 확률은 무엇인가?</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> multinomial</span><br><span class=\"line\"></span><br><span class=\"line\">multinomial.pmf(x=[<span class=\"number\">0</span>,<span class=\"number\">4</span>,<span class=\"number\">0</span>],n=<span class=\"number\">4</span>,p = [<span class=\"number\">.6</span>,<span class=\"number\">.2</span>,<span class=\"number\">.2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;<span class=\"number\">0.1295999999999999</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://youtu.be/nMsCHfrt3Cw\">https://youtu.be/nMsCHfrt3Cw</a></li>\n<li><a href=\"https://www.statisticshowto.com/multinomial-distribution/\">https://www.statisticshowto.com/multinomial-distribution/</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Probability]numpy와 scipy로 다항분포 간단하게 구현하기","path":"2022/06/13/Statistics-Prob-multinomial-dist/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Statistics","tags":["Probability"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[R]make: gfortran: No such file or directory 해결하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n시계열 분석을 할 일이 있어서 Rstudio server에다가 `forecast` package를 설치하려 하는데 dependancy package를 설치하는 도중\n\n아래와 같은 오류가 떴다.\n\n`make: gfortran: No such file or directory`\n\n그리고 라이브러리 설치가 안된다..\n\n찾아보니까 package가 소스 형태라 컴파일러가 필요한데 그 중 포트란 컴파일러가 서버에 설치되지 않아서 생긴 문제였다.\n\n터미널에다 아래 명령어를 쳐서 해결하였다.\n\n```bash\nsudo pacman -S gcc-fortran\n```\n\n\n우분투 사용자의 경우 아래와 같이 컴파일러를 설치해주면 된다.\n\n```bash\nsudo apt-get install gfortran\n```\n\n윈도우 사용자의 경우 [링크](https://fortran-lang.org/learn/os_setup/install_gfortran) 에서 설치법을 확인할 수 있다.\n\n**References & annotation**\n---\n- https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/\n- https://fortran-lang.org/learn/os_setup/install_gfortran","source":"_posts/TS-R-lib-1.md","raw":"---\ntitle: '[R]make: gfortran: No such file or directory 해결하기'\ncategories:\n   - Troubleshooting\ndate:\nupdated:\n\ntags:\n\t- R\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n시계열 분석을 할 일이 있어서 Rstudio server에다가 `forecast` package를 설치하려 하는데 dependancy package를 설치하는 도중\n\n아래와 같은 오류가 떴다.\n\n`make: gfortran: No such file or directory`\n\n그리고 라이브러리 설치가 안된다..\n\n찾아보니까 package가 소스 형태라 컴파일러가 필요한데 그 중 포트란 컴파일러가 서버에 설치되지 않아서 생긴 문제였다.\n\n터미널에다 아래 명령어를 쳐서 해결하였다.\n\n```bash\nsudo pacman -S gcc-fortran\n```\n\n\n우분투 사용자의 경우 아래와 같이 컴파일러를 설치해주면 된다.\n\n```bash\nsudo apt-get install gfortran\n```\n\n윈도우 사용자의 경우 [링크](https://fortran-lang.org/learn/os_setup/install_gfortran) 에서 설치법을 확인할 수 있다.\n\n**References & annotation**\n---\n- https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/\n- https://fortran-lang.org/learn/os_setup/install_gfortran","slug":"TS-R-lib-1","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsco003jb36q5xx5210s","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>시계열 분석을 할 일이 있어서 Rstudio server에다가 <code>forecast</code> package를 설치하려 하는데 dependancy package를 설치하는 도중</p>\n<p>아래와 같은 오류가 떴다.</p>\n<p><code>make: gfortran: No such file or directory</code></p>\n<p>그리고 라이브러리 설치가 안된다..</p>\n<p>찾아보니까 package가 소스 형태라 컴파일러가 필요한데 그 중 포트란 컴파일러가 서버에 설치되지 않아서 생긴 문제였다.</p>\n<p>터미널에다 아래 명령어를 쳐서 해결하였다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S gcc-fortran</span><br></pre></td></tr></table></figure>\n\n\n<p>우분투 사용자의 경우 아래와 같이 컴파일러를 설치해주면 된다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install gfortran</span><br></pre></td></tr></table></figure>\n\n<p>윈도우 사용자의 경우 <a href=\"https://fortran-lang.org/learn/os_setup/install_gfortran\">링크</a> 에서 설치법을 확인할 수 있다.</p>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/\">https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/</a></li>\n<li><a href=\"https://fortran-lang.org/learn/os_setup/install_gfortran\">https://fortran-lang.org/learn/os_setup/install_gfortran</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p>시계열 분석을 할 일이 있어서 Rstudio server에다가 <code>forecast</code> package를 설치하려 하는데 dependancy package를 설치하는 도중</p>\n<p>아래와 같은 오류가 떴다.</p>\n<p><code>make: gfortran: No such file or directory</code></p>\n<p>그리고 라이브러리 설치가 안된다..</p>\n<p>찾아보니까 package가 소스 형태라 컴파일러가 필요한데 그 중 포트란 컴파일러가 서버에 설치되지 않아서 생긴 문제였다.</p>\n<p>터미널에다 아래 명령어를 쳐서 해결하였다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S gcc-fortran</span><br></pre></td></tr></table></figure>\n\n\n<p>우분투 사용자의 경우 아래와 같이 컴파일러를 설치해주면 된다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install gfortran</span><br></pre></td></tr></table></figure>\n\n<p>윈도우 사용자의 경우 <a href=\"https://fortran-lang.org/learn/os_setup/install_gfortran\">링크</a> 에서 설치법을 확인할 수 있다.</p>\n<h2 id=\"References-amp-annotation\"><a href=\"#References-amp-annotation\" class=\"headerlink\" title=\"References &amp; annotation\"></a><strong>References &amp; annotation</strong></h2><ul>\n<li><a href=\"https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/\">https://www.r-bloggers.com/2021/03/gfortran-support-for-r-on-macos-2/</a></li>\n<li><a href=\"https://fortran-lang.org/learn/os_setup/install_gfortran\">https://fortran-lang.org/learn/os_setup/install_gfortran</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[R]make: gfortran: No such file or directory 해결하기","path":"2022/06/13/TS-R-lib-1/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Troubleshooting","tags":["R"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n안쓰던 노트북을 서버로 만들어서 작업중 다음 에러가 발생했다.\n\nERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides  \n\n찾아보니 SQL 서버의 변수값을 변경해 해결할 수 있었다.\n\n아래 명령을 통해 local_infile 상태가 ON 인지 OFF인지 확인 한다.\n\n```SQL\nshow global variables like 'local_infile';\n```\n\n값이 OFF일 경우 아래 명령 실행\n\n```SQL\nset global local_infile = true;\n```\n\n## References\n\n- https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client\n","source":"_posts/TS-SQL-ts-1.md","raw":"---\ntitle: '[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기'\ncategories:\n  - Troubleshooting\ndate:\nupdated:\ntags:\n\t- SQL\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n안쓰던 노트북을 서버로 만들어서 작업중 다음 에러가 발생했다.\n\nERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides  \n\n찾아보니 SQL 서버의 변수값을 변경해 해결할 수 있었다.\n\n아래 명령을 통해 local_infile 상태가 ON 인지 OFF인지 확인 한다.\n\n```SQL\nshow global variables like 'local_infile';\n```\n\n값이 OFF일 경우 아래 명령 실행\n\n```SQL\nset global local_infile = true;\n```\n\n## References\n\n- https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client\n","slug":"TS-SQL-ts-1","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qsco003nb36qadh1ett7","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<p>안쓰던 노트북을 서버로 만들어서 작업중 다음 에러가 발생했다.</p>\n<p>ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides  </p>\n<p>찾아보니 SQL 서버의 변수값을 변경해 해결할 수 있었다.</p>\n<p>아래 명령을 통해 local_infile 상태가 ON 인지 OFF인지 확인 한다.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">show</span> <span class=\"keyword\">global</span> variables <span class=\"keyword\">like</span> <span class=\"string\">&#x27;local_infile&#x27;</span>;</span><br></pre></td></tr></table></figure>\n\n<p>값이 OFF일 경우 아래 명령 실행</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"keyword\">global</span> local_infile <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client\">https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n\n#신경망이란 무엇인가?\n\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n\n<p>안쓰던 노트북을 서버로 만들어서 작업중 다음 에러가 발생했다.</p>\n<p>ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides  </p>\n<p>찾아보니 SQL 서버의 변수값을 변경해 해결할 수 있었다.</p>\n<p>아래 명령을 통해 local_infile 상태가 ON 인지 OFF인지 확인 한다.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">show</span> <span class=\"keyword\">global</span> variables <span class=\"keyword\">like</span> <span class=\"string\">&#x27;local_infile&#x27;</span>;</span><br></pre></td></tr></table></figure>\n\n<p>값이 OFF일 경우 아래 명령 실행</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span> <span class=\"keyword\">global</span> local_infile <span class=\"operator\">=</span> <span class=\"literal\">true</span>;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client\">https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기","path":"2022/06/13/TS-SQL-ts-1/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Troubleshooting","tags":["SQL"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Linux]zsh: corrupt history file 해결","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n---\n\n```bash\n$ cd ~                          \n\n\n\n$ mv .zsh_history .zsh_history.1217\n\n\n\n$ strings .zsh_history.1217 .zsh_history\n\n\n\n$ fc -R .zsh_history\n```\n\n\n## References\n\n- https://www.whatwant.com/entry/zsh-corrupt-history-file \n\n","source":"_posts/TS-linux-ts-1.md","raw":"---\ntitle: '[Linux]zsh: corrupt history file 해결'\ncategories:\n  - Troubleshooting\ndate:\nupdated:\ntags: \n  - Linux\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n---\n\n```bash\n$ cd ~                          \n\n\n\n$ mv .zsh_history .zsh_history.1217\n\n\n\n$ strings .zsh_history.1217 .zsh_history\n\n\n\n$ fc -R .zsh_history\n```\n\n\n## References\n\n- https://www.whatwant.com/entry/zsh-corrupt-history-file \n\n","slug":"TS-linux-ts-1","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscp003ob36q2a2fg8xl","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<hr>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> ~                          </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"built_in\">mv</span> .zsh_history .zsh_history.1217</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ strings .zsh_history.1217 .zsh_history</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"built_in\">fc</span> -R .zsh_history</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.whatwant.com/entry/zsh-corrupt-history-file\">https://www.whatwant.com/entry/zsh-corrupt-history-file</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<hr>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> ~                          </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"built_in\">mv</span> .zsh_history .zsh_history.1217</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ strings .zsh_history.1217 .zsh_history</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"built_in\">fc</span> -R .zsh_history</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.whatwant.com/entry/zsh-corrupt-history-file\">https://www.whatwant.com/entry/zsh-corrupt-history-file</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Linux]zsh: corrupt history file 해결","path":"2022/06/13/TS-linux-ts-1/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Troubleshooting","tags":["Linux"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Unsorted]머신러닝과 통계학의 차이","date":"2022-06-13T14:23:32.350Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n\n-->\n\n개인적으로 생각했을 때 머신러닝과 통계적 검정의 차이는 인과추론에 있어서의 엄격성의 여부이다.\n\n이는 실험설계의 엄격함의 정도와 다룰 수 있는 feature의 수 사이의 tradeoff라고도 볼 수 있다.\n\n사실 머신러닝과 딥러닝은 그렇게까지 큰 차이가 있는것이 아니고 분석하고자 하는 데이터의 차원이 다른 것 뿐이다.\n\n통계적 검정의 목적은 기본적으로 가설검정이지만 머신러닝의 목적은 패턴인식이다. \n\n이 차이는 상호보완적으로 문제해결에 각기 다르게 도움을 준다.\n\n중요한건 문제에 따라 어떤 방법론이 적합할지를 파악하는 것이다.\n\n자원은 항상 한정적이기 때문에(특히 시간이) 학습시간이 너무 오래걸리는 모델을 붙잡고 있는 것보다는 다른 방식으로 인사이트를 얻을 수 있는 방법이 있는지를 사전에 고민해 보는 편이 좋다.\n\n개인연구를 할때는 삽질이 곧 배움으로 이어지지만 실무에서는 삽질의 대가가 너무 크게 다가온다.\n","source":"_posts/thoughts-1.md","raw":"---\ntitle: '[Unsorted]머신러닝과 통계학의 차이'\ncategories:\n  - Unsorted\ntags:\n  - Unsorted\ndate:\nupdated:\n---\n\n<!--\n\n\n-->\n\n개인적으로 생각했을 때 머신러닝과 통계적 검정의 차이는 인과추론에 있어서의 엄격성의 여부이다.\n\n이는 실험설계의 엄격함의 정도와 다룰 수 있는 feature의 수 사이의 tradeoff라고도 볼 수 있다.\n\n사실 머신러닝과 딥러닝은 그렇게까지 큰 차이가 있는것이 아니고 분석하고자 하는 데이터의 차원이 다른 것 뿐이다.\n\n통계적 검정의 목적은 기본적으로 가설검정이지만 머신러닝의 목적은 패턴인식이다. \n\n이 차이는 상호보완적으로 문제해결에 각기 다르게 도움을 준다.\n\n중요한건 문제에 따라 어떤 방법론이 적합할지를 파악하는 것이다.\n\n자원은 항상 한정적이기 때문에(특히 시간이) 학습시간이 너무 오래걸리는 모델을 붙잡고 있는 것보다는 다른 방식으로 인사이트를 얻을 수 있는 방법이 있는지를 사전에 고민해 보는 편이 좋다.\n\n개인연구를 할때는 삽질이 곧 배움으로 이어지지만 실무에서는 삽질의 대가가 너무 크게 다가온다.\n","slug":"thoughts-1","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscp003tb36q621059o0","content":"<!--\n\n\n-->\n\n<p>개인적으로 생각했을 때 머신러닝과 통계적 검정의 차이는 인과추론에 있어서의 엄격성의 여부이다.</p>\n<p>이는 실험설계의 엄격함의 정도와 다룰 수 있는 feature의 수 사이의 tradeoff라고도 볼 수 있다.</p>\n<p>사실 머신러닝과 딥러닝은 그렇게까지 큰 차이가 있는것이 아니고 분석하고자 하는 데이터의 차원이 다른 것 뿐이다.</p>\n<p>통계적 검정의 목적은 기본적으로 가설검정이지만 머신러닝의 목적은 패턴인식이다. </p>\n<p>이 차이는 상호보완적으로 문제해결에 각기 다르게 도움을 준다.</p>\n<p>중요한건 문제에 따라 어떤 방법론이 적합할지를 파악하는 것이다.</p>\n<p>자원은 항상 한정적이기 때문에(특히 시간이) 학습시간이 너무 오래걸리는 모델을 붙잡고 있는 것보다는 다른 방식으로 인사이트를 얻을 수 있는 방법이 있는지를 사전에 고민해 보는 편이 좋다.</p>\n<p>개인연구를 할때는 삽질이 곧 배움으로 이어지지만 실무에서는 삽질의 대가가 너무 크게 다가온다.</p>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n\n-->\n\n<p>개인적으로 생각했을 때 머신러닝과 통계적 검정의 차이는 인과추론에 있어서의 엄격성의 여부이다.</p>\n<p>이는 실험설계의 엄격함의 정도와 다룰 수 있는 feature의 수 사이의 tradeoff라고도 볼 수 있다.</p>\n<p>사실 머신러닝과 딥러닝은 그렇게까지 큰 차이가 있는것이 아니고 분석하고자 하는 데이터의 차원이 다른 것 뿐이다.</p>\n<p>통계적 검정의 목적은 기본적으로 가설검정이지만 머신러닝의 목적은 패턴인식이다. </p>\n<p>이 차이는 상호보완적으로 문제해결에 각기 다르게 도움을 준다.</p>\n<p>중요한건 문제에 따라 어떤 방법론이 적합할지를 파악하는 것이다.</p>\n<p>자원은 항상 한정적이기 때문에(특히 시간이) 학습시간이 너무 오래걸리는 모델을 붙잡고 있는 것보다는 다른 방식으로 인사이트를 얻을 수 있는 방법이 있는지를 사전에 고민해 보는 편이 좋다.</p>\n<p>개인연구를 할때는 삽질이 곧 배움으로 이어지지만 실무에서는 삽질의 대가가 너무 크게 다가온다.</p>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Unsorted]머신러닝과 통계학의 차이","path":"2022/06/13/thoughts-1/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.350Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.350Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Unsorted","tags":["Unsorted"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Tools]Anaconda 시작시 기본설정","date":"2021-07-08T08:09:02.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"## Intro\n\nAnaconda를 자주 설치하고 지우기 때문에 Anaconda를 설치하면 하는 루틴들을 정리해두려고 합니다.\n\n## 주피터 시작경로 설정\n\n1. powershell에서 해당 명령어 실행\n```powershell\njupyter notebook --generate-config\n```\n2. 'C:\\Users\\유저명\\.jupyter\\jupyter_notebook_config.py' 경로로 이동\n\n3.  #c.NotebookApp.notebook_dir= ''  를 주석처리하고 원하는 경로 입력 \n4.  시작메뉴 주피터 속성 에서  **%USERPROFILE%/** 과 **%HOMEPATH%** 삭제\n\n## Jupyterlab 바로가기 설청\n\n1. conda 설치경로\\Script에 들어가서 activate.bat 파일을 연다.\n2. activate.bat 파일을 다른 이름으로 저장한다(activate_jupyter).\n3. 다른이름으로 저장한 파일의 맨 아래에 다음 두 줄을 추가한다.\n   ```bat\n    cd <작업경로>\n    jupyter lab\n    ```\n4. 새로만든 파일의 바로가기를 만든다.\n5. 바로가기 아이콘을 jupyter lab 아이콘으로 변경한다.\n6. 시작메뉴에 바로가기를 추가한다.\n\n## Jupyterlab theme 설치\n\n## 가상환경 생성 및 Jupyter 등록\n가상환경 자체는 단순히 자신이 필요한 python환경을 구축하기 위해 폴더 안에 필요한 패키지만을 모아 놓은 것이다. 보통 프로젝트 단위로 작업을 할때 패키지 의존성(dependancy)으로 인한 오류들을 줄이기 위해 사용한다. `vertualenv`로도 가상환경을 설치할 수 있지만 `conda`를 사용한다면 conda명령어를 통해 쉽게 가상환경을 설치하고 삭제할 수 있다.\n![png](conda_venv.png)\n### conda 가상환경 명령어\n\n1. 가상환경 만들기 \n```bash\nconda create –n 가상환경이름 python=버전 \n```\n2. 가상환경 활성화  \n```bash\nconda activate 가상환경이름\n```\n3. 가상환경 비활성화  \n```bash\nconda activate 가상환경이름\n```\n4. 가상환경에 패키지 설치 \n```bash\nconda install 패키지이름 \n```\n5. 가상환경 정보 확인 \n```bash\nconda info --enves \n```\n6. 가상환경 복사\n```bash\nconda create –n 복사된 가상환경 --clone 복사될 가상환경 \n```\n7. 가상환경 설치패키지 확인\n```bash\n# 가상환경 활성화 시킨 후 시행\nconda list\n```\n8. 가상환경 삭제\n```bash\nconda env remove -n 가상환경이름 \n```\n\n### 주피터 커널 등록\n1. ipykernel 설치\n```bash\npip install ipykernel\n```\n2. 가상환경 주피터 등록\n```bash\npython -m ipykernel install --user --name 가상환경 이름 --display-name 커널 이름\n```\n3. 가상환경 주피터 커널 삭제\n```bash\njupyter kernelspec uninstall 가상환경이름\n```\n\n## References\n\n- https://ooyoung.tistory.com/7\n- https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html","source":"_posts/tools-conda-install.md","raw":"---\ntitle: \"[Tools]Anaconda 시작시 기본설정\"\ndate: 2021-07-08 17:09:02\ntags:\ncategories:\n  - [Tools]\nupdated:\n---\n## Intro\n\nAnaconda를 자주 설치하고 지우기 때문에 Anaconda를 설치하면 하는 루틴들을 정리해두려고 합니다.\n\n## 주피터 시작경로 설정\n\n1. powershell에서 해당 명령어 실행\n```powershell\njupyter notebook --generate-config\n```\n2. 'C:\\Users\\유저명\\.jupyter\\jupyter_notebook_config.py' 경로로 이동\n\n3.  #c.NotebookApp.notebook_dir= ''  를 주석처리하고 원하는 경로 입력 \n4.  시작메뉴 주피터 속성 에서  **%USERPROFILE%/** 과 **%HOMEPATH%** 삭제\n\n## Jupyterlab 바로가기 설청\n\n1. conda 설치경로\\Script에 들어가서 activate.bat 파일을 연다.\n2. activate.bat 파일을 다른 이름으로 저장한다(activate_jupyter).\n3. 다른이름으로 저장한 파일의 맨 아래에 다음 두 줄을 추가한다.\n   ```bat\n    cd <작업경로>\n    jupyter lab\n    ```\n4. 새로만든 파일의 바로가기를 만든다.\n5. 바로가기 아이콘을 jupyter lab 아이콘으로 변경한다.\n6. 시작메뉴에 바로가기를 추가한다.\n\n## Jupyterlab theme 설치\n\n## 가상환경 생성 및 Jupyter 등록\n가상환경 자체는 단순히 자신이 필요한 python환경을 구축하기 위해 폴더 안에 필요한 패키지만을 모아 놓은 것이다. 보통 프로젝트 단위로 작업을 할때 패키지 의존성(dependancy)으로 인한 오류들을 줄이기 위해 사용한다. `vertualenv`로도 가상환경을 설치할 수 있지만 `conda`를 사용한다면 conda명령어를 통해 쉽게 가상환경을 설치하고 삭제할 수 있다.\n![png](conda_venv.png)\n### conda 가상환경 명령어\n\n1. 가상환경 만들기 \n```bash\nconda create –n 가상환경이름 python=버전 \n```\n2. 가상환경 활성화  \n```bash\nconda activate 가상환경이름\n```\n3. 가상환경 비활성화  \n```bash\nconda activate 가상환경이름\n```\n4. 가상환경에 패키지 설치 \n```bash\nconda install 패키지이름 \n```\n5. 가상환경 정보 확인 \n```bash\nconda info --enves \n```\n6. 가상환경 복사\n```bash\nconda create –n 복사된 가상환경 --clone 복사될 가상환경 \n```\n7. 가상환경 설치패키지 확인\n```bash\n# 가상환경 활성화 시킨 후 시행\nconda list\n```\n8. 가상환경 삭제\n```bash\nconda env remove -n 가상환경이름 \n```\n\n### 주피터 커널 등록\n1. ipykernel 설치\n```bash\npip install ipykernel\n```\n2. 가상환경 주피터 등록\n```bash\npython -m ipykernel install --user --name 가상환경 이름 --display-name 커널 이름\n```\n3. 가상환경 주피터 커널 삭제\n```bash\njupyter kernelspec uninstall 가상환경이름\n```\n\n## References\n\n- https://ooyoung.tistory.com/7\n- https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html","slug":"tools-conda-install","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscq003vb36qandg6abu","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>Anaconda를 자주 설치하고 지우기 때문에 Anaconda를 설치하면 하는 루틴들을 정리해두려고 합니다.</p>\n<h2 id=\"주피터-시작경로-설정\"><a href=\"#주피터-시작경로-설정\" class=\"headerlink\" title=\"주피터 시작경로 설정\"></a>주피터 시작경로 설정</h2><ol>\n<li><p>powershell에서 해당 명령어 실행</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook <span class=\"literal\">--generate-config</span></span><br></pre></td></tr></table></figure></li>\n<li><p>‘C:\\Users\\유저명.jupyter\\jupyter_notebook_config.py’ 경로로 이동</p>\n</li>\n<li><p>#c.NotebookApp.notebook_dir&#x3D; ‘’  를 주석처리하고 원하는 경로 입력 </p>\n</li>\n<li><p>시작메뉴 주피터 속성 에서  <strong>%USERPROFILE%&#x2F;</strong> 과 <strong>%HOMEPATH%</strong> 삭제</p>\n</li>\n</ol>\n<h2 id=\"Jupyterlab-바로가기-설청\"><a href=\"#Jupyterlab-바로가기-설청\" class=\"headerlink\" title=\"Jupyterlab 바로가기 설청\"></a>Jupyterlab 바로가기 설청</h2><ol>\n<li>conda 설치경로\\Script에 들어가서 activate.bat 파일을 연다.</li>\n<li>activate.bat 파일을 다른 이름으로 저장한다(activate_jupyter).</li>\n<li>다른이름으로 저장한 파일의 맨 아래에 다음 두 줄을 추가한다.<figure class=\"highlight bat\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> &lt;작업경로&gt;</span><br><span class=\"line\">jupyter lab</span><br></pre></td></tr></table></figure></li>\n<li>새로만든 파일의 바로가기를 만든다.</li>\n<li>바로가기 아이콘을 jupyter lab 아이콘으로 변경한다.</li>\n<li>시작메뉴에 바로가기를 추가한다.</li>\n</ol>\n<h2 id=\"Jupyterlab-theme-설치\"><a href=\"#Jupyterlab-theme-설치\" class=\"headerlink\" title=\"Jupyterlab theme 설치\"></a>Jupyterlab theme 설치</h2><h2 id=\"가상환경-생성-및-Jupyter-등록\"><a href=\"#가상환경-생성-및-Jupyter-등록\" class=\"headerlink\" title=\"가상환경 생성 및 Jupyter 등록\"></a>가상환경 생성 및 Jupyter 등록</h2><p>가상환경 자체는 단순히 자신이 필요한 python환경을 구축하기 위해 폴더 안에 필요한 패키지만을 모아 놓은 것이다. 보통 프로젝트 단위로 작업을 할때 패키지 의존성(dependancy)으로 인한 오류들을 줄이기 위해 사용한다. <code>vertualenv</code>로도 가상환경을 설치할 수 있지만 <code>conda</code>를 사용한다면 conda명령어를 통해 쉽게 가상환경을 설치하고 삭제할 수 있다.<br><img src=\"/conda_venv.png\" alt=\"png\"></p>\n<h3 id=\"conda-가상환경-명령어\"><a href=\"#conda-가상환경-명령어\" class=\"headerlink\" title=\"conda 가상환경 명령어\"></a>conda 가상환경 명령어</h3><ol>\n<li>가상환경 만들기 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create –n 가상환경이름 python=버전 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 활성화  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate 가상환경이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 비활성화  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate 가상환경이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경에 패키지 설치 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install 패키지이름 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 정보 확인 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda info --enves </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 복사<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create –n 복사된 가상환경 --<span class=\"built_in\">clone</span> 복사될 가상환경 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 설치패키지 확인<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 가상환경 활성화 시킨 후 시행</span></span><br><span class=\"line\">conda list</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 삭제<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda <span class=\"built_in\">env</span> remove -n 가상환경이름 </span><br></pre></td></tr></table></figure></li>\n</ol>\n<h3 id=\"주피터-커널-등록\"><a href=\"#주피터-커널-등록\" class=\"headerlink\" title=\"주피터 커널 등록\"></a>주피터 커널 등록</h3><ol>\n<li>ipykernel 설치<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install ipykernel</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 주피터 등록<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m ipykernel install --user --name 가상환경 이름 --display-name 커널 이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 주피터 커널 삭제<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter kernelspec uninstall 가상환경이름</span><br></pre></td></tr></table></figure></li>\n</ol>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://ooyoung.tistory.com/7\">https://ooyoung.tistory.com/7</a></li>\n<li><a href=\"https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html\">https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>Anaconda를 자주 설치하고 지우기 때문에 Anaconda를 설치하면 하는 루틴들을 정리해두려고 합니다.</p>\n<h2 id=\"주피터-시작경로-설정\"><a href=\"#주피터-시작경로-설정\" class=\"headerlink\" title=\"주피터 시작경로 설정\"></a>주피터 시작경로 설정</h2><ol>\n<li><p>powershell에서 해당 명령어 실행</p>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook <span class=\"literal\">--generate-config</span></span><br></pre></td></tr></table></figure></li>\n<li><p>‘C:\\Users\\유저명.jupyter\\jupyter_notebook_config.py’ 경로로 이동</p>\n</li>\n<li><p>#c.NotebookApp.notebook_dir&#x3D; ‘’  를 주석처리하고 원하는 경로 입력 </p>\n</li>\n<li><p>시작메뉴 주피터 속성 에서  <strong>%USERPROFILE%&#x2F;</strong> 과 <strong>%HOMEPATH%</strong> 삭제</p>\n</li>\n</ol>\n<h2 id=\"Jupyterlab-바로가기-설청\"><a href=\"#Jupyterlab-바로가기-설청\" class=\"headerlink\" title=\"Jupyterlab 바로가기 설청\"></a>Jupyterlab 바로가기 설청</h2><ol>\n<li>conda 설치경로\\Script에 들어가서 activate.bat 파일을 연다.</li>\n<li>activate.bat 파일을 다른 이름으로 저장한다(activate_jupyter).</li>\n<li>다른이름으로 저장한 파일의 맨 아래에 다음 두 줄을 추가한다.<figure class=\"highlight bat\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> &lt;작업경로&gt;</span><br><span class=\"line\">jupyter lab</span><br></pre></td></tr></table></figure></li>\n<li>새로만든 파일의 바로가기를 만든다.</li>\n<li>바로가기 아이콘을 jupyter lab 아이콘으로 변경한다.</li>\n<li>시작메뉴에 바로가기를 추가한다.</li>\n</ol>\n<h2 id=\"Jupyterlab-theme-설치\"><a href=\"#Jupyterlab-theme-설치\" class=\"headerlink\" title=\"Jupyterlab theme 설치\"></a>Jupyterlab theme 설치</h2><h2 id=\"가상환경-생성-및-Jupyter-등록\"><a href=\"#가상환경-생성-및-Jupyter-등록\" class=\"headerlink\" title=\"가상환경 생성 및 Jupyter 등록\"></a>가상환경 생성 및 Jupyter 등록</h2><p>가상환경 자체는 단순히 자신이 필요한 python환경을 구축하기 위해 폴더 안에 필요한 패키지만을 모아 놓은 것이다. 보통 프로젝트 단위로 작업을 할때 패키지 의존성(dependancy)으로 인한 오류들을 줄이기 위해 사용한다. <code>vertualenv</code>로도 가상환경을 설치할 수 있지만 <code>conda</code>를 사용한다면 conda명령어를 통해 쉽게 가상환경을 설치하고 삭제할 수 있다.<br><img src=\"/conda_venv.png\" alt=\"png\"></p>\n<h3 id=\"conda-가상환경-명령어\"><a href=\"#conda-가상환경-명령어\" class=\"headerlink\" title=\"conda 가상환경 명령어\"></a>conda 가상환경 명령어</h3><ol>\n<li>가상환경 만들기 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create –n 가상환경이름 python=버전 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 활성화  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate 가상환경이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 비활성화  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate 가상환경이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경에 패키지 설치 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install 패키지이름 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 정보 확인 <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda info --enves </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 복사<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create –n 복사된 가상환경 --<span class=\"built_in\">clone</span> 복사될 가상환경 </span><br></pre></td></tr></table></figure></li>\n<li>가상환경 설치패키지 확인<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 가상환경 활성화 시킨 후 시행</span></span><br><span class=\"line\">conda list</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 삭제<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda <span class=\"built_in\">env</span> remove -n 가상환경이름 </span><br></pre></td></tr></table></figure></li>\n</ol>\n<h3 id=\"주피터-커널-등록\"><a href=\"#주피터-커널-등록\" class=\"headerlink\" title=\"주피터 커널 등록\"></a>주피터 커널 등록</h3><ol>\n<li>ipykernel 설치<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install ipykernel</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 주피터 등록<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m ipykernel install --user --name 가상환경 이름 --display-name 커널 이름</span><br></pre></td></tr></table></figure></li>\n<li>가상환경 주피터 커널 삭제<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter kernelspec uninstall 가상환경이름</span><br></pre></td></tr></table></figure></li>\n</ol>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://ooyoung.tistory.com/7\">https://ooyoung.tistory.com/7</a></li>\n<li><a href=\"https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html\">https://django-easy-tutorial.blogspot.com/2015/08/python-virtual-environment-setup-in-ubuntu.html</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"/conda_venv.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Tools]Anaconda 시작시 기본설정","path":"2021/07/08/tools-conda-install/","eyeCatchImage":"/conda_venv.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-07-08T08:09:02.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-07-08T08:09:02.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Tools","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Tools]Docker 기본개념(작성중)","date":"2022-06-13T14:23:32.360Z","tagscategories":[["Tools"]],"updated":"2022-04-14T22:10:22.000Z","_content":"\n## Intro\n\n## Docker개념\n\n- 도커 이미지는 다른 이미지 위에 쌓는게 가능하다.\n- VM은 독립적으로 운영되고 독립적인 자원을 사용하기 때문에 그 경우에 효율적일 수 있다.\n- 도커의 Layer는 설계도이다.\n- 이미지는 기본적으로 아직 실행되지 않은 컨테이너이다.\n\nDocker Container는 가상환경과 다르다.\n\n가상환경은 python version만 폴더안에 있는걸 쓰는 것이고 Docker Container는 아예 독립된 환경이다.\n\n## Doker 명령어들\n\n도커 빌드하기\n```bash\n$ cd /path/to/Dockerfile\n$ sudo docker build .\n```\n\n현재 실행되고 있는 도커 프로세스들확인\n\n```\n$ sudo docker ps\n```\n\nView all processes\n\n```\n$ sudo docker ps -a\n```\n\nRun an image in a new container daemonized\n\n```\n$ sudo docker run -d <image_name>\n```\n\nRun an image in interactive mode with the command `/bin/bash`\n\n```\n$ sudo docker run -i -t <image_name> /bin/bash\n```\n\nRun an image in interactive mode with the command `/bin/bash` and link the ports.\n\n```\n$ sudo docker run -i -t --link <docker_container_name>:<docker_container_alias> <image_name> /bin/bash\n```\n\nRun an image with an ENTRYPOINT command in interactive mode with the command `/bin/bash`\n\n```\n$ sudo docker run --entrypoint /bin/bash -i -t <image_name>\n```\n\nRun an image in interactive mode with the command `/bin/bash` mounting the host director `/var/app/current` to the container directory `/usr/src/app`\n\n```\n$ sudo docker run -i -t -v /var/app/current:/usr/src/app/ <image_name> /bin/bash\n```\n\nRun an image in interactive mode with the command `/bin/bash` setting the environments variables `FOO` and `BAR`\n\n```\n$ sudo docker run -i -t -e FOO=foo -e BAR=bar <image_name> /bin/bash\n```\n\n## References\n\n- https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4\n","source":"_posts/tools-docker-basic.md","raw":"---\ntitle: \"[Tools]Docker 기본개념(작성중)\"\ndate: \ntagscategories:\n  - [Tools]\nupdated:\n---\n\n## Intro\n\n## Docker개념\n\n- 도커 이미지는 다른 이미지 위에 쌓는게 가능하다.\n- VM은 독립적으로 운영되고 독립적인 자원을 사용하기 때문에 그 경우에 효율적일 수 있다.\n- 도커의 Layer는 설계도이다.\n- 이미지는 기본적으로 아직 실행되지 않은 컨테이너이다.\n\nDocker Container는 가상환경과 다르다.\n\n가상환경은 python version만 폴더안에 있는걸 쓰는 것이고 Docker Container는 아예 독립된 환경이다.\n\n## Doker 명령어들\n\n도커 빌드하기\n```bash\n$ cd /path/to/Dockerfile\n$ sudo docker build .\n```\n\n현재 실행되고 있는 도커 프로세스들확인\n\n```\n$ sudo docker ps\n```\n\nView all processes\n\n```\n$ sudo docker ps -a\n```\n\nRun an image in a new container daemonized\n\n```\n$ sudo docker run -d <image_name>\n```\n\nRun an image in interactive mode with the command `/bin/bash`\n\n```\n$ sudo docker run -i -t <image_name> /bin/bash\n```\n\nRun an image in interactive mode with the command `/bin/bash` and link the ports.\n\n```\n$ sudo docker run -i -t --link <docker_container_name>:<docker_container_alias> <image_name> /bin/bash\n```\n\nRun an image with an ENTRYPOINT command in interactive mode with the command `/bin/bash`\n\n```\n$ sudo docker run --entrypoint /bin/bash -i -t <image_name>\n```\n\nRun an image in interactive mode with the command `/bin/bash` mounting the host director `/var/app/current` to the container directory `/usr/src/app`\n\n```\n$ sudo docker run -i -t -v /var/app/current:/usr/src/app/ <image_name> /bin/bash\n```\n\nRun an image in interactive mode with the command `/bin/bash` setting the environments variables `FOO` and `BAR`\n\n```\n$ sudo docker run -i -t -e FOO=foo -e BAR=bar <image_name> /bin/bash\n```\n\n## References\n\n- https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4\n","slug":"tools-docker-basic","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscq003zb36qf7w93ab0","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><h2 id=\"Docker개념\"><a href=\"#Docker개념\" class=\"headerlink\" title=\"Docker개념\"></a>Docker개념</h2><ul>\n<li>도커 이미지는 다른 이미지 위에 쌓는게 가능하다.</li>\n<li>VM은 독립적으로 운영되고 독립적인 자원을 사용하기 때문에 그 경우에 효율적일 수 있다.</li>\n<li>도커의 Layer는 설계도이다.</li>\n<li>이미지는 기본적으로 아직 실행되지 않은 컨테이너이다.</li>\n</ul>\n<p>Docker Container는 가상환경과 다르다.</p>\n<p>가상환경은 python version만 폴더안에 있는걸 쓰는 것이고 Docker Container는 아예 독립된 환경이다.</p>\n<h2 id=\"Doker-명령어들\"><a href=\"#Doker-명령어들\" class=\"headerlink\" title=\"Doker 명령어들\"></a>Doker 명령어들</h2><p>도커 빌드하기</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> /path/to/Dockerfile</span><br><span class=\"line\">$ sudo docker build .</span><br></pre></td></tr></table></figure>\n\n<p>현재 실행되고 있는 도커 프로세스들확인</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker ps</span><br></pre></td></tr></table></figure>\n\n<p>View all processes</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker ps -a</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in a new container daemonized</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -d &lt;image_name&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> and link the ports.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t --link &lt;docker_container_name&gt;:&lt;docker_container_alias&gt; &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image with an ENTRYPOINT command in interactive mode with the command <code>/bin/bash</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run --entrypoint /bin/bash -i -t &lt;image_name&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> mounting the host director <code>/var/app/current</code> to the container directory <code>/usr/src/app</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t -v /var/app/current:/usr/src/app/ &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> setting the environments variables <code>FOO</code> and <code>BAR</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t -e FOO=foo -e BAR=bar &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4\">https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><h2 id=\"Docker개념\"><a href=\"#Docker개념\" class=\"headerlink\" title=\"Docker개념\"></a>Docker개념</h2><ul>\n<li>도커 이미지는 다른 이미지 위에 쌓는게 가능하다.</li>\n<li>VM은 독립적으로 운영되고 독립적인 자원을 사용하기 때문에 그 경우에 효율적일 수 있다.</li>\n<li>도커의 Layer는 설계도이다.</li>\n<li>이미지는 기본적으로 아직 실행되지 않은 컨테이너이다.</li>\n</ul>\n<p>Docker Container는 가상환경과 다르다.</p>\n<p>가상환경은 python version만 폴더안에 있는걸 쓰는 것이고 Docker Container는 아예 독립된 환경이다.</p>\n<h2 id=\"Doker-명령어들\"><a href=\"#Doker-명령어들\" class=\"headerlink\" title=\"Doker 명령어들\"></a>Doker 명령어들</h2><p>도커 빌드하기</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> /path/to/Dockerfile</span><br><span class=\"line\">$ sudo docker build .</span><br></pre></td></tr></table></figure>\n\n<p>현재 실행되고 있는 도커 프로세스들확인</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker ps</span><br></pre></td></tr></table></figure>\n\n<p>View all processes</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker ps -a</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in a new container daemonized</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -d &lt;image_name&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> and link the ports.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t --link &lt;docker_container_name&gt;:&lt;docker_container_alias&gt; &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image with an ENTRYPOINT command in interactive mode with the command <code>/bin/bash</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run --entrypoint /bin/bash -i -t &lt;image_name&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> mounting the host director <code>/var/app/current</code> to the container directory <code>/usr/src/app</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t -v /var/app/current:/usr/src/app/ &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<p>Run an image in interactive mode with the command <code>/bin/bash</code> setting the environments variables <code>FOO</code> and <code>BAR</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo docker run -i -t -e FOO=foo -e BAR=bar &lt;image_name&gt; /bin/bash</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4\">https://gist.github.com/dwilkie/f8d6526edc5f1a8aca385df5113567e4</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Tools]Docker 기본개념(작성중)","path":"2022/06/13/tools-docker-basic/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.360Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.360Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Git]간단한 Git 명령어 및 용법 정리","date":"2021-07-17T12:52:37.000Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n## Intro\n\nGit은 버전관리와 협업을 위한 툴입니다. 추후 참고할 수 있도록 간단히 내용을 정리하겠습니다.\n\n---\n\n## Git 동작원리\n\n### Git의 영역들\n\n- **Project 영역(working tree)** : 현재 프로젝트에 있는 파일들 전체입니다.\n- **Stagind 영역(staging area)** : Project 영역에서 변경된 사항들을 기록하는 index 입니다.\n- **Repository** : 깃이 버전 관리를 하기 위해 필요한 데이터들을 저장하는 곳입니다.깃을 초기화해 버전관리를 한 시점부터 현 시점까지의 파일들이 저장되어 있습니다.\n  - local  : 사용자의 Local 머신에 저장된 레포지토리입니다.\n  - remote : Github, Gitlab 등의 웹 저장소에 레포지토리 입니다.\n\n### Git에서 수행하는 작업들\n\nGit은 기본적으로 Project영역에서 수정작업을 한 뒤 index에 `staging`하고 이를 로컬저장소에 `commit`하고 원격저장소에 `push` 하는 절차를 거칩니다.\n\n- **특정 프로젝트의 업데이트 내용 기록(버전관리)**\n  - git add\n  - git status\n  - git log\n  - git commit\n- **같은 파일을 여러 작업자가 수정 및 관리(협업)**\n  - git branch\n  - git checkout\n  - git remote add 저장소\n  - git fetch\n  - git merge\n  - git pull\n  - git push\n\n![ohno](git-simple.png)\n**<그림 1 git 동작원리>**\n\n## git commit\n간단한 연습용 프로젝트를 통해 Git을 이해해봅시다.\n일단 적당히 디렉토리를 만들고 git을 초기화 합니다.\n\n```bash\nmkdir new_project\ncd new_project\ngit init\n```\n적당한 파이썬 파일 하나를 디렉토리에 추가해 줍니다.\n`vim` 이나 `sublimetext` ,`vscode` 등의 편집기를 활용해 파일들을 수정할 수 있습니다.\n여기서는 bash shell에서 간단한 명령을 추가했습니다.\n```bash\ntouch this.py\n# 파일 편집을 위한 적당한 명령 추가\necho \"import this\" >> this.py\n```\n`git status` 를  통해  확인해보면 `commit` 할 파일이 없다고 나옵니다.\n이는 파일이 아직 Staging 영역의 index에 올라가지 않아서 그렇습니다.\n```bash\ngit status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        this.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n```\n방금 생성한 파일을 git이 추적하도록 git add를 해줍니다.\n만약 폴더 내 여러 파일을 편집한 상황에서 모든 변경사항을 반영하고 싶다면 `git add .`를 해주면 됩니다. \n\n```bash\ngit add this.py\n```\n이제 폴더 내 변경사항이 index에 staging되었으니까 local 저장소에 `commit`을 해줄 수 있습니다.\n`commit -m` 을 통해 어떤 부분이 변경되었는지 간단히 메시지를 적어줄 수 있습니다.\n```bash\ngit commit -m \"this.py 파일 추가\"\n```\n여기까지가 `working directory` -> `local repository`의 작업흐름입니다.\n## git remote\n`remote repostory` 는 데이터가 웹 서버에 저장된다는 것을 제외하면 로컬의 그것과 다를게 없습니다. \n\n서버에 저장소를 만들어 둠으로서 보다 여러 사람들이 저장소에 접근하고 수정할 수 있게끔 해서 소스코드 관리를 보다 편리하게 할 수있습니다.\n\n![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile8.uf.tistory.com%2Fimage%2F27532A36575F3888290EC8)\n**<그림2 remote repository>**\n\n`git remote add ` 를 통해 로컬을 외부저장소와 연결할 수 있습니다.\n```bash\ngit remote add origin '외부저장소 링크'\n```\n\n`git push`  를 통해 로컬저장소의 내용을 서버의 외부저장소로 전송할 수 있습니다.\n\n```bash\ngit push -u origin main\n```\n\n`git fetch`  를 통해 외부저장소의 변경사항을 로컬로 가져올 수 있습니다. \n이 변경사항을 로컬의 작업내역들과 비교할 수 있고 만약 누군가가 생성한 신규 커밋들이 로컬에서 작업한 부분과 중복되는 부분이 있다면 이를 알 수 있습니다.\n```bash\ngit fetch '외부저장소명' '브랜치명'\n```\n\n`git merge`  를 통해 외부저장소의 내용과 로컬의 내용을 동기화합니다.\n\n```bash\ngit push -u origin main\n```\n\n`git pull`  를 통해 `git fetch`와 `merge`를 한번에 시행할 수 있습니다.\n`git fetch`를 통해 외부저장소의 변경사항을 확인하고 `git merge`를 통해 로컬과 외부저장소를 병합합니다.\n\n```bash\ngit pull\n```\n## git branch\n\n**branch는 git의 커밋과 커밋 사이를 이동할 수 있는 일종의 포인터 입니다.** \n\ngit branch는 git의 버전관리와 협업의 핵심이 되는 컨셉입니다.\n\n여러 작업자가 동시에 작업해야 하는 큰 프로젝트가 있을때 git branch를 사용해서 생산성을 높일 수 있습니다.\n\n![](git-branch.png)\n\n`git branch` 를 통해 현재 branch들을 조회합니다. -a 를 통해 원격과 로컬 branch를 모두 조회할 수 있습니다.\n\n```bash\ngit branch\n```\n새 branch를 만듧니다.\n```bash\ngit branch 브랜치명\n```\n원래 branch를 새로운 branch로 변경합니다.\n```bash\ngit branch -m 원래브랜치 새로운브랜치\n```\nmain에서 새로운 브랜치 new를 만듭니다.\n```bash\ngit branch new main\n```\n브랜치를 삭제합니다.\n```bash\ngit branch -d 브랜치\n```\n해당 브랜치로 작업영역을 변경합니다.\n```\ngit checkout 브랜치\n```\n\nA 브랜치를 현재 브랜치로 합칩니다.\n```\ngit merge A\n```\n---\n\n## References\n\n- https://opentutorials.org/module/2676/15202\n- https://dzone.com/articles/top-20-git-commands-with-examples\n- https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\n\n","source":"_posts/tools-git-basic.md","raw":"---\ntitle: \"[Git]간단한 Git 명령어 및 용법 정리\"\ndate: 2021-07-17 21:52:37\ntags:\ncategories:\n  - [Tools]\nupdated:\n---\n\n## Intro\n\nGit은 버전관리와 협업을 위한 툴입니다. 추후 참고할 수 있도록 간단히 내용을 정리하겠습니다.\n\n---\n\n## Git 동작원리\n\n### Git의 영역들\n\n- **Project 영역(working tree)** : 현재 프로젝트에 있는 파일들 전체입니다.\n- **Stagind 영역(staging area)** : Project 영역에서 변경된 사항들을 기록하는 index 입니다.\n- **Repository** : 깃이 버전 관리를 하기 위해 필요한 데이터들을 저장하는 곳입니다.깃을 초기화해 버전관리를 한 시점부터 현 시점까지의 파일들이 저장되어 있습니다.\n  - local  : 사용자의 Local 머신에 저장된 레포지토리입니다.\n  - remote : Github, Gitlab 등의 웹 저장소에 레포지토리 입니다.\n\n### Git에서 수행하는 작업들\n\nGit은 기본적으로 Project영역에서 수정작업을 한 뒤 index에 `staging`하고 이를 로컬저장소에 `commit`하고 원격저장소에 `push` 하는 절차를 거칩니다.\n\n- **특정 프로젝트의 업데이트 내용 기록(버전관리)**\n  - git add\n  - git status\n  - git log\n  - git commit\n- **같은 파일을 여러 작업자가 수정 및 관리(협업)**\n  - git branch\n  - git checkout\n  - git remote add 저장소\n  - git fetch\n  - git merge\n  - git pull\n  - git push\n\n![ohno](git-simple.png)\n**<그림 1 git 동작원리>**\n\n## git commit\n간단한 연습용 프로젝트를 통해 Git을 이해해봅시다.\n일단 적당히 디렉토리를 만들고 git을 초기화 합니다.\n\n```bash\nmkdir new_project\ncd new_project\ngit init\n```\n적당한 파이썬 파일 하나를 디렉토리에 추가해 줍니다.\n`vim` 이나 `sublimetext` ,`vscode` 등의 편집기를 활용해 파일들을 수정할 수 있습니다.\n여기서는 bash shell에서 간단한 명령을 추가했습니다.\n```bash\ntouch this.py\n# 파일 편집을 위한 적당한 명령 추가\necho \"import this\" >> this.py\n```\n`git status` 를  통해  확인해보면 `commit` 할 파일이 없다고 나옵니다.\n이는 파일이 아직 Staging 영역의 index에 올라가지 않아서 그렇습니다.\n```bash\ngit status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        this.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n```\n방금 생성한 파일을 git이 추적하도록 git add를 해줍니다.\n만약 폴더 내 여러 파일을 편집한 상황에서 모든 변경사항을 반영하고 싶다면 `git add .`를 해주면 됩니다. \n\n```bash\ngit add this.py\n```\n이제 폴더 내 변경사항이 index에 staging되었으니까 local 저장소에 `commit`을 해줄 수 있습니다.\n`commit -m` 을 통해 어떤 부분이 변경되었는지 간단히 메시지를 적어줄 수 있습니다.\n```bash\ngit commit -m \"this.py 파일 추가\"\n```\n여기까지가 `working directory` -> `local repository`의 작업흐름입니다.\n## git remote\n`remote repostory` 는 데이터가 웹 서버에 저장된다는 것을 제외하면 로컬의 그것과 다를게 없습니다. \n\n서버에 저장소를 만들어 둠으로서 보다 여러 사람들이 저장소에 접근하고 수정할 수 있게끔 해서 소스코드 관리를 보다 편리하게 할 수있습니다.\n\n![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile8.uf.tistory.com%2Fimage%2F27532A36575F3888290EC8)\n**<그림2 remote repository>**\n\n`git remote add ` 를 통해 로컬을 외부저장소와 연결할 수 있습니다.\n```bash\ngit remote add origin '외부저장소 링크'\n```\n\n`git push`  를 통해 로컬저장소의 내용을 서버의 외부저장소로 전송할 수 있습니다.\n\n```bash\ngit push -u origin main\n```\n\n`git fetch`  를 통해 외부저장소의 변경사항을 로컬로 가져올 수 있습니다. \n이 변경사항을 로컬의 작업내역들과 비교할 수 있고 만약 누군가가 생성한 신규 커밋들이 로컬에서 작업한 부분과 중복되는 부분이 있다면 이를 알 수 있습니다.\n```bash\ngit fetch '외부저장소명' '브랜치명'\n```\n\n`git merge`  를 통해 외부저장소의 내용과 로컬의 내용을 동기화합니다.\n\n```bash\ngit push -u origin main\n```\n\n`git pull`  를 통해 `git fetch`와 `merge`를 한번에 시행할 수 있습니다.\n`git fetch`를 통해 외부저장소의 변경사항을 확인하고 `git merge`를 통해 로컬과 외부저장소를 병합합니다.\n\n```bash\ngit pull\n```\n## git branch\n\n**branch는 git의 커밋과 커밋 사이를 이동할 수 있는 일종의 포인터 입니다.** \n\ngit branch는 git의 버전관리와 협업의 핵심이 되는 컨셉입니다.\n\n여러 작업자가 동시에 작업해야 하는 큰 프로젝트가 있을때 git branch를 사용해서 생산성을 높일 수 있습니다.\n\n![](git-branch.png)\n\n`git branch` 를 통해 현재 branch들을 조회합니다. -a 를 통해 원격과 로컬 branch를 모두 조회할 수 있습니다.\n\n```bash\ngit branch\n```\n새 branch를 만듧니다.\n```bash\ngit branch 브랜치명\n```\n원래 branch를 새로운 branch로 변경합니다.\n```bash\ngit branch -m 원래브랜치 새로운브랜치\n```\nmain에서 새로운 브랜치 new를 만듭니다.\n```bash\ngit branch new main\n```\n브랜치를 삭제합니다.\n```bash\ngit branch -d 브랜치\n```\n해당 브랜치로 작업영역을 변경합니다.\n```\ngit checkout 브랜치\n```\n\nA 브랜치를 현재 브랜치로 합칩니다.\n```\ngit merge A\n```\n---\n\n## References\n\n- https://opentutorials.org/module/2676/15202\n- https://dzone.com/articles/top-20-git-commands-with-examples\n- https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\n\n","slug":"tools-git-basic","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscr0041b36q4mhxh8vs","content":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>Git은 버전관리와 협업을 위한 툴입니다. 추후 참고할 수 있도록 간단히 내용을 정리하겠습니다.</p>\n<hr>\n<h2 id=\"Git-동작원리\"><a href=\"#Git-동작원리\" class=\"headerlink\" title=\"Git 동작원리\"></a>Git 동작원리</h2><h3 id=\"Git의-영역들\"><a href=\"#Git의-영역들\" class=\"headerlink\" title=\"Git의 영역들\"></a>Git의 영역들</h3><ul>\n<li><strong>Project 영역(working tree)</strong> : 현재 프로젝트에 있는 파일들 전체입니다.</li>\n<li><strong>Stagind 영역(staging area)</strong> : Project 영역에서 변경된 사항들을 기록하는 index 입니다.</li>\n<li><strong>Repository</strong> : 깃이 버전 관리를 하기 위해 필요한 데이터들을 저장하는 곳입니다.깃을 초기화해 버전관리를 한 시점부터 현 시점까지의 파일들이 저장되어 있습니다.<ul>\n<li>local  : 사용자의 Local 머신에 저장된 레포지토리입니다.</li>\n<li>remote : Github, Gitlab 등의 웹 저장소에 레포지토리 입니다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Git에서-수행하는-작업들\"><a href=\"#Git에서-수행하는-작업들\" class=\"headerlink\" title=\"Git에서 수행하는 작업들\"></a>Git에서 수행하는 작업들</h3><p>Git은 기본적으로 Project영역에서 수정작업을 한 뒤 index에 <code>staging</code>하고 이를 로컬저장소에 <code>commit</code>하고 원격저장소에 <code>push</code> 하는 절차를 거칩니다.</p>\n<ul>\n<li><strong>특정 프로젝트의 업데이트 내용 기록(버전관리)</strong><ul>\n<li>git add</li>\n<li>git status</li>\n<li>git log</li>\n<li>git commit</li>\n</ul>\n</li>\n<li><strong>같은 파일을 여러 작업자가 수정 및 관리(협업)</strong><ul>\n<li>git branch</li>\n<li>git checkout</li>\n<li>git remote add 저장소</li>\n<li>git fetch</li>\n<li>git merge</li>\n<li>git pull</li>\n<li>git push</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/git-simple.png\" alt=\"ohno\"><br><strong>&lt;그림 1 git 동작원리&gt;</strong></p>\n<h2 id=\"git-commit\"><a href=\"#git-commit\" class=\"headerlink\" title=\"git commit\"></a>git commit</h2><p>간단한 연습용 프로젝트를 통해 Git을 이해해봅시다.<br>일단 적당히 디렉토리를 만들고 git을 초기화 합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> new_project</span><br><span class=\"line\"><span class=\"built_in\">cd</span> new_project</span><br><span class=\"line\">git init</span><br></pre></td></tr></table></figure>\n<p>적당한 파이썬 파일 하나를 디렉토리에 추가해 줍니다.<br><code>vim</code> 이나 <code>sublimetext</code> ,<code>vscode</code> 등의 편집기를 활용해 파일들을 수정할 수 있습니다.<br>여기서는 bash shell에서 간단한 명령을 추가했습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">touch</span> this.py</span><br><span class=\"line\"><span class=\"comment\"># 파일 편집을 위한 적당한 명령 추가</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;import this&quot;</span> &gt;&gt; this.py</span><br></pre></td></tr></table></figure>\n<p><code>git status</code> 를  통해  확인해보면 <code>commit</code> 할 파일이 없다고 나옵니다.<br>이는 파일이 아직 Staging 영역의 index에 올라가지 않아서 그렇습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git status</span><br><span class=\"line\">On branch master</span><br><span class=\"line\"></span><br><span class=\"line\">No commits yet</span><br><span class=\"line\"></span><br><span class=\"line\">Untracked files:</span><br><span class=\"line\">  (use <span class=\"string\">&quot;git add &lt;file&gt;...&quot;</span> to include <span class=\"keyword\">in</span> what will be committed)</span><br><span class=\"line\">        this.py</span><br><span class=\"line\"></span><br><span class=\"line\">nothing added to commit but untracked files present (use <span class=\"string\">&quot;git add&quot;</span> to track)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>방금 생성한 파일을 git이 추적하도록 git add를 해줍니다.<br>만약 폴더 내 여러 파일을 편집한 상황에서 모든 변경사항을 반영하고 싶다면 <code>git add .</code>를 해주면 됩니다. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add this.py</span><br></pre></td></tr></table></figure>\n<p>이제 폴더 내 변경사항이 index에 staging되었으니까 local 저장소에 <code>commit</code>을 해줄 수 있습니다.<br><code>commit -m</code> 을 통해 어떤 부분이 변경되었는지 간단히 메시지를 적어줄 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit -m <span class=\"string\">&quot;this.py 파일 추가&quot;</span></span><br></pre></td></tr></table></figure>\n<p>여기까지가 <code>working directory</code> -&gt; <code>local repository</code>의 작업흐름입니다.</p>\n<h2 id=\"git-remote\"><a href=\"#git-remote\" class=\"headerlink\" title=\"git remote\"></a>git remote</h2><p><code>remote repostory</code> 는 데이터가 웹 서버에 저장된다는 것을 제외하면 로컬의 그것과 다를게 없습니다. </p>\n<p>서버에 저장소를 만들어 둠으로서 보다 여러 사람들이 저장소에 접근하고 수정할 수 있게끔 해서 소스코드 관리를 보다 편리하게 할 수있습니다.</p>\n<p><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http://cfile8.uf.tistory.com/image/27532A36575F3888290EC8\"><br><strong>&lt;그림2 remote repository&gt;</strong></p>\n<p><code>git remote add </code> 를 통해 로컬을 외부저장소와 연결할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add origin <span class=\"string\">&#x27;외부저장소 링크&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<p><code>git push</code>  를 통해 로컬저장소의 내용을 서버의 외부저장소로 전송할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push -u origin main</span><br></pre></td></tr></table></figure>\n\n<p><code>git fetch</code>  를 통해 외부저장소의 변경사항을 로컬로 가져올 수 있습니다.<br>이 변경사항을 로컬의 작업내역들과 비교할 수 있고 만약 누군가가 생성한 신규 커밋들이 로컬에서 작업한 부분과 중복되는 부분이 있다면 이를 알 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch <span class=\"string\">&#x27;외부저장소명&#x27;</span> <span class=\"string\">&#x27;브랜치명&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<p><code>git merge</code>  를 통해 외부저장소의 내용과 로컬의 내용을 동기화합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push -u origin main</span><br></pre></td></tr></table></figure>\n\n<p><code>git pull</code>  를 통해 <code>git fetch</code>와 <code>merge</code>를 한번에 시행할 수 있습니다.<br><code>git fetch</code>를 통해 외부저장소의 변경사항을 확인하고 <code>git merge</code>를 통해 로컬과 외부저장소를 병합합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull</span><br></pre></td></tr></table></figure>\n<h2 id=\"git-branch\"><a href=\"#git-branch\" class=\"headerlink\" title=\"git branch\"></a>git branch</h2><p><strong>branch는 git의 커밋과 커밋 사이를 이동할 수 있는 일종의 포인터 입니다.</strong> </p>\n<p>git branch는 git의 버전관리와 협업의 핵심이 되는 컨셉입니다.</p>\n<p>여러 작업자가 동시에 작업해야 하는 큰 프로젝트가 있을때 git branch를 사용해서 생산성을 높일 수 있습니다.</p>\n<p><img src=\"/git-branch.png\"></p>\n<p><code>git branch</code> 를 통해 현재 branch들을 조회합니다. -a 를 통해 원격과 로컬 branch를 모두 조회할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n<p>새 branch를 만듧니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch 브랜치명</span><br></pre></td></tr></table></figure>\n<p>원래 branch를 새로운 branch로 변경합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -m 원래브랜치 새로운브랜치</span><br></pre></td></tr></table></figure>\n<p>main에서 새로운 브랜치 new를 만듭니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch new main</span><br></pre></td></tr></table></figure>\n<p>브랜치를 삭제합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d 브랜치</span><br></pre></td></tr></table></figure>\n<p>해당 브랜치로 작업영역을 변경합니다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout 브랜치</span><br></pre></td></tr></table></figure>\n\n<p>A 브랜치를 현재 브랜치로 합칩니다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git merge A</span><br></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://opentutorials.org/module/2676/15202\">https://opentutorials.org/module/2676/15202</a></li>\n<li><a href=\"https://dzone.com/articles/top-20-git-commands-with-examples\">https://dzone.com/articles/top-20-git-commands-with-examples</a></li>\n<li><a href=\"https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\">https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>Git은 버전관리와 협업을 위한 툴입니다. 추후 참고할 수 있도록 간단히 내용을 정리하겠습니다.</p>\n<hr>\n<h2 id=\"Git-동작원리\"><a href=\"#Git-동작원리\" class=\"headerlink\" title=\"Git 동작원리\"></a>Git 동작원리</h2><h3 id=\"Git의-영역들\"><a href=\"#Git의-영역들\" class=\"headerlink\" title=\"Git의 영역들\"></a>Git의 영역들</h3><ul>\n<li><strong>Project 영역(working tree)</strong> : 현재 프로젝트에 있는 파일들 전체입니다.</li>\n<li><strong>Stagind 영역(staging area)</strong> : Project 영역에서 변경된 사항들을 기록하는 index 입니다.</li>\n<li><strong>Repository</strong> : 깃이 버전 관리를 하기 위해 필요한 데이터들을 저장하는 곳입니다.깃을 초기화해 버전관리를 한 시점부터 현 시점까지의 파일들이 저장되어 있습니다.<ul>\n<li>local  : 사용자의 Local 머신에 저장된 레포지토리입니다.</li>\n<li>remote : Github, Gitlab 등의 웹 저장소에 레포지토리 입니다.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Git에서-수행하는-작업들\"><a href=\"#Git에서-수행하는-작업들\" class=\"headerlink\" title=\"Git에서 수행하는 작업들\"></a>Git에서 수행하는 작업들</h3><p>Git은 기본적으로 Project영역에서 수정작업을 한 뒤 index에 <code>staging</code>하고 이를 로컬저장소에 <code>commit</code>하고 원격저장소에 <code>push</code> 하는 절차를 거칩니다.</p>\n<ul>\n<li><strong>특정 프로젝트의 업데이트 내용 기록(버전관리)</strong><ul>\n<li>git add</li>\n<li>git status</li>\n<li>git log</li>\n<li>git commit</li>\n</ul>\n</li>\n<li><strong>같은 파일을 여러 작업자가 수정 및 관리(협업)</strong><ul>\n<li>git branch</li>\n<li>git checkout</li>\n<li>git remote add 저장소</li>\n<li>git fetch</li>\n<li>git merge</li>\n<li>git pull</li>\n<li>git push</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/git-simple.png\" alt=\"ohno\"><br><strong>&lt;그림 1 git 동작원리&gt;</strong></p>\n<h2 id=\"git-commit\"><a href=\"#git-commit\" class=\"headerlink\" title=\"git commit\"></a>git commit</h2><p>간단한 연습용 프로젝트를 통해 Git을 이해해봅시다.<br>일단 적당히 디렉토리를 만들고 git을 초기화 합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> new_project</span><br><span class=\"line\"><span class=\"built_in\">cd</span> new_project</span><br><span class=\"line\">git init</span><br></pre></td></tr></table></figure>\n<p>적당한 파이썬 파일 하나를 디렉토리에 추가해 줍니다.<br><code>vim</code> 이나 <code>sublimetext</code> ,<code>vscode</code> 등의 편집기를 활용해 파일들을 수정할 수 있습니다.<br>여기서는 bash shell에서 간단한 명령을 추가했습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">touch</span> this.py</span><br><span class=\"line\"><span class=\"comment\"># 파일 편집을 위한 적당한 명령 추가</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;import this&quot;</span> &gt;&gt; this.py</span><br></pre></td></tr></table></figure>\n<p><code>git status</code> 를  통해  확인해보면 <code>commit</code> 할 파일이 없다고 나옵니다.<br>이는 파일이 아직 Staging 영역의 index에 올라가지 않아서 그렇습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git status</span><br><span class=\"line\">On branch master</span><br><span class=\"line\"></span><br><span class=\"line\">No commits yet</span><br><span class=\"line\"></span><br><span class=\"line\">Untracked files:</span><br><span class=\"line\">  (use <span class=\"string\">&quot;git add &lt;file&gt;...&quot;</span> to include <span class=\"keyword\">in</span> what will be committed)</span><br><span class=\"line\">        this.py</span><br><span class=\"line\"></span><br><span class=\"line\">nothing added to commit but untracked files present (use <span class=\"string\">&quot;git add&quot;</span> to track)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>방금 생성한 파일을 git이 추적하도록 git add를 해줍니다.<br>만약 폴더 내 여러 파일을 편집한 상황에서 모든 변경사항을 반영하고 싶다면 <code>git add .</code>를 해주면 됩니다. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add this.py</span><br></pre></td></tr></table></figure>\n<p>이제 폴더 내 변경사항이 index에 staging되었으니까 local 저장소에 <code>commit</code>을 해줄 수 있습니다.<br><code>commit -m</code> 을 통해 어떤 부분이 변경되었는지 간단히 메시지를 적어줄 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit -m <span class=\"string\">&quot;this.py 파일 추가&quot;</span></span><br></pre></td></tr></table></figure>\n<p>여기까지가 <code>working directory</code> -&gt; <code>local repository</code>의 작업흐름입니다.</p>\n<h2 id=\"git-remote\"><a href=\"#git-remote\" class=\"headerlink\" title=\"git remote\"></a>git remote</h2><p><code>remote repostory</code> 는 데이터가 웹 서버에 저장된다는 것을 제외하면 로컬의 그것과 다를게 없습니다. </p>\n<p>서버에 저장소를 만들어 둠으로서 보다 여러 사람들이 저장소에 접근하고 수정할 수 있게끔 해서 소스코드 관리를 보다 편리하게 할 수있습니다.</p>\n<p><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http://cfile8.uf.tistory.com/image/27532A36575F3888290EC8\"><br><strong>&lt;그림2 remote repository&gt;</strong></p>\n<p><code>git remote add </code> 를 통해 로컬을 외부저장소와 연결할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add origin <span class=\"string\">&#x27;외부저장소 링크&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<p><code>git push</code>  를 통해 로컬저장소의 내용을 서버의 외부저장소로 전송할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push -u origin main</span><br></pre></td></tr></table></figure>\n\n<p><code>git fetch</code>  를 통해 외부저장소의 변경사항을 로컬로 가져올 수 있습니다.<br>이 변경사항을 로컬의 작업내역들과 비교할 수 있고 만약 누군가가 생성한 신규 커밋들이 로컬에서 작업한 부분과 중복되는 부분이 있다면 이를 알 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch <span class=\"string\">&#x27;외부저장소명&#x27;</span> <span class=\"string\">&#x27;브랜치명&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<p><code>git merge</code>  를 통해 외부저장소의 내용과 로컬의 내용을 동기화합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push -u origin main</span><br></pre></td></tr></table></figure>\n\n<p><code>git pull</code>  를 통해 <code>git fetch</code>와 <code>merge</code>를 한번에 시행할 수 있습니다.<br><code>git fetch</code>를 통해 외부저장소의 변경사항을 확인하고 <code>git merge</code>를 통해 로컬과 외부저장소를 병합합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull</span><br></pre></td></tr></table></figure>\n<h2 id=\"git-branch\"><a href=\"#git-branch\" class=\"headerlink\" title=\"git branch\"></a>git branch</h2><p><strong>branch는 git의 커밋과 커밋 사이를 이동할 수 있는 일종의 포인터 입니다.</strong> </p>\n<p>git branch는 git의 버전관리와 협업의 핵심이 되는 컨셉입니다.</p>\n<p>여러 작업자가 동시에 작업해야 하는 큰 프로젝트가 있을때 git branch를 사용해서 생산성을 높일 수 있습니다.</p>\n<p><img src=\"/git-branch.png\"></p>\n<p><code>git branch</code> 를 통해 현재 branch들을 조회합니다. -a 를 통해 원격과 로컬 branch를 모두 조회할 수 있습니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n<p>새 branch를 만듧니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch 브랜치명</span><br></pre></td></tr></table></figure>\n<p>원래 branch를 새로운 branch로 변경합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -m 원래브랜치 새로운브랜치</span><br></pre></td></tr></table></figure>\n<p>main에서 새로운 브랜치 new를 만듭니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch new main</span><br></pre></td></tr></table></figure>\n<p>브랜치를 삭제합니다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d 브랜치</span><br></pre></td></tr></table></figure>\n<p>해당 브랜치로 작업영역을 변경합니다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout 브랜치</span><br></pre></td></tr></table></figure>\n\n<p>A 브랜치를 현재 브랜치로 합칩니다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git merge A</span><br></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://opentutorials.org/module/2676/15202\">https://opentutorials.org/module/2676/15202</a></li>\n<li><a href=\"https://dzone.com/articles/top-20-git-commands-with-examples\">https://dzone.com/articles/top-20-git-commands-with-examples</a></li>\n<li><a href=\"https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\">https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":"/git-simple.png","popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Git]간단한 Git 명령어 및 용법 정리","path":"2021/07/17/tools-git-basic/","eyeCatchImage":"/git-simple.png","excerpt":null,"date":{"_isAMomentObject":true,"_i":"2021-07-17T12:52:37.000Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2021-07-17T12:52:37.000Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Tools","tags":[],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Git]commit, push 제외 자주쓰는 git 명령어들","date":"2022-06-13T14:23:32.360Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\nmerge 렉카 : https://kotlinworld.com/277\n\nmerge flow :\nmaster 에서 출발\n나의 작업용 브랜치를 만들기 위해 master 에서 feature 브랜치를 생성\nfeature 브랜치에서 add commit 등의 작업\n내가 지금까지 push했던것은 local에 있던 코드를 remote의 브랜치로 전달한것\nPR 메시지 작성\nremote feature 에서 remote master로 merge\nlocal master에서 최신의 remote master 내용을 반영하기 위해 git pull origin master\nlocal feature 에서 최신이 된 local master 내용을 반영하기 위해 git merge master\n충돌 발생 (local feature 에서)\n충돌 해결 후 add ,commit => 새 변경 사항\nremote feature에 새 변경 사항을 push\n충돌 해결 (remote feture 에서)\n\n-->\n\n\n\n#### Branch 생성\n\n```bash\ngit branch\n```\n\n\n#### 생성한 Branch로 이동\n\n```bash\ngit switch example\n```\n\n- branch를 만들면서 현재 branch 변경\n\n```bash\ngit switch -c example2  # c 는 아마 create를 의미\n```\n\n#### branch 삭제\n\n```bash\ngit branch -d example\n```\n\n- merge가 정상적으로 안되는 branch를 강제 삭제할 겨우\n\n```bash\ngit branch -D example\n```\n\n\n- 원격 브랜치를 삭제할 경우\n\n```\ngit push origin --delete example\n```\n\n\n#### 파일옮기기, 이름 바꾸기\n\n단순히 unix mv 명령어를 사용해도 된다.\n\n```bash\ngit mv  old_file new file \n```\n#### 커밋내역 확인하기\n\nlog를 통해 이전 커밋 내역을 확인한다.\n\n```bash\ngit log\n```\n\n#### 커밋내역 삭제하기\n\n```bash\ngit reset HEAD~n  최근 내역 n개 삭제\ngit log  # 삭제된 커밋 확인\n```\n\n\n#### 변경사항 복원하기\n\nrestore  :  변경 내역이 있는 파일을 복원할 수 있다.\n\n```bash\ngit restore a_file \n```\n\n- stage 된 파일 복원\n\n```bash\ngit restore --staged a_file\n```\n\n#### branch 합치기(merge)\n\n- 여기서는 현재 branch의 commit을 대상이 되는 branch의 commit까지 옮기는 작업인 `Fast Forward Merge` 만 다룬다.\n- `Fast Forward Merge`는 중간에 다른 커밋이 추가되면 충돌 오류가 발생한다.\n\n```bash\ngit switch example\ngit merge example2\n```\n\n\n\n## References\n\n- [브랜치와 Merge의 기초](https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EC%99%80-Merge-%EC%9D%98-%EA%B8%B0%EC%B4%88)\n- https://git-scm.com/book/ko/v2\n- https://goddaehee.tistory.com/274\n","source":"_posts/tools-git-log.md","raw":"---\ntitle: \"[Git]commit, push 제외 자주쓰는 git 명령어들\"\ndate: \ntags:\n  - Git\ncategories:\n  - Tools\nupdated:\n---\n\n<!--\n\nmerge 렉카 : https://kotlinworld.com/277\n\nmerge flow :\nmaster 에서 출발\n나의 작업용 브랜치를 만들기 위해 master 에서 feature 브랜치를 생성\nfeature 브랜치에서 add commit 등의 작업\n내가 지금까지 push했던것은 local에 있던 코드를 remote의 브랜치로 전달한것\nPR 메시지 작성\nremote feature 에서 remote master로 merge\nlocal master에서 최신의 remote master 내용을 반영하기 위해 git pull origin master\nlocal feature 에서 최신이 된 local master 내용을 반영하기 위해 git merge master\n충돌 발생 (local feature 에서)\n충돌 해결 후 add ,commit => 새 변경 사항\nremote feature에 새 변경 사항을 push\n충돌 해결 (remote feture 에서)\n\n-->\n\n\n\n#### Branch 생성\n\n```bash\ngit branch\n```\n\n\n#### 생성한 Branch로 이동\n\n```bash\ngit switch example\n```\n\n- branch를 만들면서 현재 branch 변경\n\n```bash\ngit switch -c example2  # c 는 아마 create를 의미\n```\n\n#### branch 삭제\n\n```bash\ngit branch -d example\n```\n\n- merge가 정상적으로 안되는 branch를 강제 삭제할 겨우\n\n```bash\ngit branch -D example\n```\n\n\n- 원격 브랜치를 삭제할 경우\n\n```\ngit push origin --delete example\n```\n\n\n#### 파일옮기기, 이름 바꾸기\n\n단순히 unix mv 명령어를 사용해도 된다.\n\n```bash\ngit mv  old_file new file \n```\n#### 커밋내역 확인하기\n\nlog를 통해 이전 커밋 내역을 확인한다.\n\n```bash\ngit log\n```\n\n#### 커밋내역 삭제하기\n\n```bash\ngit reset HEAD~n  최근 내역 n개 삭제\ngit log  # 삭제된 커밋 확인\n```\n\n\n#### 변경사항 복원하기\n\nrestore  :  변경 내역이 있는 파일을 복원할 수 있다.\n\n```bash\ngit restore a_file \n```\n\n- stage 된 파일 복원\n\n```bash\ngit restore --staged a_file\n```\n\n#### branch 합치기(merge)\n\n- 여기서는 현재 branch의 commit을 대상이 되는 branch의 commit까지 옮기는 작업인 `Fast Forward Merge` 만 다룬다.\n- `Fast Forward Merge`는 중간에 다른 커밋이 추가되면 충돌 오류가 발생한다.\n\n```bash\ngit switch example\ngit merge example2\n```\n\n\n\n## References\n\n- [브랜치와 Merge의 기초](https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EC%99%80-Merge-%EC%9D%98-%EA%B8%B0%EC%B4%88)\n- https://git-scm.com/book/ko/v2\n- https://goddaehee.tistory.com/274\n","slug":"tools-git-log","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscr0045b36q8f8l3adl","content":"<!--\n\nmerge 렉카 : https://kotlinworld.com/277\n\nmerge flow :\nmaster 에서 출발\n나의 작업용 브랜치를 만들기 위해 master 에서 feature 브랜치를 생성\nfeature 브랜치에서 add commit 등의 작업\n내가 지금까지 push했던것은 local에 있던 코드를 remote의 브랜치로 전달한것\nPR 메시지 작성\nremote feature 에서 remote master로 merge\nlocal master에서 최신의 remote master 내용을 반영하기 위해 git pull origin master\nlocal feature 에서 최신이 된 local master 내용을 반영하기 위해 git merge master\n충돌 발생 (local feature 에서)\n충돌 해결 후 add ,commit => 새 변경 사항\nremote feature에 새 변경 사항을 push\n충돌 해결 (remote feture 에서)\n\n-->\n\n\n\n<h4 id=\"Branch-생성\"><a href=\"#Branch-생성\" class=\"headerlink\" title=\"Branch 생성\"></a>Branch 생성</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"생성한-Branch로-이동\"><a href=\"#생성한-Branch로-이동\" class=\"headerlink\" title=\"생성한 Branch로 이동\"></a>생성한 Branch로 이동</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch example</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>branch를 만들면서 현재 branch 변경</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch -c example2  <span class=\"comment\"># c 는 아마 create를 의미</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"branch-삭제\"><a href=\"#branch-삭제\" class=\"headerlink\" title=\"branch 삭제\"></a>branch 삭제</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d example</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>merge가 정상적으로 안되는 branch를 강제 삭제할 겨우</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -D example</span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>원격 브랜치를 삭제할 경우</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push origin --delete example</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"파일옮기기-이름-바꾸기\"><a href=\"#파일옮기기-이름-바꾸기\" class=\"headerlink\" title=\"파일옮기기, 이름 바꾸기\"></a>파일옮기기, 이름 바꾸기</h4><p>단순히 unix mv 명령어를 사용해도 된다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">mv</span>  old_file new file </span><br></pre></td></tr></table></figure>\n<h4 id=\"커밋내역-확인하기\"><a href=\"#커밋내역-확인하기\" class=\"headerlink\" title=\"커밋내역 확인하기\"></a>커밋내역 확인하기</h4><p>log를 통해 이전 커밋 내역을 확인한다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">log</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"커밋내역-삭제하기\"><a href=\"#커밋내역-삭제하기\" class=\"headerlink\" title=\"커밋내역 삭제하기\"></a>커밋내역 삭제하기</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset HEAD~n  최근 내역 n개 삭제</span><br><span class=\"line\">git <span class=\"built_in\">log</span>  <span class=\"comment\"># 삭제된 커밋 확인</span></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"변경사항-복원하기\"><a href=\"#변경사항-복원하기\" class=\"headerlink\" title=\"변경사항 복원하기\"></a>변경사항 복원하기</h4><p>restore  :  변경 내역이 있는 파일을 복원할 수 있다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git restore a_file </span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>stage 된 파일 복원</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git restore --staged a_file</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"branch-합치기-merge\"><a href=\"#branch-합치기-merge\" class=\"headerlink\" title=\"branch 합치기(merge)\"></a>branch 합치기(merge)</h4><ul>\n<li>여기서는 현재 branch의 commit을 대상이 되는 branch의 commit까지 옮기는 작업인 <code>Fast Forward Merge</code> 만 다룬다.</li>\n<li><code>Fast Forward Merge</code>는 중간에 다른 커밋이 추가되면 충돌 오류가 발생한다.</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch example</span><br><span class=\"line\">git merge example2</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EC%99%80-Merge-%EC%9D%98-%EA%B8%B0%EC%B4%88\">브랜치와 Merge의 기초</a></li>\n<li><a href=\"https://git-scm.com/book/ko/v2\">https://git-scm.com/book/ko/v2</a></li>\n<li><a href=\"https://goddaehee.tistory.com/274\">https://goddaehee.tistory.com/274</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\nmerge 렉카 : https://kotlinworld.com/277\n\nmerge flow :\nmaster 에서 출발\n나의 작업용 브랜치를 만들기 위해 master 에서 feature 브랜치를 생성\nfeature 브랜치에서 add commit 등의 작업\n내가 지금까지 push했던것은 local에 있던 코드를 remote의 브랜치로 전달한것\nPR 메시지 작성\nremote feature 에서 remote master로 merge\nlocal master에서 최신의 remote master 내용을 반영하기 위해 git pull origin master\nlocal feature 에서 최신이 된 local master 내용을 반영하기 위해 git merge master\n충돌 발생 (local feature 에서)\n충돌 해결 후 add ,commit => 새 변경 사항\nremote feature에 새 변경 사항을 push\n충돌 해결 (remote feture 에서)\n\n-->\n\n\n\n<h4 id=\"Branch-생성\"><a href=\"#Branch-생성\" class=\"headerlink\" title=\"Branch 생성\"></a>Branch 생성</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"생성한-Branch로-이동\"><a href=\"#생성한-Branch로-이동\" class=\"headerlink\" title=\"생성한 Branch로 이동\"></a>생성한 Branch로 이동</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch example</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>branch를 만들면서 현재 branch 변경</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch -c example2  <span class=\"comment\"># c 는 아마 create를 의미</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"branch-삭제\"><a href=\"#branch-삭제\" class=\"headerlink\" title=\"branch 삭제\"></a>branch 삭제</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d example</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>merge가 정상적으로 안되는 branch를 강제 삭제할 겨우</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -D example</span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>원격 브랜치를 삭제할 경우</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push origin --delete example</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"파일옮기기-이름-바꾸기\"><a href=\"#파일옮기기-이름-바꾸기\" class=\"headerlink\" title=\"파일옮기기, 이름 바꾸기\"></a>파일옮기기, 이름 바꾸기</h4><p>단순히 unix mv 명령어를 사용해도 된다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">mv</span>  old_file new file </span><br></pre></td></tr></table></figure>\n<h4 id=\"커밋내역-확인하기\"><a href=\"#커밋내역-확인하기\" class=\"headerlink\" title=\"커밋내역 확인하기\"></a>커밋내역 확인하기</h4><p>log를 통해 이전 커밋 내역을 확인한다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">log</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"커밋내역-삭제하기\"><a href=\"#커밋내역-삭제하기\" class=\"headerlink\" title=\"커밋내역 삭제하기\"></a>커밋내역 삭제하기</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset HEAD~n  최근 내역 n개 삭제</span><br><span class=\"line\">git <span class=\"built_in\">log</span>  <span class=\"comment\"># 삭제된 커밋 확인</span></span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"변경사항-복원하기\"><a href=\"#변경사항-복원하기\" class=\"headerlink\" title=\"변경사항 복원하기\"></a>변경사항 복원하기</h4><p>restore  :  변경 내역이 있는 파일을 복원할 수 있다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git restore a_file </span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>stage 된 파일 복원</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git restore --staged a_file</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"branch-합치기-merge\"><a href=\"#branch-합치기-merge\" class=\"headerlink\" title=\"branch 합치기(merge)\"></a>branch 합치기(merge)</h4><ul>\n<li>여기서는 현재 branch의 commit을 대상이 되는 branch의 commit까지 옮기는 작업인 <code>Fast Forward Merge</code> 만 다룬다.</li>\n<li><code>Fast Forward Merge</code>는 중간에 다른 커밋이 추가되면 충돌 오류가 발생한다.</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git switch example</span><br><span class=\"line\">git merge example2</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-%EB%B8%8C%EB%9E%9C%EC%B9%98%EC%99%80-Merge-%EC%9D%98-%EA%B8%B0%EC%B4%88\">브랜치와 Merge의 기초</a></li>\n<li><a href=\"https://git-scm.com/book/ko/v2\">https://git-scm.com/book/ko/v2</a></li>\n<li><a href=\"https://goddaehee.tistory.com/274\">https://goddaehee.tistory.com/274</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Git]commit, push 제외 자주쓰는 git 명령어들","path":"2022/06/13/tools-git-log/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.360Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.360Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Tools","tags":["Git"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"[Git]Private repository import 하기","date":"2022-06-13T14:23:32.360Z","updated":"2022-04-14T22:10:22.000Z","_content":"\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programmingdf\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**보안이나 기타 이유로 private repository에서 관리하는 패키지를 써야할 경우가 있다.**\n\n**나중에도 자주 쓸거 같으니까 개인용 패키지를 pandas나 sklearn처럼 설치하고 사용하는 법을 정리해두자.**\n\n---\n\n## 작업용 repository 만들기\n\n적당한 이름의 private repository를 만들고 패키지를 넣을 디렉터리를 만들어준다. \n\n```bash\nmkdir some_package\n```\n\n## `__init__.py` 만들기\n\n`__init__.py` 파일을 디렉터리에 넣으면 pip에서 해당 디렉토리를 패키지로 인식한다.\n\n```python\n# example 패키지(디렉토리)의 broadcast 파일에서  전부 가져오기\n\nfrom example.broadcast import *\n```\n\n## 패키지에 함수 넣기\n\n패키지 디렉토리에 포함될 함수를 넣어준다.\n\n여기서는 numpy의 broadcast를 구현하는 함수를 넣어주었다.\n\n```python\n# broadcast.py\nimport numpy as np\nm1 = np.array([[1,2],[3,4]])\nm2 = np.array([10,20])\n\nprint(m1 * m2)\n\n```\n\n## `setup.py` 만들기\n\nsetup.py에는 패키지의 메타데이터를 넣어준다.\n\n```python\n\n\"\"\"\nPython package setup info\n\"\"\"\nimport setuptools\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name='레포이름',\n    version='0.0.1',\n    author='jinheonyoon',\n    author_email='yjinheon@gmail.com',\n    description='Package 설치 확인',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/yjinheon/레포이름.git',\n    project_urls = {\n        \"Bug Tracker\": \"https://github.com/yjinheon/레포이름/issues\"\n    },\n    license='jinheonyoon',\n    zip_safe = False,\n    packages = ['설치한 패키지 명'],\n    install_requires = [\n        'numpy==1.20.1',\n        'pandas==1.3'\n        ]\n)\n\n\n```\n\n\n## token 생성하기\n\nGithub 계정의 `Setting`의 `developer settings`에서 token을 생성할 수 있다.\n \n[여기](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token) 참조\n\n\n\n## 설치하기\n\n생성한 토큰을 로컬에 변수로 넣어준다.\n\n```\nexport token=생성한 토큰\n```\n\n$token으로 토큰을 불러와서 일반 package처럼 private repository의 package를 설치할 수 있다.\n\n설치할 때는 `powershell`이나 `git`을 관리자 모드로 열어야한다.\n\n```bash\npip install git+https://{$token}@github.com/yjinheon/toolbox\n```\n\n\n## 사용하기 & 결론\n\nrepository로 설치를 하고 불러올 때는 package명으로 불러와줘야 한다.\n\n```python\nimport seaborn as sns\nfrom eda import glimpse # 최근에 넣어둔 eda용 헬퍼함수\n\ndf = sns.load_dataset('penguins')\n\n\nglimpse(df)\nShape:  (344, 7)\nspecies           object   0 (0%) NAs : Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie\nisland            object   0 (0%) NAs : Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg\nbill_length_mm    float64  2 (1%) NAs : 39.1, 39.5, 40.3, nan, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0\nbill_depth_mm     float64  2 (1%) NAs : 18.7, 17.4, 18.0, nan, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2\nflipper_length_mm float64  2 (1%) NAs : 181.0, 186.0, 195.0, nan, 193.0, 190.0, 181.0, 195.0, 193.0, 190.0\nbody_mass_g       float64  2 (1%) NAs : 3750.0, 3800.0, 3250.0, nan, 3450.0, 3650.0, 3625.0, 4675.0, 3475.0, 4\nsex               object  11 (3%) NAs : Male, Female, Female, nan, Female, Male, Female, Male, nan, nan\n\n```\n\n아직 패키지만 만든 상태라서 편한지 어떤지는 모르겠다. 몇주 써보고 좀 익숙해져야 할 것 같다.\n\n\n## References\n\n- [Personal access token 만들기](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n- [private-python-package](https://docs.readthedocs.io/en/stable/guides/private-python-packages.html)\n- [custom-package](https://towardsdatascience.com/create-your-custom-python-package-that-you-can-pip-install-from-your-git-repository-f90465867893)","source":"_posts/tools-git-private-repo.md","raw":"---\ntitle: '[Git]Private repository import 하기'\ncategories:\n  - [Tools]\ntags:\n  - Git\ndate:\nupdated:\n---\n\n<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programmingdf\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n**보안이나 기타 이유로 private repository에서 관리하는 패키지를 써야할 경우가 있다.**\n\n**나중에도 자주 쓸거 같으니까 개인용 패키지를 pandas나 sklearn처럼 설치하고 사용하는 법을 정리해두자.**\n\n---\n\n## 작업용 repository 만들기\n\n적당한 이름의 private repository를 만들고 패키지를 넣을 디렉터리를 만들어준다. \n\n```bash\nmkdir some_package\n```\n\n## `__init__.py` 만들기\n\n`__init__.py` 파일을 디렉터리에 넣으면 pip에서 해당 디렉토리를 패키지로 인식한다.\n\n```python\n# example 패키지(디렉토리)의 broadcast 파일에서  전부 가져오기\n\nfrom example.broadcast import *\n```\n\n## 패키지에 함수 넣기\n\n패키지 디렉토리에 포함될 함수를 넣어준다.\n\n여기서는 numpy의 broadcast를 구현하는 함수를 넣어주었다.\n\n```python\n# broadcast.py\nimport numpy as np\nm1 = np.array([[1,2],[3,4]])\nm2 = np.array([10,20])\n\nprint(m1 * m2)\n\n```\n\n## `setup.py` 만들기\n\nsetup.py에는 패키지의 메타데이터를 넣어준다.\n\n```python\n\n\"\"\"\nPython package setup info\n\"\"\"\nimport setuptools\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name='레포이름',\n    version='0.0.1',\n    author='jinheonyoon',\n    author_email='yjinheon@gmail.com',\n    description='Package 설치 확인',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/yjinheon/레포이름.git',\n    project_urls = {\n        \"Bug Tracker\": \"https://github.com/yjinheon/레포이름/issues\"\n    },\n    license='jinheonyoon',\n    zip_safe = False,\n    packages = ['설치한 패키지 명'],\n    install_requires = [\n        'numpy==1.20.1',\n        'pandas==1.3'\n        ]\n)\n\n\n```\n\n\n## token 생성하기\n\nGithub 계정의 `Setting`의 `developer settings`에서 token을 생성할 수 있다.\n \n[여기](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token) 참조\n\n\n\n## 설치하기\n\n생성한 토큰을 로컬에 변수로 넣어준다.\n\n```\nexport token=생성한 토큰\n```\n\n$token으로 토큰을 불러와서 일반 package처럼 private repository의 package를 설치할 수 있다.\n\n설치할 때는 `powershell`이나 `git`을 관리자 모드로 열어야한다.\n\n```bash\npip install git+https://{$token}@github.com/yjinheon/toolbox\n```\n\n\n## 사용하기 & 결론\n\nrepository로 설치를 하고 불러올 때는 package명으로 불러와줘야 한다.\n\n```python\nimport seaborn as sns\nfrom eda import glimpse # 최근에 넣어둔 eda용 헬퍼함수\n\ndf = sns.load_dataset('penguins')\n\n\nglimpse(df)\nShape:  (344, 7)\nspecies           object   0 (0%) NAs : Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie\nisland            object   0 (0%) NAs : Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg\nbill_length_mm    float64  2 (1%) NAs : 39.1, 39.5, 40.3, nan, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0\nbill_depth_mm     float64  2 (1%) NAs : 18.7, 17.4, 18.0, nan, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2\nflipper_length_mm float64  2 (1%) NAs : 181.0, 186.0, 195.0, nan, 193.0, 190.0, 181.0, 195.0, 193.0, 190.0\nbody_mass_g       float64  2 (1%) NAs : 3750.0, 3800.0, 3250.0, nan, 3450.0, 3650.0, 3625.0, 4675.0, 3475.0, 4\nsex               object  11 (3%) NAs : Male, Female, Female, nan, Female, Male, Female, Male, nan, nan\n\n```\n\n아직 패키지만 만든 상태라서 편한지 어떤지는 모르겠다. 몇주 써보고 좀 익숙해져야 할 것 같다.\n\n\n## References\n\n- [Personal access token 만들기](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n- [private-python-package](https://docs.readthedocs.io/en/stable/guides/private-python-packages.html)\n- [custom-package](https://towardsdatascience.com/create-your-custom-python-package-that-you-can-pip-install-from-your-git-repository-f90465867893)","slug":"tools-git-private-repo","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"cl4e4qscs0048b36qcdmqaspn","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programmingdf\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>보안이나 기타 이유로 private repository에서 관리하는 패키지를 써야할 경우가 있다.</strong></p>\n<p><strong>나중에도 자주 쓸거 같으니까 개인용 패키지를 pandas나 sklearn처럼 설치하고 사용하는 법을 정리해두자.</strong></p>\n<hr>\n<h2 id=\"작업용-repository-만들기\"><a href=\"#작업용-repository-만들기\" class=\"headerlink\" title=\"작업용 repository 만들기\"></a>작업용 repository 만들기</h2><p>적당한 이름의 private repository를 만들고 패키지를 넣을 디렉터리를 만들어준다. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> some_package</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"init-py-만들기\"><a href=\"#init-py-만들기\" class=\"headerlink\" title=\"__init__.py 만들기\"></a><code>__init__.py</code> 만들기</h2><p><code>__init__.py</code> 파일을 디렉터리에 넣으면 pip에서 해당 디렉토리를 패키지로 인식한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># example 패키지(디렉토리)의 broadcast 파일에서  전부 가져오기</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> example.broadcast <span class=\"keyword\">import</span> *</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"패키지에-함수-넣기\"><a href=\"#패키지에-함수-넣기\" class=\"headerlink\" title=\"패키지에 함수 넣기\"></a>패키지에 함수 넣기</h2><p>패키지 디렉토리에 포함될 함수를 넣어준다.</p>\n<p>여기서는 numpy의 broadcast를 구현하는 함수를 넣어주었다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># broadcast.py</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">m1 = np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">m2 = np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(m1 * m2)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"setup-py-만들기\"><a href=\"#setup-py-만들기\" class=\"headerlink\" title=\"setup.py 만들기\"></a><code>setup.py</code> 만들기</h2><p>setup.py에는 패키지의 메타데이터를 넣어준다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Python package setup info</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> setuptools</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&quot;README.md&quot;</span>, <span class=\"string\">&quot;r&quot;</span>, encoding=<span class=\"string\">&quot;utf-8&quot;</span>) <span class=\"keyword\">as</span> fh:</span><br><span class=\"line\">    long_description = fh.read()</span><br><span class=\"line\"></span><br><span class=\"line\">setuptools.setup(</span><br><span class=\"line\">    name=<span class=\"string\">&#x27;레포이름&#x27;</span>,</span><br><span class=\"line\">    version=<span class=\"string\">&#x27;0.0.1&#x27;</span>,</span><br><span class=\"line\">    author=<span class=\"string\">&#x27;jinheonyoon&#x27;</span>,</span><br><span class=\"line\">    author_email=<span class=\"string\">&#x27;yjinheon@gmail.com&#x27;</span>,</span><br><span class=\"line\">    description=<span class=\"string\">&#x27;Package 설치 확인&#x27;</span>,</span><br><span class=\"line\">    long_description=long_description,</span><br><span class=\"line\">    long_description_content_type=<span class=\"string\">&quot;text/markdown&quot;</span>,</span><br><span class=\"line\">    url=<span class=\"string\">&#x27;https://github.com/yjinheon/레포이름.git&#x27;</span>,</span><br><span class=\"line\">    project_urls = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;Bug Tracker&quot;</span>: <span class=\"string\">&quot;https://github.com/yjinheon/레포이름/issues&quot;</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    license=<span class=\"string\">&#x27;jinheonyoon&#x27;</span>,</span><br><span class=\"line\">    zip_safe = <span class=\"literal\">False</span>,</span><br><span class=\"line\">    packages = [<span class=\"string\">&#x27;설치한 패키지 명&#x27;</span>],</span><br><span class=\"line\">    install_requires = [</span><br><span class=\"line\">        <span class=\"string\">&#x27;numpy==1.20.1&#x27;</span>,</span><br><span class=\"line\">        <span class=\"string\">&#x27;pandas==1.3&#x27;</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"token-생성하기\"><a href=\"#token-생성하기\" class=\"headerlink\" title=\"token 생성하기\"></a>token 생성하기</h2><p>Github 계정의 <code>Setting</code>의 <code>developer settings</code>에서 token을 생성할 수 있다.</p>\n<p><a href=\"https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token\">여기</a> 참조</p>\n<h2 id=\"설치하기\"><a href=\"#설치하기\" class=\"headerlink\" title=\"설치하기\"></a>설치하기</h2><p>생성한 토큰을 로컬에 변수로 넣어준다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export token=생성한 토큰</span><br></pre></td></tr></table></figure>\n\n<p>$token으로 토큰을 불러와서 일반 package처럼 private repository의 package를 설치할 수 있다.</p>\n<p>설치할 때는 <code>powershell</code>이나 <code>git</code>을 관리자 모드로 열어야한다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install git+https://&#123;<span class=\"variable\">$token</span>&#125;@github.com/yjinheon/toolbox</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"사용하기-amp-결론\"><a href=\"#사용하기-amp-결론\" class=\"headerlink\" title=\"사용하기 &amp; 결론\"></a>사용하기 &amp; 결론</h2><p>repository로 설치를 하고 불러올 때는 package명으로 불러와줘야 한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">from</span> eda <span class=\"keyword\">import</span> glimpse <span class=\"comment\"># 최근에 넣어둔 eda용 헬퍼함수</span></span><br><span class=\"line\"></span><br><span class=\"line\">df = sns.load_dataset(<span class=\"string\">&#x27;penguins&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">glimpse(df)</span><br><span class=\"line\">Shape:  (<span class=\"number\">344</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\">species           <span class=\"built_in\">object</span>   <span class=\"number\">0</span> (<span class=\"number\">0</span>%) NAs : Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie</span><br><span class=\"line\">island            <span class=\"built_in\">object</span>   <span class=\"number\">0</span> (<span class=\"number\">0</span>%) NAs : Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg</span><br><span class=\"line\">bill_length_mm    float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">39.1</span>, <span class=\"number\">39.5</span>, <span class=\"number\">40.3</span>, nan, <span class=\"number\">36.7</span>, <span class=\"number\">39.3</span>, <span class=\"number\">38.9</span>, <span class=\"number\">39.2</span>, <span class=\"number\">34.1</span>, <span class=\"number\">42.0</span></span><br><span class=\"line\">bill_depth_mm     float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">18.7</span>, <span class=\"number\">17.4</span>, <span class=\"number\">18.0</span>, nan, <span class=\"number\">19.3</span>, <span class=\"number\">20.6</span>, <span class=\"number\">17.8</span>, <span class=\"number\">19.6</span>, <span class=\"number\">18.1</span>, <span class=\"number\">20.2</span></span><br><span class=\"line\">flipper_length_mm float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">181.0</span>, <span class=\"number\">186.0</span>, <span class=\"number\">195.0</span>, nan, <span class=\"number\">193.0</span>, <span class=\"number\">190.0</span>, <span class=\"number\">181.0</span>, <span class=\"number\">195.0</span>, <span class=\"number\">193.0</span>, <span class=\"number\">190.0</span></span><br><span class=\"line\">body_mass_g       float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">3750.0</span>, <span class=\"number\">3800.0</span>, <span class=\"number\">3250.0</span>, nan, <span class=\"number\">3450.0</span>, <span class=\"number\">3650.0</span>, <span class=\"number\">3625.0</span>, <span class=\"number\">4675.0</span>, <span class=\"number\">3475.0</span>, <span class=\"number\">4</span></span><br><span class=\"line\">sex               <span class=\"built_in\">object</span>  <span class=\"number\">11</span> (<span class=\"number\">3</span>%) NAs : Male, Female, Female, nan, Female, Male, Female, Male, nan, nan</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>아직 패키지만 만든 상태라서 편한지 어떤지는 모르겠다. 몇주 써보고 좀 익숙해져야 할 것 같다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token\">Personal access token 만들기</a></li>\n<li><a href=\"https://docs.readthedocs.io/en/stable/guides/private-python-packages.html\">private-python-package</a></li>\n<li><a href=\"https://towardsdatascience.com/create-your-custom-python-package-that-you-can-pip-install-from-your-git-repository-f90465867893\">custom-package</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n- Statistics , Math\n- Data Engineering\n- Programmingdf\n- EDA & Visualization\n- Data Extraction & Wrangling\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<p><strong>보안이나 기타 이유로 private repository에서 관리하는 패키지를 써야할 경우가 있다.</strong></p>\n<p><strong>나중에도 자주 쓸거 같으니까 개인용 패키지를 pandas나 sklearn처럼 설치하고 사용하는 법을 정리해두자.</strong></p>\n<hr>\n<h2 id=\"작업용-repository-만들기\"><a href=\"#작업용-repository-만들기\" class=\"headerlink\" title=\"작업용 repository 만들기\"></a>작업용 repository 만들기</h2><p>적당한 이름의 private repository를 만들고 패키지를 넣을 디렉터리를 만들어준다. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> some_package</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"init-py-만들기\"><a href=\"#init-py-만들기\" class=\"headerlink\" title=\"__init__.py 만들기\"></a><code>__init__.py</code> 만들기</h2><p><code>__init__.py</code> 파일을 디렉터리에 넣으면 pip에서 해당 디렉토리를 패키지로 인식한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># example 패키지(디렉토리)의 broadcast 파일에서  전부 가져오기</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> example.broadcast <span class=\"keyword\">import</span> *</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"패키지에-함수-넣기\"><a href=\"#패키지에-함수-넣기\" class=\"headerlink\" title=\"패키지에 함수 넣기\"></a>패키지에 함수 넣기</h2><p>패키지 디렉토리에 포함될 함수를 넣어준다.</p>\n<p>여기서는 numpy의 broadcast를 구현하는 함수를 넣어주었다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># broadcast.py</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">m1 = np.array([[<span class=\"number\">1</span>,<span class=\"number\">2</span>],[<span class=\"number\">3</span>,<span class=\"number\">4</span>]])</span><br><span class=\"line\">m2 = np.array([<span class=\"number\">10</span>,<span class=\"number\">20</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(m1 * m2)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"setup-py-만들기\"><a href=\"#setup-py-만들기\" class=\"headerlink\" title=\"setup.py 만들기\"></a><code>setup.py</code> 만들기</h2><p>setup.py에는 패키지의 메타데이터를 넣어준다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Python package setup info</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> setuptools</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&quot;README.md&quot;</span>, <span class=\"string\">&quot;r&quot;</span>, encoding=<span class=\"string\">&quot;utf-8&quot;</span>) <span class=\"keyword\">as</span> fh:</span><br><span class=\"line\">    long_description = fh.read()</span><br><span class=\"line\"></span><br><span class=\"line\">setuptools.setup(</span><br><span class=\"line\">    name=<span class=\"string\">&#x27;레포이름&#x27;</span>,</span><br><span class=\"line\">    version=<span class=\"string\">&#x27;0.0.1&#x27;</span>,</span><br><span class=\"line\">    author=<span class=\"string\">&#x27;jinheonyoon&#x27;</span>,</span><br><span class=\"line\">    author_email=<span class=\"string\">&#x27;yjinheon@gmail.com&#x27;</span>,</span><br><span class=\"line\">    description=<span class=\"string\">&#x27;Package 설치 확인&#x27;</span>,</span><br><span class=\"line\">    long_description=long_description,</span><br><span class=\"line\">    long_description_content_type=<span class=\"string\">&quot;text/markdown&quot;</span>,</span><br><span class=\"line\">    url=<span class=\"string\">&#x27;https://github.com/yjinheon/레포이름.git&#x27;</span>,</span><br><span class=\"line\">    project_urls = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;Bug Tracker&quot;</span>: <span class=\"string\">&quot;https://github.com/yjinheon/레포이름/issues&quot;</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    license=<span class=\"string\">&#x27;jinheonyoon&#x27;</span>,</span><br><span class=\"line\">    zip_safe = <span class=\"literal\">False</span>,</span><br><span class=\"line\">    packages = [<span class=\"string\">&#x27;설치한 패키지 명&#x27;</span>],</span><br><span class=\"line\">    install_requires = [</span><br><span class=\"line\">        <span class=\"string\">&#x27;numpy==1.20.1&#x27;</span>,</span><br><span class=\"line\">        <span class=\"string\">&#x27;pandas==1.3&#x27;</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"token-생성하기\"><a href=\"#token-생성하기\" class=\"headerlink\" title=\"token 생성하기\"></a>token 생성하기</h2><p>Github 계정의 <code>Setting</code>의 <code>developer settings</code>에서 token을 생성할 수 있다.</p>\n<p><a href=\"https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token\">여기</a> 참조</p>\n<h2 id=\"설치하기\"><a href=\"#설치하기\" class=\"headerlink\" title=\"설치하기\"></a>설치하기</h2><p>생성한 토큰을 로컬에 변수로 넣어준다.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export token=생성한 토큰</span><br></pre></td></tr></table></figure>\n\n<p>$token으로 토큰을 불러와서 일반 package처럼 private repository의 package를 설치할 수 있다.</p>\n<p>설치할 때는 <code>powershell</code>이나 <code>git</code>을 관리자 모드로 열어야한다.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install git+https://&#123;<span class=\"variable\">$token</span>&#125;@github.com/yjinheon/toolbox</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"사용하기-amp-결론\"><a href=\"#사용하기-amp-결론\" class=\"headerlink\" title=\"사용하기 &amp; 결론\"></a>사용하기 &amp; 결론</h2><p>repository로 설치를 하고 불러올 때는 package명으로 불러와줘야 한다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\"><span class=\"keyword\">from</span> eda <span class=\"keyword\">import</span> glimpse <span class=\"comment\"># 최근에 넣어둔 eda용 헬퍼함수</span></span><br><span class=\"line\"></span><br><span class=\"line\">df = sns.load_dataset(<span class=\"string\">&#x27;penguins&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">glimpse(df)</span><br><span class=\"line\">Shape:  (<span class=\"number\">344</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\">species           <span class=\"built_in\">object</span>   <span class=\"number\">0</span> (<span class=\"number\">0</span>%) NAs : Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie</span><br><span class=\"line\">island            <span class=\"built_in\">object</span>   <span class=\"number\">0</span> (<span class=\"number\">0</span>%) NAs : Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg</span><br><span class=\"line\">bill_length_mm    float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">39.1</span>, <span class=\"number\">39.5</span>, <span class=\"number\">40.3</span>, nan, <span class=\"number\">36.7</span>, <span class=\"number\">39.3</span>, <span class=\"number\">38.9</span>, <span class=\"number\">39.2</span>, <span class=\"number\">34.1</span>, <span class=\"number\">42.0</span></span><br><span class=\"line\">bill_depth_mm     float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">18.7</span>, <span class=\"number\">17.4</span>, <span class=\"number\">18.0</span>, nan, <span class=\"number\">19.3</span>, <span class=\"number\">20.6</span>, <span class=\"number\">17.8</span>, <span class=\"number\">19.6</span>, <span class=\"number\">18.1</span>, <span class=\"number\">20.2</span></span><br><span class=\"line\">flipper_length_mm float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">181.0</span>, <span class=\"number\">186.0</span>, <span class=\"number\">195.0</span>, nan, <span class=\"number\">193.0</span>, <span class=\"number\">190.0</span>, <span class=\"number\">181.0</span>, <span class=\"number\">195.0</span>, <span class=\"number\">193.0</span>, <span class=\"number\">190.0</span></span><br><span class=\"line\">body_mass_g       float64  <span class=\"number\">2</span> (<span class=\"number\">1</span>%) NAs : <span class=\"number\">3750.0</span>, <span class=\"number\">3800.0</span>, <span class=\"number\">3250.0</span>, nan, <span class=\"number\">3450.0</span>, <span class=\"number\">3650.0</span>, <span class=\"number\">3625.0</span>, <span class=\"number\">4675.0</span>, <span class=\"number\">3475.0</span>, <span class=\"number\">4</span></span><br><span class=\"line\">sex               <span class=\"built_in\">object</span>  <span class=\"number\">11</span> (<span class=\"number\">3</span>%) NAs : Male, Female, Female, nan, Female, Male, Female, Male, nan, nan</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>아직 패키지만 만든 상태라서 편한지 어떤지는 모르겠다. 몇주 써보고 좀 익숙해져야 할 것 같다.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token\">Personal access token 만들기</a></li>\n<li><a href=\"https://docs.readthedocs.io/en/stable/guides/private-python-packages.html\">private-python-package</a></li>\n<li><a href=\"https://towardsdatascience.com/create-your-custom-python-package-that-you-can-pip-install-from-your-git-repository-f90465867893\">custom-package</a></li>\n</ul>\n","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Fri Apr 15 2022 07:10:22 GMT+0900 (Korean Standard Time)","title":"[Git]Private repository import 하기","path":"2022/06/13/tools-git-private-repo/","eyeCatchImage":null,"excerpt":null,"date":{"_isAMomentObject":true,"_i":"2022-06-13T14:23:32.360Z","_isUTC":false,"_pf":{"empty":false,"unusedTokens":[],"unusedInput":[],"overflow":-2,"charsLeftOver":0,"nullInput":false,"invalidEra":null,"invalidMonth":null,"invalidFormat":false,"userInvalidated":false,"iso":false,"parsedDateParts":[],"era":null,"meridiem":null,"rfc2822":false,"weekdayMismatch":false},"_locale":{"_calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"_longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"_invalidDate":"Invalid date","_dayOfMonthOrdinalParse":{},"_relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"_months":["January","February","March","April","May","June","July","August","September","October","November","December"],"_monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"_week":{"dow":0,"doy":6},"_weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"_weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"_weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"_meridiemParse":{},"_eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"_abbr":"en","_config":{"calendar":{"sameDay":"[Today at] LT","nextDay":"[Tomorrow at] LT","nextWeek":"dddd [at] LT","lastDay":"[Yesterday at] LT","lastWeek":"[Last] dddd [at] LT","sameElse":"L"},"longDateFormat":{"LTS":"h:mm:ss A","LT":"h:mm A","L":"MM/DD/YYYY","LL":"MMMM D, YYYY","LLL":"MMMM D, YYYY h:mm A","LLLL":"dddd, MMMM D, YYYY h:mm A"},"invalidDate":"Invalid date","dayOfMonthOrdinalParse":{},"relativeTime":{"future":"in %s","past":"%s ago","s":"a few seconds","ss":"%d seconds","m":"a minute","mm":"%d minutes","h":"an hour","hh":"%d hours","d":"a day","dd":"%d days","w":"a week","ww":"%d weeks","M":"a month","MM":"%d months","y":"a year","yy":"%d years"},"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthsShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"week":{"dow":0,"doy":6},"weekdays":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"weekdaysMin":["Su","Mo","Tu","We","Th","Fr","Sa"],"weekdaysShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"meridiemParse":{},"eras":[{"since":"0001-01-01","until":null,"offset":1,"name":"Anno Domini","narrow":"AD","abbr":"AD"},{"since":"0000-12-31","until":null,"offset":1,"name":"Before Christ","narrow":"BC","abbr":"BC"}],"abbr":"en"},"_dayOfMonthOrdinalParseLenient":{}},"_d":"2022-06-13T14:23:32.360Z","_isValid":true,"_z":null},"pv":0,"totalPV":0,"categories":"Tools","tags":["Git"],"internalLinks":[],"keywords":[],"keywordsLength":0}}],"PostAsset":[{"_id":"source/_posts/DL-backpropagation/backward_propagation.png","post":"cl4e4qsc1000cb36q6zrj7x4w","slug":"backward_propagation.png","modified":1,"renderable":1},{"_id":"source/_posts/DL-backpropagation/bp_model.png","post":"cl4e4qsc1000cb36q6zrj7x4w","slug":"bp_model.png","modified":1,"renderable":1},{"_id":"source/_posts/DL-backpropagation/chainrule.png","post":"cl4e4qsc1000cb36q6zrj7x4w","slug":"chainrule.png","modified":1,"renderable":1},{"_id":"source/_posts/DL-backpropagation/gb_ex.png","post":"cl4e4qsc1000cb36q6zrj7x4w","slug":"gb_ex.png","modified":1,"renderable":1},{"_id":"source/_posts/ML-SP-simple_regression/1_simple-regression_6_0.png","post":"cl4e4qsca001jb36qeles5pz4","slug":"1_simple-regression_6_0.png","modified":1,"renderable":1},{"_id":"source/_posts/ML-SP-simple_regression/1_simple-regression_8_0.png","post":"cl4e4qsca001jb36qeles5pz4","slug":"1_simple-regression_8_0.png","modified":1,"renderable":1},{"_id":"source/_posts/ML-US-PCA/FS_FE.png","post":"cl4e4qscb001mb36q9rr59rsy","slug":"FS_FE.png","modified":1,"renderable":1},{"_id":"source/_posts/ML-US-PCA/PCA_lc.png","post":"cl4e4qscb001mb36q9rr59rsy","slug":"PCA_lc.png","modified":1,"renderable":1},{"_id":"source/_posts/ML-US-PCA/covariance_matrix.png","post":"cl4e4qscb001mb36q9rr59rsy","slug":"covariance_matrix.png","modified":1,"renderable":1},{"_id":"source/_posts/tools-conda-install/conda_venv.png","post":"cl4e4qscq003vb36qandg6abu","slug":"conda_venv.png","modified":1,"renderable":1},{"_id":"source/_posts/tools-git-basic/git-branch.png","post":"cl4e4qscr0041b36q4mhxh8vs","slug":"git-branch.png","modified":1,"renderable":1},{"_id":"source/_posts/tools-git-basic/git-remote.png","post":"cl4e4qscr0041b36q4mhxh8vs","slug":"git-remote.png","modified":1,"renderable":1},{"_id":"source/_posts/tools-git-basic/git-simple.png","post":"cl4e4qscr0041b36q4mhxh8vs","slug":"git-simple.png","modified":1,"renderable":1},{"_id":"source/_posts/tools-git-basic/github_remote.png","post":"cl4e4qscr0041b36q4mhxh8vs","slug":"github_remote.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cl4e4qsby0007b36q7oe446yb","category_id":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc1000db36q2ck54eun"},{"post_id":"cl4e4qsbz0008b36q6db4gv4v","category_id":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc3000jb36qc40za4w3"},{"post_id":"cl4e4qsbv0003b36qbikja07i","category_id":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc4000ob36qajn343j8"},{"post_id":"cl4e4qsbx0005b36qdwzx18db","category_id":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc5000rb36qhtbebcvt"},{"post_id":"cl4e4qsbs0001b36q4lco4f4q","category_id":"cl4e4qsby0006b36qh7rsh89z","_id":"cl4e4qsc70010b36qdyu88c3d"},{"post_id":"cl4e4qsbs0001b36q4lco4f4q","category_id":"cl4e4qsc3000lb36q3fx1a729","_id":"cl4e4qsc70013b36q2pxk4k4t"},{"post_id":"cl4e4qsc5000ub36qcvy7bcez","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qsc80017b36qgzap9msj"},{"post_id":"cl4e4qsc0000bb36qht5d7kpu","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qsc90019b36qarh03xjg"},{"post_id":"cl4e4qsc1000cb36q6zrj7x4w","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qsc9001db36q0fo485yu"},{"post_id":"cl4e4qsc2000gb36q3xefcx97","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qsca001ib36qa3rx0h3t"},{"post_id":"cl4e4qsc3000ib36q3erihpe8","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qscc001qb36qbvby02or"},{"post_id":"cl4e4qsc4000nb36q6n427ulm","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qscd001ub36qff1j33f6"},{"post_id":"cl4e4qsc4000qb36q8tbc0zaq","category_id":"cl4e4qsc5000sb36qa9cqdy9f","_id":"cl4e4qsce0020b36qbi1l3ynr"},{"post_id":"cl4e4qsc6000wb36q8hsebd7r","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscf0027b36qgut03xd2"},{"post_id":"cl4e4qsc6000zb36q7ceaebpt","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscg002cb36qdismfatk"},{"post_id":"cl4e4qsc70012b36qd3is4fiq","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qsch002hb36q23a01jwc"},{"post_id":"cl4e4qsc80016b36qc3lfayrx","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qsci002ob36q2yebhomi"},{"post_id":"cl4e4qsc9001cb36qcefp4dga","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscl002zb36q6c7oea63"},{"post_id":"cl4e4qsca001fb36qecumfic7","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscm0035b36q37sd7il7"},{"post_id":"cl4e4qsca001jb36qeles5pz4","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscm003bb36q1aelfyjf"},{"post_id":"cl4e4qscb001ob36qds3ygnel","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qsco003mb36qctx58sfq"},{"post_id":"cl4e4qscc001rb36qddgu0bw1","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscp003qb36q8lb10u5q"},{"post_id":"cl4e4qscd001vb36qe70q9vuv","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscq003wb36q4zr5finq"},{"post_id":"cl4e4qscd001xb36qddedhew2","category_id":"cl4e4qscp003pb36qarth6e08","_id":"cl4e4qscr0046b36q6ek00052"},{"post_id":"cl4e4qsce001yb36qhnqi5opn","category_id":"cl4e4qscp003pb36qarth6e08","_id":"cl4e4qscs004bb36qeh1ibau7"},{"post_id":"cl4e4qsce0022b36q8k6c2zvr","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qsct004eb36q7lz7fpen"},{"post_id":"cl4e4qscf0024b36q732s13z2","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qsct004jb36qgrhibrx7"},{"post_id":"cl4e4qscf0028b36qfuvl1vuj","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qsct004ob36q8hgb0djm"},{"post_id":"cl4e4qscg002ab36qc1qq3ztt","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qsct004qb36qh6cmh33m"},{"post_id":"cl4e4qscg002db36q53p84ifr","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qscu004ub36qa5kihb39"},{"post_id":"cl4e4qsch002fb36qfmk7ho2x","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qscu004yb36qao8qam8k"},{"post_id":"cl4e4qsch002jb36qbie8avsw","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qscu0052b36qg2p6bqxl"},{"post_id":"cl4e4qsc80018b36qb7312dam","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscv0057b36qdabcbpl1"},{"post_id":"cl4e4qsc80018b36qb7312dam","category_id":"cl4e4qscu004xb36qagsr6j3p","_id":"cl4e4qscv0059b36q43h1b9ky"},{"post_id":"cl4e4qsci002mb36q4u0c2ibj","category_id":"cl4e4qscr0042b36q8j7e1is0","_id":"cl4e4qscv005cb36q0ofrhs7u"},{"post_id":"cl4e4qsci002pb36q4mcu3uiy","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscv005fb36q2osk9hqb"},{"post_id":"cl4e4qscj002rb36q8xy0eb8w","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscw005ib36q9q0sen49"},{"post_id":"cl4e4qsck002ub36q8e2w3b0y","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscw005lb36qcxdg5m4i"},{"post_id":"cl4e4qsck002xb36qhy033yoa","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscw005qb36q55z1af1w"},{"post_id":"cl4e4qscl0030b36qbvya1ny5","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscw005ub36qeytm4bls"},{"post_id":"cl4e4qscl0032b36qg6fp5amg","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscx005xb36q5g3yeyx2"},{"post_id":"cl4e4qscm0036b36q20bhfccx","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscx0061b36qby3t98sj"},{"post_id":"cl4e4qscm0039b36qh9pjdqsa","category_id":"cl4e4qscv0056b36q1dszdal0","_id":"cl4e4qscx0067b36qf9nv7i55"},{"post_id":"cl4e4qscb001mb36q9rr59rsy","category_id":"cl4e4qscd001tb36q1v7h2bpn","_id":"cl4e4qscy006bb36qdyku0ssk"},{"post_id":"cl4e4qscb001mb36q9rr59rsy","category_id":"cl4e4qscx0060b36q9q1i10dp","_id":"cl4e4qscy006cb36q30qg0fzo"},{"post_id":"cl4e4qscn003cb36q26m0cgeq","category_id":"cl4e4qscx0065b36q61bsgran","_id":"cl4e4qscy006fb36q8w4i6xnp"},{"post_id":"cl4e4qscn003db36q5snp610w","category_id":"cl4e4qscx0065b36q61bsgran","_id":"cl4e4qscz006jb36q72df7q6s"},{"post_id":"cl4e4qsco003hb36q4i952wj0","category_id":"cl4e4qscx0065b36q61bsgran","_id":"cl4e4qscz006ob36qdyoz7exr"},{"post_id":"cl4e4qsco003jb36q5xx5210s","category_id":"cl4e4qscy006ib36q1tj64v4y","_id":"cl4e4qscz006sb36qae6ig9ju"},{"post_id":"cl4e4qsco003nb36qadh1ett7","category_id":"cl4e4qscy006ib36q1tj64v4y","_id":"cl4e4qscz006vb36q5xm4739n"},{"post_id":"cl4e4qscp003ob36q2a2fg8xl","category_id":"cl4e4qscy006ib36q1tj64v4y","_id":"cl4e4qsd0006yb36qcing2frq"},{"post_id":"cl4e4qscp003tb36q621059o0","category_id":"cl4e4qscz006ub36qbvz08mci","_id":"cl4e4qsd00073b36qfmlj4l5y"},{"post_id":"cl4e4qscq003vb36qandg6abu","category_id":"cl4e4qsd00070b36qggzv5a1c","_id":"cl4e4qsd10077b36q8snnckvn"},{"post_id":"cl4e4qscr0041b36q4mhxh8vs","category_id":"cl4e4qsd00070b36qggzv5a1c","_id":"cl4e4qsd1007ab36q126t40rz"},{"post_id":"cl4e4qscr0045b36q8f8l3adl","category_id":"cl4e4qsd00070b36qggzv5a1c","_id":"cl4e4qsd1007db36qfl0kffjh"},{"post_id":"cl4e4qscs0048b36qcdmqaspn","category_id":"cl4e4qsd00070b36qggzv5a1c","_id":"cl4e4qsd1007hb36qh9vy1hsl"}],"PostTag":[{"post_id":"cl4e4qsbs0001b36q4lco4f4q","tag_id":"cl4e4qsbw0004b36qhidecvl7","_id":"cl4e4qsc2000hb36qbw5qdtas"},{"post_id":"cl4e4qsbs0001b36q4lco4f4q","tag_id":"cl4e4qsbz0009b36q69iu6v9s","_id":"cl4e4qsc3000kb36q2gmy4jqx"},{"post_id":"cl4e4qsbv0003b36qbikja07i","tag_id":"cl4e4qsc1000fb36qbzqh3mll","_id":"cl4e4qsc4000pb36q0qoi4hpp"},{"post_id":"cl4e4qsbx0005b36qdwzx18db","tag_id":"cl4e4qsc1000fb36qbzqh3mll","_id":"cl4e4qsc6000vb36q8w3y5aw0"},{"post_id":"cl4e4qsby0007b36q7oe446yb","tag_id":"cl4e4qsc1000fb36qbzqh3mll","_id":"cl4e4qsc70011b36qhz5qbbc7"},{"post_id":"cl4e4qsbz0008b36q6db4gv4v","tag_id":"cl4e4qsc1000fb36qbzqh3mll","_id":"cl4e4qsc9001eb36q8a48871n"},{"post_id":"cl4e4qsbz0008b36q6db4gv4v","tag_id":"cl4e4qsc70014b36qemn9awbb","_id":"cl4e4qsca001gb36q6az64vbs"},{"post_id":"cl4e4qsc0000bb36qht5d7kpu","tag_id":"cl4e4qsc9001bb36qcog1bo3i","_id":"cl4e4qscb001lb36q4z69d8mb"},{"post_id":"cl4e4qsc4000nb36q6n427ulm","tag_id":"cl4e4qscb001kb36q5d7b930o","_id":"cl4e4qscd001sb36qe53bb1ui"},{"post_id":"cl4e4qsc6000wb36q8hsebd7r","tag_id":"cl4e4qscc001pb36qanen14ze","_id":"cl4e4qscf0023b36qefat5r3v"},{"post_id":"cl4e4qsc6000wb36q8hsebd7r","tag_id":"cl4e4qscd001wb36qhheahxei","_id":"cl4e4qscf0026b36qdgjkawsq"},{"post_id":"cl4e4qsc6000zb36q7ceaebpt","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qsch002ib36q7sze340c"},{"post_id":"cl4e4qsc6000zb36q7ceaebpt","tag_id":"cl4e4qscg0029b36q2b06co9c","_id":"cl4e4qsci002kb36q0aah54fd"},{"post_id":"cl4e4qsc70012b36qd3is4fiq","tag_id":"cl4e4qscg0029b36q2b06co9c","_id":"cl4e4qscj002tb36qf962b04t"},{"post_id":"cl4e4qsc70012b36qd3is4fiq","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qsck002vb36qejmv4ath"},{"post_id":"cl4e4qsc80016b36qc3lfayrx","tag_id":"cl4e4qscj002qb36q4w3n4w7c","_id":"cl4e4qscm0034b36qbwmkgn9t"},{"post_id":"cl4e4qsc80016b36qc3lfayrx","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qscm0037b36q4mnl3pjg"},{"post_id":"cl4e4qsc80018b36qb7312dam","tag_id":"cl4e4qscl0031b36qgwrdc6ir","_id":"cl4e4qsco003gb36qbh6jbirp"},{"post_id":"cl4e4qsc80018b36qb7312dam","tag_id":"cl4e4qscm0038b36q593yheuf","_id":"cl4e4qsco003ib36qacz4g1tq"},{"post_id":"cl4e4qsco003nb36qadh1ett7","tag_id":"cl4e4qsc1000fb36qbzqh3mll","_id":"cl4e4qscp003sb36qatbs1sgm"},{"post_id":"cl4e4qscp003ob36q2a2fg8xl","tag_id":"cl4e4qsbz0009b36q69iu6v9s","_id":"cl4e4qscq003ub36q3k92509f"},{"post_id":"cl4e4qsc9001cb36qcefp4dga","tag_id":"cl4e4qscn003fb36qfx1cc3xn","_id":"cl4e4qscr0040b36qe6txdus9"},{"post_id":"cl4e4qsc9001cb36qcefp4dga","tag_id":"cl4e4qsco003lb36qb7zaatz2","_id":"cl4e4qscr0043b36q0pm13fqi"},{"post_id":"cl4e4qsc9001cb36qcefp4dga","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qscs0047b36qcmu50l41"},{"post_id":"cl4e4qsca001fb36qecumfic7","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qscs004cb36q8fl1g6t2"},{"post_id":"cl4e4qsca001fb36qecumfic7","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qsct004fb36qc7uvalsn"},{"post_id":"cl4e4qsca001jb36qeles5pz4","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsct004kb36q7ww9hwcj"},{"post_id":"cl4e4qsca001jb36qeles5pz4","tag_id":"cl4e4qsce0021b36qe1llcexc","_id":"cl4e4qsct004mb36qfywkh8u7"},{"post_id":"cl4e4qscb001mb36q9rr59rsy","tag_id":"cl4e4qsct004ib36qcvvb5m4b","_id":"cl4e4qscu004sb36qh2295qs9"},{"post_id":"cl4e4qscb001mb36q9rr59rsy","tag_id":"cl4e4qsct004nb36qbv1w1uzl","_id":"cl4e4qscu004vb36q91792fcx"},{"post_id":"cl4e4qscb001ob36qds3ygnel","tag_id":"cl4e4qsct004rb36qam6c3mdw","_id":"cl4e4qscu0050b36qd9ah8q93"},{"post_id":"cl4e4qscb001ob36qds3ygnel","tag_id":"cl4e4qsct004ib36qcvvb5m4b","_id":"cl4e4qscu0053b36qawj83ru6"},{"post_id":"cl4e4qscc001rb36qddgu0bw1","tag_id":"cl4e4qscu004zb36q6rmg61wx","_id":"cl4e4qscv0055b36q7ole4kke"},{"post_id":"cl4e4qscd001vb36qe70q9vuv","tag_id":"cl4e4qscu004zb36q6rmg61wx","_id":"cl4e4qscv005ab36qaf08e9cu"},{"post_id":"cl4e4qscd001xb36qddedhew2","tag_id":"cl4e4qscv0058b36q9etu0ok5","_id":"cl4e4qscw005mb36q2l0q5klw"},{"post_id":"cl4e4qscd001xb36qddedhew2","tag_id":"cl4e4qscv005db36qde76g1sa","_id":"cl4e4qscw005nb36qa9oj5aai"},{"post_id":"cl4e4qscd001xb36qddedhew2","tag_id":"cl4e4qscv005gb36q9dsq0n5g","_id":"cl4e4qscw005rb36q0i1p719r"},{"post_id":"cl4e4qsce001yb36qhnqi5opn","tag_id":"cl4e4qscw005jb36q0mt9ep61","_id":"cl4e4qscx005yb36q80e79ecs"},{"post_id":"cl4e4qsce001yb36qhnqi5opn","tag_id":"cl4e4qscv0058b36q9etu0ok5","_id":"cl4e4qscx005zb36q5h1v7n3j"},{"post_id":"cl4e4qsce001yb36qhnqi5opn","tag_id":"cl4e4qscw005sb36q978k63l3","_id":"cl4e4qscx0063b36q7ok9a7ku"},{"post_id":"cl4e4qscf0024b36q732s13z2","tag_id":"cl4e4qscx005wb36q1dsigo6r","_id":"cl4e4qscx0064b36qg2cubl8g"},{"post_id":"cl4e4qscf0028b36qfuvl1vuj","tag_id":"cl4e4qscx0062b36q66pk5jrb","_id":"cl4e4qscy0068b36q0t44bzjs"},{"post_id":"cl4e4qscg002ab36qc1qq3ztt","tag_id":"cl4e4qscx0062b36q66pk5jrb","_id":"cl4e4qscy006gb36q1utsdavq"},{"post_id":"cl4e4qscg002ab36qc1qq3ztt","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qscy006hb36q74drgirb"},{"post_id":"cl4e4qscg002db36q53p84ifr","tag_id":"cl4e4qscx0062b36q66pk5jrb","_id":"cl4e4qscz006lb36q59mt5dg9"},{"post_id":"cl4e4qsch002fb36qfmk7ho2x","tag_id":"cl4e4qscx0062b36q66pk5jrb","_id":"cl4e4qscz006pb36q0jsj5nc0"},{"post_id":"cl4e4qsch002jb36qbie8avsw","tag_id":"cl4e4qscx0062b36q66pk5jrb","_id":"cl4e4qscz006tb36q0mvk01td"},{"post_id":"cl4e4qsci002mb36q4u0c2ibj","tag_id":"cl4e4qscz006qb36qe2uc534m","_id":"cl4e4qsd0006xb36q2ljrfj32"},{"post_id":"cl4e4qsci002pb36q4mcu3uiy","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd00071b36q71x25kyp"},{"post_id":"cl4e4qscj002rb36q8xy0eb8w","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd1007bb36q0uat5g6j"},{"post_id":"cl4e4qscj002rb36q8xy0eb8w","tag_id":"cl4e4qsd00072b36q6l5r22fe","_id":"cl4e4qsd1007cb36q9lygg1hn"},{"post_id":"cl4e4qscj002rb36q8xy0eb8w","tag_id":"cl4e4qsd00075b36qgf6yat0h","_id":"cl4e4qsd1007fb36q1mxmhl0g"},{"post_id":"cl4e4qsck002ub36q8e2w3b0y","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd2007jb36q3nwvafoh"},{"post_id":"cl4e4qsck002ub36q8e2w3b0y","tag_id":"cl4e4qsd00072b36q6l5r22fe","_id":"cl4e4qsd2007kb36q94tt0dhe"},{"post_id":"cl4e4qsck002ub36q8e2w3b0y","tag_id":"cl4e4qsd00075b36qgf6yat0h","_id":"cl4e4qsd2007mb36qbipb0w5e"},{"post_id":"cl4e4qsck002xb36qhy033yoa","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd2007nb36qcdm9ddtz"},{"post_id":"cl4e4qscl0030b36qbvya1ny5","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd2007pb36qc95ndodk"},{"post_id":"cl4e4qscl0032b36qg6fp5amg","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd2007rb36qdyphhu13"},{"post_id":"cl4e4qscm0036b36q20bhfccx","tag_id":"cl4e4qsd2007qb36q29bscke0","_id":"cl4e4qsd2007ub36qhwlu7ihi"},{"post_id":"cl4e4qscm0036b36q20bhfccx","tag_id":"cl4e4qscq003yb36qac3s2zae","_id":"cl4e4qsd2007vb36qahqc7dk0"},{"post_id":"cl4e4qscm0039b36qh9pjdqsa","tag_id":"cl4e4qsd2007tb36q160j8dzw","_id":"cl4e4qsd3007xb36q36mc8r79"},{"post_id":"cl4e4qscn003db36q5snp610w","tag_id":"cl4e4qsd2007wb36qfl58a18w","_id":"cl4e4qsd3007zb36qh0pp46st"},{"post_id":"cl4e4qsco003hb36q4i952wj0","tag_id":"cl4e4qsd2007wb36qfl58a18w","_id":"cl4e4qsd30081b36q95ugcpm4"},{"post_id":"cl4e4qsco003jb36q5xx5210s","tag_id":"cl4e4qsd2007tb36q160j8dzw","_id":"cl4e4qsd30083b36qftkv25at"},{"post_id":"cl4e4qscp003tb36q621059o0","tag_id":"cl4e4qsd30082b36qdfv65s1c","_id":"cl4e4qsd30085b36qhlfm8cko"},{"post_id":"cl4e4qscr0045b36q8f8l3adl","tag_id":"cl4e4qsd30084b36q7sypdmys","_id":"cl4e4qsd30087b36qh43w8bkj"},{"post_id":"cl4e4qscs0048b36qcdmqaspn","tag_id":"cl4e4qsd30084b36q7sypdmys","_id":"cl4e4qsd30088b36q3oo8dyka"}],"Tag":[{"name":"commandline","_id":"cl4e4qsbw0004b36qhidecvl7"},{"name":"Linux","_id":"cl4e4qsbz0009b36q69iu6v9s"},{"name":"SQL","_id":"cl4e4qsc1000fb36qbzqh3mll"},{"name":"Subquery","_id":"cl4e4qsc70014b36qemn9awbb"},{"name":"RNN","_id":"cl4e4qsc9001bb36qcog1bo3i"},{"name":"Optimizer","_id":"cl4e4qscb001kb36q5d7b930o"},{"name":"Regression","_id":"cl4e4qscc001pb36qanen14ze"},{"name":"Metrics","_id":"cl4e4qscd001wb36qhheahxei"},{"name":"Supervised Learning","_id":"cl4e4qsce0021b36qe1llcexc"},{"name":"Decision Tree","_id":"cl4e4qscg0029b36q2b06co9c"},{"name":"Random Forest","_id":"cl4e4qscj002qb36q4w3n4w7c"},{"name":"SVM","_id":"cl4e4qscl0031b36qgwrdc6ir"},{"name":"hyperplane","_id":"cl4e4qscm0038b36q593yheuf"},{"name":"Logistic Regression","_id":"cl4e4qscn003fb36qfx1cc3xn"},{"name":"Cross Entropy","_id":"cl4e4qsco003lb36qb7zaatz2"},{"name":"Python","_id":"cl4e4qscq003yb36qac3s2zae"},{"name":"Unsupervised Learning","_id":"cl4e4qsct004ib36qcvvb5m4b"},{"name":"PCA","_id":"cl4e4qsct004nb36qbv1w1uzl"},{"name":"KNN","_id":"cl4e4qsct004rb36qam6c3mdw"},{"name":"XAI","_id":"cl4e4qscu004zb36q6rmg61wx"},{"name":"NLP","_id":"cl4e4qscv0058b36q9etu0ok5"},{"name":"NLU","_id":"cl4e4qscv005db36qde76g1sa"},{"name":"QA","_id":"cl4e4qscv005gb36q9dsq0n5g"},{"name":"Deep Learning","_id":"cl4e4qscw005jb36q0mt9ep61"},{"name":"WordEmbedding","_id":"cl4e4qscw005sb36q978k63l3"},{"name":"numpy","_id":"cl4e4qscx005wb36q1dsigo6r"},{"name":"pandas","_id":"cl4e4qscx0062b36q66pk5jrb"},{"name":"Sampling","_id":"cl4e4qscz006qb36qe2uc534m"},{"name":"Algorithm","_id":"cl4e4qsd00072b36q6l5r22fe"},{"name":"Data Structure","_id":"cl4e4qsd00075b36qgf6yat0h"},{"name":"regex","_id":"cl4e4qsd2007qb36q29bscke0"},{"name":"R","_id":"cl4e4qsd2007tb36q160j8dzw"},{"name":"Probability","_id":"cl4e4qsd2007wb36qfl58a18w"},{"name":"Unsorted","_id":"cl4e4qsd30082b36qdfv65s1c"},{"name":"Git","_id":"cl4e4qsd30084b36q7sypdmys"}]}}