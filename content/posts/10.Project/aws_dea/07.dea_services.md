---
created: 2024-07-04T09:22
updated: 2024-07-20T20:58
tags:
  - project
  - dea
  - status/in_progress
draft: true
---

# Data Engineering

Data engineering is the development, implementation, and maintenance of processes and systems that ingest raw data and produce high quality and consistent data to be used for analysis
, machine learning, and more.
As a data engineer, you must incorporate security, data management, orchestration, data architecture, software engineering
, and operations to manage the lifecycle of data.
The goal is to take raw data and make it easy and reliable to work with and to integrate across datasets and domains.
To accomplish this goal, you use methods, tools, and services such as streaming, extract, transform, and load, or ETL, data warehouses, and data lakes.

# DEA Scope of Services

- Analytics
- Application Intergration
- Cloud Financial Management
- Compute
- Containers
- Database
- Developer Tools
- Frontend Web and Mobile
- Machine Learning
- Management and Governance
- Migration and Transfer
- Networking and Content Delivery
- Security, Identity and Compliance
- Storage

# Analytics

## Amazon Athena

### S3-> Athena

Sales table -> S3 Bucket (S3 prefix path) -> Partitioning and bucketing

Definition: A serverless, interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.
Notable characteristics:

Serverless: No infrastructure to manage
Pay-per-que uery: Only pay for the queries you run
Works directly with data stored in S3
Supports various data formats (CSV, JSON, ORC, Avro, Parquet)
JDBC/ODBC driver support for connecting with BI tools

---

**_Concept_**

- **Amazon Athena** : 서버리스, 인터랙티브 쿼리 지원 분석 서비스. S3에서 직접 데이터를 쿼리할 수 있다. Pay-per-query : DEA ; AWS

- **Athena workgroup** :: DEA ; AWS

- **Parquet** :하둡 에코시스템을 위해 설계된 오픈 소스 파일형식. 열 기반저장. 압축효율성, 복합 중첩 데이터 구조 지원이 특징. : DEA ; AWS
- **Data Buketinge** : Dataset을 Bucket이라는 일종의 범주로 분류하는 것. Bucket간 데이터는 균등하게 분배된다. : DEA ; AWS

---

### Spark and Athena

- You must first create a Spark enabled workgroup in Athena

## Amazon EMR

## AWS Glue

---

**_Concept_**

- **AWS GLue** :: DEA ; AWS
- **AWS Glue Catalog** :: DEA ; AWS
- **AWS Glue Crawler** :: DEA ; AWS
- **AWS Glue Workflow** :: DEA ; AWS

---

### AWS Glue Data Catalog

#### Use Lamada to create Glue Partitions

- use code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition_API call.
- Use Glue Data Catalog as the central metatdata repository

### AWS Glue Crawler

- Uset AWS Glue Crawlers to connect to multiple data stores and to Update Data Catalog with metadata changes.

### AWS Glue Workflow

## AWS Glue DataBrew

- Data Quality Validation이 가능하다.
- PII detection이 가능하다.
- DataBrew gives you the Ability to create a data quality ruleset that automatically performs data quality validations as part of a profiling job.
- You can use DataBrew in combination with a Step Functions state machine to automate data validation in an ingestion pipeline

## AWS Lake Formation

---

**_Concept_**

- **AWS Lake Formation** : 데이터 레이크를 쉽게 설정, 보안설정및 관리하는 서비스. 중앙집중식 권한관리를 통한 세분화된 엑세스 제어 가능. IAM과 통합되어 RBAC 가능 : DEA ; AWS

---

- Security Policies
- Tag-Based access Control
- Integration with AWS analytics and machine learning services

Simplifies data lake creation and management
Centralized permissions management
Automated data discovery and classification
Supports governed tables for ACID transactions

### 리소스 접근제한 걸기

- Enabling fine-grained access control in AWS Lake Formation
- Register S3 path as an AWS Lake Formation location

# Analytics_Stream

## Amazon Kinesis Data Firehose

---

**_Concept_**

- **Kinesis Data Firehose** : 스트림 데이터를 변환하여 S3, Redshift, OpenSearch등 분석 서비스에 전달하는 ETL 서비스 완전관리형,서버리스, 준실시간(near real time). Lambda를 활용한 변환 지원. 데이터 전송에 중점 : DEA ; AWS

---

Producer, Source, Writing
Kinesis Data Streams
Kinesis Agent
Kinesis Data Firehose API
CloudWatch Logs & Events
Destination
S3
Redshift
Elasticsearch
Splunk
HTTP 엔드포인트

## Amazon Kinesis Data Streams

---

**_Concept_**

- **Kinesis Data Streams** : 대규모 레코드 스트림을 실시간으로 수집하고 처리하는 AWS 서비스.보통 로그 데이터, 앱로그, 시장 데이터 피드, 웹 클릭스트림 데이터 .실시간 처리 및 분석에 중점 : DEA ; AWS
- **Data Record(KDS)** : . Kinesis Stream에 저장되는 단위 : DEA ; AWS
- **Shard(KDS)** : Data Stream에서 고유하게 식별되는 레코드 시퀀스-> 스트림은 하나 이상의 샤드로 구성되며 샤드는 고정된 용량 제공 : DEA ; AWS
- **Partition Key(KDS)** : 스트림 내의 Shard 별로 데이터를 그룹화 하기 위한 Key : DEA ; AWS
- **Sequence Number(Data Streams)** : 각 데이터 레코드에는 샤드 내에 파티션-키 마다 고유한 sequence number가 존제 : DEA ; AWS

---

### 컨셉1

## Amazon Managed Service for Apache Flink

---

**_Concept_**

- **Amazon Managed Service for Apache Flink** : Apache Flink를 사용해 실시간 스트리밍 어플리케이션을 빌드하고 실행할 수 있는 완전관리형 서버리스 서비스
  . 과거 Kinesis Data Analytics란 이름이었다. 데아터 변환, 대화형 쿼리, 실시간 분석 및 다른 AWS와의 통합이 특징 : DEA ; AWS

---

## Amazon Managed Streaming for Apache Kafka(Amazon MSK)

---

**_Concept_**

- **Amazon Managed Service for Apache Kafka** : Apache Kafka 인프라와 운영을 관리하는 AWS 스트리밍 데이터 서비스. Kafka와 같이 데이터 Publisher와 Consumer를 분리하는 역할을 한다. : DEA ; AWS
- **Broker Node** : 카프카 클러스터 구성요소중 하나로 메시지를 저장하고 관리하는 역할을 한다. Producer로부터 메시지를 수신하고 Consumer에게 전달함. 토픽과 파티션을 관리한다. : DEA ; AWS
- **Zookeepr Node** : 분산시스템 조정을 위한 노드. 브로커의 상태관리, 클러스터 설정 저장 : DEA ; AWS
- **Producer(Kafka)** : 메시지를 생성하고 브로커에 전송하는 어플리케이션. 필요시 메시지 직렬화 수행 : DEA ; AWS
- **Consumer(Kafka)** : 브로커로부터 메시지를 읽는 어플리케이션 : DEA ; AWS
- **Topic** : Producer가 게시한 메시지가 저장되는 공간. Kafka에서 메시지를 구분하는 논리적인 단위. Kafka는 기본적으로 하나의 Topic을 여러 브로커로 파티셔닝하는 전략을 취한다. : DEA ; AWS

---

- 기본적으로 kafka와 동일하기 때문에 파티션 수의 증가는 가능하지만 감소는 불가능하다.
- 파티션 수를 증가시키면 데이터의 분산이 증가하고 처리량이 증가한다.
- Kinesis Data Streams의 경우 Shard의 수를 증가시키고 감소시킬 수 있다.

## Amazon OpenSearch Service

## Amazon QuickSight

# Application Integration

## Amazon AppFlow

---

**_Concept_**

- **Amazon Appflow** : 주로 SaaS 어플리케이션과 AWS 서비스간 안전한 데이터 전송을 지원하는 완전관리형 통합 서비스. 주로 여러 서드파티 어플리케이션과 S3, Redshift등 AWS 서비스와의 연결을 지원 : DEA ; AWS

---

## Amazon EventBridge

## Amazon Managed Workflows for Apache Airflow

---

**_Concept_**

- **Apache Airflow** : 데이터 오케스트레이션 툴. 자동화된 스케줄링 구축에 용이 : DEA ; AWS

---

## Amazon SNS

## Amazon SQS

## AWS Step Functions

---

**_Concept_**

- **AWS Step Functions** :서버리스 워크플로우 오케스트레이션 툴. Visual Workflow Designer. Error Handling and Retry Mechanism: DEA ; AWS

---

Step Functions uses a state machine, which is a workflow defined using the Amazon States Language (ASL), a JSON-based structured language. This state machine defines a series of steps, with the output of one step acting as input to the next step.
The workflow in Step Functions is represented as a series of states. Each state represents a unit of work that can perform some action, make a choice, or pass control to another state. The state machine handles error checking, retry logic, and can parallelize tasks, making it easier to build and manage complex workflows.

-

### Step Functions and IAM

- Make Sure that Step Functions state machine code has all IAM permissions that are necessary to create and run the EMR jobs

### Step Functions and EMR

- Query the flow logs for the VPC. Determine whether the traffic that originates from EMR cluster is reach the data providers.

# Cloud Financial Management

## AWS Budgets

## AWS Cost and Usage Report

## AWS Cost Explorer

# Compute

## AWS Batch

## Amazon EC2

### Application Auto Scaling

- to schedule the scaling of EC2 instances

## AWS Lambda

### Lambda@Edge

## AWS Serverkess Application MOdel(SAM)

# Containers

## Amazon Elastic Container Registry(ECR)

## Amazon Elastic Container Service(ECS)

## Amazon Elastic Kubernetes Service(EKS)

# Database

## Amazon DocumentDB

## Amazone DynamoDB

## Amazon Keyspaces(for Apache Cassandra)

## Amazon MemoryDB for Redis

## Amazone Neptune

## Amazon RDS

## Amazon Redshift

---

**_Concept_**

- **Redshift Spectrum** : : DEA ; AWS
- **distribution key** : : DEA ; AWS

---

# Developer Tools

## AWS CLI

## AWS Cloud9

## AWS CDK

## AWS CodeBuild

## AWS CodeCommit

## AWS CodeDeploy

## AWS CodePipeline

# Front-End Web & Mobile

## Amazon API Gateway

# Machine Learning

## Amazon SageMaker

## Amazon Comprenhend

- NLP solution
- to detect sentiment, entities, and key phrases in text.

---

**_Concept_**

- **Amazon SageMaker** : : DEA ; AWS
- **SageMaker Data Wrangler** : Data Sampling Using AWS SageMaker : DEA ; AWS

---

# Management and Governace

## AWS CloudFormation

## AWS CloudTrail

CloudTrail can send logs to CloudTrail Lake without the need to develop a custom solution. CloudTrail Lake automatically converts the JSON event type to Apache ORC format, and stores the data in an event data store. CloudTrail Lake gives you the ability to run SQL queries across multiple event data stores automatically. You can use this solution to analyze the event data automatically.

- S3로 로그를 저장하고, Athena를 통해 쿼리할 수 있다.

### CloudTrail Lake

- Automatic Data intergration from CloudTrail

- AWS Config configuration items
- AWS Audit Manager evidence

## Amazon CloudWatch

- 어플리케이션 모니터링을 위한 커스텀 지표 생성가능

### Amazon CloudWatch Logs

### CLoudWatch Logs insights

CloudWatch Log Insights automatically discovers fields in logs from AWS services, such as Amazon Route 53, Lambda, CloudTrail, and Amazon VPC, and any application or custom log that emits log events as JSON.

- Amazon Route 53

### Amazon Athena CloudWatch connector

- LogGroups as schemas
- LogStreams as tables
- all_log_streams view (all LogStreams in LogGroup)

## AWS Config

When you turn on AWS Config, it first discovers the supported AWS resources that exist in your account and generates a configuration item for each resource. AWS Config keeps track of all changes to your resources by invoking the describe or the list API call for each resource in your account. The service uses those same API calls to capture configuration details for all related resources.

## Amazon Managed Grafana

## AWS Systems Manager

## AWS Well-Architected Tool

# Migration and Transfer

## AWS Application Discovery Service

## AWS Application Migration Service

## AWS DMS

## AWS DataSync

## AWS Schema Conversion Tool (SCT)

## AWS Snow Family

## AWS Transfer Family

# Networking and Content Delivery:

## Amazon CloudFront

## AWS PrivateLink

## Amazon Route 53

## Amazon VPC

# Security, Identity, and Compliance:

## Projecting Data

- Transport Layer Security(TLS) : DEA ; AWS

## AWS Identity and Access Management (IAM)

Authentication

Authorization

IAM Access Analyzer

### IAM Identity Center

- Workforce identities
- Application assignments
- Identity center
- Multi-account permission

### Identities in AWS

- IAM user
- IAM group
- IAM role

### AWS Certiticate Manager

### IAM policies

- Trust Policy
- Identity-based policy
- Resource-based policies

AWS managed policy : Creatd and managed by AWS
Customer managed policy : Created and managed by You
Inline policy : Created for a single IAM Identity

### Access Models

---

**_Concept_**

- **RBAC** : Access based on user Role. Business logic : DEA ; AWS
- **ABAC** : Attribute Based Access Control. Access based on attributes or tags : DEA ; AWS

---

## AWS Key Management Service (AWS KMS)

AWS KMS is a managed service to create and control the cryptographic keys that are used to protect your data.

AWS KMS integrates with most other AWS services that encrypt your data. For example, AWS KMS integrates with CloudTrail to log use of your AWS KMS keys for auditing, regulatory, and compliance, and you can use the AWS KMI API to create and manage AWS KMS keys and features such as custom key stores and use AWS KMS keys in cryptographic operations. Here's a question. What AWS service can you use if you have a requirement to directly manage the AWS CloudHSM device that generates, stores, and uses encryption keys?

### Encrption fundamentals

- Client-Side Encryption : Encrypt data before sending it to AWS : DEA ; AWS
- Server-Side Encryption : Encrypt data after sending it to AWS : DEA ; AWS
- Encryption : In-Transit and At-Rest : DEA ; AWS
- Tokenization : Plain Text into a string of Characters. Replace sensitive data with non-sensitive data : DEA ; AWS
- Token Vault : Store and manage tokens : DEA ; AWS
- Salt : Random data that is used as an additional input to a one-way function that hashes data

### Lambda -> DynamoDB -> KMS

- DynamoDB Client-Side Encryption : DEA ; AWS

One design would be a serverless application that uses API Gateway, Lambda, Amazon Cognito, DynamoDB, and AWS KMS. For this design, the client authenticates with Amazon Cognito and receives an authorization token. The token is used to validate calls to the customer order Lambda function. The function calls the tokenization layer providing sensitive information in the request. This layer includes the logic to generate unique random tokens and store encrypted text in a cipher database. Lambda calls AWS KMS to obtain an encryption key. It then uses the DynamoDB client-side encryption library to encrypt the original text and store the ciphertext in the cipher database. The Lambda function retrieves the generated token in the response from the tokenization layer. This token is then stored in the application database for future reference. AWS KMS manages the creation and management of cryptographic keys. It provides logs of all key usage to help you meet regulatory and compliance needs.

### Project Sensitive Data in DW

---

**_Concept_**

- **Dynamic Data Masking** :동적 데이터 마스킹. DBMS에서 민감한 데이터에 대한 실시간 엑세스를 제어하는 보조기능. 실시간 처리 및 원본데이터 보존. 사용자 인가수준별 차등적용: DEA ; AWS

---

Here's another question. How do you protect sensitive data in your data warehouse? You can use Dynamic Data Masking, or DDM, in Amazon Redshift and manipulate how Amazon Redshift shows sensitive data to the user at query time without transforming it in the database. When attached to a table, the masking expression is applied to one or more of its columns. You can further modify masking policies to only apply them to certain users or users defined roles that you create with RBAC. You can also apply DDM on the cell level by using conditional columns when creating your masking policy, and you can apply multiple masking policies with varying levels of obfuscation to the same column in a table and assign them to different roles.

## AWS HSM

CloudHSM provides hardware security models in AWS. AWS automates the hardware provisioning, software patching, network routing, and creating encrypted backups of key stores. You are responsible for scaling your CloudHSM environment and managing the crypto accounts and credentials within the HSM. Like AWS KMS, CloudHSM is designed so that plain text keys cannot be used outside the HSM by anyone.

## Amazon Macie

## AWS Secrets Manager

A secrets lifecycle has four phases, create, store, use, and destroy. And a secrets management solution protects the secrets in each of these phases from unauthorized access. AWS Secrets Manager is a secrets management service to help you protect access to your application services and resources.

Secrets Manager offers built-in integration with IAM and you can attach access control policies to IAM principals or to secrets themselves by using resource-based policies. Secrets Manager also integrates with AWS Key Management Service, or AWS KMS. Secrets are encrypted at rest by using an AWS managed key or customer managed key. And Secrets Manager supports a rotation of secrets securely, and you can schedule automatic database credential rotation for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. You can also use customized Lambda functions to extend the Secrets Manager rotation feature to other secrets types such as API keys and OAuth tokens.

## Parameter Store

## AWS Shield

## AWS WAF

# Storage

## AWS Backup

### AWS Backup Audit Manager

AWS Backup Audit Manager lets you audit and report on the compliance of your data protection policies to help meet business and regulatory compliance requirements. To monitor the backup activity of the S3 resources you identified earlier, you can create a custom framework in AWS Backup Audit Manager with a targeted set of controls configured.

## Amazon EBS

- Root Volume 에서만 DeleteOnTermination이 True로 설정되어있다.

When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume. It is set to False for all other volume types.

## Amazon EFS

## Amazon S3

### S3 Select

S3 는 간단한 쿼리를 지원한다. S3 Select를 사용하면 S3에 저장된 객체에서 쿼리를 실행할 수 있다. 이를 통해 객체의 일부만 읽을 수 있으며, 객체의 크기가 클 경우 데이터 전송 비용을 줄일 수 있다.

---

**_Concept_**

- **Amazon S3** : : DEA ; AWS
- **Amazon S3 Select** : : DEA ; AWS

---

## Amazon Glacier

# Unsorted

### Stream Processing Style

Simple event processing

각각의 이벤트가 직접적으로 수행해야할 action과 매핑되어 처리 된다.실시간으로 작업의 흐름을 처리할 때 사용되며, 이벤트 처리 시간과 비용의 손실이 적다.
Event Stream Processing

이벤트를 중요도에 따라 필터링하여 걸러진 이벤트만을 수신자에게 전송.실시간으로 정보의 흐름을 처리할 때 사용되며, 기업에 적용될 경우 신속한 의사 결정을 가능하게 한다.(BAM)
Complex event processing

일상적인 이벤트의 패턴을 감지하여 더 복잡한 이벤트의 발생을 추론하는 것. 예를 들어 ‘주식의 등락’이라는 일상적인 이벤트의 패턴을 감지하여 ‘투자 적기’ 라는 상위의 이벤트를 추론해 낼 수 있다.
