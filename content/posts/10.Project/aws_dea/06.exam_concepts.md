---
created: 2024-01-04T09:22
updated: 2024-07-20T20:58
tags:
  - project
  - dea
  - status/in_progress
draft: true
---

# SAA의 기본적인 4가지 도메인

- 01 Design Resilient Architectures
- 02 Define Performant Architectures
- 03 Specify Secure Applications and Architectures
- 04 Design Cost-Optimized Architectures

# DEA의 기본적인 4가지 도메인

- 01 데이터 수집 및 변환
- 02 데이터 스토어 관리
- 03 데이터 운영 및 지원
- 04 데이터 보안 및 거버넌스

# Exam concepts

---

**_Concept_**

- **Latency** : 네트워크 지연시간. 데이터 패킷이 한 지점에서 다른 지점으로 이동하는데 걸리는 시간. : AWS ; WEB
- **Throughput** : 단위 시간당 처리량. 웹 어플리케이션 성능지표로 쓰일 경우 보통 단위시간 당 처리하는 요청의 수. 데이터 전송률 : AWS ; WEB
- **IOPS** : Input Output Operation Per Second. 초당 처리하는 IO의 개수로 보통 스토리지의 속도를 의미. 스토리지의 Throughput은 IOPS로서 스토리지 성능을 대표한다. : AWS ; WEB
- **Bandwidth** : 네트워크에서 특정 시간 내에 전송될 수 있는 데이터의 최대 용량을 의미 : AWS ; WEB

---

### AWS Glue

### Amazon Redshift

### Amazon Athena

---

**_Concept_**

- **Athena Query Editor** : S3와 같은 외부데이터 쿼리 : AWS ; DEA

---

### Amazon S3

# Data Engineering

Data engineering is the development, implementation, and maintenance of processes and systems that ingest raw data and produce high quality and consistent data to be used for analysis
, machine learning, and more.
As a data engineer, you must incorporate security, data management, orchestration, data architecture, software engineering
, and operations to manage the lifecycle of data.
The goal is to take raw data and make it easy and reliable to work with and to integrate across datasets and domains.
To accomplish this goal, you use methods, tools, and services such as streaming, extract, transform, and load, or ETL, data warehouses, and data lakes.

# Domain 1 Data Ingestion and Transformation

- Throughput
- latency
- cost efficiency
- high availability

## 1.1 Performing Data ingestion

### Data engineering life cycle

Generation -> Storage -> Ingestion -> Transformation -> Serving

#### Generation

### Ingestion

Ingestion: data ingestion is moving data from one place to another.

- Gather and Ingest data
- Batch and stream
- Push and Pull

#### Data Ingestion pipelines

- Reprocess
  : S3, Amazon Kinesis, Amazon EventBridge
- Idempotent
- Checkpoints
- Versioning
- Compliance and governance
- Logging and Monitoring
- Tools

#### Transactional Data

- Dynamo DB
- Amazon RDS
- AWS DataBase Migration Service (AWS DMS)

#### Streaming Data

- Kinesis

  - Amazon Kinesis Data Firehose
  - Amazon Kinesis Data Streams
  - Amazon Kinesis Videos Streams

- Amazon Managed Service for Apache Flink
- Amazon MSK
- Amazon AppFlow

#### Scenario question

- Kinesis Data Streams
  - Each stream = 1 or more shards
  - Hot Shards
- Pipelines expeted data flow
  - Capacity
  - Partition key strategy

**solution**

- UpdateShardCount
- Random partition keys
- Distribute hash key evenly across shards

#### Data Ingestion using protocols

**AWS Transfer Family**

- File Transfer Protocol(FTP)
- Secure File Transfer Protocol(SFTP)

**AWS DataSync**

- Network File System(NFS)
- Server Message Block(SMB)

**Large amounts of data**

- AWS Snow Family

#### Batch data ingestion

Batch는 기본적으로 연,분,초 등 특정 단위에 bounded 된 데이터이다.

#### Stateful vs Stateless

Stateful : Stateful data transaction store information about the current state

- Amazon ElasticCache
- Amazon RDS

Stateless data transactions do not store data or sessions and do not rely on the past state.

- Lambda
- Amazon API Gateway
- Amazon S3

## 1.2 Transform and process data

Transformation means to change the data from its original form into some other form that can be useful for downstream use cases.

### Query

### Data Modeling

#### OLTP(Online transaction processing)

- Latest State of Data
- 1~ 3 정규화
- Optimize for point queries
- Query latency matters
- Common Table Expressions can cause latency

#### OLAP(Online Analytical Process)

- Latest State and historical data
- 정규화를 할 경우 느려질 수있다.
- Optimized for Group by
- use CTE instead of subqueries

### AWS Service for transformations

#### Computing

- Lamda
- Amazon EMR
- AWS Glue : Fully managed ETL service. Data Cleansing, enrichment, and movement
- Amazon Redshift

#### Distributed Computing

- Amazon EMR : Full-featured, distributed Hadoop environment.
- AWS Batch
- AWS Step Functions

### 데이터 수집 및 처리 시나리오

#### Kinesis

### Troubleshooting

트러블 슈팅 절차

- check logs
- Verify data. Identify the following :
  - Bottlenecks
  - High-processing times , memory usage or higher I/O operations
  - Algorithms, partition strategy, or parallel processing is needes
  - Resource allocation
  - Caching is needed
- Implement incremental processing
- Add retries
- Test

**AWS Glue**

- built-in debugging capabilities for programming language and framworks

### Secure connections

#### JDBC

- Prepare the JDBC driver
- Configure the security groups
- Use a URL, user name and password

#### ODBC

- Select an ODBC driver
- Install an ODBC driver
- Set up an ODBC data source name (DSN)
- Specify a DSN

### Creating data APIs

#### 01 Data Storage

데이터 저장소는 어디인가?

- Amazon RDS
- Dynamo DB
- Amazon S3

#### 02 Data processing and transformation?

- AWS Glue
- Amazon EMR
- Lambda

#### 03 Security

- AWS IAM
- AWS Certificate Manager (ACM)
- Lambda

#### 04 Caching and optimization

- ElasticCache
- API Gateway
- Amazon Cloudwatch

#### 04 High availability and scailability

- Lambda
- API Gateway

#### 05 Test and Vaidate

- AWS Code Pilpeline
- AWS Code Build

## 1.3 Orchestrate data pipelines

### Data architecture

Data architecture is basically the design of your systems to evolving data needs for your organizaition.

시험 관련 Data architecture 예시

- Data warehouse
- data lakes
- data stacks
- lambda
- data mesh

#### Operational architecture

- Functional requirements

#### Technical architectur

- How the data is ingested, stored, transformed and served

#### Data Pipeline

a Data pipeline is a collection of data processing tasks need to be run in specific order

- Step Function

**run type of pipeline**

- Schedule based
- Event based (Event driven pipeline)

#### Handle Pipeline failures

- Data quality
- Code errors
- Endpoint errors : 보통 커넥션 에러
- Dependency errors

### Data pipeline orchestraion

- MWAA(AWS Managed Workflows for Apache Airflow)
- Step Functions
- AWS Data Pipeline
- AWS Glue

#### Use Cases

- 복잡한 환경구성
- Step function
- Multiple AWS Glue job

- 상대적으로 단순한 환경구성
- Data Pipeline
- AWS Glue workflows
- MWAA

#### BackOffRate in Stepfunction

- Retry with an interval of 10 seconds and a backoff rate of 1.5
- The second retry will wait 15 seconds
- The third retry will wait 22,5 seconds

#### Failure notifications

- Amazon SNS(Simple Notification Service)

Understand how to create an Amazon SNS topic and subscribe one ore more email addresses to the topic

### AWS Data Pipeline

- Extracts , transforms, and loads data between sources

### AWS Step Functions

- Step Functions is a serverless orchestration service that provides a visual design tool.
- It is like a design document.

### AWS Glue Workflows

- Glue 관련 작업만 있을 경우 적합

#### UseCase None AWS Glue services

### Data Pipelins Best practices

- Distributed processing
- Auto Scaling
- Data partitioning
- Fault-tolerant Storage
- Backups
- Monitoring
- Validation and quality checks
- Automated testing
- CI/CD

## 1.4 applying programming concepts

### SQL

standard language

### Spark

Complex data-processing requirements

#### Scenario question

Amazon

SQL or Spark?

### Amzon Athena

- Query tables in Data Catalog
- Athena does not support special characters like / or .

### How do you optimize code and reduce runtimes for data ingestion and transformations?

- Compression
- Memory optimization
- Resource allocation
- Optimization
- Monitoring
- IaC(Infra as Code)
- Parallel processing
- Batch processing
- Data formats
- Filters
- Partitioning

### IaC

- Cloud Formation
- AWS CDK
- CI/CDK
- AWS Serverless Application Model (AWS SAM)
- Code Commit

### Data Structure

- Relational databases
- NoSQL databases
- Data warehouses
- Distributed file systems
- Message queues
- Tree
- Hash table

### Data Algorithms

- Amazon EMR : Store data and retrieve fast
- HDFS cluster : MapReduce algorithm

중요!!

### programming Concepts in AWS

- Ingestion : Lambda
- Processing : Amazon EMR, AWS Glue and Kinesis
- Transformations : AWS Glue
- Storage : Amazona S3, Amazon RDS, Amazon Redshift
- Orchestration : Step Functions

### Amazon AppFlow

# Domain 2. Data Store Management

## 2.1 Choose a data store

### AWS Storage services

### Amazon S3

S3 Transfer Acceleration can ensure fast, secure, and private file transfers across long geographic distances. S3 Transfer Acceleration uses the globally distributed edge locations of CloudFront. Additionally, S3 Transfer Acceleration uses AWS backbone networks. You can configure S3 Transfer Acceleration in Amazon S3

### Amazon EFS

### Amazon EBS

### Amazon RDS

### Amazon DynamoDB

### Amazon Redshift

- Copy command

### Amazon FSx

- Amazon FSx for Lustre
- Parallel file system
- High-performance computing, data analytics, ML workloads

### Elastic Cache

### Data Access Control

- IAM
- Security Groups
- AWS Key Management Service (KMS)
- Parameter groups : configure database parameter
- DataBase locking mechanisms
- Auditing
- Monitoring

## 2.2 Understand data catalog systems

A data catalog is a metadata repository that stores information about the location, description, or schema and runtime metrics of the data. Users can use this central information to facilitate writing federated queries,

- Data Lake나 DW를 구축하기 전에 데이터 카탈로그 구축이 선행되어야 한다,

### AWS Lake Formation

### AWS Glue crawler

to run after new data is ingested and to have that new data automatically added to the Data Catalog.

1. Create database in AWS Glue
2. Create a crawler
3. Define a partitioning schema.
4. run the crawler
5. schedule the crawler
6. query the data using Athena, or other db engine

### Metadata and data catalogs

#### Metadata components

- Data schema
- Data type
- Data sources
- Timestamps
- Data quality metrics
- Ownership
- Access controls

#### Data catalog components

- Tables and databases
- Table schemas
- Partitioning
- location
- Statistics and metadata
- Data Lineage
- Tagging
- Access Controls
- Integrations

### Data classification

- IAM : Users, Groups , Roles and policies
- AWS KMS : encryption
- Amazon VPC
  - Security Groups
  - NACL(Network Access Control List)
- Service control policies (SCP) :
  - used to manage permissions in AWS Organizations
- Amazon Macie
  - Security Service that uses machine learning to discover, classify and protect sensitive data in aws

## 2.3 Manage the lifecycle of data

### Query access patterns

- How important is this data?
- How long should I keep this data-processing
- how often will the data be accessed?

#### HOT

- Quick access
- Higher Cost

#### Warm

- Value lower then hot but not greater than cold

#### Cold

- Lower Costs

### Data lifecycle policies

- S3 lifecycle policies
- Resize

### Data protection

- How do you ensure that data is protected with the needed resiliency and availability?

- Data replication
- Backup and restore
- Cross-region replication : RDS
- Versioning : S3
- TTL : DynamoDB

## 2.4 Design data models and schema evolution

### Data models

- Amazon Redshift
- star schemas
- snowflake schemas

### Data process frameworks

#### Spark

- EMR
- Glue
- S3
- Athena
- Amazon Comprehend

### Data Lineage

- Amazon Sagemaker ML Lineage Tracking

### Data optimization

- Indexing :
- Partitioning : distribution keys, sortkey
- Compression
- Serialization
- Caching
- Serialization : parquet, ORC

- Caching
- Lifecycle Management
- Scaling
- Queries : Amazon RDS Performance Insite,
- Data Transfer : DataSync

### Data Structures

#### Structured

- RDS
- Aurora
- Redshift

#### Semi-structured

- DynamoDB
- DocumentDB
- Athena
- S3

#### Unstructured

- S3
- Recognition
- Transcribe
- Comprehend

### Schema evolution

Schema evolution is the process of making changes to the schema or structure of the data stored in databases or data stores while preserving existing data and ensuring backward compatibility

- Dynamo DB
- RDS and Aurora
- S3 data lake : Schema on read approach
- AWS Glue
- Formats
- Transformations

#### Issues related to schema evolution

- Dead-letter queue
- Schema registry
- Validation
- Versioning
- Backups
- Monitoring and alerting

### Schema conversions and Migration

- AWS Schema Conversion Tool (SCT)
- AWS DMS

# Domain 3 Data Operations and Support

## 3.1 Automate data processing by using AWS services

### CI/CD

#### AWS CodePipeline

### Data pipeline orchestration

### Amazon MWAA(AWS Managed Workflows for Apache Airflow)

- Flexible and code based approach to define complex data workflows

#### Troubleshooting

- Check logs
  - CloudWatch logs
  - Airflow logs
- Verify Permissions
  - IAM
- Check dependencies
- Monitor and adjust
- Test Locally
- AWS Support

### Step Functions

- Serverless and event-workflows that intergrate with AWS services

### AWS Service to manage APIs

- API Gateway
- Lambda
- AWS SDK(software development kit)

### Lambda

### AWS Service to maintain API Intergrations

- CloudWatch
- AWS Auto Scaling and Amazon EC2 Auto Scaling
- API Gateway
- ElasticCache
- IAM
- AWS Backup
- Amazon CloudFront
- DynamoDB

### AWS Services to process data

- Amazon EMR
- Amazon Redshift
- AWS Glue
- Lambda
- Athena

### Managing events ans schedulers

### EventBridge

- management of event in scheduler

- Implement event-driven architectures and automate process
- Build workflows and manage events and schedule

Another part of this task statement is managing events and schedulers. At the beginning of this lesson, we mentioned EventBridge. Using EventBridge, you can implement event-driven architectures and automate processes in your AWS environment. Managing events and schedules is important for automating tasks, workflows, and processes in AWS. You can create event rules that match incoming events based on a predefined patterns. These patterns can be based on event attributes, custom event buses, or other conditions. When an event matches a rule, EventBridge routes the event to one or more targets for further processing. EventBridge integrates with CloudWatch to monitor rule and target activity through CloudWatch metrics and set up alarms based on event patterns.

You can also use AWS CloudTrail to log API calls for EventBridge. Also, the EventBridge schema registry is an optional feature that helps you to define and manage event schemas. So long story short, EventBridge can help you to monitor events, alert, remediate, and respond. Let's walk through an example of orchestrating a data pipeline to demonstrate everything that we've learned so far in this course.

### CloudTrail

## 3.2 Analyze data by using AWS services

### Data manipulation operations

#### Data aggregation

- Amazon Redshift : SQL queries with aggregate function
- Athena : SQL queries with group by
- OpenSearch : summarize by histograms, terms aggregations

#### Rolling average

- Redshift
- Amazon QuickSight

#### Grouping

- Redshift
- Athena
- QuickSight

#### Pivoting

- Redshift
- QuickSight

### Analyze and View data

- QuickSight
- AWS Glue Databrew
- Athena
- Amazon Redshift
- Amazon EMR
- AWS Glue
- Data Pipeline
- Lake Formation

### SQL Queries

- Amazon RDs
- Aurora
- Redshift
- DynamoDB
- DocumentDB
- AWS Glue
- Athena : spark

### Data analytics and exploration

- Athena
- EMR

### Verify and Clean data

- Lambda
- Athena
- QuickSight
- Jupiter Notebook
- Amazon SageMaker Data Wrangler

## 3.3 Maintain and monitor data pipelines

### monitoring and logging

#### Metrics

#### Monitoring and Observability

- CloudWatch
- AWS X-Ray
- Amazon GuardDuty
- Amazon Inspector
- AWS Security Hub

#### Monitor and Troubleshoot

- CloudWatch
- CloudWatch Logs
- CloudWatch Events
- EventBridge
- AWS X-Ray
- Lambda
- Application Load Balancer
- Amazon RDS

#### Auditing and traceability

- AWS CloudTrail
- AWS Config
  - AWS Managed Rules
- AWS CloudFormation

  - AWS CloudFormation StackSets

- VPC Flow Logs
- Security Hub

### CloudWatch

- CloudWatch Logs Insights
- Amazon CloudWatch Events
- EventBridge
- CloudWatch log groups

### AWS X-Ray

- X-Ray can provide end-to-end tracing and analysis if you have large-scale applications with distributed architecture and microservices.

- X-Ray can help to visualize and understand the interactions between components of your application and can be valuable for performance optimization and issue resolution. L

### Amazon Gaurd Duty

### Amazon Inspector

### AWS Security Hub

### CloudTrail

### AWS Config

### AWS CloudFormation

### VPC Flow Logs

VPC Flow Logs capture information about the IP traffic going to and from network interfaces in your Amazon VPC. This helps monitor and troubleshoot network traffic for security and compliance audits.

### Event -> Lambda -> S3 pattern

## 3.4 Ensure data quality

### Concepts

#### Data Profiling

- AWS Glue DataBrew
- Athena
- Amazon Redshift
- Amazon EMR
- AWS Marketplace

# Unsorted

---

**_Concept_**

- **federate** : 연합하다(연방을 이루다). : Work ; English

---
