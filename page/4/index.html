<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yjinheon.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="DataMind">
<meta property="og:url" content="https://yjinheon.github.io/page/4/index.html">
<meta property="og:site_name" content="DataMind">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="JinHeon Yoon">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://yjinheon.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DataMind</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/rss2.xml" title="DataMind" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">DataMind</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="JinHeon Yoon"
      src="/images/medium.jpg">
  <p class="site-author-name" itemprop="name">JinHeon Yoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yjinheon" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yjinheon" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yjinheon@gmail.com" title="E-Mail → mailto:yjinheon@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/ko" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-US-knn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-US-knn/" class="post-title-link" itemprop="url">[Unsupervised Learning]KNN을 활용한 분류</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

- ML
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Data Extraction & Wrangling


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<hr>
<h2 id="간단한-컨셉"><a href="#간단한-컨셉" class="headerlink" title="간단한 컨셉"></a>간단한 컨셉</h2><p><strong>KNN</strong></p>
<ul>
<li><p>새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.</p>
</li>
<li><p>회귀문제와 분류문제 해결에 모두 사용되는 지도학습</p>
</li>
<li><p>하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.</p>
</li>
<li><p><strong>K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).</strong></p>
</li>
<li><p>KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.</p>
</li>
<li><p>거리측정방법</p>
<ul>
<li>유클리디안 거리 : 데이터포인트 사이 직선 최단거리</li>
<li>마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.</li>
<li>맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.</li>
</ul>
</li>
<li><p>주의점</p>
<ul>
<li>기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.</li>
<li>불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.</li>
</ul>
</li>
<li><p>장단점</p>
<ul>
<li>장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 </li>
<li>단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림</li>
</ul>
</li>
</ul>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li>유클라디안 거리를 활용한 KNN 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">euclidean_distance</span>(<span class="params">x1,x2</span>):</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>((x1-x2)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KNN</span>:</span><br><span class="line"></span><br><span class="line">    self __init__(self, k=<span class="number">3</span>):</span><br><span class="line">        self.k = k </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>): <span class="comment"># triain sample and label</span></span><br><span class="line">        self.X_train = X</span><br><span class="line">        self.y_train = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        predicted_labels = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">return</span> np.array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        1. 거리 계산하기</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        2. k nearest sample</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        3. majority vote, get most common class</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        distances = [euclidean_distance(x,x_train) <span class="keyword">for</span> x_train <span class="keyword">in</span> X_train]</span><br><span class="line"></span><br><span class="line">        k_indices = np.argsort(distances)[:self.k]</span><br><span class="line">        k_nearest_labels = [self.y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> k_indices]</span><br><span class="line"></span><br><span class="line">        most_common = Counter(k_nearest_labels).most_common(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> most_common[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="분류문제-풀이"><a href="#분류문제-풀이" class="headerlink" title="분류문제 풀이"></a>분류문제 풀이</h2><ul>
<li>iris 데이터를 바탕으로 분류문제 풀이</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cmap = ListedColormap([<span class="string">&quot;#FF0000&quot;</span>, <span class="string">&quot;#00FF00&quot;</span>, <span class="string">&quot;#0000FF&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    accuracy = np.<span class="built_in">sum</span>(y_true == y_pred) / <span class="built_in">len</span>(y_true)</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">        X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1234</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">clf = KNN(k=k)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">predictions = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNN classification 정확도&quot;</span>, accuracy(y_test, predictions))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$KNN</span> classification accuracy 1.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>sklearn에서도 knn 분류기가 구현되어 있다.<ul>
<li>irsis data load까지는 동일하게 진행된다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeiborsClassifier</span><br><span class="line"></span><br><span class="line">clf = KNeiborsClassifier(n_neighbors =<span class="number">3</span>)</span><br><span class="line">clf.fit()</span><br><span class="line"></span><br><span class="line">pred = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNN classification 정확도&quot;</span>, clf.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="K값과-모델-복잡도의-관계"><a href="#K값과-모델-복잡도의-관계" class="headerlink" title="K값과 모델 복잡도의 관계"></a>K값과 모델 복잡도의 관계</h2><ul>
<li>위스콘신 유방암데이터로 구현한다.</li>
<li>k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.</li>
<li>k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.</li>
<li>k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.</li>
<li>중간정도의 범위에서 k의 수를 선정할 필요가 있다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train , X_test , y_train , y_test = train_test_split(cancer.data,</span><br><span class="line">                                                       cancer.target,</span><br><span class="line">                                                       stratify = cancer.target, <span class="comment"># stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)</span></span><br><span class="line">                                                       random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">train_acc = []</span><br><span class="line">test_acc = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k_indices = <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_indices:</span><br><span class="line">    clf = KNeiborsClassifier(n_neighbors=k)</span><br><span class="line">    clf.fit()</span><br><span class="line">    train_acc.append(clf.score(X_train,y_train))</span><br><span class="line">    test_acc.append(clf.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(neighbors_settings, training_accuracy, label=<span class="string">&quot;훈련 정확도&quot;</span>)</span><br><span class="line">plt.plot(neighbors_settings, test_accuracy, label=<span class="string">&quot;테스트 정확도&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;정확도&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;n_neighbors&quot;</span>)</span><br><span class="line">plt.legend(</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><img src="https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/collections.html">https://docs.python.org/3/library/collections.html</a></li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">파이싼 라이브러리를 활용한 머신러닝</a></li>
<li><a target="_blank" rel="noopener" href="https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/">https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Main-Decision-Tree-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Main-Decision-Tree-Algorithms/" class="post-title-link" itemprop="url">[Tree]주요 Decision Tree 알고리즘</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<p>주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.</p>
<hr>
<h2 id="Main-Decision-Tree-Algorithms"><a href="#Main-Decision-Tree-Algorithms" class="headerlink" title="Main Decision Tree Algorithms"></a>Main Decision Tree Algorithms</h2><h3 id="CHAID"><a href="#CHAID" class="headerlink" title="CHAID"></a><strong>CHAID</strong></h3><p>The Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.<br><strong>In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.</strong></p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a><strong>CART</strong></h3><p>CART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.</p>
<p><strong>Prepare Data for CART</strong></p>
<ul>
<li><p>The <strong>splitting of numerical features</strong> can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!</p>
</li>
<li><p>Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization&#x2F;manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.</p>
</li>
<li><p>In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.</p>
</li>
<li><p>If height or depth of the tree is exactly one then such a tree is called as a decision stump.</p>
</li>
<li><p>Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.</p>
</li>
<li><p>Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.</p>
</li>
<li><p>Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.</p>
</li>
<li><p>Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.</p>
</li>
</ul>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a><strong>ID3</strong></h3><p>The Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a><strong>C4.5</strong></h3><p>C4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.</p>
<p>Unlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.<br>Additionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14">https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Random_Forest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Random_Forest/" class="post-title-link" itemprop="url">[Tree]Random Forest의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>Bagging</strong> : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. </li>
<li><strong>weak learner</strong> : 서로 독립적으로 만들어지며 <strong>상관이 낮은</strong> 약한 분류기.</li>
<li><strong>Random Subspace Method</strong> : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용</li>
<li><strong>Random Forest</strong> : 여러 <code>week learner</code>들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.</li>
<li><strong>Bootstrap</strong> : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.</li>
<li><strong>OOB</strong> : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.</li>
</ul>
<hr>
<blockquote>
<p>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</p>
</blockquote>
<p><strong>랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.</strong></p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><img src="https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png" width="700" />

<p>배깅의 핵심적인 목표는 <strong>의사결정 트리 사이의 분산을 줄이는 것이다.</strong> . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 <code>부트스트래핑</code>을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 <code>랜덤 포레스트</code>이다. <code>랜덤포레스트</code>는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.</p>
<h3 id="Random-Subspace-method"><a href="#Random-Subspace-method" class="headerlink" title="Random Subspace method"></a>Random Subspace method</h3><ul>
<li><p>Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.</p>
</li>
<li><p>train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.</p>
</li>
<li><p><strong>feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 <code>weak learner</code>에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.</strong></p>
</li>
<li><p>기본모델 생성시 <strong>특성 m개 중 일부분 k개의 특성을 선택(sampling)한다</strong> </p>
</li>
<li><p>k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.</p>
</li>
<li><p>$\sqrt{m}$을 k로 활용할 수도 있다.</p>
</li>
<li><p>k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.</p>
</li>
<li><p>k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.</p>
</li>
<li><p>k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 </p>
</li>
<li><p>서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**</p>
</li>
<li><p>트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.<br><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/caLvgA/btraSXTXHT3/T0aBmdkrhHd3FCKGsSWN9k/img.png"></p>
</li>
</ul>
<p><strong>배깅과 Random subspave method의 비교</strong></p>
<ul>
<li>bagging: it is better when the training samples are sparse(결측값이 많은 경우)</li>
<li>Random subspace method: it is better when the classes are compact and the boundaries are smooth.</li>
</ul>
<h3 id="Random-Forest-알고리즘"><a href="#Random-Forest-알고리즘" class="headerlink" title="Random Forest 알고리즘"></a>Random Forest 알고리즘</h3><ul>
<li>m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.</li>
<li>m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. </li>
<li>각각의 약한 분류기로 결과를 예측한다.</li>
<li>각 분류기의 예측결과를 모아 최종결과를 도출한다.<ul>
<li>분류 문제일 경우 다수결을 통해 최종 결과를 도출한다</li>
<li>회귀 문제일 경우 평균을 통해 최종결과를 도출한다.</li>
</ul>
</li>
</ul>
<h3 id="Random-Forest-주요-hyperparameter"><a href="#Random-Forest-주요-hyperparameter" class="headerlink" title="Random Forest 주요 hyperparameter"></a>Random Forest 주요 hyperparameter</h3><p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/">Randomforest Hyperparameter</a><br>sklearn에서 제공하는 하이퍼파라미터 기준으로 정리</p>
<ul>
<li>max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.</li>
<li>n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.</li>
<li>min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.</li>
<li>oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 보통 고려하는 것들</span></span><br><span class="line">&#123;<span class="string">&#x27;bootstrap&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line"> <span class="string">&#x27;criterion&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="comment"># depth가 3일때까지만 split</span></span><br><span class="line"> <span class="string">&#x27;max_features&#x27;</span>: <span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;max_leaf_nodes&#x27;</span>: <span class="number">4</span>, <span class="comment"># leaf node가 4개일때까지만 split</span></span><br><span class="line"> <span class="string">&#x27;min_impurity_decrease&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"> <span class="string">&#x27;min_impurity_split&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line"> <span class="string">&#x27;min_samples_leaf&#x27;</span>: <span class="number">3</span>, <span class="comment"># 생성될 노드들의 샘플 수가 3개 이상이여만 split </span></span><br><span class="line"> <span class="string">&#x27;min_samples_split&#x27;</span>: <span class="number">5</span>, <span class="comment"># 5개 이상의 샘플만 split</span></span><br><span class="line"> <span class="string">&#x27;min_weight_fraction_leaf&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"> <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">&#x27;n_jobs&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;oob_score&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line"> <span class="string">&#x27;random_state&#x27;</span>: <span class="number">42</span>,</span><br><span class="line"> <span class="string">&#x27;verbose&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">&#x27;warm_start&#x27;</span>: <span class="literal">False</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Random-Forest-장단점"><a href="#Random-Forest-장단점" class="headerlink" title="Random Forest 장단점"></a>Random Forest 장단점</h3><p><strong>장점</strong></p>
<ul>
<li>과적합에 강하다.</li>
<li>이상치에 크게 영향받지 않는다.</li>
<li>Scaling이 필요가 없다.</li>
<li>결측값에 크게 영향받지 않는다.</li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li>고차원의 희소한 데이터에 대해 성능이 저하된다.</li>
<li>training 속도 느림(메모리 소모)</li>
<li>개별 트리 분석이 어럽다.</li>
</ul>
<h3 id="Random-Forest-구현"><a href="#Random-Forest-구현" class="headerlink" title="Random Forest 구현"></a>Random Forest 구현</h3><ul>
<li>sklearn을 활용한 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">4</span>,</span><br><span class="line">                            n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifier(max_depth=<span class="number">2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">RandomForestClassifier(...)</span><br><span class="line"><span class="built_in">print</span>(clf.predict([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li>numpy를 활용한 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .decision_tree <span class="keyword">import</span> DecisionTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bootstrap_sample</span>(<span class="params">X, y</span>):</span><br><span class="line">    n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">    idxs = np.random.choice(n_samples, n_samples, replace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X[idxs], y[idxs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">most_common_label</span>(<span class="params">y</span>):</span><br><span class="line">    counter = Counter(y)</span><br><span class="line">    most_common = counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> most_common</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomForest</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_trees=<span class="number">10</span>, min_samples_split=<span class="number">2</span>, max_depth=<span class="number">100</span>, n_feats=<span class="literal">None</span></span>):</span><br><span class="line">        self.n_trees = n_trees</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.n_feats = n_feats</span><br><span class="line">        self.trees = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        self.trees = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_trees):</span><br><span class="line">            tree = DecisionTree(</span><br><span class="line">                min_samples_split=self.min_samples_split,</span><br><span class="line">                max_depth=self.max_depth,</span><br><span class="line">                n_feats=self.n_feats,</span><br><span class="line">            )</span><br><span class="line">            X_samp, y_samp = bootstrap_sample(X, y)</span><br><span class="line">            tree.fit(X_samp, y_samp)</span><br><span class="line">            self.trees.append(tree)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        tree_preds = np.array([tree.predict(X) <span class="keyword">for</span> tree <span class="keyword">in</span> self.trees])</span><br><span class="line">        tree_preds = np.swapaxes(tree_preds, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        y_pred = [most_common_label(tree_pred) <span class="keyword">for</span> tree_pred <span class="keyword">in</span> tree_preds]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_pred)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a></li>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-SVM/" class="post-title-link" itemprop="url">[SVM]서포트벡터머신의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/Supervised-Learning/" itemprop="url" rel="index"><span itemprop="name">Supervised Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning



- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<!--
진짜 ref
https://excelsior-cjh.tistory.com/165


진짜 가장중요한 ref
https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin

-->

<hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>결정 경계</strong>: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.</li>
<li><strong>초평면(hyperplane)</strong> : flat affine subspace of p-1 (p는 데이터의 차원) </li>
<li><strong>Support Vector</strong> : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트</li>
<li><strong>Margin</strong> : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)</li>
<li><strong>Support Vector Machine</strong> : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.<ul>
<li><strong>Soft Margin</strong> : <strong>Allow misclassification</strong>. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.</li>
<li><strong>Hard Margin</strong>: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><hr>
<ul>
<li>데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.</li>
<li>기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.</li>
<li>margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)</li>
<li>마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,</li>
<li>패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.<ul>
<li>maximizing the margin and minimizing the loss</li>
</ul>
</li>
</ul>
<p><img src="https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png"></p>
<p>margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.<br>이는 <code>wx+b=0</code>에 수직인 벡터(법선벡터)인 $\frac{2}{|\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)<br>따라서 $\frac{2}{|\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.<br>Graidient 계산을 보다 용이하게 하기 위해 $\frac{2}{|\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.</p>
<p>$$\min _{\boldsymbol{w}, b} \frac{1}{2}|\boldsymbol{w}|^{2} \equiv \min _{\boldsymbol{w}, b} \frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}$$</p>
<p>class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.</p>
<ul>
<li><strong>양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.</strong></li>
</ul>
<p>이를 모두 만족하는 제약식은 아래와 같다.</p>
<p>$\quad y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1$</p>
<p>따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.</p>
<p>$$\min _{\boldsymbol{w}, b} \frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}$$</p>
<p>$$\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1$$</p>
<h3 id="Soft-Margin"><a href="#Soft-Margin" class="headerlink" title="Soft Margin"></a>Soft Margin</h3><hr>
<p>소프트마진은 분류기에 오차를 나타내는 slack variable $\zeta$ 를 목적함수에 추가한다. </p>
<p>hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.</p>
<p>반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.</p>
<p>소프트 마진 SVM의 최적화 함수는 다음과 같다.</p>
<p>아래의 제약조건을 포함해 생각하면 slack vairable $\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.</p>
<p>$$\min \frac{1}{2}|\mathbf{w}|^{2}+C \sum_{i&#x3D;1}^{m} \zeta_{i}$$</p>


$$\quad y_{i}\left(\mathbf{w}^{T} \mathbf{x}_{i}+b\right) \geq 1-\zeta_{i} \quad i=1, \ldots, n, \quad \zeta_{i} \geq 0$$



<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><hr>
<p>max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.</p>
<p>SVM의 loss function은 <code>hinge loss</code> 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.</p>
<p>SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.</p>
<p>결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.</p>
<p>결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.</p>
<p><img src="https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png"></p>
<h3 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h3><ul>
<li>iris data set에 대해 soft margin 구현</li>
</ul>
<p>사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> svc</span><br><span class="line">linear_svm = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>,C=<span class="number">1.0</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">linear_svm.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>

<ul>
<li>numpy로 직접구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy로 svm구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SVM</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,learning_rate=<span class="number">0.0001</span>,lambda_param =<span class="number">0.01</span>,n_iter =<span class="number">1000</span></span>):</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.lambda_param = lambda_param</span><br><span class="line">        self.n_iters = n_iters</span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.b = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,X,y</span>):</span><br><span class="line">        y_ = np.where(y&lt;=<span class="number">0</span> ,-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        n_samples = X.shape</span><br><span class="line"></span><br><span class="line">        self.w = np.zeros(n_features) <span class="comment"># 가중치 초기화</span></span><br><span class="line">        self.b = <span class="number">0</span> <span class="comment"># 편향 초기화</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_iters):</span><br><span class="line">            <span class="keyword">for</span> idx, x_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">                <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                current index, data point</span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= <span class="number">1</span> <span class="comment"># 제약조건 구현</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 가중치 업데이트(hinge loss의 gradient update)</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> condition:</span><br><span class="line">                    self.w -= self.lr * (<span class="number">2</span> * self.lambda_param * self.w)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.w -= self.lr * (<span class="number">2</span> * self.lambda_param * self.w - np.dot(x_i,y_[idx]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X</span>):</span><br><span class="line">        linear_output = np.dot(X,self.w) - self.b</span><br><span class="line">        <span class="keyword">return</span> np.sign(linear_output) <span class="comment"># numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># weight가 주어졌을 경우 SVM을 시각화하는 함수</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_svm</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_hyperplane_value</span>(<span class="params">x, w, b, offset</span>):</span><br><span class="line">        <span class="keyword">return</span> (-w[<span class="number">0</span>] * x + b + offset) / w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">&quot;o&quot;</span>, c=y)</span><br><span class="line"></span><br><span class="line">    x0_1 = np.amin(X[:, <span class="number">0</span>])</span><br><span class="line">    x0_2 = np.amax(X[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, <span class="number">0</span>)</span><br><span class="line">    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -<span class="number">1</span>)</span><br><span class="line">    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, <span class="number">1</span>)</span><br><span class="line">    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1, x1_2], <span class="string">&quot;y--&quot;</span>)</span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], <span class="string">&quot;k&quot;</span>)</span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], <span class="string">&quot;k&quot;</span>)</span><br><span class="line"></span><br><span class="line">    x1_min = np.amin(X[:, <span class="number">1</span>])</span><br><span class="line">    x1_max = np.amax(X[:, <span class="number">1</span>])</span><br><span class="line">    ax.set_ylim([x1_min - <span class="number">3</span>, x1_max + <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>Reference &amp; Annotaion</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://youtu.be/efR1C6CvhmE">https://youtu.be/efR1C6CvhmE</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Support-vector_machine">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1">https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1</a></li>
<li>데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.</li>
<li>법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Decision_Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Decision_Tree/" class="post-title-link" itemprop="url">[Tree]Decision Tree의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h2 id="Decision-Tree의-이해"><a href="#Decision-Tree의-이해" class="headerlink" title="Decision Tree의 이해"></a>Decision Tree의 이해</h2><hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>Decision Tree(결정트리)</strong>: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.</li>
<li><strong>threshold</strong> : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,</li>
<li><strong>full tree</strong> : 모든 학습데이터에 대해 분기한 상태.</li>
<li><strong>Entropy</strong> : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치</li>
<li><strong>지니 불순도</strong> : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</li>
<li><strong>정보 이득</strong> : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.</li>
<li><strong>Root Node</strong> : 초기노드. 데이터셋 혹은 샘플 전체. </li>
<li><strong>Leaf Node(Terminal Node)</strong> : 자식이 없는 노드.하위노드가 없다.</li>
<li><strong>Pure Node</strong> : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.</li>
<li><strong>Branch</strong> : sub-section of an entire tree.</li>
<li><strong>Splitting</strong> : 특정 노드를 나눠 하위노드를 생성하는 것.</li>
<li><strong>Pruning</strong> : 특정 노드의 하위노드를 날리는 것(삭제).</li>
<li><strong>Pre-prune</strong>: When you stop growing DT branches when information becomes unreliable.</li>
<li><strong>Post-prune</strong>: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.</li>
</ul>
<p><img src="https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png"></p>
<hr>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><hr>
<ul>
<li>SVM처럼 <strong>분기점(threshold)을 학습한다.</strong></li>
<li>기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.</li>
<li><strong>분기의 기준이 정보이득이라는 것이 핵심이다.</strong></li>
<li>과적합을 방지하기 위해 pruning이 필요하다.</li>
<li>선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.</li>
<li>특성을 해석하기 좋아 많이 쓰임</li>
<li><strong>샘플에 민감해 트리 고저가 자주 바뀐다.</strong></li>
<li>앙상블 방법의 기초가 된다.</li>
<li><strong>결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예&#x2F;아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.</strong></li>
<li>학습 데이터셋에 과대적합되는 경향이 있다.</li>
<li>결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)</li>
<li>따라서 사전&#x2F;사후 가지치기를 통해 과대적합을 방지한다.</li>
<li>알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.</li>
</ul>
<h3 id="불순도-지표"><a href="#불순도-지표" class="headerlink" title="불순도 지표"></a>불순도 지표</h3><hr>
<h4 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h4><hr>
<p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/">엔트로피 중요개념</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8">매우중요</a></p>
<ul>
<li>Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)</li>
<li>0~1의 값을 가진다.<ul>
<li>클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. </li>
<li>데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.</li>
</ul>
</li>
<li>트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -&gt; 엔트로피가 최소화되는 방향으로 학습을 진행한다.</li>
</ul>
<p align="center">
<img src="https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png" alt="drawing" width="400"/>
</p>

<ul>
<li><strong>정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)</strong></li>
<li>N은 범주의 개수</li>
<li>$p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.</li>
</ul>
<p>$$\text { Entropy }(p)&#x3D;-\sum_{i&#x3D;1}^{N} p_{i} \log <em>{2} p</em>{i}$$</p>
<h4 id="지니불순도"><a href="#지니불순도" class="headerlink" title="지니불순도"></a>지니불순도</h4><hr>
<ul>
<li><strong>잘못 분류될 확률을 최소화하기 위한 기준이다.</strong><ul>
<li>정확히는 <code>데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</code></li>
<li>기본적으로 Single Node에 대해 계산한 값이다,</li>
</ul>
</li>
<li>클래스의 비율이 완벽히 균등할 때 최대가 된다.</li>
<li>기본적으로 노드가 중요할수록 불순도가 크게 감소한다.</li>
<li>범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.</li>
<li><strong>Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.</strong></li>
<li><strong>지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.</strong>(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)<ul>
<li>불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.</li>
</ul>
</li>
<li>최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.</li>
<li>Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.</li>
<li>Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.</li>
</ul>
<p>$$\text{Gini Impurity}&#x3D;\sum_{i&#x3D;1}^{N} p(i) *(1-p(i))$$</p>
<h4 id="information-Gain"><a href="#information-Gain" class="headerlink" title="information Gain"></a>information Gain</h4><hr>
<ul>
<li>leaf의 결과는 기본적으로 majority 를 반환한다.</li>
<li><strong>정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.</strong><ul>
<li>이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.</li>
</ul>
</li>
<li>Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.</li>
</ul>
<p>$$IG(Parent,Children) &#x3D; E(Parent) - E(Parent | Children)$$</p>
<ul>
<li><strong>자식 노드의 불순도가 낮을수록 정보 이득이 커진다.</strong> </li>
<li>보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.</li>
</ul>
<p>$$\text {E(parent)} - [\text {weighted average}] * E(children)$$</p>
<p><img src="https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png"></p>
<ul>
<li>엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.</li>
<li>엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.</li>
</ul>
<h3 id="결정트리의-최적화-문제"><a href="#결정트리의-최적화-문제" class="headerlink" title="결정트리의 최적화 문제"></a>결정트리의 최적화 문제</h3><hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a">최적화 원리와 코드</a></li>
</ul>
<p><strong>Training algorithm</strong></p>
<ul>
<li><p><strong>기본적으로 Best Threshold를 찾는 문제이다</strong></p>
</li>
<li><p>Start at the top node and at each node select the best split based o the best information gain</p>
</li>
<li><p>Greedy Search : Loop over all features and over all thresholds (<strong>all possible feature values</strong>)</p>
</li>
<li><p>Save the best split features and split threshold at each node</p>
</li>
<li><p>Build the tree recursively</p>
</li>
<li><p>Apply some stopping criteria to stop growing</p>
<ul>
<li>maximum depth</li>
<li>minimum samples</li>
<li>etc..</li>
</ul>
</li>
<li><p>When we have a leaf node, store the most common class label of this node</p>
</li>
</ul>
<p><strong>Predict :&#x3D; Traverse tree</strong></p>
<ul>
<li>Traverse the tree recursively.</li>
<li>At each node look at the best split feature of the test feature vector x and go left or right <strong>depending on x[feature idx] &lt;&#x3D; threshold</strong></li>
<li>When we reach the leaf node we return the stored most common class label</li>
</ul>
<h3 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h3><hr>
<p><strong>Put limits in How trees grow</strong></p>
<h4 id="PrePruning"><a href="#PrePruning" class="headerlink" title="PrePruning"></a>PrePruning</h4><hr>
<ul>
<li><p>트리의 최대 깊이 제한하기(max_depth)</p>
</li>
<li><p>리프의 최대 개수 제한하기</p>
</li>
<li><p>노드가 분할하기 위한 데이터 포인트의 최소 개수 지정</p>
</li>
<li><p>sklearn에서 제공하는 관련 Hyperparameter</p>
<ul>
<li>max_depth : 일반화 성능관련. 트리의 최대깊이<ul>
<li>min_sample_splite</li>
<li>max_feature : 최대 피처 사용수</li>
<li>random_state : random state</li>
<li>class_weight : 가중치 balance 맟추기</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="PostPruning"><a href="#PostPruning" class="headerlink" title="PostPruning"></a>PostPruning</h4><hr>
<p>Post-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.</p>
<h3 id="Feature-Importance-in-Decision-Tree"><a href="#Feature-Importance-in-Decision-Tree" class="headerlink" title="Feature Importance in Decision Tree"></a>Feature Importance in Decision Tree</h3><hr>
<h3 id="More-to-learn"><a href="#More-to-learn" class="headerlink" title="More to learn"></a>More to learn</h3><hr>
<ul>
<li>Pruning</li>
<li>Handling missing data</li>
<li>Building Trees for regression</li>
<li>Using trees to explore datasets</li>
</ul>
<p><strong>more</strong></p>
<ul>
<li>Gini-Index is providing us with the highest accuracy with max depth &#x3D; 6.</li>
<li>Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.</li>
<li>With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.</li>
</ul>
<p>Therefore with taking the criteria as Gini and max_depth &#x3D; 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.</p>
<p><strong>결정트리의 장점</strong></p>
<ul>
<li>설명가능성</li>
</ul>
<p><strong>결정트리의 단점</strong></p>
<ul>
<li>과적합</li>
</ul>
<h3 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h3><hr>
<ul>
<li>numpy로 구현</li>
<li><strong>기본적으로 Best Split Threshold를 찾는 것이 목적이다.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the entropy of a label vector</span></span><br><span class="line"><span class="string">    :param y: label vector</span></span><br><span class="line"><span class="string">    :return: entropy</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    hist = np.bincount(y) <span class="comment"># class distribution # 0부터 max까지 class label의 빈도</span></span><br><span class="line">    ps = hist / <span class="built_in">len</span>(y) <span class="comment"># probability of each class</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>([p * np.log2(p) <span class="keyword">for</span> p <span class="keyword">in</span> ps <span class="keyword">if</span> p != <span class="number">0</span>]) <span class="comment"># 음수에 대해서는 정의하지 않음</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,feature=<span class="literal">None</span>,threshold=<span class="literal">None</span>,left=<span class="literal">None</span>,right=<span class="literal">None</span>,*,value=<span class="literal">None</span></span>):</span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.threshold = threshold</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line">        self.value = value</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_leaf</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="comment"># leaf node의 경우 value가 있다.</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecisionTree</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, min_samples_split=<span class="number">2</span>, max_depth=<span class="number">100</span>, n_feats = <span class="literal">None</span></span>):</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.n_feats = n_feats</span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="comment"># grow tree</span></span><br><span class="line">        <span class="comment"># X.shape[1] : feature의 개수</span></span><br><span class="line">        </span><br><span class="line">        self.n_feats = X.shape[<span class="number">1</span>] <span class="keyword">if</span> <span class="keyword">not</span> self.n_feats <span class="keyword">else</span> <span class="built_in">min</span>(self.n_feats, X.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># if not self.n_feats -&gt; n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) </span></span><br><span class="line">        <span class="comment"># input의 feature 수보다 n_feats기 커지지 않게끔하는 </span></span><br><span class="line">        self.root = self._grow_tree(X, y)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_grow_tree</span>(<span class="params">self, X, y, depth=<span class="number">0</span></span>):</span><br><span class="line">        n_sample, n_feats = X.shape</span><br><span class="line">        n_labels = <span class="built_in">len</span>(np.unique(y))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            depth &gt;= self.max_depth </span><br><span class="line">            <span class="keyword">or</span> n_labels == <span class="number">1</span> </span><br><span class="line">            <span class="keyword">or</span> n_sample &lt; self.min_samples_split</span><br><span class="line">        ):</span><br><span class="line">            leaf_value = self._most_common_label(y)</span><br><span class="line">            <span class="keyword">return</span> Node(value=leaf_value)</span><br><span class="line">        </span><br><span class="line">        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># calculate information gain</span></span><br><span class="line">        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># grow the children that result from splitting on the best feature</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 정보이득을 계산한 best_feature와 best threshold 기준으로 분할</span></span><br><span class="line">        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)</span><br><span class="line">        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+<span class="number">1</span>) <span class="comment"># depth+1</span></span><br><span class="line">        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Node(best_feat, best_threshold, left, right) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_best_criteria</span>(<span class="params">self,X,y,feat_idxs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Find the best criteria to split the data</span></span><br><span class="line"><span class="string">        :param X: input data</span></span><br><span class="line"><span class="string">        :param y: label</span></span><br><span class="line"><span class="string">        :param feat_idxs: indices of features to consider</span></span><br><span class="line"><span class="string">        :return: best feature index, best threshold</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        best_gain = -<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        split_idx, split_threshold = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> feat_idx <span class="keyword">in</span> feat_idxs:</span><br><span class="line">            X_col = X[:,feat_idx] <span class="comment"># X의 각 feature</span></span><br><span class="line">            thresholds = np.unique(X_col) <span class="comment"># 각 feature의 cardianlity</span></span><br><span class="line">            <span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:</span><br><span class="line">                gain = self._information_gain(y,X_col,threshold) <span class="comment"># 각 feuture의 모든 threshold에 대해서 gain을 계산</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> gain &gt; best_gain:</span><br><span class="line">                    best_gain = gain</span><br><span class="line">                    split_idx = feat_idx</span><br><span class="line">                    split_threshold = threshold</span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">return</span> split_idx, split_threshold</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_information_gain</span>(<span class="params">self,y,X_column,split_threshold</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculate information gain</span></span><br><span class="line"><span class="string">        E(parent) - [weight average] * E(Children)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># parent entropy</span></span><br><span class="line">        </span><br><span class="line">        parent_entropy = entropy(y)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># generate split</span></span><br><span class="line">        left_idxs, right_idxs = self._split(X_column, split_threshold)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 더이상 분할이 안될 경우 정보이득이 0</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">len</span>(left_idxs) == <span class="number">0</span>) <span class="keyword">or</span> (<span class="built_in">len</span>(right_idxs)) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the weighted avg. of the loss for the children</span></span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        n_l, n_r = <span class="built_in">len</span>(left_idxs), <span class="built_in">len</span>(right_idxs)</span><br><span class="line">        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])</span><br><span class="line">        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r</span><br><span class="line"></span><br><span class="line">        <span class="comment"># information gain is difference in loss before vs. after split</span></span><br><span class="line">        ig = parent_entropy - child_entropy</span><br><span class="line">        <span class="keyword">return</span> ig</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_split</span>(<span class="params">self, X_column, split_threshold</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Split data according to the threshold</span></span><br><span class="line"><span class="string">        :param X_column: input data</span></span><br><span class="line"><span class="string">        :param split_threshold: threshold to split</span></span><br><span class="line"><span class="string">        :return: left and right indices</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># np.argwhere을 사용 조건에 해당하는 인덱스 반환.</span></span><br><span class="line">        left_idxs = np.argwhere(X_column &lt;= split_threshold).flatten()</span><br><span class="line">        right_idxs = np.argwhere(X_column &gt; split_threshold).flatten()</span><br><span class="line">        <span class="keyword">return</span> left_idxs, right_idxs</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_most_common_label</span>(<span class="params">self, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Find the most common label in the dataset</span></span><br><span class="line"><span class="string">        :param y: labels</span></span><br><span class="line"><span class="string">        :return: most common label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        counter = Counter(y)</span><br><span class="line">        <span class="comment"># counter.most_common(1) -&gt; [(label, count)] # 리스트 안에 튜플</span></span><br><span class="line">        most_common = counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> most_common <span class="comment"># Counter(y) : Counter(&#123;0: 2, 1: 2&#125;) #value과 count중 value만 반환 </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># traverse the tree</span></span><br><span class="line">        <span class="keyword">return</span> np.array([self._traverse_tree(x,self.root) <span class="keyword">for</span> x <span class="keyword">in</span> X]) <span class="comment"># X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_traverse_tree</span>(<span class="params">self, x, node</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Traverse the tree from the root</span></span><br><span class="line"><span class="string">        :param x: input data</span></span><br><span class="line"><span class="string">        :param node: root node</span></span><br><span class="line"><span class="string">        :return: label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> node.is_leaf(): <span class="comment"># check if leaf node</span></span><br><span class="line">            <span class="keyword">return</span> node.value</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> x[node.feature] &lt;= node.threshold:</span><br><span class="line">            <span class="keyword">return</span> self._traverse_tree(x, node.left)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._traverse_tree(x, node.right)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y,y_pred</span>):</span><br><span class="line">        acc = np.<span class="built_in">sum</span>(y == y_pred) / <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">    data = datasets.load_breast_cancer()</span><br><span class="line">    X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    </span><br><span class="line">    clf = DecisionTree(max_depth=<span class="number">10</span>)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    </span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    acc = accuracy(y_test, y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Accuracy : <span class="subst">&#123;acc&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="References-amp-annotation"><a href="#References-amp-annotation" class="headerlink" title="References &amp; annotation"></a><strong>References &amp; annotation</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html">결정트리의 최적화 문제</a></li>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/information-gain-and-mutual-information/">정보이득</a></li>
<li><a target="_blank" rel="noopener" href="https://victorzhou.com/blog/gini-impurity/">지니불순도</a></li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/">불순도 지표들</a></li>
<li><a href="-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b">Post_Pruning</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2022/03/02/NLP-wordembedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/02/NLP-wordembedding/" class="post-title-link" itemprop="url">[NLP]Word Embedding과 Text Classification</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-02 19:28:01" itemprop="dateCreated datePublished" datetime="2022-03-02T19:28:01+09:00">2022-03-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Data Extraction & Wrangling


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h1 id="NLP-subtask에-대한-정리"><a href="#NLP-subtask에-대한-정리" class="headerlink" title="NLP subtask에 대한 정리"></a>NLP subtask에 대한 정리</h1><p>: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리</p>
<h2 id="1-Word-Embedding"><a href="#1-Word-Embedding" class="headerlink" title="1.Word Embedding"></a>1.Word Embedding</h2><h3 id="1-1-Word-Embedding에서-고려하는-task"><a href="#1-1-Word-Embedding에서-고려하는-task" class="headerlink" title="1.1 Word Embedding에서 고려하는 task"></a>1.1 Word Embedding에서 고려하는 task</h3><p>Word Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.</p>
<p>Embedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.</p>
<p><strong>단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.</strong></p>
<p><img src="https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png"></p>
<h4 id="1-1-1-차원의-저주"><a href="#1-1-1-차원의-저주" class="headerlink" title="1.1.1 차원의 저주"></a>1.1.1 차원의 저주</h4><p>단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.</p>
<p>문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.</p>
<p>이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.</p>
<h4 id="1-1-2-Distribution-Hypothesis"><a href="#1-1-2-Distribution-Hypothesis" class="headerlink" title="1.1.2 Distribution Hypothesis"></a>1.1.2 Distribution Hypothesis</h4><p>Distribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.</p>
<p>Word embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. </p>
<p>따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.</p>
<h4 id="1-1-3-Predictive-Method"><a href="#1-1-3-Predictive-Method" class="headerlink" title="1.1.3 Predictive Method"></a>1.1.3 Predictive Method</h4><p>Word Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.</p>
<h3 id="1-2-대표적인-데이터셋"><a href="#1-2-대표적인-데이터셋" class="headerlink" title="1.2 대표적인 데이터셋"></a>1.2 대표적인 데이터셋</h3><h4 id="Words-in-Context"><a href="#Words-in-Context" class="headerlink" title="Words in Context"></a>Words in Context</h4><p>Word in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.</p>
<p>과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.</p>
<p>Word in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.</p>
<p>데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.</p>
<p><img src="https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png"></p>
<h3 id="Contextual-Embedding-SOTA-Technique"><a href="#Contextual-Embedding-SOTA-Technique" class="headerlink" title="Contextual Embedding(SOTA Technique)"></a>Contextual Embedding(SOTA Technique)</h3><p>Word Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.</p>
<p>과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.</p>
<p>이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.</p>
<h4 id="biLM-bidirectional-Language-Model-as-function"><a href="#biLM-bidirectional-Language-Model-as-function" class="headerlink" title="biLM(bidirectional Language Model) as function"></a>biLM(bidirectional Language Model) as function</h4><p>Contextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.</p>
<p>ELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.</p>
<p>여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.</p>
<h2 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h2><h3 id="Text-Classification의-주요-task"><a href="#Text-Classification의-주요-task" class="headerlink" title="Text Classification의 주요 task"></a>Text Classification의 주요 task</h3><p>Text Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.</p>
<h3 id="대표적인-데이터셋"><a href="#대표적인-데이터셋" class="headerlink" title="대표적인 데이터셋"></a>대표적인 데이터셋</h3><h4 id="IMDB-Movie-Review"><a href="#IMDB-Movie-Review" class="headerlink" title="IMDB Movie Review"></a>IMDB Movie Review</h4><p>IMDB에 게시된 영화 리뷰와 Positive&#x2F;Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.</p>
<h3 id="BERT-SOTA-Technique"><a href="#BERT-SOTA-Technique" class="headerlink" title="BERT(SOTA Technique)"></a>BERT(SOTA Technique)</h3><p>BERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.</p>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><ul>
<li>Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.</li>
<li>기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.</li>
<li>Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.</li>
<li>문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.<ul>
<li>Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.</li>
</ul>
</li>
<li>BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.</li>
</ul>
<p>BERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)</p>
<p>단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.</p>
<h4 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine tuning"></a>fine tuning</h4><p>Transformer와 함께 BERT의 핵심 컨셉중 하나로 <em>기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것</em>을 뜻한다.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/what-are-word-embeddings/">https://machinelearningmastery.com/what-are-word-embeddings/</a></li>
<li><a target="_blank" rel="noopener" href="https://pilehvar.github.io/wic/">https://pilehvar.github.io/wic/</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.09121v3.pdf">WiC: the Word-in-Context Dataset</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/method/elmo">https://paperswithcode.com/method/elmo</a></li>
<li><a target="_blank" rel="noopener" href="https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80">Positional Encoding의 이해</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2021/12/29/DE-Pyspark-Hadoop.md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/29/DE-Pyspark-Hadoop.md/" class="post-title-link" itemprop="url">[Pyspark]하둡의 컨셉 이해하기</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-29 00:00:00" itemprop="dateCreated datePublished" datetime="2021-12-29T00:00:00+09:00">2021-12-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-11 15:12:22" itemprop="dateModified" datetime="2023-03-11T15:12:22+09:00">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Data-Engineering/" itemprop="url" rel="index"><span itemprop="name">Data Engineering</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>Pyspark를 본격적으로 쓰기 전에 하둡의 컨셉을 간단히 정리하자.</p>
</blockquote>
<h2 id="하둡"><a href="#하둡" class="headerlink" title="하둡"></a>하둡</h2><p>: 하둡은 Data Locality에 바탕을 둔 분산컴퓨팅을 위한 소프트웨어 플랫폼 및 프레임워크이다. 기본적인 컨셉은 분산처리(큰 문제를 작은 문제의 집합으로 나누고 정리하는 것)이다.</p>
<p>하둡은 HDFS (Hadoop Distributed File System) 와 YARN(Yet Another Resource Navigator)로 구성된다.</p>
<ul>
<li>하둡은 데이터가 비공유 접근을 허용하는 클러스터의 노드에서 지역적으로 처리될 수 있게 한다</li>
<li>각 노드는 다른 노드들과 통신할 필요 없이 전체 데이터의 훨씬 작은 부분을 독립적으로 처리할 수 있다.</li>
<li>기본적으로 분산시스템이기에 네트워크 연결을 통해 여러 연산자원(노드들)의 사용을 조정한다.</li>
<li>선형적 확장성을 가진다. 이는 노드의 수 , 스토리지의 양 , 잡 루틴이 선형적 관계로 엮여있다는 것을 의미한다. (만약 노드의 수를 늘린다면 그만큼 처리시간이 줄어들 것이라고 예측할 수 있다.)</li>
</ul>
<p><strong>데이터가 기본적으로 분산되어있고 확장가능하며(노드의 수를 늘리는 방식으로) 오류에 대처할 수 있다.</strong></p>
<p>이는 분산파일시스템의 구현을 통해 가능해진다.</p>
<h3 id="Schima-on-Write"><a href="#Schima-on-Write" class="headerlink" title="Schima-on-Write"></a>Schima-on-Write</h3><p>: 데이터를 저장할때 스키마를 우선 정의하는 것</p>
<p>주로 RBDMS에 자주 쓰인다. 데이터에 대해 익숙하고 자주 쓴다고 가정할 경우 Schima-on-Write 방식이 보다 적합할 수 있다.</p>
<h3 id="Schima-On-Read"><a href="#Schima-On-Read" class="headerlink" title="Schima-On-Read"></a>Schima-On-Read</h3><p>: 분석을 위해 파일 시스템에서 데이터를 읽을 때 스키마가 정의되는 것.</p>
<p>반구조화를 포함한 광범위한 데이터를 저장하고 처리할 수 있다.이는 데이터를 원본 그대로 저장할 수있다는 것을 의미하며 빅데이터 처리 측면에서 강점을 가진다는 것을 의미한다.</p>
<img src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/hwyn_0105-2d2be3e87d610d5bece7f647d47d5fcc.png" alt="drawing" width="500"/>

<h3 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h3><p>: 데이터가 있는 곳으로 이동해서 계산하는 것. 데이터의 이동이 아닌 계산의 이동.</p>
<p><strong>데이터 지역성은 계산하기 위해 데이터를 이동하는 것이 아니라 데이터를 그대로 두고 계산을 이동시키는 것이다.</strong><br>빅 데이터를 계산하기 위해 데이터를 이동(move)를 최대한 줄여<br>시스템 쓰루풋(throughput)과 혼잡도를 늦추게 하는 것이다.<br>따라서 통신 대역폭이 당연히 줄어들고 성능은 늘어난다.</p>
<p>기본적으로 대용량데이터를 다룰경우 데이터를 옮기는 것보다 프로그램 자체를 이동시키는 것이 효율적이라는 아이디어에 기반한 개념이다.</p>
<h2 id="비공유-아키텍처"><a href="#비공유-아키텍처" class="headerlink" title="비공유 아키텍처"></a>비공유 아키텍처</h2><p>: Shared Nothing Architecture</p>
<p>분산 컴퓨팅에 사용되는 아키텍처로 RAM, Disk 등의 자원을 공유하지 않는 독립적인 노드들로 이루어진다.</p>
<p>각각의 노드는 독립적인 처리장치와 프로세스, 디스크를 가지고 있다.</p>
<img src="https://phoenixnap.com/kb/wp-content/uploads/2021/09/shared-nothing-architecture-diagram.png" alt="drawing" width="800"/>

<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><h3 id="Mater-x2F-Slave"><a href="#Mater-x2F-Slave" class="headerlink" title="Mater&#x2F;Slave"></a>Mater&#x2F;Slave</h3><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shared-nothing_architecture">https://en.wikipedia.org/wiki/Shared-nothing_architecture</a></li>
<li><a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/shared-nothing-architecture">https://phoenixnap.com/kb/shared-nothing-architecture</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2021/12/01/TS-SQL-ts-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/01/TS-SQL-ts-1/" class="post-title-link" itemprop="url">[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-01 13:21:48" itemprop="dateCreated datePublished" datetime="2021-12-01T13:21:48+09:00">2021-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-11 15:12:22" itemprop="dateModified" datetime="2023-03-11T15:12:22+09:00">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Troubleshooting/" itemprop="url" rel="index"><span itemprop="name">Troubleshooting</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning



- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing



#신경망이란 무엇인가?


https://www.youtube.com/watch?v=aircAruvnKk



#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->


<p>안쓰던 노트북을 서버로 만들어서 작업중 다음 에러가 발생했다.</p>
<p>ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides  </p>
<p>찾아보니 SQL 서버의 변수값을 변경해 해결할 수 있었다.</p>
<p>아래 명령을 통해 local_infile 상태가 ON 인지 OFF인지 확인 한다.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> variables <span class="keyword">like</span> <span class="string">&#x27;local_infile&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>값이 OFF일 경우 아래 명령 실행</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> local_infile <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client">https://stackoverflow.com/questions/59993844/error-loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2021/10/08/DE-Linux-commandline/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/10/08/DE-Linux-commandline/" class="post-title-link" itemprop="url">[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-10-08 17:09:02" itemprop="dateCreated datePublished" datetime="2021-10-08T17:09:02+09:00">2021-10-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-11 15:12:22" itemprop="dateModified" datetime="2023-03-11T15:12:22+09:00">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Data-Engineering/" itemprop="url" rel="index"><span itemprop="name">Data Engineering</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Data-Engineering/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--


<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h2 id="데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음"><a href="#데이터-관련-프로젝트시-자주-사용하는-commandline-명령어-모음" class="headerlink" title="데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음"></a>데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음</h2><h3 id="도움말"><a href="#도움말" class="headerlink" title="도움말"></a>도움말</h3><ul>
<li>man</li>
</ul>
<h3 id="파일관리"><a href="#파일관리" class="headerlink" title="파일관리"></a>파일관리</h3><ul>
<li>pwd</li>
<li>cd</li>
<li>ls</li>
<li>mkdir</li>
<li>rmdir</li>
<li>cp</li>
<li>mv</li>
<li>rm</li>
<li>ln</li>
<li>chmod</li>
</ul>
<h3 id="파일처리"><a href="#파일처리" class="headerlink" title="파일처리"></a>파일처리</h3><ul>
<li>cat</li>
<li>echo</li>
<li>head</li>
<li>tail</li>
<li>more&#x2F;less</li>
<li>grep</li>
<li>sed*</li>
<li>awk*</li>
<li>find*</li>
<li>which</li>
<li>sort</li>
<li>uniq</li>
<li>cut</li>
<li>tr</li>
<li>zip</li>
<li>unzip</li>
<li>gunzip</li>
<li>tar</li>
</ul>
<h3 id="프로세스-관리"><a href="#프로세스-관리" class="headerlink" title="프로세스 관리"></a>프로세스 관리</h3><ul>
<li>top</li>
<li>ps</li>
<li>kill</li>
<li>fg</li>
<li>bg</li>
</ul>
<h3 id="네트워크"><a href="#네트워크" class="headerlink" title="네트워크"></a>네트워크</h3><ul>
<li>ssh</li>
<li>scp</li>
<li>ping</li>
<li>traceroute</li>
<li>curl</li>
<li>finger</li>
<li>who</li>
</ul>
<h3 id="편집기"><a href="#편집기" class="headerlink" title="편집기"></a>편집기</h3><ul>
<li>vi</li>
<li>vim</li>
<li>nvim </li>
<li>nano</li>
</ul>
<h3 id="파이프와-리디렉션"><a href="#파이프와-리디렉션" class="headerlink" title="파이프와 리디렉션"></a>파이프와 리디렉션</h3><h3 id="셀-환경변수"><a href="#셀-환경변수" class="headerlink" title="셀 환경변수"></a>셀 환경변수</h3><ul>
<li>export</li>
<li>$path</li>
<li>$ps1</li>
</ul>
<h3 id="Unsorted"><a href="#Unsorted" class="headerlink" title="Unsorted"></a>Unsorted</h3><ul>
<li>cal</li>
<li>history</li>
</ul>
<h2 id="References-amp-annotation"><a href="#References-amp-annotation" class="headerlink" title="References &amp; annotation"></a><strong>References &amp; annotation</strong></h2><ul>
<li>따라하며 배우는 데이터과학 (책)</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2021/09/28/DL-backpropagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/09/28/DL-backpropagation/" class="post-title-link" itemprop="url">[Neural Network]역전파 알고리즘(backpropagation)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-09-28 16:59:23" itemprop="dateCreated datePublished" datetime="2021-09-28T16:59:23+09:00">2021-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-11 15:12:22" itemprop="dateModified" datetime="2023-03-11T15:12:22+09:00">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Neural-Network/" itemprop="url" rel="index"><span itemprop="name">Neural Network</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Data Extraction & Wrangling

https://edgeaiguru.com/Feedforward-and-Backpropagation
https://www.youtube.com/watch?v=aircAruvnKk
#참고

https://cinema4dr12.tistory.com/1016?category=515283
https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면<br>역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다. 간단히 역전파의 컨셉을 알아보자.</strong></p>
<hr>
<h2 id="역전파-알고리즘"><a href="#역전파-알고리즘" class="headerlink" title="역전파 알고리즘"></a>역전파 알고리즘</h2><p><strong>순전파가 입력층에서 신호를 받아 은닉층의 가중치(+bias)와 연산을 한 뒤 출력층에서 벡터를 출력하는 과정이라면 역전파는 예측값과 실제값의 차이(에러)를 줄이기 위해  손실함수가 최소가 되도록 출력층으로부터 순전파의 역방향으로 편미분을 통해 가중치를 업데이트 하는 것이다.</strong></p>
<p>역전파 알고리즘에서 가중치를 업데이트 한다는 것은 가중치 매개변수의 기울기(Graidant)를 예측값을 바탕으로 다시 계산한다는 것이다.</p>
<p>기본적으로 <strong>타겟과 예측값의 차이를 줄이기 위해</strong> 가중치를 업데이트한다.</p>
<p>$$<br> y &#x3D; activiate(\sum(\theta_{1}x_{1} + \theta_{2}x_{2} + … + \theta_{n}x_{n}) + bias)<br>$$</p>
<p>타겟과 예측값의 차이는 <code>loss function(cost function)</code>이라고 볼 수 있다.</p>
<p>비용함수는 수식으로 나타내면 아래와 같다.</p>
<p>$$<br>J(\theta) &#x3D; y - h_\theta(x)<br>$$</p>
<p>여기서 $h_\theta(x)$ 는 가설함수(모형)이다.</p>
<p>역전파(backward Propagation)는 예측값의 국소적 미분값을 순전파(Forward Propagation)의 반대방향으로 곱한 후 다음노드로 전달하는 것이다.</p>
<p>비용함수의 편미분을 통해 기울기를 구하는 이유는 <strong>기울기가 비용함수의 값을 최소화 하는 방향을 제시하기 때문이다.</strong></p>
<p><img src="https://i.imgur.com/Olxv64J.png"></p>
<center><b>그림 1. Gradiant Boosting을 통한 global optimum 찾기</b></center>


<p>만약 모형의 loss function이 MSE(Mear Sqared Error)일 경우, 비용함수는 아래와 같이 나타낼 수 있다. (m은 sample의 수를 의미)</p>
<p>$$<br>J(\theta)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2}<br>$$</p>
<p>이를 미분할 경우 직접 계산하기 어렵거나 불가능하기 때문에 Chain Rule을 사용한 합성함수의 편미분을 통해 구한다.</p>
<p>아래와 같은 신경망이 있고 output node의 활성화함수는 sigmoid라고 할 경우</p>
<p><img src="https://i.imgur.com/bGCvYVJ.png"></p>
<center><b>그림 2. 신경망으로 나타낸 backpropagation</b></center>

<p>하나의 가중치에 대한 Gradiant를 아래와 같이 나타날 수 있다.</p>
<p>$$<br>Gradiant&#x3D;\frac{\partial J(\theta)}{\partial \theta_{i}}&#x3D;\frac{\frac{1}{2 m} \sum_{i&#x3D;1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)} {\partial \theta_{i}}<br>$$</p>
<p>이 때 분자는 가중치 $\theta_i$에 대해 미분할 수 없기 때문에 아래와 같이 <code>Chain Rule</code> 을 사용해서 Gradiant를 도출한다.</p>
<p>$$<br>\frac{\partial J(\theta)}{\partial \theta_{i}} &#x3D;\frac{\partial J(\theta)}{\partial z_{2}} \times \frac{\partial z_{2}}{\partial s_{2}} \times \frac{\partial s_{2}}{\partial \theta_{i}}<br>$$</p>
<p><strong>chain rule(연쇄법칙)</strong><br>chain rule은 합성함수의 미분규칙이며 역전파과정에서 Gradiant를 도출할 때 사용된다.</p>
<p>기본적으로 바깥함수의 도함수에 안쪽함수를 인자로 넣어주고 안쪽함수의 도함수를 곱해주면 된다.</p>
<p><img src="https://i.imgur.com/4eSVZW0.png"></p>
<center><b>그림 3. Chain Rule 예시</b></center>


<p><strong>가중치 업데이트</strong></p>
<p>도출된 값을 learning rate와 곱해서 기존 가중치에서 빼주면 새로운 가중치는 다음과 같이 나타낼 수 있다.</p>
<p>$$<br>\theta_{j}&#x3D;\theta_{j}-\eta \frac{\partial}{\partial \theta_{j}} J(\theta)<br>$$</p>
<p><strong>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</strong></p>
<h3 id="신경망-학습-알고리즘-절차-정리"><a href="#신경망-학습-알고리즘-절차-정리" class="headerlink" title="신경망 학습 알고리즘 절차 정리"></a>신경망 학습 알고리즘 절차 정리</h3><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>
<ol>
<li><p>학습할 신경망 구조를 선택</p>
<ul>
<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>
<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>
<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>
<li>hyperparameter의 영역이다. </li>
<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>
</ul>
</li>
</ul>
</li>
<li><p>가중치 초기화</p>
</li>
<li><p>순방향 전파를 통해 $h_{\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>
<ul>
<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\theta)$를 계산</li>
</ul>
</li>
<li><p>역방향 전파를 통해 편미분 값들 $\frac{\delta}{\delta\theta_{jk}^{l}}{J(\theta)}$ 을 계산</p>
</li>
<li><p>optimizer를 통해 loss function을 최소화</p>
</li>
<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>
<ul>
<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>
<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>
<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>
</ul>
</li>
</ol>
<h3 id="머신러닝에서의-학습"><a href="#머신러닝에서의-학습" class="headerlink" title="머신러닝에서의 학습"></a>머신러닝에서의 학습</h3><p>미분은 순간의 변화율을 구하는 것이다.<br><strong>역전파는 모형에서 계산한 예측값과 실제값의 차이를 바탕으로 미분을 통해 가중치를 보정하는 것을 최대한 반복해서 수행하는 것이다.</strong><br>구체적으로는 손실함수의 국소적 미분값(local deravitive)를 구해서 학습률과 곱한 값을 기존 파라미터에서 빼주는 것을 손실함수가 최소가 될 때까지 반복하는 것이다.<br><code>손실함수의 각 피처에 대한 편미분을 벡터로 묶은 것을 그래디언트(Gradient)라고 부른다.</code><br>결국 역전파는 gradient를 조정하는 것을 반복하는 것이라고 볼 수 있다.<br>결국 순전파와 역전파를 통해 가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정이  기계학습에서의 <code>학습</code>이라는 것을 알 수 있다.</p>
<h2 id="신경망-학습-알고리즘-절차-정리-1"><a href="#신경망-학습-알고리즘-절차-정리-1" class="headerlink" title="신경망 학습 알고리즘 절차 정리"></a>신경망 학습 알고리즘 절차 정리</h2><p>퍼셉트론과 역전파 알고리즘에 대한 이해를 바탕으로 지금까지의 절차를 아래와 같이 정리할 수 있다.</p>
<ol start="0">
<li><p>학습할 신경망 구조를 선택</p>
<ul>
<li>입력층 유닛의 수 &#x3D; Feature 수 (input layer)</li>
<li>출력층 유닛의 수 &#x3D; target class 수 (output layer)</li>
<li>은닉층 수, 각 은닉층의 노드 수 (hidden layer)<ul>
<li>hyperparameter의 영역이다. </li>
<li>sqrt(input layer 수 * output layer 수 ) 로 구해줄 수 있지만 방식이 정해진 것은 아니다.</li>
</ul>
</li>
</ul>
</li>
<li><p>가중치 초기화</p>
</li>
<li><p>순방향 전파를 통해 $h_{\theta}(x^{(i)})$(출력층 y값) 을 모든 입력 $x^{(i)}$에 대해 계산</p>
<ul>
<li>입력벡터와 가중치벡터의 내적을 산출<br>비용함수 $J(\theta)$를 계산</li>
</ul>
</li>
<li><p>역방향 전파를 통해 편미분 값들 $\frac{\delta}{\delta\theta_{jk}^{l}}{J(\theta)}$ 을 계산</p>
</li>
<li><p>optimizer를 통해 loss function을 최소화</p>
</li>
<li><p>어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복한다.</p>
<ul>
<li>한번 학습할때의 sample의 size 를 <code>batch</code> 라고 한다.</li>
<li>전체 sample에 대해 2-5 의 과정을 반복한 것을 <code>epoch</code>라고 한다.</li>
<li><code>iteration</code>은 batch 기준으로 학습의 횟수를 카운팅 한 것이다. 100개의 sample의 batch가 50이고 epoch를 50으로 할 경우 전체 iteration의 수는 100이 된다.</li>
</ul>
</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd">https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd</a></li>
<li><a target="_blank" rel="noopener" href="https://edgeaiguru.com/Feedforward-and-Backpropagation">https://edgeaiguru.com/Feedforward-and-Backpropagation</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JinHeon Yoon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yjinheon" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.0/mermaid.min.js","integrity":"sha256-3JloMMI/ZQx6ryuhhZTsQJQmGAkXeni6PkshX7UUO2s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
