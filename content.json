{"meta":{"title":"DataMind","subtitle":"","description":"","author":"JinHeon Yoon","url":"https://yjinheon.github.io","root":"/"},"posts":[{"title":"[SVM]서포트벡터머신의 이해","date":"2022-06-14T15:05:35.592Z","path":"2022/06/15/ML-SP-SVM/","text":"Concept 결정 경계: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다. 초평면(hyperplane) : flat affine subspace of p-1 (p는 데이터의 차원) Support Vector : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트 Margin : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리) Support Vector Machine : 마진을 최대화 하는 결정 경계를 찾는 알고리즘. Soft Margin : Allow misclassification. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다. Hard Margin: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다. Note 데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다. 기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다. margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.) 마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다, 패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다. maximizing the margin and minimizing the loss margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.이는 wx+b=0에 수직인 벡터(법선벡터)인 $\\frac{2}{|\\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)따라서 $\\frac{2}{|\\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.Graidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{|\\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다. $$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}|\\boldsymbol{w}|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$ class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다. 양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다. 이를 모두 만족하는 제약식은 아래와 같다. $\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$ 따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다. $$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$ $$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$ Soft Margin 소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다. 반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다. 소프트 마진 SVM의 최적화 함수는 다음과 같다. 아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이 Soft Margin SVM의 목적이 된다. $$\\min \\frac{1}{2}|\\mathbf{w}|^{2}+C \\sum_{i&#x3D;1}^{m} \\zeta_{i}$$ $$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$ Hinge Loss max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다. SVM의 loss function은 hinge loss 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다. SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다. 결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다. 결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우 loss는 0으로 수렴한다. 구현 iris data set에 대해 soft margin 구현 사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다. 1234from sklearn.svm import svclinear_svm = SVC(kernel=&#x27;linear&#x27;,C=1.0, random_state=42)linear_svm.fit(X_train,y_train) numpy로 직접구현 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# numpy로 svm구현import numpy as npclass SVM: def __init__(self,learning_rate=0.0001,lambda_param =0.01,n_iter =1000): self.lr = learning_rate self.lambda_param = lambda_param self.n_iters = n_iters self.w = None self.b = None def fit(self,X,y): y_ = np.where(y&lt;=0 ,-1, 1) n_samples = X.shape self.w = np.zeros(n_features) # 가중치 초기화 self.b = 0 # 편향 초기화 for _ in range(self.n_iters): for idx, x_i in enumerate(X): &quot;&quot;&quot; current index, data point &quot;&quot;&quot; condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= 1 # 제약조건 구현 # 가중치 업데이트(hinge loss의 gradient update) if condition: self.w -= self.lr * (2 * self.lambda_param * self.w) else: self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i,y_[idx])) def predict(self,X): linear_output = np.dot(X,self.w) - self.b return np.sign(linear_output) # numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환# weight가 주어졌을 경우 SVM을 시각화하는 함수def visualize_svm(): def get_hyperplane_value(x, w, b, offset): return (-w[0] * x + b + offset) / w[1] fig = plt.figure() ax = fig.add_subplot(1, 1, 1) plt.scatter(X[:, 0], X[:, 1], marker=&quot;o&quot;, c=y) x0_1 = np.amin(X[:, 0]) x0_2 = np.amax(X[:, 0]) x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0) x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0) x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1) x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1) x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1) x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1) ax.plot([x0_1, x0_2], [x1_1, x1_2], &quot;y--&quot;) ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], &quot;k&quot;) ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], &quot;k&quot;) x1_min = np.amin(X[:, 1]) x1_max = np.amax(X[:, 1]) ax.set_ylim([x1_min - 3, x1_max + 3]) plt.show() Reference &amp; Annotaion https://youtu.be/efR1C6CvhmE https://en.wikipedia.org/wiki/Support-vector_machine https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1 데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다. 법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.","content":"<!--\n\n<center>Kaggle Customer Score Dataset</center>\n\n- Machine Learning\n\n\n\n- Statistics , Math\n- Data Engineering\n- Programming\n- EDA & Visualization\n- Preprocessing\n\n\n#신경망이란 무엇인가?\n\nhttps://www.youtube.com/watch?v=aircAruvnKk\n\n\n#참고\n\nhttps://cinema4dr12.tistory.com/1016?category=515283\n\nhttps://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html\n-->\n\n<!--\n진짜 ref\nhttps://excelsior-cjh.tistory.com/165\n\n\n진짜 가장중요한 ref\nhttps://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin\n\n-->\n\n<hr>\n<p><strong><em>Concept</em></strong></p>\n<ul>\n<li><strong>결정 경계</strong>: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.</li>\n<li><strong>초평면(hyperplane)</strong> : flat affine subspace of p-1 (p는 데이터의 차원) </li>\n<li><strong>Support Vector</strong> : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트</li>\n<li><strong>Margin</strong> : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)</li>\n<li><strong>Support Vector Machine</strong> : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.<ul>\n<li><strong>Soft Margin</strong> : <strong>Allow misclassification</strong>. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.</li>\n<li><strong>Hard Margin</strong>: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h3><hr>\n<ul>\n<li>데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.</li>\n<li>기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.</li>\n<li>margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)</li>\n<li>마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,</li>\n<li>패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.<ul>\n<li>maximizing the margin and minimizing the loss</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png\"></p>\n<p>margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.<br>이는 <code>wx+b=0</code>에 수직인 벡터(법선벡터)인 $\\frac{2}{|\\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)<br>따라서 $\\frac{2}{|\\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.<br>Graidient 계산을 보다 용이하게 하기 위해 $\\frac{2}{|\\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2}|\\boldsymbol{w}|^{2} \\equiv \\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.</p>\n<ul>\n<li><strong>양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.</strong></li>\n</ul>\n<p>이를 모두 만족하는 제약식은 아래와 같다.</p>\n<p>$\\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$</p>\n<p>따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.</p>\n<p>$$\\min _{\\boldsymbol{w}, b} \\frac{1}{2} \\boldsymbol{w}^{T} \\boldsymbol{w}$$</p>\n<p>$$\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}+b\\right) \\geq 1$$</p>\n<h3 id=\"Soft-Margin\"><a href=\"#Soft-Margin\" class=\"headerlink\" title=\"Soft Margin\"></a>Soft Margin</h3><hr>\n<p>소프트마진은 분류기에 오차를 나타내는 slack variable $\\zeta$ 를 목적함수에 추가한다. </p>\n<p>hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.</p>\n<p>반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.</p>\n<p>소프트 마진 SVM의 최적화 함수는 다음과 같다.</p>\n<p>아래의 제약조건을 포함해 생각하면 slack vairable $\\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.</p>\n<p>$$\\min \\frac{1}{2}|\\mathbf{w}|^{2}+C \\sum_{i&#x3D;1}^{m} \\zeta_{i}$$</p>\n\n\n$$\\quad y_{i}\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\zeta_{i} \\quad i=1, \\ldots, n, \\quad \\zeta_{i} \\geq 0$$\n\n\n\n<h3 id=\"Hinge-Loss\"><a href=\"#Hinge-Loss\" class=\"headerlink\" title=\"Hinge Loss\"></a>Hinge Loss</h3><hr>\n<p>max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.</p>\n<p>SVM의 loss function은 <code>hinge loss</code> 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.</p>\n<p>SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.</p>\n<p>결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.</p>\n<p>결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.</p>\n<p><img src=\"https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png\"></p>\n<h3 id=\"구현\"><a href=\"#구현\" class=\"headerlink\" title=\"구현\"></a>구현</h3><ul>\n<li>iris data set에 대해 soft margin 구현</li>\n</ul>\n<p>사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> svc</span><br><span class=\"line\">linear_svm = SVC(kernel=<span class=\"string\">&#x27;linear&#x27;</span>,C=<span class=\"number\">1.0</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">linear_svm.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>numpy로 직접구현</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># numpy로 svm구현</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SVM</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,learning_rate=<span class=\"number\">0.0001</span>,lambda_param =<span class=\"number\">0.01</span>,n_iter =<span class=\"number\">1000</span></span>):</span><br><span class=\"line\">        self.lr = learning_rate</span><br><span class=\"line\">        self.lambda_param = lambda_param</span><br><span class=\"line\">        self.n_iters = n_iters</span><br><span class=\"line\">        self.w = <span class=\"literal\">None</span></span><br><span class=\"line\">        self.b = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self,X,y</span>):</span><br><span class=\"line\">        y_ = np.where(y&lt;=<span class=\"number\">0</span> ,-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        n_samples = X.shape</span><br><span class=\"line\"></span><br><span class=\"line\">        self.w = np.zeros(n_features) <span class=\"comment\"># 가중치 초기화</span></span><br><span class=\"line\">        self.b = <span class=\"number\">0</span> <span class=\"comment\"># 편향 초기화</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(self.n_iters):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> idx, x_i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(X):</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">                current index, data point</span></span><br><span class=\"line\"><span class=\"string\">                &quot;&quot;&quot;</span></span><br><span class=\"line\">                condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= <span class=\"number\">1</span> <span class=\"comment\"># 제약조건 구현</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"comment\"># 가중치 업데이트(hinge loss의 gradient update)</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> condition:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    self.w -= self.lr * (<span class=\"number\">2</span> * self.lambda_param * self.w - np.dot(x_i,y_[idx]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self,X</span>):</span><br><span class=\"line\">        linear_output = np.dot(X,self.w) - self.b</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.sign(linear_output) <span class=\"comment\"># numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># weight가 주어졌을 경우 SVM을 시각화하는 함수</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">visualize_svm</span>():</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">get_hyperplane_value</span>(<span class=\"params\">x, w, b, offset</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (-w[<span class=\"number\">0</span>] * x + b + offset) / w[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    fig = plt.figure()</span><br><span class=\"line\">    ax = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&quot;o&quot;</span>, c=y)</span><br><span class=\"line\"></span><br><span class=\"line\">    x0_1 = np.amin(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    x0_2 = np.amax(X[:, <span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\">    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\">    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1, x1_2], <span class=\"string\">&quot;y--&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\">    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], <span class=\"string\">&quot;k&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    x1_min = np.amin(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1_max = np.amax(X[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    ax.set_ylim([x1_min - <span class=\"number\">3</span>, x1_max + <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>Reference &amp; Annotaion</strong></p>\n<ul>\n<li><a href=\"https://youtu.be/efR1C6CvhmE\">https://youtu.be/efR1C6CvhmE</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>\n<li><a href=\"https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1\">https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1</a></li>\n<li>데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.</li>\n<li>법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.</li>\n</ul>\n","comments":true,"permalink":"https://yjinheon.github.io/2022/06/15/ML-SP-SVM/","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://yjinheon.github.io/categories/Machine-Learning/"},{"name":"Supervised Learning","slug":"Machine-Learning/Supervised-Learning","permalink":"https://yjinheon.github.io/categories/Machine-Learning/Supervised-Learning/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://yjinheon.github.io/tags/SVM/"},{"name":"hyperplane","slug":"hyperplane","permalink":"https://yjinheon.github.io/tags/hyperplane/"}]},{"title":"Hello World","date":"2022-06-14T15:00:43.052Z","path":"2022/06/15/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","comments":true,"permalink":"https://yjinheon.github.io/2022/06/15/hello-world/","categories":[],"tags":[]}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://yjinheon.github.io/categories/Machine-Learning/"},{"name":"Supervised Learning","slug":"Machine-Learning/Supervised-Learning","permalink":"https://yjinheon.github.io/categories/Machine-Learning/Supervised-Learning/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://yjinheon.github.io/tags/SVM/"},{"name":"hyperplane","slug":"hyperplane","permalink":"https://yjinheon.github.io/tags/hyperplane/"}]}