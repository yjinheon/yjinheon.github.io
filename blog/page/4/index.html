<!DOCTYPE html>
<html lang="en">
<head><!-- hexo injector head_begin start --><link href="/css/hexo-widget-tree.css" rel="stylesheet"/><!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yjinheon.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="DataMind">
<meta property="og:url" content="https://yjinheon.github.io/blog/page/4/index.html">
<meta property="og:site_name" content="DataMind">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="JinHeon Yoon">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://yjinheon.github.io/blog/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"blog/page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DataMind</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/rss2.xml" title="DataMind" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">DataMind</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-search"><a href="/search/" rel="section"><i class="fa fa-search fa-fw"></i>Search</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="JinHeon Yoon"
      src="/images/medium.jpg">
  <p class="site-author-name" itemprop="name">JinHeon Yoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yjinheon" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yjinheon" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yjinheon@gmail.com" title="E-Mail → mailto:yjinheon@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/ko" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/Preprocessing-pandas_tricks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/Preprocessing-pandas_tricks/" class="post-title-link" itemprop="url">[pandas]pandas 함수와 기초용법들</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-06-28 22:52:16" itemprop="dateModified" datetime="2022-06-28T22:52:16+09:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Preprocessing/" itemprop="url" rel="index"><span itemprop="name">Preprocessing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="pandas-tricks"><a href="#pandas-tricks" class="headerlink" title="pandas tricks"></a><strong>pandas tricks</strong></h2><blockquote>
<p>pandas관련 자주 사용할만한 코드 정리</p>
</blockquote>
<h3 id="pandas-version확인"><a href="#pandas-version확인" class="headerlink" title="pandas version확인"></a>pandas version확인</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.__version__ <span class="comment"># pandas version확인</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.show_versions() <span class="comment">#의존성 패키지 확인</span></span><br></pre></td></tr></table></figure>

<h3 id="DF-생성하기"><a href="#DF-생성하기" class="headerlink" title="DF 생성하기"></a>DF 생성하기</h3><blockquote>
<p>여러 방법이 있지만 보통 dictionary를 사용한다.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;col one&#x27;</span>:[<span class="number">100</span>, <span class="number">200</span>], <span class="string">&#x27;col two&#x27;</span>:[<span class="number">300</span>, <span class="number">400</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 난수생성을 통핸 DF생성</span></span><br><span class="line">pd.DataFrame(np.random.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br></pre></td></tr></table></figure>

<h3 id="열이름-변경하기"><a href="#열이름-변경하기" class="headerlink" title="열이름 변경하기"></a>열이름 변경하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dictionary 형태로 변경하기</span></span><br><span class="line">df = df.rename(&#123;<span class="string">&#x27;col one&#x27;</span> : <span class="string">&#x27;col_one&#x27;</span>,<span class="string">&#x27;col two&#x27;</span>: <span class="string">&#x27;col_two&#x27;</span>&#125;, axis = <span class="string">&#x27;columns&#x27;</span> ) <span class="comment"># 적용할 axis지정 rename</span></span><br><span class="line"></span><br><span class="line">df.add_prefix(<span class="string">&#x27;X_&#x27;</span>) <span class="comment">#컬럼에 접두어 X 추가</span></span><br><span class="line">df.add_suffix(<span class="string">&#x27;_Y&#x27;</span>) <span class="comment">#컬럼에 접미어 Y 추가</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list를 매핑해 변경하기</span></span><br><span class="line">df.columns = [<span class="string">&#x27;col_one&#x27;</span>, <span class="string">&#x27;col_two&#x27;</span>]</span><br></pre></td></tr></table></figure>


<h3 id="행순서-뒤집기"><a href="#행순서-뒤집기" class="headerlink" title="행순서 뒤집기"></a>행순서 뒤집기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drinks.loc[::-<span class="number">1</span>].head()</span><br></pre></td></tr></table></figure>

<h3 id="reverse-column-order"><a href="#reverse-column-order" class="headerlink" title="reverse column order"></a>reverse column order</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">drinks.loc[:, ::-<span class="number">1</span>].head() <span class="comment"># [start:end:(step)]에 대한 이해 필요</span></span><br><span class="line"><span class="comment"># start, end가 비어있고 step이 -1이기에 순서가 역순으로 바뀜</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="datatype-기준으로-컬럼-선택하기"><a href="#datatype-기준으로-컬럼-선택하기" class="headerlink" title="datatype 기준으로 컬럼 선택하기"></a>datatype 기준으로 컬럼 선택하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drinks.dtypes <span class="comment"># 모든 열의 dtype 확인</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drinks.select_dtypes(include=<span class="string">&#x27;number&#x27;</span>).head() <span class="comment"># dtype이 numeric인 데이터 추출</span></span><br><span class="line"></span><br><span class="line">drinks.select_dtypes(include=[<span class="string">&#x27;number&#x27;</span>, <span class="string">&#x27;object&#x27;</span>, <span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;datetime&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>
<h3 id="문자열-numeric으로-변환하기"><a href="#문자열-numeric으로-변환하기" class="headerlink" title="문자열 numeric으로 변환하기"></a>문자열 numeric으로 변환하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;col_one&#x27;</span>:[<span class="string">&#x27;1.1&#x27;</span>, <span class="string">&#x27;2.2&#x27;</span>, <span class="string">&#x27;3.3&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;col_two&#x27;</span>:[<span class="string">&#x27;4.4&#x27;</span>, <span class="string">&#x27;5.5&#x27;</span>, <span class="string">&#x27;6.6&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;col_three&#x27;</span>:[<span class="string">&#x27;7.7&#x27;</span>, <span class="string">&#x27;8.8&#x27;</span>, <span class="string">&#x27;-&#x27;</span>]&#125;)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># astype()을 활용한 변환</span></span><br><span class="line">df.astype(&#123;<span class="string">&#x27;col_one&#x27;</span>:<span class="string">&#x27;float&#x27;</span>, <span class="string">&#x27;col_two&#x27;</span>:<span class="string">&#x27;float&#x27;</span>&#125;).dtypes</span><br><span class="line"></span><br><span class="line"><span class="comment"># to_numeric을 활용한 변환</span></span><br><span class="line">pd.to_numeric(df.col_three, errors=<span class="string">&#x27;coerce&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># df 전체에 적용(numeric 변환 후 fillna)</span></span><br><span class="line">df = df.apply(pd.to_numeric, errors=<span class="string">&#x27;coerce&#x27;</span>).fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># , 이 포함된 숫자형태의 문자열의 경우 replace사용</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">toInt</span>(<span class="params">string</span>):</span><br><span class="line">    string = <span class="built_in">int</span>(string.replace(<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;&#x27;</span>))</span><br><span class="line">    returen string</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="DF-사이즈-줄이기"><a href="#DF-사이즈-줄이기" class="headerlink" title="DF 사이즈 줄이기"></a>DF 사이즈 줄이기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 메모리 사용정도 확인</span></span><br><span class="line">drinks.info(memory_usage=<span class="string">&#x27;deep&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 컬럼지정을 활용한 데이터 줄이기</span></span><br><span class="line">dtypes = &#123;<span class="string">&#x27;continent&#x27;</span>:<span class="string">&#x27;category&#x27;</span>&#125;</span><br><span class="line">smaller_drinks = pd.read_csv(<span class="string">&#x27;http://bit.ly/drinksbycountry&#x27;</span>, usecols=cols, dtype=dtypes)</span><br><span class="line">smaller_drinks.info(memory_usage=<span class="string">&#x27;deep&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Build-a-DataFrame-from-mulfiple-files-row-wise"><a href="#Build-a-DataFrame-from-mulfiple-files-row-wise" class="headerlink" title="Build a DataFrame from mulfiple files (row-wise)"></a>Build a DataFrame from mulfiple files (row-wise)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="comment"># 정규식,와일드카드 관련문서 참고</span></span><br><span class="line"><span class="comment"># stocks로 시작하는 data폴더 내 모든 csv 파일 </span></span><br><span class="line">stock_files = <span class="built_in">sorted</span>(glob(<span class="string">&#x27;data/stocks*.csv&#x27;</span>))</span><br><span class="line">stock_files</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;data/stocks1.csv&#x27;</span>, <span class="string">&#x27;data/stocks2.csv&#x27;</span>, <span class="string">&#x27;data/stocks3.csv&#x27;</span>] <span class="comment"># 리스트 형태로 반환</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일합치기</span></span><br><span class="line">pd.concat((pd.read_csv(file) <span class="keyword">for</span> file <span class="keyword">in</span> stock_files), ignore_index=<span class="literal">True</span>) <span class="comment"># ignore index는 각 파일의 index를 무시하고 초기화하는 옵션이다.</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Build-a-DataFrame-from-mulfiple-files-column-wise"><a href="#Build-a-DataFrame-from-mulfiple-files-column-wise" class="headerlink" title="Build a DataFrame from mulfiple files (column-wise)"></a>Build a DataFrame from mulfiple files (column-wise)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 축옵션만 넣어주면 된다</span></span><br><span class="line">pd.concat((pd.read_csv(file) <span class="keyword">for</span> file <span class="keyword">in</span> drink_files), axis=<span class="string">&#x27;columns&#x27;</span>).head()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="클립보드에서-df불러오기"><a href="#클립보드에서-df불러오기" class="headerlink" title="클립보드에서 df불러오기"></a>클립보드에서 df불러오기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_clipboard()</span><br><span class="line">df</span><br></pre></td></tr></table></figure>

<h3 id="DF-subsetting-하기"><a href="#DF-subsetting-하기" class="headerlink" title="DF subsetting 하기"></a>DF subsetting 하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># frac으로 원db의 75% 할당</span></span><br><span class="line">movies_1 = movies.sample(frac=<span class="number">0.75</span>, random_state=<span class="number">1234</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 나머지</span></span><br><span class="line">movies_2 = movies.drop(movies_1.index)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="isin을-활용한-DF필터링"><a href="#isin을-활용한-DF필터링" class="headerlink" title="isin을 활용한 DF필터링"></a>isin을 활용한 DF필터링</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># inin을 사용해 특정열에 대해 값에 대한조건을 넣어줄 수 있다.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 포함하고 뽑기</span></span><br><span class="line">movies[movies.genre.isin([<span class="string">&#x27;Action&#x27;</span>,<span class="string">&#x27;Drama&#x27;</span>,<span class="string">&#x27;Western&#x27;</span>])].head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제외하고 뽑기</span></span><br><span class="line">movies[~movies.genre.isin([<span class="string">&#x27;Action&#x27;</span>, <span class="string">&#x27;Drama&#x27;</span>, <span class="string">&#x27;Western&#x27;</span>])].head()</span><br></pre></td></tr></table></figure>

<h3 id="value-counts-를-관측값-구하기"><a href="#value-counts-를-관측값-구하기" class="headerlink" title="value_counts()를 관측값 구하기"></a>value_counts()를 관측값 구하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 우선 카테고리(장르)별 관측값를 구한다</span></span><br><span class="line">counts = movies.genre.value_counts()</span><br><span class="line">counts</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># count에서 상위3개를 구한다.</span></span><br><span class="line">counts.nlargest(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="결측값-처리하기"><a href="#결측값-처리하기" class="headerlink" title="결측값 처리하기"></a>결측값 처리하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 결측값 조건걸기</span></span><br><span class="line">ufo = dropna(thresh = <span class="built_in">len</span>(ufo)*<span class="number">0.9</span>, axis = <span class="string">&#x27;columns&#x27;</span>) <span class="comment"># 90% 이상 값이 있는 컬럼만 유지</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 열별로 결측값의 수 세기</span></span><br><span class="line">ufo.isna().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NA가 하나라도 있는 열 삭제</span></span><br><span class="line">ufo.dropna(axis=<span class="string">&#x27;columns&#x27;</span>).head()</span><br></pre></td></tr></table></figure>
<h3 id="split를-활용한-문자열-나누기"><a href="#split를-활용한-문자열-나누기" class="headerlink" title=".split를 활용한 문자열 나누기"></a>.split를 활용한 문자열 나누기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;name&#x27;</span>:[<span class="string">&#x27;John Arthur Doe&#x27;</span>, <span class="string">&#x27;Jane Ann Smith&#x27;</span>],</span><br><span class="line">                   <span class="string">&#x27;location&#x27;</span>:[<span class="string">&#x27;Los Angeles, CA&#x27;</span>, <span class="string">&#x27;Washington, DC&#x27;</span>]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[[<span class="string">&#x27;first&#x27;</span>, <span class="string">&#x27;middle&#x27;</span>, <span class="string">&#x27;last&#x27;</span>]] = df.name.<span class="built_in">str</span>.split(<span class="string">&#x27; &#x27;</span>, expand=<span class="literal">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<p><img src="/image/output.png"></p>
<h3 id="list를-DF로-변환하기"><a href="#list를-DF로-변환하기" class="headerlink" title="list를 DF로 변환하기"></a>list를 DF로 변환하기</h3><p>이건 자주 쓴다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;col_one&#x27;</span>:[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], <span class="string">&#x27;col_two&#x27;</span>:[[<span class="number">10</span>, <span class="number">40</span>], [<span class="number">20</span>, <span class="number">50</span>], [<span class="number">30</span>, <span class="number">60</span>]]&#125;)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<p><img src="/image/output2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_new = df.col_two.apply(pd.Series) <span class="comment"># apply를 활용한 df 생성</span></span><br><span class="line">df_new</span><br></pre></td></tr></table></figure>
<h3 id="Aggregate-by-multiple-funtions"><a href="#Aggregate-by-multiple-funtions" class="headerlink" title="Aggregate by multiple funtions"></a>Aggregate by multiple funtions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># aggregate를 활용한 요약통계량 산출하기</span></span><br><span class="line">orders.groupby(<span class="string">&#x27;order_id&#x27;</span>).item_price.agg([<span class="string">&#x27;sum&#x27;</span>, <span class="string">&#x27;count&#x27;</span>]).head()</span><br></pre></td></tr></table></figure>

<h3 id="Combine-the-output-of-an-aggregation-by-multiple-funtions"><a href="#Combine-the-output-of-an-aggregation-by-multiple-funtions" class="headerlink" title="Combine the output of an aggregation by multiple funtions"></a>Combine the output of an aggregation by multiple funtions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transform()은 입력된 개체와 동일하게 인덱스된 객체를 반환하며 다중연산에 쓰인다.</span></span><br><span class="line">total_price = orders.groupby(<span class="string">&#x27;order_id&#x27;</span>).item_price.transform(<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform() 관련레퍼런스</span></span><br><span class="line"><span class="comment"># https://kongdols-room.tistory.com/169 </span></span><br></pre></td></tr></table></figure>
<h3 id="loc를-활용한-행열-슬라이싱"><a href="#loc를-활용한-행열-슬라이싱" class="headerlink" title=".loc를 활용한 행열 슬라이싱"></a>.loc를 활용한 행열 슬라이싱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">titanic.describe().loc[<span class="string">&#x27;min&#x27;</span>:<span class="string">&#x27;max&#x27;</span>]</span><br><span class="line"></span><br><span class="line">titanic.describe().loc[<span class="string">&#x27;min&#x27;</span>:<span class="string">&#x27;max&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>:<span class="string">&#x27;Parch&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="계층적-index를-가지는-Series-DF로-변환하기"><a href="#계층적-index를-가지는-Series-DF로-변환하기" class="headerlink" title="계층적 index를 가지는 Series DF로 변환하기"></a>계층적 index를 가지는 Series DF로 변환하기</h3><ul>
<li>부모자식 노드처럼 계층이 있는 인덱스를 가지는 DF를 만들 수있다</li>
<li>잘 쓰진 않는 것 같다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 계층적</span></span><br><span class="line"><span class="comment"># https://nittaku.tistory.com/122</span></span><br><span class="line"></span><br><span class="line">titanic.groupby([<span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>]).Survived.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># changing multiple Series into a DF</span></span><br><span class="line">titanic.groupby([<span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>]).Survived.mean().unstack()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="피벗테이블-만들기"><a href="#피벗테이블-만들기" class="headerlink" title="피벗테이블 만들기"></a>피벗테이블 만들기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">titanic.pivot_table(index=<span class="string">&#x27;Sex&#x27;</span>, columns=<span class="string">&#x27;Pclass&#x27;</span>, values=<span class="string">&#x27;Survived&#x27;</span>, aggfunc=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="comment"># pivot_table에서 aggfunc 파라미터를 &#x27;count&#x27; 으로 바꿀 경우 단순 crosstable을 반환한다</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># margins = True option으로 행열합을 DF에 추가한다</span></span><br><span class="line">titanic.pivot_table(index=<span class="string">&#x27;Sex&#x27;</span>, columns=<span class="string">&#x27;Pclass&#x27;</span>, values=<span class="string">&#x27;Survived&#x27;</span>, aggfunc=<span class="string">&#x27;mean&#x27;</span>,</span><br><span class="line">                    margins=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기"><a href="#bin과-labels를-활용해-수치형-변수-범주형-변수로-바꾸기" class="headerlink" title="bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기"></a>bin과 labels를 활용해 수치형 변수 범주형 변수로 바꾸기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use bin with the labels</span></span><br><span class="line">pd.cut(titanic.Age, bins=[<span class="number">0</span>, <span class="number">18</span>, <span class="number">25</span>, <span class="number">99</span>], labels=[<span class="string">&#x27;child&#x27;</span>, <span class="string">&#x27;young adult&#x27;</span>, <span class="string">&#x27;adult&#x27;</span>]).head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h3 id="DF-표시형식-바꾸기"><a href="#DF-표시형식-바꾸기" class="headerlink" title="DF 표시형식 바꾸기"></a>DF 표시형식 바꾸기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set_option을 통해 표시형식 바꾸기</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, <span class="string">&#x27;&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>)</span><br></pre></td></tr></table></figure>
<h3 id="DF-꾸미기-Style-a-DataFrame"><a href="#DF-꾸미기-Style-a-DataFrame" class="headerlink" title="DF 꾸미기 (Style a DataFrame)"></a>DF 꾸미기 (Style a DataFrame)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">format_dict = &#123;<span class="string">&#x27;Date&#x27;</span>:<span class="string">&#x27;&#123;:%m/%d/%y&#125;&#x27;</span>, <span class="string">&#x27;Close&#x27;</span>:<span class="string">&#x27;$&#123;:.2f&#125;&#x27;</span>, <span class="string">&#x27;Volume&#x27;</span>:<span class="string">&#x27;&#123;:,&#125;&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">df.style.<span class="built_in">format</span>(format_dict) <span class="comment"># 스타일 바꾸기</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="ProfileReport를-통해-DF-구조-통계량-한번에-확인하기"><a href="#ProfileReport를-통해-DF-구조-통계량-한번에-확인하기" class="headerlink" title="ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기"></a>ProfileReport를 통해 DF 구조, 통계량 한번에 확인하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i</span><br><span class="line">mport pandas_profiling</span><br><span class="line">pandas_profiliing.PrifileReport(titanic)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="glob을-사용해-여러-csv파일을-하나의-df로-합치기"><a href="#glob을-사용해-여러-csv파일을-하나의-df로-합치기" class="headerlink" title="glob을 사용해 여러 csv파일을 하나의 df로 합치기"></a>glob을 사용해 여러 csv파일을 하나의 df로 합치기</h3><ul>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line">path = <span class="string">r&#x27;C:\DRO\DCL_rawdata_files&#x27;</span> <span class="comment"># use your path</span></span><br><span class="line">all_files = glob.glob(path + <span class="string">&quot;/*.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">li = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> all_files:</span><br><span class="line">    df = pd.read_csv(filename, index_col=<span class="literal">None</span>, header=<span class="number">0</span>)</span><br><span class="line">    li.append(df)</span><br><span class="line"></span><br><span class="line">frame = pd.concat(li, axis=<span class="number">0</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="DF에-컬럼-추가하기"><a href="#DF에-컬럼-추가하기" class="headerlink" title="DF에 컬럼 추가하기"></a>DF에 컬럼 추가하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dic = &#123;<span class="string">&#x27;Name&#x27;</span>: [<span class="string">&#x27;Jai&#x27;</span>, <span class="string">&#x27;Princi&#x27;</span>, <span class="string">&#x27;Gaurav&#x27;</span>, <span class="string">&#x27;Anuj&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;Height&#x27;</span>: [<span class="number">5.1</span>, <span class="number">6.2</span>, <span class="number">5.1</span>, <span class="number">5.2</span>],</span><br><span class="line">        <span class="string">&#x27;Qualification&#x27;</span>: [<span class="string">&#x27;Msc&#x27;</span>, <span class="string">&#x27;MA&#x27;</span>, <span class="string">&#x27;Msc&#x27;</span>, <span class="string">&#x27;Msc&#x27;</span>]&#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Convert the dictionary into DataFrame</span></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Declare a list that is to be converted into a column</span></span><br><span class="line">address = [<span class="string">&#x27;Delhi&#x27;</span>, <span class="string">&#x27;Bangalore&#x27;</span>, <span class="string">&#x27;Chennai&#x27;</span>, <span class="string">&#x27;Patna&#x27;</span>]</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Using &#x27;Address&#x27; as the column name</span></span><br><span class="line"><span class="comment"># and equating it to the list</span></span><br><span class="line">df[<span class="string">&#x27;Address&#x27;</span>] = address</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="apply-등을-활용한-파생변수-생성하기"><a href="#apply-등을-활용한-파생변수-생성하기" class="headerlink" title="apply 등을 활용한 파생변수 생성하기"></a>apply 등을 활용한 파생변수 생성하기</h3><p>-DF전체에 적용하거나 DF일부에 적용할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="lambda를-활용한-함수-적용"><a href="#lambda를-활용한-함수-적용" class="headerlink" title="lambda를 활용한 함수 적용"></a>lambda를 활용한 함수 적용</h3><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/">http://www.leejungmin.org/post/2018/04/21/pandas_apply_and_map/</a></p>
<p><a target="_blank" rel="noopener" href="https://wikidocs.net/46758">https://wikidocs.net/46758</a></p>
<p><a target="_blank" rel="noopener" href="https://data-make.tistory.com/123">https://data-make.tistory.com/123</a></p>
<h2 id="3-References"><a href="#3-References" class="headerlink" title="3. References"></a>3. References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/">https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=RlIiVeig3hc">https://www.youtube.com/watch?v=RlIiVeig3hc</a></li>
<li><a target="_blank" rel="noopener" href="https://kongdols-room.tistory.com/169">https://kongdols-room.tistory.com/169</a> </li>
<li><a target="_blank" rel="noopener" href="https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/">https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/Preprocessing-sampling-imbalance-data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/Preprocessing-sampling-imbalance-data/" class="post-title-link" itemprop="url">[Sampling]Class Imbalance 다루기</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Preprocessing/" itemprop="url" rel="index"><span itemprop="name">Preprocessing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning

- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk

#참고

https://cinema4dr12.tistory.com/1016?category=515283
https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html

오버 샘플링 렉카
https://wyatt37.tistory.com/10
-->

<h1 id="Dealing-with-Class-Imbalance-클래스-불균형-다루기"><a href="#Dealing-with-Class-Imbalance-클래스-불균형-다루기" class="headerlink" title="Dealing with Class Imbalance(클래스 불균형 다루기)"></a>Dealing with Class Imbalance(클래스 불균형 다루기)</h1><hr>
<!--
오버 샘플링 렉카
https://wyatt37.tistory.com/10

-->

<p><strong>여기서 해결하는 문제</strong></p>
<ul>
<li>Biased predictions</li>
<li>Misleading accuracy</li>
</ul>
<p><strong>보통 고려하는 해결방법</strong></p>
<ul>
<li>데이터 합성(Synthesisis of new minority class instances)</li>
<li>Over-sampling </li>
<li>Under-sampling </li>
<li>class weight 조정하기(상향&#x2F;하향가중치 적용)</li>
<li>cost function 조정</li>
</ul>
<h2 id="Random-Under-Sampling"><a href="#Random-Under-Sampling" class="headerlink" title="Random Under-Sampling"></a>Random Under-Sampling</h2><hr>
<ul>
<li><p><strong>Advantages</strong></p>
<ul>
<li>It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.</li>
</ul>
</li>
<li><p><strong>Disadvantages</strong></p>
<ul>
<li>It can discard potentially useful information which could be important for building rule classifiers.</li>
<li>The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.</li>
</ul>
</li>
</ul>
<h3 id="Tomeck-Links"><a href="#Tomeck-Links" class="headerlink" title="Tomeck Links"></a>Tomeck Links</h3><p>Tomek Links란 두 샘플 사이에 다른 관측치가 없는 경우를 말한다.</p>
<p><img src="https://blog.dominodatalab.com/hubfs/Imported_Blog_Media/machine-learning-challenges-for-automated-prompting-in-smart-homes-23-638-2.jpg"></p>
<p>Tomek Links 방법은 Tomeck links 중에 major에 속하는 데이터포인트를 제거하는 undersampling 기법의 일종이다. 이 경우 데이터 불균형을 해결하면서 클래스 간 거리가 확보 되지만 여전히 정보 자체를 잃어버린다는 단점은 남는다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> TomekLinks</span><br><span class="line"></span><br><span class="line">tomek = TomekLinks(random_state = <span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">X_tm, y_tm = tomek.fit_sample(X, y)</span><br></pre></td></tr></table></figure>

<h2 id="Random-Over-Sampling"><a href="#Random-Over-Sampling" class="headerlink" title="Random Over-Sampling"></a>Random Over-Sampling</h2><hr>
<p>minor class의 데이터를 반복적으로 replace하는 것</p>
<p>단순히 부트스트래핑을 통한 업샘플링의 변형이다.</p>
<ul>
<li><strong>Advantages</strong><ul>
<li>no information loss</li>
</ul>
</li>
<li><strong>Disadvantages</strong><ul>
<li>prone to overfitting due to copying same information</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomOverSampler(random_state=<span class="number">0</span>).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>부트스트래핑을 직접 구현할 경우</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bootstrap</span>(<span class="params">X, n = <span class="literal">None</span>, iterations = <span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="literal">None</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(X)</span><br><span class="line">        X_resampled = np.random.choice(X, size = (iterations, n), replace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_resampled</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="SMOTE-Synthetic-Minority-Oversample-Technique"><a href="#SMOTE-Synthetic-Minority-Oversample-Technique" class="headerlink" title="SMOTE(Synthetic Minority Oversample Technique)"></a>SMOTE(Synthetic Minority Oversample Technique)</h3><p>임의의 마이너 클래스 데이터 포인트와 근접한 마이너 클래스 데이터 포인트 사이에 새로운 데이터 포인트를 생성하는 것</p>
<p><strong>반드시 training set에 대해서만 SMOTE 시행. 이는 data leakage 문제와 관련이 있다.</strong></p>
<p>$$syntetic &#x3D; x_{minor} + u * (x_{nn}-x_{minor})$$</p>
<p>synthetic 합성 값은 minor class의 데이터 포인트와 근접한 minor class의 데이터포인트의 차이에 uniform distribution을 곱한 뒤 minor class의 데이터포인트를 더해준 값이다.</p>
<!--
- Process
  + Identify the feature vectore and its nearest neighbor
  + take the the difference between the two
  + multiply the difference with a random number between 0 and 1
  + identify a new point on the line segment by adding the randomg number to feature vector
  + repeat the process of identified feature vectors

- 절차
-->

<ul>
<li>numpy로 SMOTE 구현하기</li>
</ul>
<p>알고리즘을 구현하는 것 자체는 어렵지 않지만 실제로 작업을 할때는 <code>imblearn</code> 모듈에서 제공하는 SMOTE함수를 사용하는 것이 훨씬 낫다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># SMOTE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">euclidean_dist</span>(<span class="params">x1,x2</span>):</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(<span class="built_in">sum</span>((x1-x2)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_neighbors</span>(<span class="params">X, x, k</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  minor 클레스 데이터에 대해서 k개의 nearest neighbor를 구한다</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">    X_len = <span class="built_in">len</span>(X)</span><br><span class="line">    euclidean_dist = [euclidean_dist(X[i],x) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_len)]</span><br><span class="line">    euclidean_dist = np.sort(euclidean_dist)</span><br><span class="line">    neighbors = euclidean_dist[:k]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> neighbors</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SMOTE</span>(<span class="params">X,k</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  smote algorithm 적용한 합성 데이터 생성</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">    X_len = <span class="built_in">len</span>(X)</span><br><span class="line">    synthetic = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,X_len):</span><br><span class="line">        w = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> w == <span class="number">0</span>:</span><br><span class="line">            w = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        add = get_neighbors(X,X[i],k)</span><br><span class="line">        rand_idx = random.randint(<span class="number">0</span>,k-<span class="number">1</span>)</span><br><span class="line">        add = add[rand_idx]</span><br><span class="line">        </span><br><span class="line">        diff = X[i] - add</span><br><span class="line">        </span><br><span class="line">        synthetic.append(X[i] + w*diff)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(synthetic)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>imblearn을 활용한 target resampling</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"></span><br><span class="line">rs = SMOTE(random_state=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">X_new, y_new = rs.fit_sample(X, y)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline-SMOTE"></a>Borderline-SMOTE</h3><p>: major와 minor를 구분하는 경계선에 있는 Borderline에 속하는 데이터데 대해 SMOTE을 적용하는 것</p>
<p>Minor class data X와 근접한 K개의 데이터포인트의 클래스의 수에 따라 SMOTE 적용 여부를 결정</p>
<ul>
<li><p>0 &lt;&#x3D; K’ &lt;&#x3D; K&#x2F;2 : Safe</p>
</li>
<li><p>K &#x3D; K’ : Noise</p>
</li>
<li><p>K&#x2F;2 &lt; K’ &lt; K : Danger : 이 경우에 SMOTE을 적용한다.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Borderline-SMOTE</span></span><br><span class="line"></span><br><span class="line">bsmote = BorderlineSMOTE(random_state = <span class="number">1234</span>, k_neighbors=<span class="number">3</span>, m_neighbors=<span class="number">10</span>)</span><br><span class="line">X, y_new = bsmote.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Original_y %s&#x27;</span> % Counter(y))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;BorderlineSMOTE_y %s&#x27;</span> % Counter(y_new))</span><br></pre></td></tr></table></figure>

<h3 id="ADASYN"><a href="#ADASYN" class="headerlink" title="ADASYN"></a>ADASYN</h3><p>: Adaptive Synthetic Sampling</p>
<ul>
<li>가중치를 적용해 SMOTE을 다르게 진행</li>
<li>인접한 major class의 비율에 따라 SMOTE을 다르게 적용하는 것</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = ADASYN(random_state=<span class="number">0</span>).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="모델링과-평가-단계에서-Class-Imbalance-다루기"><a href="#모델링과-평가-단계에서-Class-Imbalance-다루기" class="headerlink" title="모델링과 평가 단계에서 Class Imbalance 다루기"></a>모델링과 평가 단계에서 Class Imbalance 다루기</h2><hr>
<p>샘플링 단계가 아니라 모델링과 평가단계에서 Class Imbalance 문제를 처리한다.</p>
<h3 id="Change-the-performance-metric"><a href="#Change-the-performance-metric" class="headerlink" title="Change the performance metric"></a>Change the performance metric</h3><p>class weight에 영향을 덜 받게끔 평가지표 자체를 바꿀 수 있다.</p>
<p>다른 방법보다 품이 덜 들어서 의외로 괜찮은 방법이다.</p>
<ul>
<li><p><strong>Confusion Matrix</strong>: a table showing correct predictions and types of incorrect predictions.</p>
</li>
<li><p><strong>Precision</strong>: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.</p>
</li>
<li><p><strong>Recall</strong>: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.</p>
</li>
<li><p><strong>F1</strong>: Score: the weighted average of precision and recall.</p>
</li>
<li><p><strong>Area Under ROC Curve (AUROC)</strong>: AUROC represents the likelihood of your model distinguishing observations from two classes.<br>In other words, if you randomly select one observation from each class, what’s the probability that your model will be able to “rank” them correctly?</p>
</li>
</ul>
<h3 id="Penalize-Algorithms-class-weight"><a href="#Penalize-Algorithms-class-weight" class="headerlink" title="Penalize Algorithms(class_weight)"></a>Penalize Algorithms(class_weight)</h3><ul>
<li>Cost-Sensitive Training</li>
<li>minority class로의 오분류에 대한 패널티를 크게 만듦</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># class weight </span></span><br><span class="line">svc_model = SVC(class_weight=<span class="string">&#x27;balanced&#x27;</span>, probability=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">svc_model.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">svc_predict = svc_model.predict(x_test)<span class="comment"># check performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ROCAUC score:&#x27;</span>,roc_auc_score(y_test, svc_predict))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy score:&#x27;</span>,accuracy_score(y_test, svc_predict))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 score:&#x27;</span>,f1_score(y_test, svc_predict))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>sklearn를 활용한 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Classweight  계산 </span></span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight</span><br><span class="line">classes = np.unique(y_train)</span><br><span class="line">weights = compute_class_weight(class_weight=<span class="string">&#x27;balanced&#x27;</span>, classes=classes, y=y_train)</span><br><span class="line">class_weights = <span class="built_in">dict</span>(<span class="built_in">zip</span>(classes, weights)) <span class="comment"># 모델의 인수로 들어간다.</span></span><br></pre></td></tr></table></figure>

<ul>
<li>R 을 활용한 구현</li>
</ul>
<p>대출연체가 minor이기에 연제에 대한 가중치를 1&#x2F;p로 적용.<br>p는 연체의 확률값</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wt 가중치 벡터 만들기</span></span><br><span class="line">wt <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>loan_all_data<span class="operator">$</span>outcome <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="number">1</span><span class="operator">/</span>mean<span class="punctuation">(</span>loan_all_data<span class="operator">$</span>outcome <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">clf <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio<span class="operator">+</span>purpose_<span class="operator">+</span>home_<span class="operator">+</span>emp_len<span class="punctuation">,</span></span><br><span class="line">           data<span class="operator">=</span> loan_all_data<span class="punctuation">,</span></span><br><span class="line">           weight <span class="operator">=</span>wt<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&quot;binomial&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Novelty-Detection-단일클래스-분류기법"><a href="#Novelty-Detection-단일클래스-분류기법" class="headerlink" title="Novelty Detection(단일클래스 분류기법)"></a>Novelty Detection(단일클래스 분류기법)</h3><ul>
<li>단일클래스 분류기법</li>
<li>Minor를 무시하고 Major class 에 속하는 데이터를 결정하는 일종의 바운더리를 생성하고 그 바운더리에 들어가냐 들어가지 않냐의 boolen으로 클래스를 결정한다.</li>
<li>outlier 를 판별하는 알고리즘</li>
</ul>
<h2 id="Reference-amp-annotation"><a href="#Reference-amp-annotation" class="headerlink" title="Reference &amp; annotation"></a><strong>Reference &amp; annotation</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18">https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18</a></li>
<li><strong>Class weight를 적용하는 방식이 minor를 oversampling하거나 major를 undersampling하는 방법을 대체할 수 있다.(Practical Statistics for Data Scientist)</strong></li>
<li><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/">https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/programming/Programming-Python-Generator/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/programming/Programming-Python-Generator/" class="post-title-link" itemprop="url">[Python]Iterator,Generator,yield에 대한 정리</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-09 14:13:13" itemprop="dateModified" datetime="2023-04-09T14:13:13+09:00">2023-04-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Programming/" itemprop="url" rel="index"><span itemprop="name">Programming</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<p><strong>Iterator,Generator,yield에 대한 정리</strong></p>
<hr>
<h3 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h3><ul>
<li><strong>Iterators are objects that allow you to traverse through all the elements of a collection and return one element at a time.</strong></li>
<li>iterator는 iterable로 생성되는 값을 순서대로 꺼낼 수 있는 객체이다. </li>
<li>iter(collections) : returns unmodified iterator</li>
<li>iter(<function>, to_exclusive) : A sequence of return values until ‘to_exclusive’</li>
<li>next(<iter>,default) :Raises StopIteration or returns ‘default’ on end.</li>
<li><list> &#x3D; list(<iter>) : Return a list of iterator’s remaining elements</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">temp = (<span class="string">&quot;apple&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;cherry&quot;</span>)</span><br><span class="line">myit = <span class="built_in">iter</span>(temp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(myit))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(myit))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(myit))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">30</span>]: iv = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: io = <span class="built_in">iter</span>(iv)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    ...:     <span class="keyword">try</span>:</span><br><span class="line">    ...:</span><br><span class="line">    ...:         item = <span class="built_in">next</span>(io)</span><br><span class="line">    ...:         <span class="built_in">print</span>(item)</span><br><span class="line">    ...:     <span class="keyword">except</span> StopIteration:</span><br><span class="line">    ...:         <span class="keyword">break</span></span><br><span class="line">    ...:</span><br><span class="line">    ...:</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools"><a href="#itertools" class="headerlink" title="itertools"></a>itertools</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count, repeat, cycle, chain, islice</span><br></pre></td></tr></table></figure>

<ul>
<li><code>count</code> :  count(시작, [step]) 의 함수로 시작 숫자부터 step만큼(없으면 1) 씩 무한히 증가하는 generator 반환</li>
<li><code>islice</code> : islice(iterable객체, [시작], 정지[,step])의 함수로, iterable한 객체를 특정 범위로 슬라이싱하고 iterator로 반환.</li>
<li><code>chain</code> : chain(<strong>iterable</strong>)은 iterable한 객체들을 인수로 받아 하나의 iterator로 반환</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># chain</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line">e1 = [<span class="string">&#x27;Happiness&#x27;</span>,<span class="string">&#x27;Caring&#x27;</span>,<span class="string">&#x27;Energy&#x27;</span>]</span><br><span class="line">e2 = [<span class="string">&#x27;Fear&#x27;</span>,<span class="string">&#x27;Hurt&#x27;</span>,<span class="string">&#x27;Tired&#x27;</span>]</span><br><span class="line">emotions = chain(e1, e2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">next</span>(emotions) &gt;&gt;&gt; <span class="string">&#x27;Happiness&#x27;</span></span><br><span class="line"><span class="built_in">next</span>(emotions) &gt;&gt;&gt; <span class="string">&#x27;Caring&#x27;</span></span><br><span class="line"><span class="built_in">next</span>(emotions) &gt;&gt;&gt; <span class="string">&#x27;Energy&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/python-itertools/">itertools</a></li>
<li><a target="_blank" rel="noopener" href="https://realpython.com/python-itertools/">https://realpython.com/python-itertools/</a></li>
<li><a target="_blank" rel="noopener" href="https://hamait.tistory.com/803">https://hamait.tistory.com/803</a></li>
</ul>
<p><strong>itertools.product를 활용한 이중 반복문 변형</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기존 반복문</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> i_ex:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> j_ex:</span><br><span class="line">        <span class="built_in">print</span>(i,j)</span><br><span class="line"></span><br><span class="line"><span class="comment"># itertools활용</span></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(i_ex, j_ex):</span><br><span class="line">    <span class="built_in">print</span>(i, j)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><ul>
<li><p>Any function that contains a yield statement returns a generator.</p>
</li>
<li><p>Generators and iterators are interchangeable.</p>
</li>
<li><p><strong>Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly</strong></p>
</li>
<li><p>Lazy-evaluation : 값을 미리 생성하여 메모리에 저장하고 있는게 아니며, 요청이 있을 때마다  함수를 실행하고 값을 공급(yield)해 줌</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: my_gen = (x*x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="built_in">type</span>(my_gen)</span><br><span class="line">Out[<span class="number">2</span>]: generator</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> my_gen:</span><br><span class="line">   ...:     <span class="built_in">print</span>(i)</span><br><span class="line">   ...:</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>It is just the same except you used () instead of []. BUT, you cannot perform for i in generator a second time since <strong>generators can only be used once</strong>: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one</p>
<h4 id="yield-form"><a href="#yield-form" class="headerlink" title="yield form"></a>yield form</h4><ul>
<li><code>yield</code>는 <code>return</code>과 유사하지만 generator를 반환한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: <span class="keyword">def</span> <span class="title function_">gen_count</span>(<span class="params">start,step</span>):</span><br><span class="line">   ...:     <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">   ...:         <span class="keyword">yield</span> start</span><br><span class="line">   ...:         start += step</span><br><span class="line">   ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: counter = gen_count(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">next</span>(counter)</span><br><span class="line">Out[<span class="number">8</span>]: <span class="number">10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: <span class="built_in">next</span>(counter)</span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">12</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: <span class="built_in">next</span>(counter)</span><br><span class="line">Out[<span class="number">10</span>]: <span class="number">14</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: <span class="built_in">next</span>(counter)</span><br><span class="line">Out[<span class="number">11</span>]: <span class="number">16</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>yield from a</code>를 통해 <code>iterable</code>의 전체 요소들을 반환할 수 있다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">three_generator</span>():</span><br><span class="line"><span class="meta">... </span>    a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">yield</span> <span class="keyword">from</span> a</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gen = three_generator()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(gen)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h2 id="References-amp-annotation"><a href="#References-amp-annotation" class="headerlink" title="References &amp; annotation"></a><strong>References &amp; annotation</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do">핵심 stackoverflow ref</a></li>
<li>Python Comprehensive Cheat Sheet</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-US-knn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-US-knn/" class="post-title-link" itemprop="url">[Unsupervised Learning]KNN을 활용한 분류</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

- ML
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Data Extraction & Wrangling


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<hr>
<h2 id="간단한-컨셉"><a href="#간단한-컨셉" class="headerlink" title="간단한 컨셉"></a>간단한 컨셉</h2><p><strong>KNN</strong></p>
<ul>
<li><p>새로운 데이터에 대해 기존 데이터 가운데 가장 가까운 K개 이웃의 정보로 새로운 데이터를 예측하는 방법론.</p>
</li>
<li><p>회귀문제와 분류문제 해결에 모두 사용되는 지도학습</p>
</li>
<li><p>하이퍼파라미터는 기본적으로 거리측정방법과 탐색할 이웃 수 2가지 이다.</p>
</li>
<li><p><strong>K(이웃)을 적게 사용하면 모델 복잡도가 높아지고 많이 사용하면 복잡도가 낮아진다(K의 수를 늘릴수록 결정경계가 부드러워진다.).</strong></p>
</li>
<li><p>KNN은 회귀분석에도 쓰이며 여러개의 K를 사용할 경우 이웃들의 종속변수의 평균이 예측된다.</p>
</li>
<li><p>거리측정방법</p>
<ul>
<li>유클리디안 거리 : 데이터포인트 사이 직선 최단거리</li>
<li>마할라노비스 거리 : 공분산을 고려해 거리를 계산한다. 변수간 상관관계를 고려한 거리지표.</li>
<li>맨해튼 거리 : 각 좌표축 방향으로만 이동할 경우 계산된다. 격자모양의 길을 따라간다.</li>
</ul>
</li>
<li><p>주의점</p>
<ul>
<li>기본적으로 거리기반이기 때문에 KNN을 돌리기 전 반드시 변수를 정규화 해야 한다.</li>
<li>불균형 데이터의 분류문제를 풀 경우 학습데이터 범주의 사전확률(Prior Probability)를 고려해야핟다.</li>
</ul>
</li>
<li><p>장단점</p>
<ul>
<li>장점 : 학습 데이터 내 노이즈의 영향들 덜받음. 학습데이터가 많으면 효과적 </li>
<li>단점 : 어떤 거리척도가 분석에 적랍한지 불분명. 계산시간이 오래 걸림</li>
</ul>
</li>
</ul>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li>유클라디안 거리를 활용한 KNN 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">euclidean_distance</span>(<span class="params">x1,x2</span>):</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>((x1-x2)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KNN</span>:</span><br><span class="line"></span><br><span class="line">    self __init__(self, k=<span class="number">3</span>):</span><br><span class="line">        self.k = k </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>): <span class="comment"># triain sample and label</span></span><br><span class="line">        self.X_train = X</span><br><span class="line">        self.y_train = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        predicted_labels = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">return</span> np.array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        1. 거리 계산하기</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        2. k nearest sample</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        3. majority vote, get most common class</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        distances = [euclidean_distance(x,x_train) <span class="keyword">for</span> x_train <span class="keyword">in</span> X_train]</span><br><span class="line"></span><br><span class="line">        k_indices = np.argsort(distances)[:self.k]</span><br><span class="line">        k_nearest_labels = [self.y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> k_indices]</span><br><span class="line"></span><br><span class="line">        most_common = Counter(k_nearest_labels).most_common(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> most_common[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="분류문제-풀이"><a href="#분류문제-풀이" class="headerlink" title="분류문제 풀이"></a>분류문제 풀이</h2><ul>
<li>iris 데이터를 바탕으로 분류문제 풀이</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cmap = ListedColormap([<span class="string">&quot;#FF0000&quot;</span>, <span class="string">&quot;#00FF00&quot;</span>, <span class="string">&quot;#0000FF&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    accuracy = np.<span class="built_in">sum</span>(y_true == y_pred) / <span class="built_in">len</span>(y_true)</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">        X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1234</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">clf = KNN(k=k)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">predictions = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNN classification 정확도&quot;</span>, accuracy(y_test, predictions))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$KNN</span> classification accuracy 1.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>sklearn에서도 knn 분류기가 구현되어 있다.<ul>
<li>irsis data load까지는 동일하게 진행된다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeiborsClassifier</span><br><span class="line"></span><br><span class="line">clf = KNeiborsClassifier(n_neighbors =<span class="number">3</span>)</span><br><span class="line">clf.fit()</span><br><span class="line"></span><br><span class="line">pred = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNN classification 정확도&quot;</span>, clf.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="K값과-모델-복잡도의-관계"><a href="#K값과-모델-복잡도의-관계" class="headerlink" title="K값과 모델 복잡도의 관계"></a>K값과 모델 복잡도의 관계</h2><ul>
<li>위스콘신 유방암데이터로 구현한다.</li>
<li>k의 수가 1개일 때는(적을 때는) train 데이터에 대해서만 예측력이 높고 test에서는 낮은 과적합된 모습을 보인다.</li>
<li>k의 수가 많을 수록 모델이 단순해지고 train 데이터의 정확도는 줄어든다.</li>
<li>k의 수가 10개일 때는 모델이 너무 단순해 train과 test모두에서 예측력이 낮은 모습을 보인다.</li>
<li>중간정도의 범위에서 k의 수를 선정할 필요가 있다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train , X_test , y_train , y_test = train_test_split(cancer.data,</span><br><span class="line">                                                       cancer.target,</span><br><span class="line">                                                       stratify = cancer.target, <span class="comment"># stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해준다. (한 쪽에 쏠려서 분배되는 것을 방지)</span></span><br><span class="line">                                                       random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">train_acc = []</span><br><span class="line">test_acc = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k_indices = <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_indices:</span><br><span class="line">    clf = KNeiborsClassifier(n_neighbors=k)</span><br><span class="line">    clf.fit()</span><br><span class="line">    train_acc.append(clf.score(X_train,y_train))</span><br><span class="line">    test_acc.append(clf.score(X_test,y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(neighbors_settings, training_accuracy, label=<span class="string">&quot;훈련 정확도&quot;</span>)</span><br><span class="line">plt.plot(neighbors_settings, test_accuracy, label=<span class="string">&quot;테스트 정확도&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;정확도&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;n_neighbors&quot;</span>)</span><br><span class="line">plt.legend(</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><img src="https://tensorflowkorea.files.wordpress.com/2017/06/2-7.png?w=1024"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/collections.html">https://docs.python.org/3/library/collections.html</a></li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.blog/%EA%B0%9C%EC%A0%95%ED%8C%90-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">파이싼 라이브러리를 활용한 머신러닝</a></li>
<li><a target="_blank" rel="noopener" href="https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/">https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Main-Decision-Tree-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Main-Decision-Tree-Algorithms/" class="post-title-link" itemprop="url">[Tree]주요 Decision Tree 알고리즘</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<p>주요 의사결정트리 알고리즘 4개에 대해 간단히 살펴보자.</p>
<hr>
<h2 id="Main-Decision-Tree-Algorithms"><a href="#Main-Decision-Tree-Algorithms" class="headerlink" title="Main Decision Tree Algorithms"></a>Main Decision Tree Algorithms</h2><h3 id="CHAID"><a href="#CHAID" class="headerlink" title="CHAID"></a><strong>CHAID</strong></h3><p>The Chi-squared Automatic Interaction Detection (CHAID) is one of the oldest DT algorithms methods that produces multiway DTs (splits can have more than two branches) suitable for classification and regression tasks. When building Classification Trees (where the dependent variable is categorical in nature), CHAID relies on the Chi-square independence tests to determine the best split at each step. Chi-square tests check if there is a relationship between two variables, and are applied at each stage of the DT to ensure that each branch is significantly associated with a statistically significant predictor of the response variable.<br><strong>In other words, it chooses the independent variable that has the strongest interaction with the dependent variable.</strong></p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a><strong>CART</strong></h3><p>CART is a DT algorithm that produces binary Classification or Regression Trees, depending on whether the dependent (or target) variable is categorical or numeric, respectively. It handles data in its raw form (no preprocessing needed), and can use the same variables more than once in different parts of the same DT, which may uncover complex interdependencies between sets of variables.</p>
<p><strong>Prepare Data for CART</strong></p>
<ul>
<li><p>The <strong>splitting of numerical features</strong> can be performed by sorting the features in the ascending order and trying each value as the threshold point and calculating the information gain for each value as the threshold. Finally, if that value obtained is equal to the threshold which gives the maximum I.G value then hurray..!!</p>
</li>
<li><p>Feature scaling(column standardization) not necessary to perform in decision trees. However, it helps with data visualization&#x2F;manipulation and might be useful if you intend to compare performance with other data or other methods like SVM.</p>
</li>
<li><p>In order to handle categorical features in Decision trees, we must never perform one hot encoding on a categorical variable even if the categorical variables are nominal since most of the libraries can handle categorical variables automatically. we can still assign a number for each variable if desired.</p>
</li>
<li><p>If height or depth of the tree is exactly one then such a tree is called as a decision stump.</p>
</li>
<li><p>Imbalanced class does have a detrimental impact on the tree’s structure so it can be avoided by either using upsampling or by using downsampling depending upon the dataset.</p>
</li>
<li><p>Apart from skewed classes, high dimensionality can also have an adverse effect on the structure of the tree if dimensionality is very high that means we have a lot of features which means that to find the splitting criterion on each node it will consume a lot of time.</p>
</li>
<li><p>Outliers also impact the tree’s structure as the depth increases the chance of outliers in the tree increases.</p>
</li>
<li><p>Feature importance can be determined by calculating the normalized sum at every level as we have t reduce the entropy and we then select the feature that helps to reduce the entropy by the large margin. so for whichever feature the normalized sum is highest, we can then think of it as the most important feature. similarly, feature which has the second highest normalized sum can be thought of as a second important feature.</p>
</li>
</ul>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a><strong>ID3</strong></h3><p>The Iterative Dichotomiser 3 (ID3) is a DT algorithm that is mainly used to produce Classification Trees. Since it hasn’t proved to be so effective building Regression Trees in its raw data, ID3 is mostly used for classification tasks (although some techniques such as building numerical intervals can improve its performance on Regression Trees).</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a><strong>C4.5</strong></h3><p>C4.5 is the successor of ID3 and represents an improvement in several aspects. C4.5 can handle both continuous and categorical data, making it suitable to generate Regression and Classification Trees. Additionally, it can deal with missing values by ignoring instances that include non-existing data.</p>
<p>Unlike ID3 (which uses Information Gain as splitting criteria), C4.5 uses Gain Ratio for its splitting process. Gain Ratio is a modification of the Information Gain concept that reduces the bias on DTs with huge amount of branches, by taking into account the number and size of the branches when choosing an attribute. Since Information Gain shows an unfair favoritism towards attributes with many outcomes, Gain Ratio corrects this trend by considering the intrinsic information of each split (it basically “normalizes” the Information Gain by using a split information value). This way, the attribute with the maximum Gain Ratio is selected as the splitting attribute.<br>Additionally, C4.5 includes a technique called windowing, which was originally developed to overcome the memory limitations of earlier computers. Windowing means that the algorithm randomly selects a subset of the training data (called a “window”) and builds a DT from that selection. This DT is then used to classify the remaining training data, and if it performs a correct classification, the DT is finished. Otherwise, all the misclassified data points are added to the windows, and the cycle repeats until every instance in the training set is correctly classified by the current DT. This technique generally results in DTs that are more accurate than those produced by the standard process due to the use of randomization, since it captures all the “rare” instances together with sufficient “ordinary” cases.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14">https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Random_Forest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Random_Forest/" class="post-title-link" itemprop="url">[Tree]Random Forest의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>Bagging</strong> : 랜덤 복원추출을 통해 샘플링한 데이터를 바탕으로 피팅한 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 것. </li>
<li><strong>weak learner</strong> : 서로 독립적으로 만들어지며 <strong>상관이 낮은</strong> 약한 분류기.</li>
<li><strong>Random Subspace Method</strong> : Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용</li>
<li><strong>Random Forest</strong> : 여러 <code>week learner</code>들을 합쳐서 하나의 트리를 만드는 것. boosting에 비해 과적합이 덜되는 경향이 있다.</li>
<li><strong>Bootstrap</strong> : datapoint가 n개일 때 n의 크기를 가지는 표본을 복원추출하는 것. 기본적으로 데이터가 편중되지 않게끔 한다.</li>
<li><strong>OOB</strong> : Out of Bag. 부트스트랩에서 추출되지 않는 36.8% 의 샘플.</li>
</ul>
<hr>
<blockquote>
<p>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</p>
</blockquote>
<p><strong>랜덤포레스트의 핵심적인 컨셉은 위의 인용처럼 서로 상관이 낮은 약한 분류기들을을 합쳐서 강력한 하나의 모델을 만드는 것이다.</strong></p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><img src="https://www.researchgate.net/profile/Xiaogang_He2/publication/309031320/figure/fig1/AS:422331542708224@1477703094069/Schematic-of-the-RF-algorithm-based-on-the-Bagging-Bootstrap-Aggregating-method.png" width="700" />

<p>배깅의 핵심적인 목표는 <strong>의사결정 트리 사이의 분산을 줄이는 것이다.</strong> . Bagging은 기본적으로 모델의 bias를 상승시키지 않으면서 variance를 줄이는 방법이다.이를 위해 배깅에서는 <code>부트스트래핑</code>을 통한 데이터의 서브셋을 각각 학습시켜 독립적이고 서로 상관이 낮은 여러 기본모델들을 만든다. 이렇게 만든 여러 기본모델들의 앙상블이 <code>랜덤 포레스트</code>이다. <code>랜덤포레스트</code>는 한 트리의 오류가 전파되지 않아서 노이즈(이상치)에 강하며 따라서 일반적인 의사결정나무의 약점인 과적합에 강한 모습을 보인다.</p>
<h3 id="Random-Subspace-method"><a href="#Random-Subspace-method" class="headerlink" title="Random Subspace method"></a>Random Subspace method</h3><ul>
<li><p>Bagging과 유사하지만 Bagging에 추가로 feature를 일부 선택해서 분할하는 것. Random Forest에서 사용.</p>
</li>
<li><p>train dataset의 feature가 1개만 있다면 랜덤포레스트와 배깅의 알고리즘이 동일해진다.</p>
</li>
<li><p><strong>feature를 일부 선택해서 분할하는 이유는 설명력이 높은 feature가 모든 <code>weak learner</code>에서 선택되어 모델 간의 예측값의 상관이 높아지는 것을 방지하기 위함이다.</strong></p>
</li>
<li><p>기본모델 생성시 <strong>특성 m개 중 일부분 k개의 특성을 선택(sampling)한다</strong> </p>
</li>
<li><p>k개에서 최적의(information gain이 가장 높은) 특성을 찾아내어 분할함. k개는 일반적으로 $log_2 m$ 를 사용.</p>
</li>
<li><p>$\sqrt{m}$을 k로 활용할 수도 있다.</p>
</li>
<li><p>k가 작아질 수록 각 트리들이 모두 다르게 구성되어 예측력이 향상.</p>
</li>
<li><p>k가 너무 작아지면 가중치가 적은 feature가 상위노드에 들어가 불순도가 높아진다.</p>
</li>
<li><p>k가 너무 커지면 각 트리간 상관이 높아짐(트리들이 비슷해짐).예측력이 하락한다 </p>
</li>
<li><p>서로 상관이 높은 feature가 많은 경우 k를 적게 하는 것이 유리하다.**</p>
</li>
<li><p>트리의 수가 증가해도 과적합되지 않는다. 일정 수준이상으로 많아지면 error rate는 안정되는 경향을 보인다.<br><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/caLvgA/btraSXTXHT3/T0aBmdkrhHd3FCKGsSWN9k/img.png"></p>
</li>
</ul>
<p><strong>배깅과 Random subspave method의 비교</strong></p>
<ul>
<li>bagging: it is better when the training samples are sparse(결측값이 많은 경우)</li>
<li>Random subspace method: it is better when the classes are compact and the boundaries are smooth.</li>
</ul>
<h3 id="Random-Forest-알고리즘"><a href="#Random-Forest-알고리즘" class="headerlink" title="Random Forest 알고리즘"></a>Random Forest 알고리즘</h3><ul>
<li>m개의 feature와 n개의 데이터포인트가 있는 학습데이터에서 부트스트래핑을 통해 서브셋을 추출한다.</li>
<li>m개의 feature에서 각각 k개의 feature를 추출한 서브셋을 가지고 학습해 약한 분류기를 여러개 만든다. </li>
<li>각각의 약한 분류기로 결과를 예측한다.</li>
<li>각 분류기의 예측결과를 모아 최종결과를 도출한다.<ul>
<li>분류 문제일 경우 다수결을 통해 최종 결과를 도출한다</li>
<li>회귀 문제일 경우 평균을 통해 최종결과를 도출한다.</li>
</ul>
</li>
</ul>
<h3 id="Random-Forest-주요-hyperparameter"><a href="#Random-Forest-주요-hyperparameter" class="headerlink" title="Random Forest 주요 hyperparameter"></a>Random Forest 주요 hyperparameter</h3><p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/">Randomforest Hyperparameter</a><br>sklearn에서 제공하는 하이퍼파라미터 기준으로 정리</p>
<ul>
<li>max_featuers : 기본트리에 사용되는 feature의 수. default는 전부 사요하는 것.</li>
<li>n_estimators : 기본트리 수. 커질수록 퍼포먼스가 좋아지지만 학습시간이 오래걸린다.</li>
<li>min_sample_leaf : 리프노드 샘플의 최소값. 작을 수록 학습데이터의 이상치를 잡기 어려워진다. 보통 50이상으로 놓는다.</li>
<li>oob_score : boolen 값. cross validation이랑 비슷. oob sample을 바탕으로 평가를 수행하는 것.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 보통 고려하는 것들</span></span><br><span class="line">&#123;<span class="string">&#x27;bootstrap&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line"> <span class="string">&#x27;criterion&#x27;</span>: <span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>, <span class="comment"># depth가 3일때까지만 split</span></span><br><span class="line"> <span class="string">&#x27;max_features&#x27;</span>: <span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;max_leaf_nodes&#x27;</span>: <span class="number">4</span>, <span class="comment"># leaf node가 4개일때까지만 split</span></span><br><span class="line"> <span class="string">&#x27;min_impurity_decrease&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"> <span class="string">&#x27;min_impurity_split&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line"> <span class="string">&#x27;min_samples_leaf&#x27;</span>: <span class="number">3</span>, <span class="comment"># 생성될 노드들의 샘플 수가 3개 이상이여만 split </span></span><br><span class="line"> <span class="string">&#x27;min_samples_split&#x27;</span>: <span class="number">5</span>, <span class="comment"># 5개 이상의 샘플만 split</span></span><br><span class="line"> <span class="string">&#x27;min_weight_fraction_leaf&#x27;</span>: <span class="number">0.0</span>,</span><br><span class="line"> <span class="string">&#x27;n_estimators&#x27;</span>: <span class="number">10</span>,</span><br><span class="line"> <span class="string">&#x27;n_jobs&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;oob_score&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line"> <span class="string">&#x27;random_state&#x27;</span>: <span class="number">42</span>,</span><br><span class="line"> <span class="string">&#x27;verbose&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">&#x27;warm_start&#x27;</span>: <span class="literal">False</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Random-Forest-장단점"><a href="#Random-Forest-장단점" class="headerlink" title="Random Forest 장단점"></a>Random Forest 장단점</h3><p><strong>장점</strong></p>
<ul>
<li>과적합에 강하다.</li>
<li>이상치에 크게 영향받지 않는다.</li>
<li>Scaling이 필요가 없다.</li>
<li>결측값에 크게 영향받지 않는다.</li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li>고차원의 희소한 데이터에 대해 성능이 저하된다.</li>
<li>training 속도 느림(메모리 소모)</li>
<li>개별 트리 분석이 어럽다.</li>
</ul>
<h3 id="Random-Forest-구현"><a href="#Random-Forest-구현" class="headerlink" title="Random Forest 구현"></a>Random Forest 구현</h3><ul>
<li>sklearn을 활용한 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">4</span>,</span><br><span class="line">                            n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifier(max_depth=<span class="number">2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">RandomForestClassifier(...)</span><br><span class="line"><span class="built_in">print</span>(clf.predict([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li>numpy를 활용한 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .decision_tree <span class="keyword">import</span> DecisionTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bootstrap_sample</span>(<span class="params">X, y</span>):</span><br><span class="line">    n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">    idxs = np.random.choice(n_samples, n_samples, replace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X[idxs], y[idxs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">most_common_label</span>(<span class="params">y</span>):</span><br><span class="line">    counter = Counter(y)</span><br><span class="line">    most_common = counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> most_common</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomForest</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_trees=<span class="number">10</span>, min_samples_split=<span class="number">2</span>, max_depth=<span class="number">100</span>, n_feats=<span class="literal">None</span></span>):</span><br><span class="line">        self.n_trees = n_trees</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.n_feats = n_feats</span><br><span class="line">        self.trees = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        self.trees = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_trees):</span><br><span class="line">            tree = DecisionTree(</span><br><span class="line">                min_samples_split=self.min_samples_split,</span><br><span class="line">                max_depth=self.max_depth,</span><br><span class="line">                n_feats=self.n_feats,</span><br><span class="line">            )</span><br><span class="line">            X_samp, y_samp = bootstrap_sample(X, y)</span><br><span class="line">            tree.fit(X_samp, y_samp)</span><br><span class="line">            self.trees.append(tree)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        tree_preds = np.array([tree.predict(X) <span class="keyword">for</span> tree <span class="keyword">in</span> self.trees])</span><br><span class="line">        tree_preds = np.swapaxes(tree_preds, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        y_pred = [most_common_label(tree_pred) <span class="keyword">for</span> tree_pred <span class="keyword">in</span> tree_preds]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_pred)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a></li>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-SVM/" class="post-title-link" itemprop="url">[SVM]서포트벡터머신의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/Supervised-Learning/" itemprop="url" rel="index"><span itemprop="name">Supervised Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning



- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<!--
진짜 ref
https://excelsior-cjh.tistory.com/165


진짜 가장중요한 ref
https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin

-->

<hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>결정 경계</strong>: 서로 다른 두 데이터를 구분하는 기준선(threshold). 선형 SVM의 결정 경계는 데이터 feature의 n차원의 초평면(hyperplane)이다.</li>
<li><strong>초평면(hyperplane)</strong> : flat affine subspace of p-1 (p는 데이터의 차원) </li>
<li><strong>Support Vector</strong> : 결정 경계와 가장 가까운 데이터 포인트. Soft Margin의 끝에 있는 데이터포인트</li>
<li><strong>Margin</strong> : 결정경계와 Support Vector사이의 거리(threshold와 데이터포인트 사이의 최소거리)</li>
<li><strong>Support Vector Machine</strong> : 마진을 최대화 하는 결정 경계를 찾는 알고리즘.<ul>
<li><strong>Soft Margin</strong> : <strong>Allow misclassification</strong>. outlier의 오분류를 허용함으로써 과적합으로 인한 문제(low bias, high variance) 를 완화시키려고 하는 것. Soft Margin은 오분류를 허용한 경우의 Margin을 뜻한다.</li>
<li><strong>Hard Margin</strong>: 결정경계면이 선형이며 오분류를 허용하지 않는 Margin. 오차항이 없는 경우의 soft margin 을 hard margin이라 한다.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><hr>
<ul>
<li>데이터가 p차원일 경우 분류기(Support Vector Classifier)는 p-1차원의 subspace에 존재한다. 이를 hyperplane이라 한다.</li>
<li>기본적인 컨셉은 margin을 최대화 하는 결정경계를 찾는 것이다.</li>
<li>margin을 크게 할 수록 일반화 성능이 좋아진다.(과적합이 덜 된다.)</li>
<li>마진이 커질경우 일반화 성능이 좋아지지만 bias가 상승한다,</li>
<li>패널티 항을 추가해서 생각하면 SVM에서의 최적화는 결국 마진을 크게 하는 것과 에러에 대한 페널티를 크게 하는 것의 균형으로 볼 수 있다.<ul>
<li>maximizing the margin and minimizing the loss</li>
</ul>
</li>
</ul>
<p><img src="https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/svm-all.png"></p>
<p>margin이 최대화 하려면 결정경계에 해당하는 wx+b&#x3D;0이 되게끔 하는 w를 찾아야 한다.<br>이는 <code>wx+b=0</code>에 수직인 벡터(법선벡터)인 $\frac{2}{|\boldsymbol{w}|}$ 최대화 하는 것이다.(w의 유클리드 norm에 대해 2를 곱해준 것)<br>따라서 $\frac{2}{|\boldsymbol{w}|}$ 를 최대화 하는 것이 SVM의 기본적인 목적이 된다.<br>Graidient 계산을 보다 용이하게 하기 위해 $\frac{2}{|\boldsymbol{w}|}$을 최대화하는 문제를 아래와 같이 치환할 수 있다.</p>
<p>$$\min _{\boldsymbol{w}, b} \frac{1}{2}|\boldsymbol{w}|^{2} \equiv \min _{\boldsymbol{w}, b} \frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}$$</p>
<p>class label을 각각 1,-1로 가정할 때 데이터포인트를 정하게 분류하기 위해 다음과 같은 제약조건이 필요하다.</p>
<ul>
<li><strong>양성 plane 보다 위에 있는 관측치는 1보다 커야하고 음성 plane 보다 아래 있는 관측치들은 -1 보다 작아야 한다.</strong></li>
</ul>
<p>이를 모두 만족하는 제약식은 아래와 같다.</p>
<p>$\quad y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1$</p>
<p>따라서 최적화 문제를 최종적으로 아래와 같이 정리 할 수 있다.</p>
<p>$$\min _{\boldsymbol{w}, b} \frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}$$</p>
<p>$$\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1$$</p>
<h3 id="Soft-Margin"><a href="#Soft-Margin" class="headerlink" title="Soft Margin"></a>Soft Margin</h3><hr>
<p>소프트마진은 분류기에 오차를 나타내는 slack variable $\zeta$ 를 목적함수에 추가한다. </p>
<p>hyperparameter C를 통해 loss에 대한 비용을 조정할 수 있다. C가 클 수록 분류오차에 민감해진다. 즉 C값이 커질 경우 마진이 커진다.</p>
<p>반대로 C값을 줄일 경우 bias가 늘어나는 대신 variance가 줄어든다.</p>
<p>소프트 마진 SVM의 최적화 함수는 다음과 같다.</p>
<p>아래의 제약조건을 포함해 생각하면 slack vairable $\zeta$가 0&gt;인 경우를 최소화하고 margin을 최대화 하는 hyperplane을 찾는 것이  Soft Margin SVM의 목적이 된다.</p>
<p>$$\min \frac{1}{2}|\mathbf{w}|^{2}+C \sum_{i&#x3D;1}^{m} \zeta_{i}$$</p>


$$\quad y_{i}\left(\mathbf{w}^{T} \mathbf{x}_{i}+b\right) \geq 1-\zeta_{i} \quad i=1, \ldots, n, \quad \zeta_{i} \geq 0$$



<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><hr>
<p>max(0, 1−yi(wTxi − b)) 는 SVM의 loss function으로 기능한다.</p>
<p>SVM의 loss function은 <code>hinge loss</code> 라고 불리는 데 yi(wTxi − b)이 safety margin인 1보다 크면 loss를 0으로 두고 1보다 작을수록 loss가 크도록 유도한 것이다.</p>
<p>SVM의 hyperparmeter C 는 단순히 hinge loss에 대한 계수이다.</p>
<p>결정경계로 부터의 거리가 0보다 작을 경우 hinge loss가 커지고 이는 데이터포인트가 결정경계의 잘못된 부분에 있는 것을 의미한다.</p>
<p>결정경계로 부터의 거리가 0 과 1 사이에 있는 경우에도 기본적인 loss가 존재하지만 기본적으로 결정경계로부터의 거리가 0보다 커질 경우  loss는 0으로 수렴한다.</p>
<p><img src="https://miro.medium.com/max/1150/1*PGqpYm7o5GCbDXxXErr2JA.png"></p>
<h3 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h3><ul>
<li>iris data set에 대해 soft margin 구현</li>
</ul>
<p>사실 직접 구현보다는 그냥 잘 만들어진 프레임워크를 쓰는 것이 훨씬 낫다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> svc</span><br><span class="line">linear_svm = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>,C=<span class="number">1.0</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">linear_svm.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>

<ul>
<li>numpy로 직접구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy로 svm구현</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SVM</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,learning_rate=<span class="number">0.0001</span>,lambda_param =<span class="number">0.01</span>,n_iter =<span class="number">1000</span></span>):</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.lambda_param = lambda_param</span><br><span class="line">        self.n_iters = n_iters</span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.b = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,X,y</span>):</span><br><span class="line">        y_ = np.where(y&lt;=<span class="number">0</span> ,-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        n_samples = X.shape</span><br><span class="line"></span><br><span class="line">        self.w = np.zeros(n_features) <span class="comment"># 가중치 초기화</span></span><br><span class="line">        self.b = <span class="number">0</span> <span class="comment"># 편향 초기화</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.n_iters):</span><br><span class="line">            <span class="keyword">for</span> idx, x_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">                <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                current index, data point</span></span><br><span class="line"><span class="string">                &quot;&quot;&quot;</span></span><br><span class="line">                condition = y_[idx] * (np.dot(x_i,self.w)) &gt;= <span class="number">1</span> <span class="comment"># 제약조건 구현</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 가중치 업데이트(hinge loss의 gradient update)</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> condition:</span><br><span class="line">                    self.w -= self.lr * (<span class="number">2</span> * self.lambda_param * self.w)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.w -= self.lr * (<span class="number">2</span> * self.lambda_param * self.w - np.dot(x_i,y_[idx]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X</span>):</span><br><span class="line">        linear_output = np.dot(X,self.w) - self.b</span><br><span class="line">        <span class="keyword">return</span> np.sign(linear_output) <span class="comment"># numpy 부호 판별 함수 부호에 따라 -1,1,0 중 하나를 반환</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># weight가 주어졌을 경우 SVM을 시각화하는 함수</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_svm</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_hyperplane_value</span>(<span class="params">x, w, b, offset</span>):</span><br><span class="line">        <span class="keyword">return</span> (-w[<span class="number">0</span>] * x + b + offset) / w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">&quot;o&quot;</span>, c=y)</span><br><span class="line"></span><br><span class="line">    x0_1 = np.amin(X[:, <span class="number">0</span>])</span><br><span class="line">    x0_2 = np.amax(X[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, <span class="number">0</span>)</span><br><span class="line">    x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -<span class="number">1</span>)</span><br><span class="line">    x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, <span class="number">1</span>)</span><br><span class="line">    x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1, x1_2], <span class="string">&quot;y--&quot;</span>)</span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], <span class="string">&quot;k&quot;</span>)</span><br><span class="line">    ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], <span class="string">&quot;k&quot;</span>)</span><br><span class="line"></span><br><span class="line">    x1_min = np.amin(X[:, <span class="number">1</span>])</span><br><span class="line">    x1_max = np.amax(X[:, <span class="number">1</span>])</span><br><span class="line">    ax.set_ylim([x1_min - <span class="number">3</span>, x1_max + <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>Reference &amp; Annotaion</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://youtu.be/efR1C6CvhmE">https://youtu.be/efR1C6CvhmE</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Support-vector_machine">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1">https://towardsdatascience.com/a-definitive-explanation-to-hinge-loss-for-support-vector-machines-ab6d8d3178f1</a></li>
<li>데이터가 비선형일 경우 커널 트릭을 활용한 고차원 매핑을 시행한다.</li>
<li>법선벡터를 최대화 하는 문제를 최적화 문제로 바꾸는 변환에 주의할 것.</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2023/03/03/ML-SP-Decision_Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/03/ML-SP-Decision_Tree/" class="post-title-link" itemprop="url">[Algorithms]Decision Tree의 이해</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-03 18:41:22" itemprop="dateCreated datePublished" datetime="2023-03-03T18:41:22+09:00">2023-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-19 14:43:18" itemprop="dateModified" datetime="2023-03-19T14:43:18+09:00">2023-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Preprocessing


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h2 id="Decision-Tree의-이해"><a href="#Decision-Tree의-이해" class="headerlink" title="Decision Tree의 이해"></a>Decision Tree의 이해</h2><hr>
<p><strong><em>Concept</em></strong></p>
<ul>
<li><strong>Decision Tree(결정트리)</strong>: 질문을 던지고 답을 하는 과정을 연쇄적으로 반복해 집단을 분류하거나 예측하는 분석방법.</li>
<li><strong>threshold</strong> : 결정트리에서의 학습대상. 정확히는 데이터를 나누는 best feature의 best threshold를 찾는 것이 학습의 목적이다,</li>
<li><strong>full tree</strong> : 모든 학습데이터에 대해 분기한 상태.</li>
<li><strong>Entropy</strong> : Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치</li>
<li><strong>지니 불순도</strong> : 데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</li>
<li><strong>정보 이득</strong> : 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이.</li>
<li><strong>Root Node</strong> : 초기노드. 데이터셋 혹은 샘플 전체. </li>
<li><strong>Leaf Node(Terminal Node)</strong> : 자식이 없는 노드.하위노드가 없다.</li>
<li><strong>Pure Node</strong> : 노드의 모든 데이터포인트가 하나의 클래스에 할당되어 있을 경우. 타깃 한개로만 이루어진 Leaf Node.</li>
<li><strong>Branch</strong> : sub-section of an entire tree.</li>
<li><strong>Splitting</strong> : 특정 노드를 나눠 하위노드를 생성하는 것.</li>
<li><strong>Pruning</strong> : 특정 노드의 하위노드를 날리는 것(삭제).</li>
<li><strong>Pre-prune</strong>: When you stop growing DT branches when information becomes unreliable.</li>
<li><strong>Post-prune</strong>: When you take a fully grown DT and then remove leaf nodes only if it results in a better model performance. This way, you stop removing nodes when no further improvements can be made.</li>
</ul>
<p><img src="https://miro.medium.com/max/888/1*FYEZGG-gEijSb87KuxSE_Q.png"></p>
<hr>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><hr>
<ul>
<li>SVM처럼 <strong>분기점(threshold)을 학습한다.</strong></li>
<li>기본적으로 정보이득량이 가장 커지는 방식으로 반복적으로 분할을 진행(recursive partitioning)한다.</li>
<li><strong>분기의 기준이 정보이득이라는 것이 핵심이다.</strong></li>
<li>과적합을 방지하기 위해 pruning이 필요하다.</li>
<li>선형모델과 달리 비선형(non-linear), 비단조(non-monotonic), 특성상호작용(feature interactions) 특징을 가지고 있는 데이터 분석에 용이하다.</li>
<li>특성을 해석하기 좋아 많이 쓰임</li>
<li><strong>샘플에 민감해 트리 고저가 자주 바뀐다.</strong></li>
<li>앙상블 방법의 기초가 된다.</li>
<li><strong>결정트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예&#x2F;아니오 질문 목록을 학습한다는 것이다. 이러한 질문들을 test라고 한다.</strong></li>
<li>학습 데이터셋에 과대적합되는 경향이 있다.</li>
<li>결정트리의 트리를 제어하지 않으면 트리는 무한정 깁어지고 복잡해진다.(일반화 성능이 낮아진다.)</li>
<li>따라서 사전&#x2F;사후 가지치기를 통해 과대적합을 방지한다.</li>
<li>알고리즘 특성상 feature scaling이 필요하지 않지만 주로 다른 알고리즘과의 비교(시각화)를 위해 scaling을 해주는 경우도 있다.</li>
</ul>
<h3 id="불순도-지표"><a href="#불순도-지표" class="headerlink" title="불순도 지표"></a>불순도 지표</h3><hr>
<h4 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h4><hr>
<p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2020/11/[]entropy-a-key-concept-for-all-data-science-beginners/">엔트로피 중요개념</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8">매우중요</a></p>
<ul>
<li>Entropy 는 데이터셋의 불순도와 무질서한 정도를 나타내는 측정치이다.(measure disorder)</li>
<li>0~1의 값을 가진다.<ul>
<li>클래스가 완전히 균일하게 분포되어있을 경우(0.5) Entropy가 최대인 1이된다. </li>
<li>데이터셋의 요소의 분포가 특정 클래스에 치우쳐있을수록 Entropy가 0에 가까워진다.</li>
</ul>
</li>
<li>트리를 만들때 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다. -&gt; 엔트로피가 최소화되는 방향으로 학습을 진행한다.</li>
</ul>
<p align="center">
<img src="https://miro.medium.com/max/750/1*M15RZMSk8nGEyOnD8haF-A.png" alt="drawing" width="400"/>
</p>

<ul>
<li><strong>정보이득은 엔트로피의 변화량으로 계산된다.(1-엔트로피)</strong></li>
<li>N은 범주의 개수</li>
<li>$p_{i}$ 는 p 영역에 속한 데이터 중 i 범주에 속하는 데이터의 비율.</li>
</ul>
<p>$$\text { Entropy }(p)&#x3D;-\sum_{i&#x3D;1}^{N} p_{i} \log <em>{2} p</em>{i}$$</p>
<h4 id="지니불순도"><a href="#지니불순도" class="headerlink" title="지니불순도"></a>지니불순도</h4><hr>
<ul>
<li><strong>잘못 분류될 확률을 최소화하기 위한 기준이다.</strong><ul>
<li>정확히는 <code>데이터 집합에서 클래스 분포에 따라 무작위로 라벨이 지정된 경우 무작위로 선택한 요소들을 잘못 분류할 확률이다.(Chance of being incorrect if you randomly assign a label to an example in the same set)</code></li>
<li>기본적으로 Single Node에 대해 계산한 값이다,</li>
</ul>
</li>
<li>클래스의 비율이 완벽히 균등할 때 최대가 된다.</li>
<li>기본적으로 노드가 중요할수록 불순도가 크게 감소한다.</li>
<li>범주형데이터가 라벨이라면 카디널리티가 적을 수록 불순도는 낮아진다.</li>
<li><strong>Entropy와 지니불순도의 차이는 불순도의 max가 Entopy가 보다 높다는 것이다.</strong></li>
<li><strong>지니불순도가 가장 낮은 Feature statement를 의사결정 트리의 가장 위에 놓는다.</strong>(지니인덱스가 낮으면 불순도가 낮기 때문에 루트노드에 올 가능성이 높아진다.)<ul>
<li>불순도가 낮다는 것은 해당 Feature statement로 인한 정보이득이 높다는 것이다.</li>
</ul>
</li>
<li>최초 노드의 impurity(unsertainty)에서 마지막 노드의 uncertainty를 뺀 값이 information Gain 이다.</li>
<li>Entropy와 달리 식에 log가 없어 계산시 약간 유리하다.</li>
<li>Gain이 가장 큰쪽으로 가지치기를 반복하는 것이 기본적인 의사결정 트리 알고리즘이다.</li>
</ul>
<p>$$\text{Gini Impurity}&#x3D;\sum_{i&#x3D;1}^{N} p(i) *(1-p(i))$$</p>
<h4 id="information-Gain"><a href="#information-Gain" class="headerlink" title="information Gain"></a>information Gain</h4><hr>
<ul>
<li>leaf의 결과는 기본적으로 majority 를 반환한다.</li>
<li><strong>정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이이다.</strong><ul>
<li>이진트리의 경우 자식트리인 왼쪽,오른쪽 트리의 불순도의 합을 부모노드에서 뺀다.</li>
</ul>
</li>
<li>Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.</li>
</ul>
<p>$$IG(Parent,Children) &#x3D; E(Parent) - E(Parent | Children)$$</p>
<ul>
<li><strong>자식 노드의 불순도가 낮을수록 정보 이득이 커진다.</strong> </li>
<li>보통 모듈에서 이진 결정 트리를 사용하므로 부모노드는 두 개의 자식 노드로 나눠진다.</li>
</ul>
<p>$$\text {E(parent)} - [\text {weighted average}] * E(children)$$</p>
<p><img src="https://tensorflowkorea.files.wordpress.com/2018/03/overview-plot.png"></p>
<ul>
<li>엔트로피보다 지니 불순도 방식이 불순도 값을 줄이기 위해 더 클래스 확률을 낮추어야 한다.</li>
<li>엔트로피를 불순도 지표로 사용할 경우 지니불순도를 사용하는 것보다 더 균형잡힌 트리를 만들 가능성이 높다.</li>
</ul>
<h3 id="결정트리의-최적화-문제"><a href="#결정트리의-최적화-문제" class="headerlink" title="결정트리의 최적화 문제"></a>결정트리의 최적화 문제</h3><hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://data-notes.co/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a">최적화 원리와 코드</a></li>
</ul>
<p><strong>Training algorithm</strong></p>
<ul>
<li><p><strong>기본적으로 Best Threshold를 찾는 문제이다</strong></p>
</li>
<li><p>Start at the top node and at each node select the best split based o the best information gain</p>
</li>
<li><p>Greedy Search : Loop over all features and over all thresholds (<strong>all possible feature values</strong>)</p>
</li>
<li><p>Save the best split features and split threshold at each node</p>
</li>
<li><p>Build the tree recursively</p>
</li>
<li><p>Apply some stopping criteria to stop growing</p>
<ul>
<li>maximum depth</li>
<li>minimum samples</li>
<li>etc..</li>
</ul>
</li>
<li><p>When we have a leaf node, store the most common class label of this node</p>
</li>
</ul>
<p><strong>Predict :&#x3D; Traverse tree</strong></p>
<ul>
<li>Traverse the tree recursively.</li>
<li>At each node look at the best split feature of the test feature vector x and go left or right <strong>depending on x[feature idx] &lt;&#x3D; threshold</strong></li>
<li>When we reach the leaf node we return the stored most common class label</li>
</ul>
<h3 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h3><hr>
<p><strong>Put limits in How trees grow</strong></p>
<h4 id="PrePruning"><a href="#PrePruning" class="headerlink" title="PrePruning"></a>PrePruning</h4><hr>
<ul>
<li><p>트리의 최대 깊이 제한하기(max_depth)</p>
</li>
<li><p>리프의 최대 개수 제한하기</p>
</li>
<li><p>노드가 분할하기 위한 데이터 포인트의 최소 개수 지정</p>
</li>
<li><p>sklearn에서 제공하는 관련 Hyperparameter</p>
<ul>
<li>max_depth : 일반화 성능관련. 트리의 최대깊이<ul>
<li>min_sample_splite</li>
<li>max_feature : 최대 피처 사용수</li>
<li>random_state : random state</li>
<li>class_weight : 가중치 balance 맟추기</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="PostPruning"><a href="#PostPruning" class="headerlink" title="PostPruning"></a>PostPruning</h4><hr>
<p>Post-pruning is also known as backward pruning. In this, first generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately. There are various methods for the post pruning.</p>
<h3 id="Feature-Importance-in-Decision-Tree"><a href="#Feature-Importance-in-Decision-Tree" class="headerlink" title="Feature Importance in Decision Tree"></a>Feature Importance in Decision Tree</h3><hr>
<h3 id="More-to-learn"><a href="#More-to-learn" class="headerlink" title="More to learn"></a>More to learn</h3><hr>
<ul>
<li>Pruning</li>
<li>Handling missing data</li>
<li>Building Trees for regression</li>
<li>Using trees to explore datasets</li>
</ul>
<p><strong>more</strong></p>
<ul>
<li>Gini-Index is providing us with the highest accuracy with max depth &#x3D; 6.</li>
<li>Entropy and Gini-index can behave similarly with appropriately selected min_weight_fraction_leaf.</li>
<li>With min_samples_split as 7, Entropy is outperforming Gini for a rudimentary assumption that More samples will provide more information gain and tend to skew the Gini index as the impurity increases.</li>
</ul>
<p>Therefore with taking the criteria as Gini and max_depth &#x3D; 6, we obtained the accuracy as 32% which is an 18% increase from without using parametric optimization. Hence, Optimizing the parameter rightfully, will increase the model accuracy and provide better results.</p>
<p><strong>결정트리의 장점</strong></p>
<ul>
<li>설명가능성</li>
</ul>
<p><strong>결정트리의 단점</strong></p>
<ul>
<li>과적합</li>
</ul>
<h3 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h3><hr>
<ul>
<li>numpy로 구현</li>
<li><strong>기본적으로 Best Split Threshold를 찾는 것이 목적이다.</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the entropy of a label vector</span></span><br><span class="line"><span class="string">    :param y: label vector</span></span><br><span class="line"><span class="string">    :return: entropy</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    hist = np.bincount(y) <span class="comment"># class distribution # 0부터 max까지 class label의 빈도</span></span><br><span class="line">    ps = hist / <span class="built_in">len</span>(y) <span class="comment"># probability of each class</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>([p * np.log2(p) <span class="keyword">for</span> p <span class="keyword">in</span> ps <span class="keyword">if</span> p != <span class="number">0</span>]) <span class="comment"># 음수에 대해서는 정의하지 않음</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,feature=<span class="literal">None</span>,threshold=<span class="literal">None</span>,left=<span class="literal">None</span>,right=<span class="literal">None</span>,*,value=<span class="literal">None</span></span>):</span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.threshold = threshold</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line">        self.value = value</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_leaf</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="comment"># leaf node의 경우 value가 있다.</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecisionTree</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, min_samples_split=<span class="number">2</span>, max_depth=<span class="number">100</span>, n_feats = <span class="literal">None</span></span>):</span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line">        self.n_feats = n_feats</span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="comment"># grow tree</span></span><br><span class="line">        <span class="comment"># X.shape[1] : feature의 개수</span></span><br><span class="line">        </span><br><span class="line">        self.n_feats = X.shape[<span class="number">1</span>] <span class="keyword">if</span> <span class="keyword">not</span> self.n_feats <span class="keyword">else</span> <span class="built_in">min</span>(self.n_feats, X.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># if not self.n_feats -&gt; n.feats가 정의되있지 않을 경우  min(self.n_feats,X.shape[1]) </span></span><br><span class="line">        <span class="comment"># input의 feature 수보다 n_feats기 커지지 않게끔하는 </span></span><br><span class="line">        self.root = self._grow_tree(X, y)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_grow_tree</span>(<span class="params">self, X, y, depth=<span class="number">0</span></span>):</span><br><span class="line">        n_sample, n_feats = X.shape</span><br><span class="line">        n_labels = <span class="built_in">len</span>(np.unique(y))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># stopping criteria # 더 이상 분류할 수 없는 경우 혹은 pruning 기준에 도달한 경우</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            depth &gt;= self.max_depth </span><br><span class="line">            <span class="keyword">or</span> n_labels == <span class="number">1</span> </span><br><span class="line">            <span class="keyword">or</span> n_sample &lt; self.min_samples_split</span><br><span class="line">        ):</span><br><span class="line">            leaf_value = self._most_common_label(y)</span><br><span class="line">            <span class="keyword">return</span> Node(value=leaf_value)</span><br><span class="line">        </span><br><span class="line">        feat_idxs = np.random.choice(n_feats, self.n_feats, replace=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># calculate information gain</span></span><br><span class="line">        best_feat, best_threshold = self._best_criteria(X, y, feat_idxs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># grow the children that result from splitting on the best feature</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 정보이득을 계산한 best_feature와 best threshold 기준으로 분할</span></span><br><span class="line">        left_idxs , right_idxs = self._split(X[:,best_feat], best_threshold)</span><br><span class="line">        left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+<span class="number">1</span>) <span class="comment"># depth+1</span></span><br><span class="line">        right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Node(best_feat, best_threshold, left, right) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_best_criteria</span>(<span class="params">self,X,y,feat_idxs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Find the best criteria to split the data</span></span><br><span class="line"><span class="string">        :param X: input data</span></span><br><span class="line"><span class="string">        :param y: label</span></span><br><span class="line"><span class="string">        :param feat_idxs: indices of features to consider</span></span><br><span class="line"><span class="string">        :return: best feature index, best threshold</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        best_gain = -<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        split_idx, split_threshold = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> feat_idx <span class="keyword">in</span> feat_idxs:</span><br><span class="line">            X_col = X[:,feat_idx] <span class="comment"># X의 각 feature</span></span><br><span class="line">            thresholds = np.unique(X_col) <span class="comment"># 각 feature의 cardianlity</span></span><br><span class="line">            <span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:</span><br><span class="line">                gain = self._information_gain(y,X_col,threshold) <span class="comment"># 각 feuture의 모든 threshold에 대해서 gain을 계산</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> gain &gt; best_gain:</span><br><span class="line">                    best_gain = gain</span><br><span class="line">                    split_idx = feat_idx</span><br><span class="line">                    split_threshold = threshold</span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">return</span> split_idx, split_threshold</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_information_gain</span>(<span class="params">self,y,X_column,split_threshold</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Calculate information gain</span></span><br><span class="line"><span class="string">        E(parent) - [weight average] * E(Children)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># parent entropy</span></span><br><span class="line">        </span><br><span class="line">        parent_entropy = entropy(y)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># generate split</span></span><br><span class="line">        left_idxs, right_idxs = self._split(X_column, split_threshold)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 더이상 분할이 안될 경우 정보이득이 0</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">len</span>(left_idxs) == <span class="number">0</span>) <span class="keyword">or</span> (<span class="built_in">len</span>(right_idxs)) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the weighted avg. of the loss for the children</span></span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        n_l, n_r = <span class="built_in">len</span>(left_idxs), <span class="built_in">len</span>(right_idxs)</span><br><span class="line">        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])</span><br><span class="line">        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r</span><br><span class="line"></span><br><span class="line">        <span class="comment"># information gain is difference in loss before vs. after split</span></span><br><span class="line">        ig = parent_entropy - child_entropy</span><br><span class="line">        <span class="keyword">return</span> ig</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_split</span>(<span class="params">self, X_column, split_threshold</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Split data according to the threshold</span></span><br><span class="line"><span class="string">        :param X_column: input data</span></span><br><span class="line"><span class="string">        :param split_threshold: threshold to split</span></span><br><span class="line"><span class="string">        :return: left and right indices</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># np.argwhere을 사용 조건에 해당하는 인덱스 반환.</span></span><br><span class="line">        left_idxs = np.argwhere(X_column &lt;= split_threshold).flatten()</span><br><span class="line">        right_idxs = np.argwhere(X_column &gt; split_threshold).flatten()</span><br><span class="line">        <span class="keyword">return</span> left_idxs, right_idxs</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_most_common_label</span>(<span class="params">self, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Find the most common label in the dataset</span></span><br><span class="line"><span class="string">        :param y: labels</span></span><br><span class="line"><span class="string">        :return: most common label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        counter = Counter(y)</span><br><span class="line">        <span class="comment"># counter.most_common(1) -&gt; [(label, count)] # 리스트 안에 튜플</span></span><br><span class="line">        most_common = counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> most_common <span class="comment"># Counter(y) : Counter(&#123;0: 2, 1: 2&#125;) #value과 count중 value만 반환 </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># traverse the tree</span></span><br><span class="line">        <span class="keyword">return</span> np.array([self._traverse_tree(x,self.root) <span class="keyword">for</span> x <span class="keyword">in</span> X]) <span class="comment"># X의 각 데이터포인트에 대해서 트리를 순회하며 각 데이터포인트에 대한 결과를 반환</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_traverse_tree</span>(<span class="params">self, x, node</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Traverse the tree from the root</span></span><br><span class="line"><span class="string">        :param x: input data</span></span><br><span class="line"><span class="string">        :param node: root node</span></span><br><span class="line"><span class="string">        :return: label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> node.is_leaf(): <span class="comment"># check if leaf node</span></span><br><span class="line">            <span class="keyword">return</span> node.value</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> x[node.feature] &lt;= node.threshold:</span><br><span class="line">            <span class="keyword">return</span> self._traverse_tree(x, node.left)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._traverse_tree(x, node.right)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y,y_pred</span>):</span><br><span class="line">        acc = np.<span class="built_in">sum</span>(y == y_pred) / <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">    data = datasets.load_breast_cancer()</span><br><span class="line">    X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    </span><br><span class="line">    clf = DecisionTree(max_depth=<span class="number">10</span>)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    </span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    acc = accuracy(y_test, y_pred)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Accuracy : <span class="subst">&#123;acc&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="References-amp-annotation"><a href="#References-amp-annotation" class="headerlink" title="References &amp; annotation"></a><strong>References &amp; annotation</strong></h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html">결정트리의 최적화 문제</a></li>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/information-gain-and-mutual-information/">정보이득</a></li>
<li><a target="_blank" rel="noopener" href="https://victorzhou.com/blog/gini-impurity/">지니불순도</a></li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.blog/tag/%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84/">불순도 지표들</a></li>
<li><a href="-https://xzz201920.medium.com/post-pruning-techniques-in-decision-tree-4be56636172b">Post_Pruning</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2022/03/02/NLP-wordembedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/02/NLP-wordembedding/" class="post-title-link" itemprop="url">[NLP]Word Embedding과 Text Classification</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-02 19:28:01" itemprop="dateCreated datePublished" datetime="2022-03-02T19:28:01+09:00">2022-03-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-04-15 07:10:22" itemprop="dateModified" datetime="2022-04-15T07:10:22+09:00">2022-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!--

<center>Kaggle Customer Score Dataset</center>

- Machine Learning
- Statistics , Math
- Data Engineering
- Programming
- EDA & Visualization
- Data Extraction & Wrangling


#신경망이란 무엇인가?

https://www.youtube.com/watch?v=aircAruvnKk


#참고

https://cinema4dr12.tistory.com/1016?category=515283

https://www.kdnuggets.com/2021/07/top-python-data-science-interview-questions.html
-->

<h1 id="NLP-subtask에-대한-정리"><a href="#NLP-subtask에-대한-정리" class="headerlink" title="NLP subtask에 대한 정리"></a>NLP subtask에 대한 정리</h1><p>: 주요 NLP task인 Word Embedding과 Text Classfication에 대해 간단히 정리</p>
<h2 id="1-Word-Embedding"><a href="#1-Word-Embedding" class="headerlink" title="1.Word Embedding"></a>1.Word Embedding</h2><h3 id="1-1-Word-Embedding에서-고려하는-task"><a href="#1-1-Word-Embedding에서-고려하는-task" class="headerlink" title="1.1 Word Embedding에서 고려하는 task"></a>1.1 Word Embedding에서 고려하는 task</h3><p>Word Embedding은 단어를 저차원의 실수벡터로 dense mapping하는 word representation 방식의 하나이다.</p>
<p>Embedding 자체는 토큰을 고정된 길이의 벡터로 표현하는 것을 뜻한다.</p>
<p><strong>단어의 구문(Syntax)와 의미(Semantics)를 실수벡터의 형태로 표현하는 것이 그 목적이다.</strong></p>
<p><img src="https://miro.medium.com/max/1050/1*lzjgo2KaWFRPkV3LCJDr7Q.png"></p>
<h4 id="1-1-1-차원의-저주"><a href="#1-1-1-차원의-저주" class="headerlink" title="1.1.1 차원의 저주"></a>1.1.1 차원의 저주</h4><p>단어를 실수벡터의 형태로 dense mapping하는 이유는 차원의 저주를 피하기 위함이다.</p>
<p>문서의 모든 단어를 One Hot encoding으로 표현할 경우 feature가 기하급수적으로 많아진 희소행렬이 생성되고 이 경우 연산비용이 증가하는 문제점이 발생한다.</p>
<p>이를 피하기 위해 Word Embedding을 통해 단어를 저차원 벡터에 고정시켜 나타내게 된다.</p>
<h4 id="1-1-2-Distribution-Hypothesis"><a href="#1-1-2-Distribution-Hypothesis" class="headerlink" title="1.1.2 Distribution Hypothesis"></a>1.1.2 Distribution Hypothesis</h4><p>Distribution Hypothesis는 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가설이다.</p>
<p>Word embedding은 이 분포 가설에 기반하여 주변 단어 분포를 기준으로 타겟이 되는 단어의 벡터 표현을 결정한다. </p>
<p>따라서 Word Embedding을 통해 생성된 두 단어 벡터의 거리가 가까울 수록 원문에서 두 단어가 유사한 의미와 용법을 가졌다고 볼 수 있다.</p>
<h4 id="1-1-3-Predictive-Method"><a href="#1-1-3-Predictive-Method" class="headerlink" title="1.1.3 Predictive Method"></a>1.1.3 Predictive Method</h4><p>Word Embedding은 기본적으로 단어의 예측을 학습하는 것으로 이루어진다.</p>
<h3 id="1-2-대표적인-데이터셋"><a href="#1-2-대표적인-데이터셋" class="headerlink" title="1.2 대표적인 데이터셋"></a>1.2 대표적인 데이터셋</h3><h4 id="Words-in-Context"><a href="#Words-in-Context" class="headerlink" title="Words in Context"></a>Words in Context</h4><p>Word in Context는 문맥에 따른 단어의 용법을 모아놓은 데이터 셋이다.</p>
<p>과거의 Word Embedding 기법들이 문맥에 따라 달라지는 단어의 의미를 구분하지 못한다는 문제점을 보완하기 위해 만들어졌다.</p>
<p>Word in Context을 통해 문맥정보를 학습한 임베딩을 생성할 수 있다.</p>
<p>데이터셋은 타겟단어 , 타겟이 되는 타겟단어의 Context 문장 2개와 해당 문장이 문맥상 같은 의미로 쓰여졌는지에 대한 label로 구성되어 있다.</p>
<p><img src="https://images.velog.io/images/yjinheon/post/b44f7201-b8ac-4c98-9646-510f7b2ef6a3/Velog_1_10.png"></p>
<h3 id="Contextual-Embedding-SOTA-Technique"><a href="#Contextual-Embedding-SOTA-Technique" class="headerlink" title="Contextual Embedding(SOTA Technique)"></a>Contextual Embedding(SOTA Technique)</h3><p>Word Embedding은 기본적으로 모델링이 아니라 NLP task의 input을 만드는 작업이기 때문에  BERT, ELMO, GPT-1와 같은 SOTA 모델에서 사용하는 Embedding 방식인 Contextual Embedding에 대해 기술하고자 한다.</p>
<p>과거의 Word Embedding 대표적인 문제점은 하나의 단어당 하나의 벡터 값 만이 매핑된다는 것이다. 따라서 단어의 문맥에 따라 달라지는 의미를 고려하기 어려워지고 성능에 부적인 영향을 주게 된다.</p>
<p>이러한 문제점을 보완하기 위해 Deep contextualized word representations(ELMO)에서 Contextual Embedding이 제시되었다.</p>
<h4 id="biLM-bidirectional-Language-Model-as-function"><a href="#biLM-bidirectional-Language-Model-as-function" class="headerlink" title="biLM(bidirectional Language Model) as function"></a>biLM(bidirectional Language Model) as function</h4><p>Contextual Embedding과 기존 임베딩의 차이점은 각 단어마다 고정된 크기의 벡터를 사용한 것이 아니라 pretrained model 자체를 일종의 함수으로 기능하게끔 하여 문맥정보를 학습에 반영한다는 것이다.</p>
<p>ELMo(Embedding from Language Model)는 여기서 단순한 Language Model이 아니라 일종의 함수이며 문장에 따라 같은 단어라도 다른 임베딩(단어 벡터)을 출력할 수 있다.</p>
<p>여기서 biLM은 단순히 forword LSTM(앞의 단어들로 뒤에 나올 단어를 예측)과 backword LSTM(뒤의 단어들로 앞의 단어를 예픅)을 합친 양방향 모델을 말하며 ELmo의 학습에 사용된다.</p>
<h2 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h2><h3 id="Text-Classification의-주요-task"><a href="#Text-Classification의-주요-task" class="headerlink" title="Text Classification의 주요 task"></a>Text Classification의 주요 task</h3><p>Text Classification은 문서의 내용을 바탕으로 특징을 추출해서 특정한 카테고리에 분류하는 것을 그 목적으로 한다.</p>
<h3 id="대표적인-데이터셋"><a href="#대표적인-데이터셋" class="headerlink" title="대표적인 데이터셋"></a>대표적인 데이터셋</h3><h4 id="IMDB-Movie-Review"><a href="#IMDB-Movie-Review" class="headerlink" title="IMDB Movie Review"></a>IMDB Movie Review</h4><p>IMDB에 게시된 영화 리뷰와 Positive&#x2F;Negative label로 구성된 데이터셋이며 주로 감성분석과 추천시스템 구현에 사용된다.</p>
<h3 id="BERT-SOTA-Technique"><a href="#BERT-SOTA-Technique" class="headerlink" title="BERT(SOTA Technique)"></a>BERT(SOTA Technique)</h3><p>BERT는 구글에서 개발한 신경망 구조이며 Text Classification 뿐 만 아니라 질의응답, 기계번역 , 문서요약과 같은 다양한 task에 적용할 수 있는 대표적인 SOTA Model이다.</p>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><ul>
<li>Transformer는 Encoder Decoder 구조를 가지는 딥러닝 모델이다.</li>
<li>기본적으로 여러개의 Encoder Decoder Layer가 존재하기 때문에 순차적으로 단어정보를 입력받지 않아 연산에서의 부담이 상대적으로 적은 편이다.</li>
<li>Encoder 내부에서는 self attention 기법으로 한 문장에서 한 단어가 다른 단어와 어떤 관계를 갖고 있는지 수치화한다.</li>
<li>문장의 Context를 학습하기 위해 Positional Encoding이라는 특수한 Input을 사용한다.<ul>
<li>Positional Encoding 을 통해 input으로 주어지는 단어의 vector안에 단어의 위치정보를 포함시킬 수 있다.</li>
</ul>
</li>
<li>BERT(Bidirectional Encoder Representations from Transformers)는 양방향 입력을 받는 Encoder를 여러개 쌓아올린 구조로 이루어져 있다.</li>
</ul>
<p>BERT에서는 일부 단어를 마스킹하고 해당 단어를 예측하거나(Masked L). 문장단위로 예측을 수행하는 기법(Next Sentence Prediction)</p>
<p>단어 토큰을 보다 세분화하는 WordPiece 기법을 사용한다.</p>
<h4 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine tuning"></a>fine tuning</h4><p>Transformer와 함께 BERT의 핵심 컨셉중 하나로 <em>기존의 학습된 모델을 기반으로 레이어를 새로운 task에 맞게 변형하고 이미 학습된 모델가중치를 업데이트하거나 모델의 파라미터를 재조정하는 것</em>을 뜻한다.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/what-are-word-embeddings/">https://machinelearningmastery.com/what-are-word-embeddings/</a></li>
<li><a target="_blank" rel="noopener" href="https://pilehvar.github.io/wic/">https://pilehvar.github.io/wic/</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.09121v3.pdf">WiC: the Word-in-Context Dataset</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/method/elmo">https://paperswithcode.com/method/elmo</a></li>
<li><a target="_blank" rel="noopener" href="https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80">Positional Encoding의 이해</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://yjinheon.github.io/2021/12/29/DE-Pyspark-Hadoop.md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/medium.jpg">
      <meta itemprop="name" content="JinHeon Yoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DataMind">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | DataMind">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/12/29/DE-Pyspark-Hadoop.md/" class="post-title-link" itemprop="url">[Pyspark]하둡의 컨셉 이해하기</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-29 00:00:00" itemprop="dateCreated datePublished" datetime="2021-12-29T00:00:00+09:00">2021-12-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-11 15:12:22" itemprop="dateModified" datetime="2023-03-11T15:12:22+09:00">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Data-Engineering/" itemprop="url" rel="index"><span itemprop="name">Data Engineering</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>Pyspark를 본격적으로 쓰기 전에 하둡의 컨셉을 간단히 정리하자.</p>
</blockquote>
<h2 id="하둡"><a href="#하둡" class="headerlink" title="하둡"></a>하둡</h2><p>: 하둡은 Data Locality에 바탕을 둔 분산컴퓨팅을 위한 소프트웨어 플랫폼 및 프레임워크이다. 기본적인 컨셉은 분산처리(큰 문제를 작은 문제의 집합으로 나누고 정리하는 것)이다.</p>
<p>하둡은 HDFS (Hadoop Distributed File System) 와 YARN(Yet Another Resource Navigator)로 구성된다.</p>
<ul>
<li>하둡은 데이터가 비공유 접근을 허용하는 클러스터의 노드에서 지역적으로 처리될 수 있게 한다</li>
<li>각 노드는 다른 노드들과 통신할 필요 없이 전체 데이터의 훨씬 작은 부분을 독립적으로 처리할 수 있다.</li>
<li>기본적으로 분산시스템이기에 네트워크 연결을 통해 여러 연산자원(노드들)의 사용을 조정한다.</li>
<li>선형적 확장성을 가진다. 이는 노드의 수 , 스토리지의 양 , 잡 루틴이 선형적 관계로 엮여있다는 것을 의미한다. (만약 노드의 수를 늘린다면 그만큼 처리시간이 줄어들 것이라고 예측할 수 있다.)</li>
</ul>
<p><strong>데이터가 기본적으로 분산되어있고 확장가능하며(노드의 수를 늘리는 방식으로) 오류에 대처할 수 있다.</strong></p>
<p>이는 분산파일시스템의 구현을 통해 가능해진다.</p>
<h3 id="Schima-on-Write"><a href="#Schima-on-Write" class="headerlink" title="Schima-on-Write"></a>Schima-on-Write</h3><p>: 데이터를 저장할때 스키마를 우선 정의하는 것</p>
<p>주로 RBDMS에 자주 쓰인다. 데이터에 대해 익숙하고 자주 쓴다고 가정할 경우 Schima-on-Write 방식이 보다 적합할 수 있다.</p>
<h3 id="Schima-On-Read"><a href="#Schima-On-Read" class="headerlink" title="Schima-On-Read"></a>Schima-On-Read</h3><p>: 분석을 위해 파일 시스템에서 데이터를 읽을 때 스키마가 정의되는 것.</p>
<p>반구조화를 포함한 광범위한 데이터를 저장하고 처리할 수 있다.이는 데이터를 원본 그대로 저장할 수있다는 것을 의미하며 빅데이터 처리 측면에서 강점을 가진다는 것을 의미한다.</p>
<img src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/hwyn_0105-2d2be3e87d610d5bece7f647d47d5fcc.png" alt="drawing" width="500"/>

<h3 id="Data-Locality"><a href="#Data-Locality" class="headerlink" title="Data Locality"></a>Data Locality</h3><p>: 데이터가 있는 곳으로 이동해서 계산하는 것. 데이터의 이동이 아닌 계산의 이동.</p>
<p><strong>데이터 지역성은 계산하기 위해 데이터를 이동하는 것이 아니라 데이터를 그대로 두고 계산을 이동시키는 것이다.</strong><br>빅 데이터를 계산하기 위해 데이터를 이동(move)를 최대한 줄여<br>시스템 쓰루풋(throughput)과 혼잡도를 늦추게 하는 것이다.<br>따라서 통신 대역폭이 당연히 줄어들고 성능은 늘어난다.</p>
<p>기본적으로 대용량데이터를 다룰경우 데이터를 옮기는 것보다 프로그램 자체를 이동시키는 것이 효율적이라는 아이디어에 기반한 개념이다.</p>
<h2 id="비공유-아키텍처"><a href="#비공유-아키텍처" class="headerlink" title="비공유 아키텍처"></a>비공유 아키텍처</h2><p>: Shared Nothing Architecture</p>
<p>분산 컴퓨팅에 사용되는 아키텍처로 RAM, Disk 등의 자원을 공유하지 않는 독립적인 노드들로 이루어진다.</p>
<p>각각의 노드는 독립적인 처리장치와 프로세스, 디스크를 가지고 있다.</p>
<img src="https://phoenixnap.com/kb/wp-content/uploads/2021/09/shared-nothing-architecture-diagram.png" alt="drawing" width="800"/>

<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><h3 id="Mater-x2F-Slave"><a href="#Mater-x2F-Slave" class="headerlink" title="Mater&#x2F;Slave"></a>Mater&#x2F;Slave</h3><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shared-nothing_architecture">https://en.wikipedia.org/wiki/Shared-nothing_architecture</a></li>
<li><a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/shared-nothing-architecture">https://phoenixnap.com/kb/shared-nothing-architecture</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/blog/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/blog/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/blog/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JinHeon Yoon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yjinheon" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  





<!-- hexo injector body_end start --><script src="/js/hexo-widget-tree.js"></script><div id="widget-tree">
      <ul>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Data-Engineering/">
          Data Engineering
        </a>
      <span class="tree-list-count">7</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Data-Engineering/Linux/">
          Linux
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/10/08/DE-Linux-commandline/" title="[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음"><i class="post-icon gg-file-document"></i>[Unix]데이터 관련 프로젝트시 자주 사용하는 commandline 명령어 모음</a></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Neural-Network/">
          Neural Network
        </a>
      <span class="tree-list-count">7</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/08/05/DL-RNN/" title="[Neural Network]Recurrent Neural Network"><i class="post-icon gg-file-document"></i>[Neural Network]Recurrent Neural Network</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/09/28/DL-backpropagation/" title="[Neural Network]역전파 알고리즘(backpropagation)"><i class="post-icon gg-file-document"></i>[Neural Network]역전파 알고리즘(backpropagation)</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/30/DL-hyperparameter/" title="[Neural Network]하이퍼파라미터"><i class="post-icon gg-file-document"></i>[Neural Network]하이퍼파라미터</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/15/DL-lossfunction/" title="[Deep Learning]Loss function"><i class="post-icon gg-file-document"></i>[Deep Learning]Loss function</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/10/DL-optimizer/" title="[Deep Learning]Optimizer"><i class="post-icon gg-file-document"></i>[Deep Learning]Optimizer</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/03/DL-perceptron/" title="[Neural Network]Perceptron의 이해"><i class="post-icon gg-file-document"></i>[Neural Network]Perceptron의 이해</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/08/02/DL-regularization/" title="[Deep Leearing] 학습 규제하기(Handling Overfitting)"><i class="post-icon gg-file-document"></i>[Deep Leearing] 학습 규제하기(Handling Overfitting)</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Machine-Learning/">
          Machine Learning
        </a>
      <span class="tree-list-count">12</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Machine-Learning/Supervised-Learning/">
          Supervised Learning
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/ML-SP-SVM/" title="[SVM]서포트벡터머신의 이해"><i class="post-icon gg-file-document"></i>[SVM]서포트벡터머신의 이해</a></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/NLP/">
          NLP
        </a>
      <span class="tree-list-count">2</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2022/03/02/NLP-wordembedding/" title="[NLP]Word Embedding과 Text Classification"><i class="post-icon gg-file-document"></i>[NLP]Word Embedding과 Text Classification</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/NLP-NLU/" title="[NLP]NLU & QA task"><i class="post-icon gg-file-document"></i>[NLP]NLU & QA task</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Preprocessing/">
          Preprocessing
        </a>
      <span class="tree-list-count">8</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-dt-Scaler/" title="[Data Transformation]Feature Scaling의 이해"><i class="post-icon gg-file-document"></i>[Data Transformation]Feature Scaling의 이해</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-numpy-basics/" title="[Python]numpy 연산과 활용법"><i class="post-icon gg-file-document"></i>[Python]numpy 연산과 활용법</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-pandas-collection-to-df/" title="[pandas]기본자료형을 DataFrame으로 변환하기"><i class="post-icon gg-file-document"></i>[pandas]기본자료형을 DataFrame으로 변환하기</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-pandas-remove_col/" title="[pandas]pandas의 특정 열 제외한 모든 컬럼 출력하기"><i class="post-icon gg-file-document"></i>[pandas]pandas의 특정 열 제외한 모든 컬럼 출력하기</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-pandas_groupby/" title="[pandas]Pandas Groupby용법 간단히 정리"><i class="post-icon gg-file-document"></i>[pandas]Pandas Groupby용법 간단히 정리</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-pandas_overview/" title="[pandas]Pandas를 활용한 데이터분석 시작하기"><i class="post-icon gg-file-document"></i>[pandas]Pandas를 활용한 데이터분석 시작하기</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-pandas_tricks/" title="[pandas]pandas 함수와 기초용법들"><i class="post-icon gg-file-document"></i>[pandas]pandas 함수와 기초용법들</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Preprocessing-sampling-imbalance-data/" title="[Sampling]Class Imbalance 다루기"><i class="post-icon gg-file-document"></i>[Sampling]Class Imbalance 다루기</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Programming/">
          Programming
        </a>
      <span class="tree-list-count">11</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Programming/Python/">
          Python
        </a>
      <span class="tree-list-count">3</span><ul class="tree-list-children">
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Programming/Python/FastApi/">
          FastApi
        </a>
      <span class="tree-list-count">1</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/11/Python-FastApi-Pydantic/" title="[FastApi]Pydantic  데이터 업데이트"><i class="post-icon gg-file-document"></i>[FastApi]Pydantic  데이터 업데이트</a></li></ul></li></ul></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Statistics/">
          Statistics
        </a>
      <span class="tree-list-count">5</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Statistics-GLM-1/" title="[GLM]선형모델과 비선형 모델의 차이"><i class="post-icon gg-file-document"></i>[GLM]선형모델과 비선형 모델의 차이</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/Statistics-Prob-chi-square-dist/" title="[Probability]Python을 활용한 카이스퀘어 검정 구현"><i class="post-icon gg-file-document"></i>[Probability]Python을 활용한 카이스퀘어 검정 구현</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/11/Statistics-Math-derivatives/" title="[Math]미분 기초개념"><i class="post-icon gg-file-document"></i>[Math]미분 기초개념</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/11/Statistics-Prob-binary-dist/" title="[Probability]이항분포의 이해"><i class="post-icon gg-file-document"></i>[Probability]이항분포의 이해</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/08/10/Statistics-Prob-multinomial-dist/" title="[Probability]numpy와 scipy로 다항분포 간단하게 구현하기"><i class="post-icon gg-file-document"></i>[Probability]numpy와 scipy로 다항분포 간단하게 구현하기</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Troubleshooting/">
          Troubleshooting
        </a>
      <span class="tree-list-count">3</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/TS-R-lib-1/" title="[R]make: gfortran: No such file or directory 해결하기"><i class="post-icon gg-file-document"></i>[R]make: gfortran: No such file or directory 해결하기</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/12/01/TS-SQL-ts-1/" title="[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기"><i class="post-icon gg-file-document"></i>[SQL]ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides 해결하기</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/TS-linux-ts-1/" title="[Linux]zsh: corrupt history file 해결"><i class="post-icon gg-file-document"></i>[Linux]zsh: corrupt history file 해결</a></li></ul></li>
      <li class="tree-list-item">
        <i class="toggle-post-icon gg-folder-add"></i>
        <a class="tree-list-link" href="/categories/Tools/">
          Tools
        </a>
      <span class="tree-list-count">5</span><ul class="tree-list-children"><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/08/tools-conda-install/" title="Anaconda 시작시 기본설정"><i class="post-icon gg-file-document"></i>Anaconda 시작시 기본설정</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/11/tools-docker-commands/" title="[Docker]Docker 자주쓰는 명령어"><i class="post-icon gg-file-document"></i>[Docker]Docker 자주쓰는 명령어</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/07/17/tools-git-basic/" title="[Git]간단한 Git 명령어 및 용법 정리"><i class="post-icon gg-file-document"></i>[Git]간단한 Git 명령어 및 용법 정리</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2021/08/02/tools-git-log/" title="[Git]commit, push 제외 자주쓰는 git 명령어들"><i class="post-icon gg-file-document"></i>[Git]commit, push 제외 자주쓰는 git 명령어들</a></li><li class="tree-list-item"><a class="tree-list-post-link" href="/2023/03/03/tools-git-private-repo/" title="[Git]Private repository import 하기"><i class="post-icon gg-file-document"></i>[Git]Private repository import 하기</a></li></ul></li></ul>
        <div id="widget-tree-button">
          <i class="gg-chevron-right"></i>
        </div>
      </div><!-- hexo injector body_end end --></body>
</html>
